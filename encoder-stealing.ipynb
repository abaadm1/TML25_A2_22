{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12227920,"sourceType":"datasetVersion","datasetId":7704196},{"sourceId":12227951,"sourceType":"datasetVersion","datasetId":7704219}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install onnxruntime","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:41:09.334261Z","iopub.execute_input":"2025-06-21T14:41:09.334544Z","iopub.status.idle":"2025-06-21T14:41:16.674643Z","shell.execute_reply.started":"2025-06-21T14:41:09.334523Z","shell.execute_reply":"2025-06-21T14:41:16.673725Z"}},"outputs":[{"name":"stdout","text":"Collecting onnxruntime\n  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nCollecting coloredlogs (from onnxruntime)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\nRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (3.20.3)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.6->onnxruntime) (2.4.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->onnxruntime) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.6->onnxruntime) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.6->onnxruntime) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.6->onnxruntime) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.6->onnxruntime) (2024.2.0)\nDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.22.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:41:16.676335Z","iopub.execute_input":"2025-06-21T14:41:16.676628Z","iopub.status.idle":"2025-06-21T14:41:19.688112Z","shell.execute_reply.started":"2025-06-21T14:41:16.676604Z","shell.execute_reply":"2025-06-21T14:41:19.687194Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optuna) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport glob\nimport pickle\nimport os\nfrom PIL import Image\nimport numpy as np\nimport onnxruntime as ort\nimport requests\nfrom torch import optim\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nimport optuna\nfrom copy import deepcopy\nfrom datetime import datetime\n\n# ===== Configuration =====\nTOKEN = \"12910150\"\nSEED = \"69713536\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nMEAN = [0.2980, 0.2962, 0.2987]\nSTD = [0.2886, 0.2875, 0.2889]\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Initializing Model Stealing Attack\")\nprint(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"Device: {DEVICE}\")\nprint(f\"Normalization - Mean: {MEAN}, Std: {STD}\")\nprint(f\"{'='*50}\\n\")\n\n# ===== Dataset =====\n# Dataset used for loading queried images and their corresponding embeddings\nclass TaskDataset(Dataset):\n    def __init__(self, transform=None):\n        self.ids = []\n        self.imgs = []\n        self.labels = []\n        self.transform = transform\n\n    def __getitem__(self, index):\n        id_ = self.ids[index]\n        img = self.imgs[index]\n        if self.transform is not None:\n            img = self.transform(img)\n        label = self.labels[index]\n        return id_, img, label\n\n    def __len__(self):\n        return len(self.ids)\n\ntorch.serialization.add_safe_globals({'TaskDataset': TaskDataset})\n\n# Dataset class to load embeddings and match them with input images\nclass StealingDataset(Dataset):\n    def __init__(self, pickle_dir, images):\n        print(f\"\\n[Data] Initializing dataset from {pickle_dir}\")\n        print(f\"[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\")\n        self.transform = transforms.Compose([\n            transforms.Resize(32),\n            transforms.CenterCrop(32),\n            transforms.RandomHorizontalFlip(),\n            transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=MEAN, std=STD)\n        ])\n        self.data = []\n\n        # Load embedding data from multiple pickle files\n        pickle_files = sorted(glob.glob(os.path.join(pickle_dir, \"out*.pickle\")))\n        print(f\"[Data] Found {len(pickle_files)} pickle files\")\n        \n        for i, file in enumerate(pickle_files):\n            with open(file, \"rb\") as f:\n                d = pickle.load(f)\n                loaded = 0\n                for idx, rep in zip(d[\"indices\"], d[\"embeddings\"]):\n                    if idx < len(images):\n                        self.data.append((images[idx], torch.tensor(rep, dtype=torch.float32)))\n                        loaded += 1\n                print(f\"[Data] Loaded {loaded} samples from {os.path.basename(file)} ({i+1}/{len(pickle_files)})\")\n        \n        print(f\"[Data] Total samples: {len(self.data)}\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img, emb = self.data[idx]\n        img = img.convert(\"RGB\")\n        return self.transform(img), emb\n\n\n# ===== Early Stopping =====\n# Class for early stopping during training based on validation loss\nclass EarlyStopping:\n    def __init__(self, patience=7, delta=0, verbose=True):\n        self.patience = patience\n        self.delta = delta\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.best_model = None\n        self.best_loss = np.inf\n\n    def __call__(self, loss, model):\n        score = -loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'[EarlyStopping] Counter: {self.counter}/{self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, loss, model):\n        if self.verbose:\n            print(f'[EarlyStopping] Loss improved ({self.best_loss:.6f} → {loss:.6f}). Saving model...')\n        self.best_loss = loss\n        self.best_model = deepcopy(model.state_dict())\n        \n\n# ===== Model Architecture =====\n# Residual block used in the encoder\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(channels)\n    \n    def forward(self, x):\n        out = F.gelu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out)) + x\n        return F.gelu(out)\n\n# Main encoder model with residual blocks and bottleneck layers\nclass EnhancedResNetEncoder(nn.Module):\n    def __init__(self, bottleneck_width=1024, dropout_rate=0.3):\n        super().__init__()\n        print(f\"\\n[Model] Initializing encoder with:\")\n        print(f\"  - Bottleneck width: {bottleneck_width}\")\n        print(f\"  - Dropout rate: {dropout_rate}\")\n        \n        self.conv_in = nn.Conv2d(3, 64, 3, padding=1, bias=False)\n        self.bn_in = nn.BatchNorm2d(64)\n        self.layer0 = ResidualBlock(64)\n        self.layer1 = ResidualBlock(64)\n        self.pool1 = nn.MaxPool2d(2, 2)\n        self.layer2 = ResidualBlock(64)\n        self.pool2 = nn.MaxPool2d(2, 2)\n        self.layer3 = ResidualBlock(64)\n        self.pool3 = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.bottleneck = nn.Sequential(\n            nn.Linear(64 * 4 * 4, bottleneck_width),\n            nn.GELU(),  # Using GELU activation\n            nn.Linear(bottleneck_width, 1024)\n        )\n        \n        # Initialize weights with proper nonlinearity\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        \n        print(\"[Model] Architecture initialized successfully\")\n\n    def forward(self, x):\n        x = F.gelu(self.bn_in(self.conv_in(x)))\n        x = self.layer0(x)\n        x = self.pool1(self.layer1(x))\n        x = self.pool2(self.layer2(x))\n        x = self.pool3(self.layer3(x))\n        x = self.dropout(x)\n        return self.bottleneck(torch.flatten(x, 1))\n\n# ===== Hybrid Loss =====\n# Loss function combining MSE, cosine similarity, and contrastive-like term\nclass HybridLoss(nn.Module):\n    def __init__(self, alpha=0.7):\n        super().__init__()\n        print(f\"\\n[Loss] Initializing hybrid loss with alpha={alpha}\")\n        self.alpha = alpha\n    \n    def forward(self, pred, target):\n        mse_loss = F.mse_loss(pred, target)\n        cosine_loss = 1 - F.cosine_similarity(pred, target).mean()\n        shuffled_target = target[torch.randperm(target.size(0))]\n        contrastive_loss = F.cosine_similarity(pred, shuffled_target).mean()\n        return (self.alpha * mse_loss + \n                (1-self.alpha) * cosine_loss + \n                0.1 * contrastive_loss)\n\n# ===== Optuna Optimization =====\n# Objective function for Optuna to minimize the training loss\n# It trains the model with trial hyperparameters and uses early stopping\ndef objective(trial):\n    print(f\"\\n{'='*50}\")\n    print(f\"Starting Optuna Trial {trial.number}\")\n    print(f\"{'='*50}\")\n    \n    # Hyperparameters to tune\n    config = {\n        'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),\n        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128, 256]),\n        'bottleneck_width': trial.suggest_categorical('bottleneck_width', [512, 1024, 2048]),\n        'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),\n        'alpha': trial.suggest_float('alpha', 0.4, 0.9),\n        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True)\n    }\n    \n    print(\"\\n[Optuna] Suggested hyperparameters:\")\n    for k, v in config.items():\n        print(f\"  {k}: {v}\")\n\n    # Load data\n    print(\"\\n[Data] Loading dataset...\")\n    dataset_raw = torch.load(\"/kaggle/input/modelstealingpub2/ModelStealingPub.pt\", weights_only=False)\n    dataset = StealingDataset(\"/kaggle/input/embeddings-dataset\", dataset_raw.imgs)\n    train_loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n    print(f\"[Data] Batch size: {config['batch_size']}, Total batches: {len(train_loader)}\")\n\n    # Model\n    print(\"\\n[Model] Initializing model...\")\n    model = EnhancedResNetEncoder(\n        bottleneck_width=config['bottleneck_width'],\n        dropout_rate=config['dropout_rate']\n    ).to(DEVICE)\n    print(f\"[Model] Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Optimizer\n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=config['lr'],\n        weight_decay=config['weight_decay']\n    )\n    print(f\"[Optimizer] Initialized with lr={config['lr']:.2e}, weight_decay={config['weight_decay']:.2e}\")\n    \n    # Scheduler\n    scheduler = CosineAnnealingWarmRestarts(\n        optimizer,\n        T_0=10,\n        T_mult=1,\n        eta_min=1e-6\n    )\n    print(\"[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\")\n    \n    criterion = HybridLoss(alpha=config['alpha'])\n    early_stopping = EarlyStopping(patience=5, verbose=True)\n\n    # Training loop\n    print(\"\\n[Training] Starting training...\")\n    for epoch in range(100):\n        model.train()\n        total_loss = 0.0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            total_loss += loss.item()\n            \n            if batch_idx % 10 == 0:\n                print(f\"[Training] Epoch {epoch+1:03d} | Batch {batch_idx:03d}/{len(train_loader):03d} | Current Loss: {loss.item():.6f}\")\n        \n        scheduler.step()\n        avg_loss = total_loss / len(train_loader)\n        print(f\"\\n[Training] Epoch {epoch+1:03d} Summary:\")\n        print(f\"  Avg Loss: {avg_loss:.6f}\")\n        print(f\"  Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n        \n        early_stopping(avg_loss, model)\n        if early_stopping.early_stop:\n            print(\"\\n[Training] Early stopping triggered\")\n            break\n            \n        trial.report(avg_loss, epoch)\n        if trial.should_prune():\n            print(\"\\n[Optuna] Trial pruned\")\n            raise optuna.TrialPruned()\n\n    print(f\"\\n[Optuna] Trial {trial.number} completed with best loss: {early_stopping.best_loss:.6f}\")\n    return early_stopping.best_loss\n\n# Runs Optuna study for hyperparameter search\ndef optimize_hyperparameters():\n    print(f\"\\n{'='*50}\")\n    print(\"Starting Hyperparameter Optimization with Optuna\")\n    print(f\"{'='*50}\")\n    \n    study = optuna.create_study(\n        direction='minimize',\n        sampler=optuna.samplers.TPESampler(),\n        pruner=optuna.pruners.MedianPruner()\n    )\n    study.optimize(objective, n_trials=20)\n    \n    print(\"\\n[Optuna] Optimization completed\")\n    print(f\"Best trial:\")\n    trial = study.best_trial\n    print(f\"  Value (loss): {trial.value:.6f}\")\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(f\"    {key}: {value}\")\n    \n    return trial.params\n\n# ===== Final Training =====\n# Trains the final model using best hyperparameters found by Optuna\n# Saves the best model based on early stopping\ndef train_final_model(params):\n    print(f\"\\n{'='*50}\")\n    print(\"Starting Final Model Training\")\n    print(f\"Using parameters:\")\n    for k, v in params.items():\n        print(f\"  {k}: {v}\")\n    print(f\"{'='*50}\")\n    \n    # Load data\n    print(\"\\n[Data] Loading dataset...\")\n    dataset_raw = torch.load(\"/kaggle/input/modelstealingpub2/ModelStealingPub.pt\", weights_only=False)\n    dataset = StealingDataset(\"/kaggle/input/embeddings-dataset\", dataset_raw.imgs)\n    train_loader = DataLoader(dataset, batch_size=params['batch_size'], shuffle=True, pin_memory=True)\n    print(f\"[Data] Batch size: {params['batch_size']}, Total batches: {len(train_loader)}\")\n\n    # Model\n    print(\"\\n[Model] Initializing final model...\")\n    model = EnhancedResNetEncoder(\n        bottleneck_width=params['bottleneck_width'],\n        dropout_rate=params['dropout_rate']\n    ).to(DEVICE)\n    print(f\"[Model] Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n    # Optimizer\n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=params['lr'],\n        weight_decay=params['weight_decay']\n    )\n    print(f\"[Optimizer] Initialized with lr={params['lr']:.2e}, weight_decay={params['weight_decay']:.2e}\")\n\n    # Scheduler\n    scheduler = CosineAnnealingWarmRestarts(\n        optimizer,\n        T_0=10,\n        T_mult=1,\n        eta_min=1e-6\n    )\n    print(\"[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\")\n\n    criterion = HybridLoss(alpha=params['alpha'])\n    early_stopping = EarlyStopping(patience=10, verbose=True)\n\n    # Training loop\n    print(\"\\n[Training] Starting final training...\")\n    for epoch in range(100):\n        model.train()\n        total_loss = 0.0\n        \n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            total_loss += loss.item()\n            \n            if batch_idx % 10 == 0:\n                print(f\"[Training] Epoch {epoch+1:03d} | Batch {batch_idx:03d}/{len(train_loader):03d} | Current Loss: {loss.item():.6f}\")\n        \n        scheduler.step()\n        avg_loss = total_loss / len(train_loader)\n        print(f\"\\n[Training] Epoch {epoch+1:03d} Summary:\")\n        print(f\"  Avg Loss: {avg_loss:.6f}\")\n        print(f\"  Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n        \n        early_stopping(avg_loss, model)\n        if early_stopping.early_stop:\n            print(\"\\n[Training] Early stopping triggered\")\n            break\n\n    model.load_state_dict(early_stopping.best_model)\n    print(f\"\\n[Training] Final training completed with best loss: {early_stopping.best_loss:.6f}\")\n\n    # Save the final model\n    model_path = \"stolen_encoder_final.pt\"\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'best_loss': early_stopping.best_loss,\n        'params': params\n    }, model_path)\n    print(f\"\\n[Training] Model saved to {model_path} with best loss: {early_stopping.best_loss:.6f}\")\n    \n    return model\n\n# ===== Export & Submit =====\n# Converts trained model to ONNX format and verifies it\ndef export_model(model):\n    print(f\"\\n{'='*50}\")\n    print(\"Exporting Model to ONNX\")\n    print(f\"{'='*50}\")\n    \n    dummy_input = torch.randn(1, 3, 32, 32).to(DEVICE)\n    onnx_path = \"stolen_encoder_optimized.onnx\"\n    \n    print(\"\\n[Export] Converting model to ONNX format...\")\n    torch.onnx.export(\n        model,\n        dummy_input,\n        onnx_path,\n        input_names=[\"x\"],\n        output_names=[\"output\"],\n        dynamic_axes={\n            \"x\": {0: \"batch_size\"},\n            \"output\": {0: \"batch_size\"}\n        },\n        opset_version=11\n    )\n    print(f\"[Export] Model saved to {onnx_path}\")\n\n    # Verify\n    print(\"\\n[Export] Verifying ONNX model...\")\n    try:\n        ort_session = ort.InferenceSession(onnx_path)\n        test_input = np.random.randn(1, 3, 32, 32).astype(np.float32)\n        ort_out = ort_session.run(None, {\"x\": test_input})[0]\n        assert ort_out.shape == (1, 1024)\n        print(\"[Export] Verification successful!\")\n        print(f\"[Export] Output shape: {ort_out.shape}\")\n    except Exception as e:\n        print(f\"[Export] Verification failed: {str(e)}\")\n        raise\n\n# Submits ONNX model to the remote evaluation server\ndef submit_model():\n    print(f\"\\n{'='*50}\")\n    print(\"Submitting Model to Server\")\n    print(f\"{'='*50}\")\n    \n    try:\n        with open(\"stolen_encoder_optimized.onnx\", \"rb\") as f:\n            print(\"[Submission] Sending model to server...\")\n            response = requests.post(\n                \"http://34.122.51.94:9090/stealing\",\n                files={\"file\": f},\n                headers={\"token\": TOKEN, \"seed\": SEED}\n            )\n        print(\"[Submission] Server response:\")\n        print(response.json())\n    except Exception as e:\n        print(f\"[Submission] Failed: {str(e)}\")\n\n# ===== Main Execution =====\nif __name__ == \"__main__\":\n    try:\n        # Step 1: Hyperparameter optimization\n        best_params = optimize_hyperparameters()\n        \n        # Step 2: Train final model with best params\n        final_model = train_final_model(best_params)\n        \n        # Step 3: Export and submit\n        export_model(final_model)\n        # submit_model()\n        \n        print(\"\\n[Main] Pipeline completed successfully!\")\n    except Exception as e:\n        print(f\"\\n[Main] Error encountered: {str(e)}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T14:41:19.689321Z","iopub.execute_input":"2025-06-21T14:41:19.689560Z","iopub.status.idle":"2025-06-21T18:11:04.569888Z","shell.execute_reply.started":"2025-06-21T14:41:19.689537Z","shell.execute_reply":"2025-06-21T18:11:04.569153Z"}},"outputs":[{"name":"stderr","text":"[I 2025-06-21 14:41:20,587] A new study created in memory with name: no-name-b7c1733c-1524-4aeb-87c1-bd57f4a07b79\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nInitializing Model Stealing Attack\nTimestamp: 2025-06-21 14:41:20\nDevice: cuda\nNormalization - Mean: [0.298, 0.2962, 0.2987], Std: [0.2886, 0.2875, 0.2889]\n==================================================\n\n\n==================================================\nStarting Hyperparameter Optimization with Optuna\n==================================================\n\n==================================================\nStarting Optuna Trial 0\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.0009467904939391108\n  batch_size: 32\n  bottleneck_width: 512\n  dropout_rate: 0.3979044543867316\n  alpha: 0.5586241729095667\n  weight_decay: 1.0133275210700108e-06\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 32, Total batches: 407\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 512\n  - Dropout rate: 0.3979044543867316\n[Model] Architecture initialized successfully\n[Model] Total parameters: 1,347,904\n[Optimizer] Initialized with lr=9.47e-04, weight_decay=1.01e-06\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.5586241729095667\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/407 | Current Loss: 1.745630\n[Training] Epoch 001 | Batch 010/407 | Current Loss: 0.168781\n[Training] Epoch 001 | Batch 020/407 | Current Loss: 0.144868\n[Training] Epoch 001 | Batch 030/407 | Current Loss: 0.118106\n[Training] Epoch 001 | Batch 040/407 | Current Loss: 0.112060\n[Training] Epoch 001 | Batch 050/407 | Current Loss: 0.114526\n[Training] Epoch 001 | Batch 060/407 | Current Loss: 0.108066\n[Training] Epoch 001 | Batch 070/407 | Current Loss: 0.110177\n[Training] Epoch 001 | Batch 080/407 | Current Loss: 0.110580\n[Training] Epoch 001 | Batch 090/407 | Current Loss: 0.108266\n[Training] Epoch 001 | Batch 100/407 | Current Loss: 0.107574\n[Training] Epoch 001 | Batch 110/407 | Current Loss: 0.104555\n[Training] Epoch 001 | Batch 120/407 | Current Loss: 0.105628\n[Training] Epoch 001 | Batch 130/407 | Current Loss: 0.104259\n[Training] Epoch 001 | Batch 140/407 | Current Loss: 0.104561\n[Training] Epoch 001 | Batch 150/407 | Current Loss: 0.107125\n[Training] Epoch 001 | Batch 160/407 | Current Loss: 0.103954\n[Training] Epoch 001 | Batch 170/407 | Current Loss: 0.103714\n[Training] Epoch 001 | Batch 180/407 | Current Loss: 0.103104\n[Training] Epoch 001 | Batch 190/407 | Current Loss: 0.103668\n[Training] Epoch 001 | Batch 200/407 | Current Loss: 0.105939\n[Training] Epoch 001 | Batch 210/407 | Current Loss: 0.102923\n[Training] Epoch 001 | Batch 220/407 | Current Loss: 0.103438\n[Training] Epoch 001 | Batch 230/407 | Current Loss: 0.103197\n[Training] Epoch 001 | Batch 240/407 | Current Loss: 0.103916\n[Training] Epoch 001 | Batch 250/407 | Current Loss: 0.102696\n[Training] Epoch 001 | Batch 260/407 | Current Loss: 0.103070\n[Training] Epoch 001 | Batch 270/407 | Current Loss: 0.102188\n[Training] Epoch 001 | Batch 280/407 | Current Loss: 0.102733\n[Training] Epoch 001 | Batch 290/407 | Current Loss: 0.103722\n[Training] Epoch 001 | Batch 300/407 | Current Loss: 0.103411\n[Training] Epoch 001 | Batch 310/407 | Current Loss: 0.103982\n[Training] Epoch 001 | Batch 320/407 | Current Loss: 0.103102\n[Training] Epoch 001 | Batch 330/407 | Current Loss: 0.102631\n[Training] Epoch 001 | Batch 340/407 | Current Loss: 0.101950\n[Training] Epoch 001 | Batch 350/407 | Current Loss: 0.102450\n[Training] Epoch 001 | Batch 360/407 | Current Loss: 0.102129\n[Training] Epoch 001 | Batch 370/407 | Current Loss: 0.102448\n[Training] Epoch 001 | Batch 380/407 | Current Loss: 0.103109\n[Training] Epoch 001 | Batch 390/407 | Current Loss: 0.102548\n[Training] Epoch 001 | Batch 400/407 | Current Loss: 0.102397\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.117062\n  Current LR: 9.24e-04\n[EarlyStopping] Loss improved (inf → 0.117062). Saving model...\n[Training] Epoch 002 | Batch 000/407 | Current Loss: 0.103949\n[Training] Epoch 002 | Batch 010/407 | Current Loss: 0.102337\n[Training] Epoch 002 | Batch 020/407 | Current Loss: 0.101630\n[Training] Epoch 002 | Batch 030/407 | Current Loss: 0.102872\n[Training] Epoch 002 | Batch 040/407 | Current Loss: 0.102011\n[Training] Epoch 002 | Batch 050/407 | Current Loss: 0.101642\n[Training] Epoch 002 | Batch 060/407 | Current Loss: 0.101903\n[Training] Epoch 002 | Batch 070/407 | Current Loss: 0.103187\n[Training] Epoch 002 | Batch 080/407 | Current Loss: 0.101671\n[Training] Epoch 002 | Batch 090/407 | Current Loss: 0.102680\n[Training] Epoch 002 | Batch 100/407 | Current Loss: 0.104025\n[Training] Epoch 002 | Batch 110/407 | Current Loss: 0.101853\n[Training] Epoch 002 | Batch 120/407 | Current Loss: 0.101859\n[Training] Epoch 002 | Batch 130/407 | Current Loss: 0.102236\n[Training] Epoch 002 | Batch 140/407 | Current Loss: 0.101397\n[Training] Epoch 002 | Batch 150/407 | Current Loss: 0.101887\n[Training] Epoch 002 | Batch 160/407 | Current Loss: 0.101795\n[Training] Epoch 002 | Batch 170/407 | Current Loss: 0.101643\n[Training] Epoch 002 | Batch 180/407 | Current Loss: 0.101219\n[Training] Epoch 002 | Batch 190/407 | Current Loss: 0.101254\n[Training] Epoch 002 | Batch 200/407 | Current Loss: 0.102505\n[Training] Epoch 002 | Batch 210/407 | Current Loss: 0.101720\n[Training] Epoch 002 | Batch 220/407 | Current Loss: 0.101763\n[Training] Epoch 002 | Batch 230/407 | Current Loss: 0.101044\n[Training] Epoch 002 | Batch 240/407 | Current Loss: 0.101394\n[Training] Epoch 002 | Batch 250/407 | Current Loss: 0.101160\n[Training] Epoch 002 | Batch 260/407 | Current Loss: 0.101454\n[Training] Epoch 002 | Batch 270/407 | Current Loss: 0.101037\n[Training] Epoch 002 | Batch 280/407 | Current Loss: 0.101145\n[Training] Epoch 002 | Batch 290/407 | Current Loss: 0.101154\n[Training] Epoch 002 | Batch 300/407 | Current Loss: 0.101579\n[Training] Epoch 002 | Batch 310/407 | Current Loss: 0.100813\n[Training] Epoch 002 | Batch 320/407 | Current Loss: 0.102175\n[Training] Epoch 002 | Batch 330/407 | Current Loss: 0.100876\n[Training] Epoch 002 | Batch 340/407 | Current Loss: 0.101421\n[Training] Epoch 002 | Batch 350/407 | Current Loss: 0.101042\n[Training] Epoch 002 | Batch 360/407 | Current Loss: 0.101603\n[Training] Epoch 002 | Batch 370/407 | Current Loss: 0.100928\n[Training] Epoch 002 | Batch 380/407 | Current Loss: 0.100887\n[Training] Epoch 002 | Batch 390/407 | Current Loss: 0.100995\n[Training] Epoch 002 | Batch 400/407 | Current Loss: 0.100768\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.101688\n  Current LR: 8.56e-04\n[EarlyStopping] Loss improved (0.117062 → 0.101688). Saving model...\n[Training] Epoch 003 | Batch 000/407 | Current Loss: 0.101167\n[Training] Epoch 003 | Batch 010/407 | Current Loss: 0.100941\n[Training] Epoch 003 | Batch 020/407 | Current Loss: 0.101314\n[Training] Epoch 003 | Batch 030/407 | Current Loss: 0.100903\n[Training] Epoch 003 | Batch 040/407 | Current Loss: 0.101160\n[Training] Epoch 003 | Batch 050/407 | Current Loss: 0.101025\n[Training] Epoch 003 | Batch 060/407 | Current Loss: 0.101028\n[Training] Epoch 003 | Batch 070/407 | Current Loss: 0.100735\n[Training] Epoch 003 | Batch 080/407 | Current Loss: 0.101011\n[Training] Epoch 003 | Batch 090/407 | Current Loss: 0.100990\n[Training] Epoch 003 | Batch 100/407 | Current Loss: 0.100749\n[Training] Epoch 003 | Batch 110/407 | Current Loss: 0.100858\n[Training] Epoch 003 | Batch 120/407 | Current Loss: 0.100753\n[Training] Epoch 003 | Batch 130/407 | Current Loss: 0.100598\n[Training] Epoch 003 | Batch 140/407 | Current Loss: 0.100829\n[Training] Epoch 003 | Batch 150/407 | Current Loss: 0.100760\n[Training] Epoch 003 | Batch 160/407 | Current Loss: 0.101014\n[Training] Epoch 003 | Batch 170/407 | Current Loss: 0.100866\n[Training] Epoch 003 | Batch 180/407 | Current Loss: 0.101259\n[Training] Epoch 003 | Batch 190/407 | Current Loss: 0.100629\n[Training] Epoch 003 | Batch 200/407 | Current Loss: 0.100622\n[Training] Epoch 003 | Batch 210/407 | Current Loss: 0.101225\n[Training] Epoch 003 | Batch 220/407 | Current Loss: 0.100817\n[Training] Epoch 003 | Batch 230/407 | Current Loss: 0.100837\n[Training] Epoch 003 | Batch 240/407 | Current Loss: 0.100875\n[Training] Epoch 003 | Batch 250/407 | Current Loss: 0.100738\n[Training] Epoch 003 | Batch 260/407 | Current Loss: 0.100806\n[Training] Epoch 003 | Batch 270/407 | Current Loss: 0.100585\n[Training] Epoch 003 | Batch 280/407 | Current Loss: 0.100582\n[Training] Epoch 003 | Batch 290/407 | Current Loss: 0.100822\n[Training] Epoch 003 | Batch 300/407 | Current Loss: 0.100698\n[Training] Epoch 003 | Batch 310/407 | Current Loss: 0.100762\n[Training] Epoch 003 | Batch 320/407 | Current Loss: 0.100852\n[Training] Epoch 003 | Batch 330/407 | Current Loss: 0.100501\n[Training] Epoch 003 | Batch 340/407 | Current Loss: 0.100451\n[Training] Epoch 003 | Batch 350/407 | Current Loss: 0.100818\n[Training] Epoch 003 | Batch 360/407 | Current Loss: 0.100250\n[Training] Epoch 003 | Batch 370/407 | Current Loss: 0.100477\n[Training] Epoch 003 | Batch 380/407 | Current Loss: 0.100518\n[Training] Epoch 003 | Batch 390/407 | Current Loss: 0.100638\n[Training] Epoch 003 | Batch 400/407 | Current Loss: 0.100692\n\n[Training] Epoch 003 Summary:\n  Avg Loss: 0.100786\n  Current LR: 7.52e-04\n[EarlyStopping] Loss improved (0.101688 → 0.100786). Saving model...\n[Training] Epoch 004 | Batch 000/407 | Current Loss: 0.100385\n[Training] Epoch 004 | Batch 010/407 | Current Loss: 0.100578\n[Training] Epoch 004 | Batch 020/407 | Current Loss: 0.100540\n[Training] Epoch 004 | Batch 030/407 | Current Loss: 0.100704\n[Training] Epoch 004 | Batch 040/407 | Current Loss: 0.100780\n[Training] Epoch 004 | Batch 050/407 | Current Loss: 0.100639\n[Training] Epoch 004 | Batch 060/407 | Current Loss: 0.100531\n[Training] Epoch 004 | Batch 070/407 | Current Loss: 0.100515\n[Training] Epoch 004 | Batch 080/407 | Current Loss: 0.100550\n[Training] Epoch 004 | Batch 090/407 | Current Loss: 0.100432\n[Training] Epoch 004 | Batch 100/407 | Current Loss: 0.100442\n[Training] Epoch 004 | Batch 110/407 | Current Loss: 0.100737\n[Training] Epoch 004 | Batch 120/407 | Current Loss: 0.100451\n[Training] Epoch 004 | Batch 130/407 | Current Loss: 0.100634\n[Training] Epoch 004 | Batch 140/407 | Current Loss: 0.100636\n[Training] Epoch 004 | Batch 150/407 | Current Loss: 0.100390\n[Training] Epoch 004 | Batch 160/407 | Current Loss: 0.100677\n[Training] Epoch 004 | Batch 170/407 | Current Loss: 0.100421\n[Training] Epoch 004 | Batch 180/407 | Current Loss: 0.100507\n[Training] Epoch 004 | Batch 190/407 | Current Loss: 0.100323\n[Training] Epoch 004 | Batch 200/407 | Current Loss: 0.100844\n[Training] Epoch 004 | Batch 210/407 | Current Loss: 0.100490\n[Training] Epoch 004 | Batch 220/407 | Current Loss: 0.100560\n[Training] Epoch 004 | Batch 230/407 | Current Loss: 0.100439\n[Training] Epoch 004 | Batch 240/407 | Current Loss: 0.100484\n[Training] Epoch 004 | Batch 250/407 | Current Loss: 0.100356\n[Training] Epoch 004 | Batch 260/407 | Current Loss: 0.100466\n[Training] Epoch 004 | Batch 270/407 | Current Loss: 0.100559\n[Training] Epoch 004 | Batch 280/407 | Current Loss: 0.100521\n[Training] Epoch 004 | Batch 290/407 | Current Loss: 0.100399\n[Training] Epoch 004 | Batch 300/407 | Current Loss: 0.100482\n[Training] Epoch 004 | Batch 310/407 | Current Loss: 0.100262\n[Training] Epoch 004 | Batch 320/407 | Current Loss: 0.100575\n[Training] Epoch 004 | Batch 330/407 | Current Loss: 0.100409\n[Training] Epoch 004 | Batch 340/407 | Current Loss: 0.100543\n[Training] Epoch 004 | Batch 350/407 | Current Loss: 0.100562\n[Training] Epoch 004 | Batch 360/407 | Current Loss: 0.100591\n[Training] Epoch 004 | Batch 370/407 | Current Loss: 0.100452\n[Training] Epoch 004 | Batch 380/407 | Current Loss: 0.100408\n[Training] Epoch 004 | Batch 390/407 | Current Loss: 0.100696\n[Training] Epoch 004 | Batch 400/407 | Current Loss: 0.100525\n\n[Training] Epoch 004 Summary:\n  Avg Loss: 0.100508\n  Current LR: 6.20e-04\n[EarlyStopping] Loss improved (0.100786 → 0.100508). Saving model...\n[Training] Epoch 005 | Batch 000/407 | Current Loss: 0.100365\n[Training] Epoch 005 | Batch 010/407 | Current Loss: 0.100268\n[Training] Epoch 005 | Batch 020/407 | Current Loss: 0.100350\n[Training] Epoch 005 | Batch 030/407 | Current Loss: 0.100431\n[Training] Epoch 005 | Batch 040/407 | Current Loss: 0.100404\n[Training] Epoch 005 | Batch 050/407 | Current Loss: 0.100289\n[Training] Epoch 005 | Batch 060/407 | Current Loss: 0.100538\n[Training] Epoch 005 | Batch 070/407 | Current Loss: 0.100441\n[Training] Epoch 005 | Batch 080/407 | Current Loss: 0.100493\n[Training] Epoch 005 | Batch 090/407 | Current Loss: 0.100483\n[Training] Epoch 005 | Batch 100/407 | Current Loss: 0.100290\n[Training] Epoch 005 | Batch 110/407 | Current Loss: 0.100407\n[Training] Epoch 005 | Batch 120/407 | Current Loss: 0.100489\n[Training] Epoch 005 | Batch 130/407 | Current Loss: 0.100378\n[Training] Epoch 005 | Batch 140/407 | Current Loss: 0.100343\n[Training] Epoch 005 | Batch 150/407 | Current Loss: 0.100393\n[Training] Epoch 005 | Batch 160/407 | Current Loss: 0.100450\n[Training] Epoch 005 | Batch 170/407 | Current Loss: 0.100345\n[Training] Epoch 005 | Batch 180/407 | Current Loss: 0.100433\n[Training] Epoch 005 | Batch 190/407 | Current Loss: 0.100414\n[Training] Epoch 005 | Batch 200/407 | Current Loss: 0.100449\n[Training] Epoch 005 | Batch 210/407 | Current Loss: 0.100401\n[Training] Epoch 005 | Batch 220/407 | Current Loss: 0.100386\n[Training] Epoch 005 | Batch 230/407 | Current Loss: 0.100261\n[Training] Epoch 005 | Batch 240/407 | Current Loss: 0.100474\n[Training] Epoch 005 | Batch 250/407 | Current Loss: 0.100160\n[Training] Epoch 005 | Batch 260/407 | Current Loss: 0.100436\n[Training] Epoch 005 | Batch 270/407 | Current Loss: 0.100337\n[Training] Epoch 005 | Batch 280/407 | Current Loss: 0.100349\n[Training] Epoch 005 | Batch 290/407 | Current Loss: 0.100263\n[Training] Epoch 005 | Batch 300/407 | Current Loss: 0.100382\n[Training] Epoch 005 | Batch 310/407 | Current Loss: 0.100279\n[Training] Epoch 005 | Batch 320/407 | Current Loss: 0.100392\n[Training] Epoch 005 | Batch 330/407 | Current Loss: 0.100459\n[Training] Epoch 005 | Batch 340/407 | Current Loss: 0.100454\n[Training] Epoch 005 | Batch 350/407 | Current Loss: 0.100491\n[Training] Epoch 005 | Batch 360/407 | Current Loss: 0.100672\n[Training] Epoch 005 | Batch 370/407 | Current Loss: 0.100398\n[Training] Epoch 005 | Batch 380/407 | Current Loss: 0.100418\n[Training] Epoch 005 | Batch 390/407 | Current Loss: 0.100238\n[Training] Epoch 005 | Batch 400/407 | Current Loss: 0.100207\n\n[Training] Epoch 005 Summary:\n  Avg Loss: 0.100491\n  Current LR: 4.74e-04\n[EarlyStopping] Loss improved (0.100508 → 0.100491). Saving model...\n[Training] Epoch 006 | Batch 000/407 | Current Loss: 0.108856\n[Training] Epoch 006 | Batch 010/407 | Current Loss: 0.108445\n[Training] Epoch 006 | Batch 020/407 | Current Loss: 0.102724\n[Training] Epoch 006 | Batch 030/407 | Current Loss: 0.101806\n[Training] Epoch 006 | Batch 040/407 | Current Loss: 0.101292\n[Training] Epoch 006 | Batch 050/407 | Current Loss: 0.101064\n[Training] Epoch 006 | Batch 060/407 | Current Loss: 0.100662\n[Training] Epoch 006 | Batch 070/407 | Current Loss: 0.100633\n[Training] Epoch 006 | Batch 080/407 | Current Loss: 0.100680\n[Training] Epoch 006 | Batch 090/407 | Current Loss: 0.100677\n[Training] Epoch 006 | Batch 100/407 | Current Loss: 0.100463\n[Training] Epoch 006 | Batch 110/407 | Current Loss: 0.100779\n[Training] Epoch 006 | Batch 120/407 | Current Loss: 0.100509\n[Training] Epoch 006 | Batch 130/407 | Current Loss: 0.100874\n[Training] Epoch 006 | Batch 140/407 | Current Loss: 0.100695\n[Training] Epoch 006 | Batch 150/407 | Current Loss: 0.100538\n[Training] Epoch 006 | Batch 160/407 | Current Loss: 0.100596\n[Training] Epoch 006 | Batch 170/407 | Current Loss: 0.100514\n[Training] Epoch 006 | Batch 180/407 | Current Loss: 0.100626\n[Training] Epoch 006 | Batch 190/407 | Current Loss: 0.100391\n[Training] Epoch 006 | Batch 200/407 | Current Loss: 0.100496\n[Training] Epoch 006 | Batch 210/407 | Current Loss: 0.100637\n[Training] Epoch 006 | Batch 220/407 | Current Loss: 0.100307\n[Training] Epoch 006 | Batch 230/407 | Current Loss: 0.100578\n[Training] Epoch 006 | Batch 240/407 | Current Loss: 0.100527\n[Training] Epoch 006 | Batch 250/407 | Current Loss: 0.100392\n[Training] Epoch 006 | Batch 260/407 | Current Loss: 0.100374\n[Training] Epoch 006 | Batch 270/407 | Current Loss: 0.100381\n[Training] Epoch 006 | Batch 280/407 | Current Loss: 0.100418\n[Training] Epoch 006 | Batch 290/407 | Current Loss: 0.100522\n[Training] Epoch 006 | Batch 300/407 | Current Loss: 0.100638\n[Training] Epoch 006 | Batch 310/407 | Current Loss: 0.100661\n[Training] Epoch 006 | Batch 320/407 | Current Loss: 0.100353\n[Training] Epoch 006 | Batch 330/407 | Current Loss: 0.100327\n[Training] Epoch 006 | Batch 340/407 | Current Loss: 0.100393\n[Training] Epoch 006 | Batch 350/407 | Current Loss: 0.100432\n[Training] Epoch 006 | Batch 360/407 | Current Loss: 0.100370\n[Training] Epoch 006 | Batch 370/407 | Current Loss: 0.100354\n[Training] Epoch 006 | Batch 380/407 | Current Loss: 0.100424\n[Training] Epoch 006 | Batch 390/407 | Current Loss: 0.100504\n[Training] Epoch 006 | Batch 400/407 | Current Loss: 0.100370\n\n[Training] Epoch 006 Summary:\n  Avg Loss: 0.100801\n  Current LR: 3.28e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 007 | Batch 000/407 | Current Loss: 0.100565\n[Training] Epoch 007 | Batch 010/407 | Current Loss: 0.100611\n[Training] Epoch 007 | Batch 020/407 | Current Loss: 0.100336\n[Training] Epoch 007 | Batch 030/407 | Current Loss: 0.100351\n[Training] Epoch 007 | Batch 040/407 | Current Loss: 0.100428\n[Training] Epoch 007 | Batch 050/407 | Current Loss: 0.100541\n[Training] Epoch 007 | Batch 060/407 | Current Loss: 0.100349\n[Training] Epoch 007 | Batch 070/407 | Current Loss: 0.100426\n[Training] Epoch 007 | Batch 080/407 | Current Loss: 0.100327\n[Training] Epoch 007 | Batch 090/407 | Current Loss: 0.100365\n[Training] Epoch 007 | Batch 100/407 | Current Loss: 0.100525\n[Training] Epoch 007 | Batch 110/407 | Current Loss: 0.100634\n[Training] Epoch 007 | Batch 120/407 | Current Loss: 0.100504\n[Training] Epoch 007 | Batch 130/407 | Current Loss: 0.100515\n[Training] Epoch 007 | Batch 140/407 | Current Loss: 0.100319\n[Training] Epoch 007 | Batch 150/407 | Current Loss: 0.100348\n[Training] Epoch 007 | Batch 160/407 | Current Loss: 0.100418\n[Training] Epoch 007 | Batch 170/407 | Current Loss: 0.100431\n[Training] Epoch 007 | Batch 180/407 | Current Loss: 0.100256\n[Training] Epoch 007 | Batch 190/407 | Current Loss: 0.100348\n[Training] Epoch 007 | Batch 200/407 | Current Loss: 0.100300\n[Training] Epoch 007 | Batch 210/407 | Current Loss: 0.100227\n[Training] Epoch 007 | Batch 220/407 | Current Loss: 0.100432\n[Training] Epoch 007 | Batch 230/407 | Current Loss: 0.100233\n[Training] Epoch 007 | Batch 240/407 | Current Loss: 0.100492\n[Training] Epoch 007 | Batch 250/407 | Current Loss: 0.100423\n[Training] Epoch 007 | Batch 260/407 | Current Loss: 0.100375\n[Training] Epoch 007 | Batch 270/407 | Current Loss: 0.100241\n[Training] Epoch 007 | Batch 280/407 | Current Loss: 0.100305\n[Training] Epoch 007 | Batch 290/407 | Current Loss: 0.100422\n[Training] Epoch 007 | Batch 300/407 | Current Loss: 0.100405\n[Training] Epoch 007 | Batch 310/407 | Current Loss: 0.100217\n[Training] Epoch 007 | Batch 320/407 | Current Loss: 0.100357\n[Training] Epoch 007 | Batch 330/407 | Current Loss: 0.100267\n[Training] Epoch 007 | Batch 340/407 | Current Loss: 0.100400\n[Training] Epoch 007 | Batch 350/407 | Current Loss: 0.100406\n[Training] Epoch 007 | Batch 360/407 | Current Loss: 0.100454\n[Training] Epoch 007 | Batch 370/407 | Current Loss: 0.100426\n[Training] Epoch 007 | Batch 380/407 | Current Loss: 0.100292\n[Training] Epoch 007 | Batch 390/407 | Current Loss: 0.100448\n[Training] Epoch 007 | Batch 400/407 | Current Loss: 0.100493\n\n[Training] Epoch 007 Summary:\n  Avg Loss: 0.100384\n  Current LR: 1.96e-04\n[EarlyStopping] Loss improved (0.100491 → 0.100384). Saving model...\n[Training] Epoch 008 | Batch 000/407 | Current Loss: 0.100407\n[Training] Epoch 008 | Batch 010/407 | Current Loss: 0.100826\n[Training] Epoch 008 | Batch 020/407 | Current Loss: 0.100245\n[Training] Epoch 008 | Batch 030/407 | Current Loss: 0.100405\n[Training] Epoch 008 | Batch 040/407 | Current Loss: 0.100270\n[Training] Epoch 008 | Batch 050/407 | Current Loss: 0.100226\n[Training] Epoch 008 | Batch 060/407 | Current Loss: 0.100379\n[Training] Epoch 008 | Batch 070/407 | Current Loss: 0.100335\n[Training] Epoch 008 | Batch 080/407 | Current Loss: 0.100224\n[Training] Epoch 008 | Batch 090/407 | Current Loss: 0.100347\n[Training] Epoch 008 | Batch 100/407 | Current Loss: 0.100376\n[Training] Epoch 008 | Batch 110/407 | Current Loss: 0.100322\n[Training] Epoch 008 | Batch 120/407 | Current Loss: 0.100396\n[Training] Epoch 008 | Batch 130/407 | Current Loss: 0.100334\n[Training] Epoch 008 | Batch 140/407 | Current Loss: 0.100383\n[Training] Epoch 008 | Batch 150/407 | Current Loss: 0.100271\n[Training] Epoch 008 | Batch 160/407 | Current Loss: 0.100206\n[Training] Epoch 008 | Batch 170/407 | Current Loss: 0.100298\n[Training] Epoch 008 | Batch 180/407 | Current Loss: 0.100359\n[Training] Epoch 008 | Batch 190/407 | Current Loss: 0.100320\n[Training] Epoch 008 | Batch 200/407 | Current Loss: 0.100481\n[Training] Epoch 008 | Batch 210/407 | Current Loss: 0.100344\n[Training] Epoch 008 | Batch 220/407 | Current Loss: 0.100375\n[Training] Epoch 008 | Batch 230/407 | Current Loss: 0.100495\n[Training] Epoch 008 | Batch 240/407 | Current Loss: 0.100207\n[Training] Epoch 008 | Batch 250/407 | Current Loss: 0.100407\n[Training] Epoch 008 | Batch 260/407 | Current Loss: 0.100246\n[Training] Epoch 008 | Batch 270/407 | Current Loss: 0.100345\n[Training] Epoch 008 | Batch 280/407 | Current Loss: 0.100292\n[Training] Epoch 008 | Batch 290/407 | Current Loss: 0.100182\n[Training] Epoch 008 | Batch 300/407 | Current Loss: 0.100348\n[Training] Epoch 008 | Batch 310/407 | Current Loss: 0.100427\n[Training] Epoch 008 | Batch 320/407 | Current Loss: 0.100253\n[Training] Epoch 008 | Batch 330/407 | Current Loss: 0.100276\n[Training] Epoch 008 | Batch 340/407 | Current Loss: 0.100310\n[Training] Epoch 008 | Batch 350/407 | Current Loss: 0.100259\n[Training] Epoch 008 | Batch 360/407 | Current Loss: 0.100170\n[Training] Epoch 008 | Batch 370/407 | Current Loss: 0.100283\n[Training] Epoch 008 | Batch 380/407 | Current Loss: 0.100197\n[Training] Epoch 008 | Batch 390/407 | Current Loss: 0.100236\n[Training] Epoch 008 | Batch 400/407 | Current Loss: 0.100365\n\n[Training] Epoch 008 Summary:\n  Avg Loss: 0.100319\n  Current LR: 9.13e-05\n[EarlyStopping] Loss improved (0.100384 → 0.100319). Saving model...\n[Training] Epoch 009 | Batch 000/407 | Current Loss: 0.100406\n[Training] Epoch 009 | Batch 010/407 | Current Loss: 0.100306\n[Training] Epoch 009 | Batch 020/407 | Current Loss: 0.100535\n[Training] Epoch 009 | Batch 030/407 | Current Loss: 0.100196\n[Training] Epoch 009 | Batch 040/407 | Current Loss: 0.100292\n[Training] Epoch 009 | Batch 050/407 | Current Loss: 0.100246\n[Training] Epoch 009 | Batch 060/407 | Current Loss: 0.100157\n[Training] Epoch 009 | Batch 070/407 | Current Loss: 0.100222\n[Training] Epoch 009 | Batch 080/407 | Current Loss: 0.100338\n[Training] Epoch 009 | Batch 090/407 | Current Loss: 0.100385\n[Training] Epoch 009 | Batch 100/407 | Current Loss: 0.100314\n[Training] Epoch 009 | Batch 110/407 | Current Loss: 0.100107\n[Training] Epoch 009 | Batch 120/407 | Current Loss: 0.100355\n[Training] Epoch 009 | Batch 130/407 | Current Loss: 0.100123\n[Training] Epoch 009 | Batch 140/407 | Current Loss: 0.100343\n[Training] Epoch 009 | Batch 150/407 | Current Loss: 0.100285\n[Training] Epoch 009 | Batch 160/407 | Current Loss: 0.100275\n[Training] Epoch 009 | Batch 170/407 | Current Loss: 0.100223\n[Training] Epoch 009 | Batch 180/407 | Current Loss: 0.100440\n[Training] Epoch 009 | Batch 190/407 | Current Loss: 0.100199\n[Training] Epoch 009 | Batch 200/407 | Current Loss: 0.100403\n[Training] Epoch 009 | Batch 210/407 | Current Loss: 0.100326\n[Training] Epoch 009 | Batch 220/407 | Current Loss: 0.100486\n[Training] Epoch 009 | Batch 230/407 | Current Loss: 0.100395\n[Training] Epoch 009 | Batch 240/407 | Current Loss: 0.100184\n[Training] Epoch 009 | Batch 250/407 | Current Loss: 0.100394\n[Training] Epoch 009 | Batch 260/407 | Current Loss: 0.100224\n[Training] Epoch 009 | Batch 270/407 | Current Loss: 0.100365\n[Training] Epoch 009 | Batch 280/407 | Current Loss: 0.100240\n[Training] Epoch 009 | Batch 290/407 | Current Loss: 0.100312\n[Training] Epoch 009 | Batch 300/407 | Current Loss: 0.100066\n[Training] Epoch 009 | Batch 310/407 | Current Loss: 0.100418\n[Training] Epoch 009 | Batch 320/407 | Current Loss: 0.100234\n[Training] Epoch 009 | Batch 330/407 | Current Loss: 0.100339\n[Training] Epoch 009 | Batch 340/407 | Current Loss: 0.100291\n[Training] Epoch 009 | Batch 350/407 | Current Loss: 0.100210\n[Training] Epoch 009 | Batch 360/407 | Current Loss: 0.100401\n[Training] Epoch 009 | Batch 370/407 | Current Loss: 0.100215\n[Training] Epoch 009 | Batch 380/407 | Current Loss: 0.100356\n[Training] Epoch 009 | Batch 390/407 | Current Loss: 0.100208\n[Training] Epoch 009 | Batch 400/407 | Current Loss: 0.100232\n\n[Training] Epoch 009 Summary:\n  Avg Loss: 0.100290\n  Current LR: 2.41e-05\n[EarlyStopping] Loss improved (0.100319 → 0.100290). Saving model...\n[Training] Epoch 010 | Batch 000/407 | Current Loss: 0.100326\n[Training] Epoch 010 | Batch 010/407 | Current Loss: 0.100326\n[Training] Epoch 010 | Batch 020/407 | Current Loss: 0.100259\n[Training] Epoch 010 | Batch 030/407 | Current Loss: 0.100305\n[Training] Epoch 010 | Batch 040/407 | Current Loss: 0.100499\n[Training] Epoch 010 | Batch 050/407 | Current Loss: 0.100379\n[Training] Epoch 010 | Batch 060/407 | Current Loss: 0.100418\n[Training] Epoch 010 | Batch 070/407 | Current Loss: 0.100228\n[Training] Epoch 010 | Batch 080/407 | Current Loss: 0.100236\n[Training] Epoch 010 | Batch 090/407 | Current Loss: 0.100534\n[Training] Epoch 010 | Batch 100/407 | Current Loss: 0.100280\n[Training] Epoch 010 | Batch 110/407 | Current Loss: 0.100084\n[Training] Epoch 010 | Batch 120/407 | Current Loss: 0.100186\n[Training] Epoch 010 | Batch 130/407 | Current Loss: 0.100371\n[Training] Epoch 010 | Batch 140/407 | Current Loss: 0.100384\n[Training] Epoch 010 | Batch 150/407 | Current Loss: 0.100511\n[Training] Epoch 010 | Batch 160/407 | Current Loss: 0.100306\n[Training] Epoch 010 | Batch 170/407 | Current Loss: 0.100370\n[Training] Epoch 010 | Batch 180/407 | Current Loss: 0.100384\n[Training] Epoch 010 | Batch 190/407 | Current Loss: 0.100149\n[Training] Epoch 010 | Batch 200/407 | Current Loss: 0.100455\n[Training] Epoch 010 | Batch 210/407 | Current Loss: 0.100415\n[Training] Epoch 010 | Batch 220/407 | Current Loss: 0.100296\n[Training] Epoch 010 | Batch 230/407 | Current Loss: 0.100312\n[Training] Epoch 010 | Batch 240/407 | Current Loss: 0.100389\n[Training] Epoch 010 | Batch 250/407 | Current Loss: 0.100226\n[Training] Epoch 010 | Batch 260/407 | Current Loss: 0.100278\n[Training] Epoch 010 | Batch 270/407 | Current Loss: 0.100406\n[Training] Epoch 010 | Batch 280/407 | Current Loss: 0.100266\n[Training] Epoch 010 | Batch 290/407 | Current Loss: 0.100333\n[Training] Epoch 010 | Batch 300/407 | Current Loss: 0.100280\n[Training] Epoch 010 | Batch 310/407 | Current Loss: 0.100244\n[Training] Epoch 010 | Batch 320/407 | Current Loss: 0.100242\n[Training] Epoch 010 | Batch 330/407 | Current Loss: 0.100343\n[Training] Epoch 010 | Batch 340/407 | Current Loss: 0.100281\n[Training] Epoch 010 | Batch 350/407 | Current Loss: 0.100218\n[Training] Epoch 010 | Batch 360/407 | Current Loss: 0.100279\n[Training] Epoch 010 | Batch 370/407 | Current Loss: 0.100350\n[Training] Epoch 010 | Batch 380/407 | Current Loss: 0.100496\n[Training] Epoch 010 | Batch 390/407 | Current Loss: 0.100200\n[Training] Epoch 010 | Batch 400/407 | Current Loss: 0.100325\n\n[Training] Epoch 010 Summary:\n  Avg Loss: 0.100285\n  Current LR: 9.47e-04\n[EarlyStopping] Loss improved (0.100290 → 0.100285). Saving model...\n[Training] Epoch 011 | Batch 000/407 | Current Loss: 0.100108\n[Training] Epoch 011 | Batch 010/407 | Current Loss: 0.100396\n[Training] Epoch 011 | Batch 020/407 | Current Loss: 0.100498\n[Training] Epoch 011 | Batch 030/407 | Current Loss: 0.100342\n[Training] Epoch 011 | Batch 040/407 | Current Loss: 0.100569\n[Training] Epoch 011 | Batch 050/407 | Current Loss: 0.100400\n[Training] Epoch 011 | Batch 060/407 | Current Loss: 0.100182\n[Training] Epoch 011 | Batch 070/407 | Current Loss: 0.100409\n[Training] Epoch 011 | Batch 080/407 | Current Loss: 0.100515\n[Training] Epoch 011 | Batch 090/407 | Current Loss: 0.100354\n[Training] Epoch 011 | Batch 100/407 | Current Loss: 0.100432\n[Training] Epoch 011 | Batch 110/407 | Current Loss: 0.100588\n[Training] Epoch 011 | Batch 120/407 | Current Loss: 0.100413\n[Training] Epoch 011 | Batch 130/407 | Current Loss: 0.100492\n[Training] Epoch 011 | Batch 140/407 | Current Loss: 0.100497\n[Training] Epoch 011 | Batch 150/407 | Current Loss: 0.100423\n[Training] Epoch 011 | Batch 160/407 | Current Loss: 0.100470\n[Training] Epoch 011 | Batch 170/407 | Current Loss: 0.100955\n[Training] Epoch 011 | Batch 180/407 | Current Loss: 0.100555\n[Training] Epoch 011 | Batch 190/407 | Current Loss: 0.100436\n[Training] Epoch 011 | Batch 200/407 | Current Loss: 0.100407\n[Training] Epoch 011 | Batch 210/407 | Current Loss: 0.100484\n[Training] Epoch 011 | Batch 220/407 | Current Loss: 0.100307\n[Training] Epoch 011 | Batch 230/407 | Current Loss: 0.100347\n[Training] Epoch 011 | Batch 240/407 | Current Loss: 0.100316\n[Training] Epoch 011 | Batch 250/407 | Current Loss: 0.100369\n[Training] Epoch 011 | Batch 260/407 | Current Loss: 0.100292\n[Training] Epoch 011 | Batch 270/407 | Current Loss: 0.100564\n[Training] Epoch 011 | Batch 280/407 | Current Loss: 0.100519\n[Training] Epoch 011 | Batch 290/407 | Current Loss: 0.100379\n[Training] Epoch 011 | Batch 300/407 | Current Loss: 0.100265\n[Training] Epoch 011 | Batch 310/407 | Current Loss: 0.100697\n[Training] Epoch 011 | Batch 320/407 | Current Loss: 0.100346\n[Training] Epoch 011 | Batch 330/407 | Current Loss: 0.100232\n[Training] Epoch 011 | Batch 340/407 | Current Loss: 0.100494\n[Training] Epoch 011 | Batch 350/407 | Current Loss: 0.100938\n[Training] Epoch 011 | Batch 360/407 | Current Loss: 0.100389\n[Training] Epoch 011 | Batch 370/407 | Current Loss: 0.100539\n[Training] Epoch 011 | Batch 380/407 | Current Loss: 0.100359\n[Training] Epoch 011 | Batch 390/407 | Current Loss: 0.100390\n[Training] Epoch 011 | Batch 400/407 | Current Loss: 0.100643\n\n[Training] Epoch 011 Summary:\n  Avg Loss: 0.100467\n  Current LR: 9.24e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 012 | Batch 000/407 | Current Loss: 0.100377\n[Training] Epoch 012 | Batch 010/407 | Current Loss: 0.100344\n[Training] Epoch 012 | Batch 020/407 | Current Loss: 0.100362\n[Training] Epoch 012 | Batch 030/407 | Current Loss: 0.100308\n[Training] Epoch 012 | Batch 040/407 | Current Loss: 0.100306\n[Training] Epoch 012 | Batch 050/407 | Current Loss: 0.100395\n[Training] Epoch 012 | Batch 060/407 | Current Loss: 0.100331\n[Training] Epoch 012 | Batch 070/407 | Current Loss: 0.100516\n[Training] Epoch 012 | Batch 080/407 | Current Loss: 0.100201\n[Training] Epoch 012 | Batch 090/407 | Current Loss: 0.100409\n[Training] Epoch 012 | Batch 100/407 | Current Loss: 0.100473\n[Training] Epoch 012 | Batch 110/407 | Current Loss: 0.100362\n[Training] Epoch 012 | Batch 120/407 | Current Loss: 0.100184\n[Training] Epoch 012 | Batch 130/407 | Current Loss: 0.100281\n[Training] Epoch 012 | Batch 140/407 | Current Loss: 0.100389\n[Training] Epoch 012 | Batch 150/407 | Current Loss: 0.100143\n[Training] Epoch 012 | Batch 160/407 | Current Loss: 0.100319\n[Training] Epoch 012 | Batch 170/407 | Current Loss: 0.100174\n[Training] Epoch 012 | Batch 180/407 | Current Loss: 0.100293\n[Training] Epoch 012 | Batch 190/407 | Current Loss: 0.100326\n[Training] Epoch 012 | Batch 200/407 | Current Loss: 0.100308\n[Training] Epoch 012 | Batch 210/407 | Current Loss: 0.100285\n[Training] Epoch 012 | Batch 220/407 | Current Loss: 0.100403\n[Training] Epoch 012 | Batch 230/407 | Current Loss: 0.100153\n[Training] Epoch 012 | Batch 240/407 | Current Loss: 0.100180\n[Training] Epoch 012 | Batch 250/407 | Current Loss: 0.100329\n[Training] Epoch 012 | Batch 260/407 | Current Loss: 0.100377\n[Training] Epoch 012 | Batch 270/407 | Current Loss: 0.100308\n[Training] Epoch 012 | Batch 280/407 | Current Loss: 0.100201\n[Training] Epoch 012 | Batch 290/407 | Current Loss: 0.100285\n[Training] Epoch 012 | Batch 300/407 | Current Loss: 0.100402\n[Training] Epoch 012 | Batch 310/407 | Current Loss: 0.100195\n[Training] Epoch 012 | Batch 320/407 | Current Loss: 0.100031\n[Training] Epoch 012 | Batch 330/407 | Current Loss: 0.100399\n[Training] Epoch 012 | Batch 340/407 | Current Loss: 0.100061\n[Training] Epoch 012 | Batch 350/407 | Current Loss: 0.100203\n[Training] Epoch 012 | Batch 360/407 | Current Loss: 0.100220\n[Training] Epoch 012 | Batch 370/407 | Current Loss: 0.100226\n[Training] Epoch 012 | Batch 380/407 | Current Loss: 0.100177\n[Training] Epoch 012 | Batch 390/407 | Current Loss: 0.100317\n[Training] Epoch 012 | Batch 400/407 | Current Loss: 0.100253\n\n[Training] Epoch 012 Summary:\n  Avg Loss: 0.100310\n  Current LR: 8.56e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 013 | Batch 000/407 | Current Loss: 0.100276\n[Training] Epoch 013 | Batch 010/407 | Current Loss: 0.100155\n[Training] Epoch 013 | Batch 020/407 | Current Loss: 0.100437\n[Training] Epoch 013 | Batch 030/407 | Current Loss: 0.100332\n[Training] Epoch 013 | Batch 040/407 | Current Loss: 0.100464\n[Training] Epoch 013 | Batch 050/407 | Current Loss: 0.100223\n[Training] Epoch 013 | Batch 060/407 | Current Loss: 0.100171\n[Training] Epoch 013 | Batch 070/407 | Current Loss: 0.100250\n[Training] Epoch 013 | Batch 080/407 | Current Loss: 0.100478\n[Training] Epoch 013 | Batch 090/407 | Current Loss: 0.100510\n[Training] Epoch 013 | Batch 100/407 | Current Loss: 0.100342\n[Training] Epoch 013 | Batch 110/407 | Current Loss: 0.100499\n[Training] Epoch 013 | Batch 120/407 | Current Loss: 0.100308\n[Training] Epoch 013 | Batch 130/407 | Current Loss: 0.100220\n[Training] Epoch 013 | Batch 140/407 | Current Loss: 0.100153\n[Training] Epoch 013 | Batch 150/407 | Current Loss: 0.100329\n[Training] Epoch 013 | Batch 160/407 | Current Loss: 0.100171\n[Training] Epoch 013 | Batch 170/407 | Current Loss: 0.100237\n[Training] Epoch 013 | Batch 180/407 | Current Loss: 0.100311\n[Training] Epoch 013 | Batch 190/407 | Current Loss: 0.100159\n[Training] Epoch 013 | Batch 200/407 | Current Loss: 0.100333\n[Training] Epoch 013 | Batch 210/407 | Current Loss: 0.100342\n[Training] Epoch 013 | Batch 220/407 | Current Loss: 0.100228\n[Training] Epoch 013 | Batch 230/407 | Current Loss: 0.100161\n[Training] Epoch 013 | Batch 240/407 | Current Loss: 0.100233\n[Training] Epoch 013 | Batch 250/407 | Current Loss: 0.100337\n[Training] Epoch 013 | Batch 260/407 | Current Loss: 0.100724\n[Training] Epoch 013 | Batch 270/407 | Current Loss: 0.100173\n[Training] Epoch 013 | Batch 280/407 | Current Loss: 0.100245\n[Training] Epoch 013 | Batch 290/407 | Current Loss: 0.100263\n[Training] Epoch 013 | Batch 300/407 | Current Loss: 0.100353\n[Training] Epoch 013 | Batch 310/407 | Current Loss: 0.100146\n[Training] Epoch 013 | Batch 320/407 | Current Loss: 0.100197\n[Training] Epoch 013 | Batch 330/407 | Current Loss: 0.100330\n[Training] Epoch 013 | Batch 340/407 | Current Loss: 0.100302\n[Training] Epoch 013 | Batch 350/407 | Current Loss: 0.100237\n[Training] Epoch 013 | Batch 360/407 | Current Loss: 0.100332\n[Training] Epoch 013 | Batch 370/407 | Current Loss: 0.100299\n[Training] Epoch 013 | Batch 380/407 | Current Loss: 0.100394\n[Training] Epoch 013 | Batch 390/407 | Current Loss: 0.100344\n[Training] Epoch 013 | Batch 400/407 | Current Loss: 0.100431\n\n[Training] Epoch 013 Summary:\n  Avg Loss: 0.100285\n  Current LR: 7.52e-04\n[EarlyStopping] Loss improved (0.100285 → 0.100285). Saving model...\n[Training] Epoch 014 | Batch 000/407 | Current Loss: 0.100264\n[Training] Epoch 014 | Batch 010/407 | Current Loss: 0.100202\n[Training] Epoch 014 | Batch 020/407 | Current Loss: 0.100186\n[Training] Epoch 014 | Batch 030/407 | Current Loss: 0.100218\n[Training] Epoch 014 | Batch 040/407 | Current Loss: 0.100154\n[Training] Epoch 014 | Batch 050/407 | Current Loss: 0.100225\n[Training] Epoch 014 | Batch 060/407 | Current Loss: 0.100174\n[Training] Epoch 014 | Batch 070/407 | Current Loss: 0.100215\n[Training] Epoch 014 | Batch 080/407 | Current Loss: 0.100212\n[Training] Epoch 014 | Batch 090/407 | Current Loss: 0.100159\n[Training] Epoch 014 | Batch 100/407 | Current Loss: 0.100254\n[Training] Epoch 014 | Batch 110/407 | Current Loss: 0.100060\n[Training] Epoch 014 | Batch 120/407 | Current Loss: 0.100368\n[Training] Epoch 014 | Batch 130/407 | Current Loss: 0.100282\n[Training] Epoch 014 | Batch 140/407 | Current Loss: 0.100354\n[Training] Epoch 014 | Batch 150/407 | Current Loss: 0.100211\n[Training] Epoch 014 | Batch 160/407 | Current Loss: 0.100187\n[Training] Epoch 014 | Batch 170/407 | Current Loss: 0.100277\n[Training] Epoch 014 | Batch 180/407 | Current Loss: 0.100313\n[Training] Epoch 014 | Batch 190/407 | Current Loss: 0.100217\n[Training] Epoch 014 | Batch 200/407 | Current Loss: 0.100172\n[Training] Epoch 014 | Batch 210/407 | Current Loss: 0.100259\n[Training] Epoch 014 | Batch 220/407 | Current Loss: 0.100135\n[Training] Epoch 014 | Batch 230/407 | Current Loss: 0.100295\n[Training] Epoch 014 | Batch 240/407 | Current Loss: 0.100340\n[Training] Epoch 014 | Batch 250/407 | Current Loss: 0.100143\n[Training] Epoch 014 | Batch 260/407 | Current Loss: 0.100354\n[Training] Epoch 014 | Batch 270/407 | Current Loss: 0.100277\n[Training] Epoch 014 | Batch 280/407 | Current Loss: 0.100406\n[Training] Epoch 014 | Batch 290/407 | Current Loss: 0.100251\n[Training] Epoch 014 | Batch 300/407 | Current Loss: 0.100142\n[Training] Epoch 014 | Batch 310/407 | Current Loss: 0.100248\n[Training] Epoch 014 | Batch 320/407 | Current Loss: 0.100352\n[Training] Epoch 014 | Batch 330/407 | Current Loss: 0.100291\n[Training] Epoch 014 | Batch 340/407 | Current Loss: 0.100207\n[Training] Epoch 014 | Batch 350/407 | Current Loss: 0.100150\n[Training] Epoch 014 | Batch 360/407 | Current Loss: 0.100213\n[Training] Epoch 014 | Batch 370/407 | Current Loss: 0.100120\n[Training] Epoch 014 | Batch 380/407 | Current Loss: 0.100380\n[Training] Epoch 014 | Batch 390/407 | Current Loss: 0.100214\n[Training] Epoch 014 | Batch 400/407 | Current Loss: 0.100188\n\n[Training] Epoch 014 Summary:\n  Avg Loss: 0.100232\n  Current LR: 6.20e-04\n[EarlyStopping] Loss improved (0.100285 → 0.100232). Saving model...\n[Training] Epoch 015 | Batch 000/407 | Current Loss: 0.100197\n[Training] Epoch 015 | Batch 010/407 | Current Loss: 0.100154\n[Training] Epoch 015 | Batch 020/407 | Current Loss: 0.100186\n[Training] Epoch 015 | Batch 030/407 | Current Loss: 0.100125\n[Training] Epoch 015 | Batch 040/407 | Current Loss: 0.100191\n[Training] Epoch 015 | Batch 050/407 | Current Loss: 0.100129\n[Training] Epoch 015 | Batch 060/407 | Current Loss: 0.100194\n[Training] Epoch 015 | Batch 070/407 | Current Loss: 0.100178\n[Training] Epoch 015 | Batch 080/407 | Current Loss: 0.100124\n[Training] Epoch 015 | Batch 090/407 | Current Loss: 0.100293\n[Training] Epoch 015 | Batch 100/407 | Current Loss: 0.100190\n[Training] Epoch 015 | Batch 110/407 | Current Loss: 0.100116\n[Training] Epoch 015 | Batch 120/407 | Current Loss: 0.100185\n[Training] Epoch 015 | Batch 130/407 | Current Loss: 0.100141\n[Training] Epoch 015 | Batch 140/407 | Current Loss: 0.100177\n[Training] Epoch 015 | Batch 150/407 | Current Loss: 0.100169\n[Training] Epoch 015 | Batch 160/407 | Current Loss: 0.100315\n[Training] Epoch 015 | Batch 170/407 | Current Loss: 0.100140\n[Training] Epoch 015 | Batch 180/407 | Current Loss: 0.100220\n[Training] Epoch 015 | Batch 190/407 | Current Loss: 0.100312\n[Training] Epoch 015 | Batch 200/407 | Current Loss: 0.100177\n[Training] Epoch 015 | Batch 210/407 | Current Loss: 0.100167\n[Training] Epoch 015 | Batch 220/407 | Current Loss: 0.100102\n[Training] Epoch 015 | Batch 230/407 | Current Loss: 0.100240\n[Training] Epoch 015 | Batch 240/407 | Current Loss: 0.100327\n[Training] Epoch 015 | Batch 250/407 | Current Loss: 0.100177\n[Training] Epoch 015 | Batch 260/407 | Current Loss: 0.100359\n[Training] Epoch 015 | Batch 270/407 | Current Loss: 0.100337\n[Training] Epoch 015 | Batch 280/407 | Current Loss: 0.100182\n[Training] Epoch 015 | Batch 290/407 | Current Loss: 0.100126\n[Training] Epoch 015 | Batch 300/407 | Current Loss: 0.100207\n[Training] Epoch 015 | Batch 310/407 | Current Loss: 0.100191\n[Training] Epoch 015 | Batch 320/407 | Current Loss: 0.100171\n[Training] Epoch 015 | Batch 330/407 | Current Loss: 0.100318\n[Training] Epoch 015 | Batch 340/407 | Current Loss: 0.100176\n[Training] Epoch 015 | Batch 350/407 | Current Loss: 0.100153\n[Training] Epoch 015 | Batch 360/407 | Current Loss: 0.100431\n[Training] Epoch 015 | Batch 370/407 | Current Loss: 0.100226\n[Training] Epoch 015 | Batch 380/407 | Current Loss: 0.100313\n[Training] Epoch 015 | Batch 390/407 | Current Loss: 0.100103\n[Training] Epoch 015 | Batch 400/407 | Current Loss: 0.100299\n\n[Training] Epoch 015 Summary:\n  Avg Loss: 0.100197\n  Current LR: 4.74e-04\n[EarlyStopping] Loss improved (0.100232 → 0.100197). Saving model...\n[Training] Epoch 016 | Batch 000/407 | Current Loss: 0.100174\n[Training] Epoch 016 | Batch 010/407 | Current Loss: 0.100187\n[Training] Epoch 016 | Batch 020/407 | Current Loss: 0.100191\n[Training] Epoch 016 | Batch 030/407 | Current Loss: 0.100107\n[Training] Epoch 016 | Batch 040/407 | Current Loss: 0.100044\n[Training] Epoch 016 | Batch 050/407 | Current Loss: 0.100150\n[Training] Epoch 016 | Batch 060/407 | Current Loss: 0.100323\n[Training] Epoch 016 | Batch 070/407 | Current Loss: 0.100149\n[Training] Epoch 016 | Batch 080/407 | Current Loss: 0.100133\n[Training] Epoch 016 | Batch 090/407 | Current Loss: 0.100027\n[Training] Epoch 016 | Batch 100/407 | Current Loss: 0.100240\n[Training] Epoch 016 | Batch 110/407 | Current Loss: 0.100071\n[Training] Epoch 016 | Batch 120/407 | Current Loss: 0.100126\n[Training] Epoch 016 | Batch 130/407 | Current Loss: 0.100156\n[Training] Epoch 016 | Batch 140/407 | Current Loss: 0.100102\n[Training] Epoch 016 | Batch 150/407 | Current Loss: 0.100265\n[Training] Epoch 016 | Batch 160/407 | Current Loss: 0.100099\n[Training] Epoch 016 | Batch 170/407 | Current Loss: 0.100081\n[Training] Epoch 016 | Batch 180/407 | Current Loss: 0.100227\n[Training] Epoch 016 | Batch 190/407 | Current Loss: 0.100066\n[Training] Epoch 016 | Batch 200/407 | Current Loss: 0.100214\n[Training] Epoch 016 | Batch 210/407 | Current Loss: 0.100175\n[Training] Epoch 016 | Batch 220/407 | Current Loss: 0.100212\n[Training] Epoch 016 | Batch 230/407 | Current Loss: 0.100109\n[Training] Epoch 016 | Batch 240/407 | Current Loss: 0.100054\n[Training] Epoch 016 | Batch 250/407 | Current Loss: 0.100322\n[Training] Epoch 016 | Batch 260/407 | Current Loss: 0.100252\n[Training] Epoch 016 | Batch 270/407 | Current Loss: 0.100234\n[Training] Epoch 016 | Batch 280/407 | Current Loss: 0.100103\n[Training] Epoch 016 | Batch 290/407 | Current Loss: 0.100368\n[Training] Epoch 016 | Batch 300/407 | Current Loss: 0.100282\n[Training] Epoch 016 | Batch 310/407 | Current Loss: 0.100249\n[Training] Epoch 016 | Batch 320/407 | Current Loss: 0.100098\n[Training] Epoch 016 | Batch 330/407 | Current Loss: 0.100231\n[Training] Epoch 016 | Batch 340/407 | Current Loss: 0.100240\n[Training] Epoch 016 | Batch 350/407 | Current Loss: 0.100230\n[Training] Epoch 016 | Batch 360/407 | Current Loss: 0.100230\n[Training] Epoch 016 | Batch 370/407 | Current Loss: 0.100165\n[Training] Epoch 016 | Batch 380/407 | Current Loss: 0.100127\n[Training] Epoch 016 | Batch 390/407 | Current Loss: 0.100276\n[Training] Epoch 016 | Batch 400/407 | Current Loss: 0.100165\n\n[Training] Epoch 016 Summary:\n  Avg Loss: 0.100176\n  Current LR: 3.28e-04\n[EarlyStopping] Loss improved (0.100197 → 0.100176). Saving model...\n[Training] Epoch 017 | Batch 000/407 | Current Loss: 0.100278\n[Training] Epoch 017 | Batch 010/407 | Current Loss: 0.100138\n[Training] Epoch 017 | Batch 020/407 | Current Loss: 0.100307\n[Training] Epoch 017 | Batch 030/407 | Current Loss: 0.100194\n[Training] Epoch 017 | Batch 040/407 | Current Loss: 0.100042\n[Training] Epoch 017 | Batch 050/407 | Current Loss: 0.100349\n[Training] Epoch 017 | Batch 060/407 | Current Loss: 0.100207\n[Training] Epoch 017 | Batch 070/407 | Current Loss: 0.100235\n[Training] Epoch 017 | Batch 080/407 | Current Loss: 0.100133\n[Training] Epoch 017 | Batch 090/407 | Current Loss: 0.100167\n[Training] Epoch 017 | Batch 100/407 | Current Loss: 0.100307\n[Training] Epoch 017 | Batch 110/407 | Current Loss: 0.100131\n[Training] Epoch 017 | Batch 120/407 | Current Loss: 0.100100\n[Training] Epoch 017 | Batch 130/407 | Current Loss: 0.100190\n[Training] Epoch 017 | Batch 140/407 | Current Loss: 0.100033\n[Training] Epoch 017 | Batch 150/407 | Current Loss: 0.100175\n[Training] Epoch 017 | Batch 160/407 | Current Loss: 0.100204\n[Training] Epoch 017 | Batch 170/407 | Current Loss: 0.100096\n[Training] Epoch 017 | Batch 180/407 | Current Loss: 0.100345\n[Training] Epoch 017 | Batch 190/407 | Current Loss: 0.100153\n[Training] Epoch 017 | Batch 200/407 | Current Loss: 0.100140\n[Training] Epoch 017 | Batch 210/407 | Current Loss: 0.100308\n[Training] Epoch 017 | Batch 220/407 | Current Loss: 0.100137\n[Training] Epoch 017 | Batch 230/407 | Current Loss: 0.100038\n[Training] Epoch 017 | Batch 240/407 | Current Loss: 0.100189\n[Training] Epoch 017 | Batch 250/407 | Current Loss: 0.100197\n[Training] Epoch 017 | Batch 260/407 | Current Loss: 0.100096\n[Training] Epoch 017 | Batch 270/407 | Current Loss: 0.100097\n[Training] Epoch 017 | Batch 280/407 | Current Loss: 0.100130\n[Training] Epoch 017 | Batch 290/407 | Current Loss: 0.100118\n[Training] Epoch 017 | Batch 300/407 | Current Loss: 0.100174\n[Training] Epoch 017 | Batch 310/407 | Current Loss: 0.100102\n[Training] Epoch 017 | Batch 320/407 | Current Loss: 0.100140\n[Training] Epoch 017 | Batch 330/407 | Current Loss: 0.100246\n[Training] Epoch 017 | Batch 340/407 | Current Loss: 0.100301\n[Training] Epoch 017 | Batch 350/407 | Current Loss: 0.099971\n[Training] Epoch 017 | Batch 360/407 | Current Loss: 0.100096\n[Training] Epoch 017 | Batch 370/407 | Current Loss: 0.100147\n[Training] Epoch 017 | Batch 380/407 | Current Loss: 0.100234\n[Training] Epoch 017 | Batch 390/407 | Current Loss: 0.100317\n[Training] Epoch 017 | Batch 400/407 | Current Loss: 0.100215\n\n[Training] Epoch 017 Summary:\n  Avg Loss: 0.100163\n  Current LR: 1.96e-04\n[EarlyStopping] Loss improved (0.100176 → 0.100163). Saving model...\n[Training] Epoch 018 | Batch 000/407 | Current Loss: 0.100294\n[Training] Epoch 018 | Batch 010/407 | Current Loss: 0.100227\n[Training] Epoch 018 | Batch 020/407 | Current Loss: 0.100280\n[Training] Epoch 018 | Batch 030/407 | Current Loss: 0.100201\n[Training] Epoch 018 | Batch 040/407 | Current Loss: 0.100205\n[Training] Epoch 018 | Batch 050/407 | Current Loss: 0.100073\n[Training] Epoch 018 | Batch 060/407 | Current Loss: 0.100135\n[Training] Epoch 018 | Batch 070/407 | Current Loss: 0.100116\n[Training] Epoch 018 | Batch 080/407 | Current Loss: 0.100069\n[Training] Epoch 018 | Batch 090/407 | Current Loss: 0.100143\n[Training] Epoch 018 | Batch 100/407 | Current Loss: 0.099964\n[Training] Epoch 018 | Batch 110/407 | Current Loss: 0.100258\n[Training] Epoch 018 | Batch 120/407 | Current Loss: 0.099988\n[Training] Epoch 018 | Batch 130/407 | Current Loss: 0.100132\n[Training] Epoch 018 | Batch 140/407 | Current Loss: 0.100098\n[Training] Epoch 018 | Batch 150/407 | Current Loss: 0.100154\n[Training] Epoch 018 | Batch 160/407 | Current Loss: 0.100107\n[Training] Epoch 018 | Batch 170/407 | Current Loss: 0.100201\n[Training] Epoch 018 | Batch 180/407 | Current Loss: 0.100068\n[Training] Epoch 018 | Batch 190/407 | Current Loss: 0.100090\n[Training] Epoch 018 | Batch 200/407 | Current Loss: 0.100301\n[Training] Epoch 018 | Batch 210/407 | Current Loss: 0.100099\n[Training] Epoch 018 | Batch 220/407 | Current Loss: 0.100247\n[Training] Epoch 018 | Batch 230/407 | Current Loss: 0.100021\n[Training] Epoch 018 | Batch 240/407 | Current Loss: 0.099987\n[Training] Epoch 018 | Batch 250/407 | Current Loss: 0.100152\n[Training] Epoch 018 | Batch 260/407 | Current Loss: 0.100094\n[Training] Epoch 018 | Batch 270/407 | Current Loss: 0.100173\n[Training] Epoch 018 | Batch 280/407 | Current Loss: 0.100261\n[Training] Epoch 018 | Batch 290/407 | Current Loss: 0.100126\n[Training] Epoch 018 | Batch 300/407 | Current Loss: 0.100113\n[Training] Epoch 018 | Batch 310/407 | Current Loss: 0.100055\n[Training] Epoch 018 | Batch 320/407 | Current Loss: 0.100162\n[Training] Epoch 018 | Batch 330/407 | Current Loss: 0.100153\n[Training] Epoch 018 | Batch 340/407 | Current Loss: 0.100138\n[Training] Epoch 018 | Batch 350/407 | Current Loss: 0.100136\n[Training] Epoch 018 | Batch 360/407 | Current Loss: 0.100038\n[Training] Epoch 018 | Batch 370/407 | Current Loss: 0.100134\n[Training] Epoch 018 | Batch 380/407 | Current Loss: 0.100151\n[Training] Epoch 018 | Batch 390/407 | Current Loss: 0.100164\n[Training] Epoch 018 | Batch 400/407 | Current Loss: 0.100242\n\n[Training] Epoch 018 Summary:\n  Avg Loss: 0.100144\n  Current LR: 9.13e-05\n[EarlyStopping] Loss improved (0.100163 → 0.100144). Saving model...\n[Training] Epoch 019 | Batch 000/407 | Current Loss: 0.100102\n[Training] Epoch 019 | Batch 010/407 | Current Loss: 0.100092\n[Training] Epoch 019 | Batch 020/407 | Current Loss: 0.099988\n[Training] Epoch 019 | Batch 030/407 | Current Loss: 0.100034\n[Training] Epoch 019 | Batch 040/407 | Current Loss: 0.100117\n[Training] Epoch 019 | Batch 050/407 | Current Loss: 0.100135\n[Training] Epoch 019 | Batch 060/407 | Current Loss: 0.100062\n[Training] Epoch 019 | Batch 070/407 | Current Loss: 0.100211\n[Training] Epoch 019 | Batch 080/407 | Current Loss: 0.100041\n[Training] Epoch 019 | Batch 090/407 | Current Loss: 0.100146\n[Training] Epoch 019 | Batch 100/407 | Current Loss: 0.100144\n[Training] Epoch 019 | Batch 110/407 | Current Loss: 0.100194\n[Training] Epoch 019 | Batch 120/407 | Current Loss: 0.100109\n[Training] Epoch 019 | Batch 130/407 | Current Loss: 0.100043\n[Training] Epoch 019 | Batch 140/407 | Current Loss: 0.100146\n[Training] Epoch 019 | Batch 150/407 | Current Loss: 0.100175\n[Training] Epoch 019 | Batch 160/407 | Current Loss: 0.100228\n[Training] Epoch 019 | Batch 170/407 | Current Loss: 0.100194\n[Training] Epoch 019 | Batch 180/407 | Current Loss: 0.100142\n[Training] Epoch 019 | Batch 190/407 | Current Loss: 0.100049\n[Training] Epoch 019 | Batch 200/407 | Current Loss: 0.100115\n[Training] Epoch 019 | Batch 210/407 | Current Loss: 0.100069\n[Training] Epoch 019 | Batch 220/407 | Current Loss: 0.100093\n[Training] Epoch 019 | Batch 230/407 | Current Loss: 0.100234\n[Training] Epoch 019 | Batch 240/407 | Current Loss: 0.100031\n[Training] Epoch 019 | Batch 250/407 | Current Loss: 0.100213\n[Training] Epoch 019 | Batch 260/407 | Current Loss: 0.100104\n[Training] Epoch 019 | Batch 270/407 | Current Loss: 0.100369\n[Training] Epoch 019 | Batch 280/407 | Current Loss: 0.100135\n[Training] Epoch 019 | Batch 290/407 | Current Loss: 0.100008\n[Training] Epoch 019 | Batch 300/407 | Current Loss: 0.100071\n[Training] Epoch 019 | Batch 310/407 | Current Loss: 0.100076\n[Training] Epoch 019 | Batch 320/407 | Current Loss: 0.100109\n[Training] Epoch 019 | Batch 330/407 | Current Loss: 0.100141\n[Training] Epoch 019 | Batch 340/407 | Current Loss: 0.100297\n[Training] Epoch 019 | Batch 350/407 | Current Loss: 0.100055\n[Training] Epoch 019 | Batch 360/407 | Current Loss: 0.100110\n[Training] Epoch 019 | Batch 370/407 | Current Loss: 0.099963\n[Training] Epoch 019 | Batch 380/407 | Current Loss: 0.100004\n[Training] Epoch 019 | Batch 390/407 | Current Loss: 0.100141\n[Training] Epoch 019 | Batch 400/407 | Current Loss: 0.100129\n\n[Training] Epoch 019 Summary:\n  Avg Loss: 0.100134\n  Current LR: 2.41e-05\n[EarlyStopping] Loss improved (0.100144 → 0.100134). Saving model...\n[Training] Epoch 020 | Batch 000/407 | Current Loss: 0.100203\n[Training] Epoch 020 | Batch 010/407 | Current Loss: 0.100212\n[Training] Epoch 020 | Batch 020/407 | Current Loss: 0.100250\n[Training] Epoch 020 | Batch 030/407 | Current Loss: 0.100026\n[Training] Epoch 020 | Batch 040/407 | Current Loss: 0.100026\n[Training] Epoch 020 | Batch 050/407 | Current Loss: 0.100135\n[Training] Epoch 020 | Batch 060/407 | Current Loss: 0.100024\n[Training] Epoch 020 | Batch 070/407 | Current Loss: 0.100104\n[Training] Epoch 020 | Batch 080/407 | Current Loss: 0.100226\n[Training] Epoch 020 | Batch 090/407 | Current Loss: 0.099982\n[Training] Epoch 020 | Batch 100/407 | Current Loss: 0.100257\n[Training] Epoch 020 | Batch 110/407 | Current Loss: 0.100137\n[Training] Epoch 020 | Batch 120/407 | Current Loss: 0.100198\n[Training] Epoch 020 | Batch 130/407 | Current Loss: 0.100217\n[Training] Epoch 020 | Batch 140/407 | Current Loss: 0.100062\n[Training] Epoch 020 | Batch 150/407 | Current Loss: 0.100127\n[Training] Epoch 020 | Batch 160/407 | Current Loss: 0.100177\n[Training] Epoch 020 | Batch 170/407 | Current Loss: 0.100115\n[Training] Epoch 020 | Batch 180/407 | Current Loss: 0.100207\n[Training] Epoch 020 | Batch 190/407 | Current Loss: 0.100106\n[Training] Epoch 020 | Batch 200/407 | Current Loss: 0.100106\n[Training] Epoch 020 | Batch 210/407 | Current Loss: 0.100223\n[Training] Epoch 020 | Batch 220/407 | Current Loss: 0.100319\n[Training] Epoch 020 | Batch 230/407 | Current Loss: 0.100263\n[Training] Epoch 020 | Batch 240/407 | Current Loss: 0.100117\n[Training] Epoch 020 | Batch 250/407 | Current Loss: 0.100045\n[Training] Epoch 020 | Batch 260/407 | Current Loss: 0.099992\n[Training] Epoch 020 | Batch 270/407 | Current Loss: 0.100242\n[Training] Epoch 020 | Batch 280/407 | Current Loss: 0.100174\n[Training] Epoch 020 | Batch 290/407 | Current Loss: 0.100300\n[Training] Epoch 020 | Batch 300/407 | Current Loss: 0.100203\n[Training] Epoch 020 | Batch 310/407 | Current Loss: 0.100191\n[Training] Epoch 020 | Batch 320/407 | Current Loss: 0.100039\n[Training] Epoch 020 | Batch 330/407 | Current Loss: 0.100200\n[Training] Epoch 020 | Batch 340/407 | Current Loss: 0.100034\n[Training] Epoch 020 | Batch 350/407 | Current Loss: 0.100155\n[Training] Epoch 020 | Batch 360/407 | Current Loss: 0.100144\n[Training] Epoch 020 | Batch 370/407 | Current Loss: 0.100178\n[Training] Epoch 020 | Batch 380/407 | Current Loss: 0.100165\n[Training] Epoch 020 | Batch 390/407 | Current Loss: 0.100168\n[Training] Epoch 020 | Batch 400/407 | Current Loss: 0.100155\n\n[Training] Epoch 020 Summary:\n  Avg Loss: 0.100122\n  Current LR: 9.47e-04\n[EarlyStopping] Loss improved (0.100134 → 0.100122). Saving model...\n[Training] Epoch 021 | Batch 000/407 | Current Loss: 0.099999\n[Training] Epoch 021 | Batch 010/407 | Current Loss: 0.111988\n[Training] Epoch 021 | Batch 020/407 | Current Loss: 0.102673\n[Training] Epoch 021 | Batch 030/407 | Current Loss: 0.102666\n[Training] Epoch 021 | Batch 040/407 | Current Loss: 0.101011\n[Training] Epoch 021 | Batch 050/407 | Current Loss: 0.101212\n[Training] Epoch 021 | Batch 060/407 | Current Loss: 0.100688\n[Training] Epoch 021 | Batch 070/407 | Current Loss: 0.100403\n[Training] Epoch 021 | Batch 080/407 | Current Loss: 0.100457\n[Training] Epoch 021 | Batch 090/407 | Current Loss: 0.100671\n[Training] Epoch 021 | Batch 100/407 | Current Loss: 0.100608\n[Training] Epoch 021 | Batch 110/407 | Current Loss: 0.100568\n[Training] Epoch 021 | Batch 120/407 | Current Loss: 0.100571\n[Training] Epoch 021 | Batch 130/407 | Current Loss: 0.100431\n[Training] Epoch 021 | Batch 140/407 | Current Loss: 0.100426\n[Training] Epoch 021 | Batch 150/407 | Current Loss: 0.100449\n[Training] Epoch 021 | Batch 160/407 | Current Loss: 0.100484\n[Training] Epoch 021 | Batch 170/407 | Current Loss: 0.100326\n[Training] Epoch 021 | Batch 180/407 | Current Loss: 0.100558\n[Training] Epoch 021 | Batch 190/407 | Current Loss: 0.100425\n[Training] Epoch 021 | Batch 200/407 | Current Loss: 0.100511\n[Training] Epoch 021 | Batch 210/407 | Current Loss: 0.100350\n[Training] Epoch 021 | Batch 220/407 | Current Loss: 0.100318\n[Training] Epoch 021 | Batch 230/407 | Current Loss: 0.100455\n[Training] Epoch 021 | Batch 240/407 | Current Loss: 0.100323\n[Training] Epoch 021 | Batch 250/407 | Current Loss: 0.100402\n[Training] Epoch 021 | Batch 260/407 | Current Loss: 0.100333\n[Training] Epoch 021 | Batch 270/407 | Current Loss: 0.100217\n[Training] Epoch 021 | Batch 280/407 | Current Loss: 0.100235\n[Training] Epoch 021 | Batch 290/407 | Current Loss: 0.100362\n[Training] Epoch 021 | Batch 300/407 | Current Loss: 0.100489\n[Training] Epoch 021 | Batch 310/407 | Current Loss: 0.100433\n[Training] Epoch 021 | Batch 320/407 | Current Loss: 0.100394\n[Training] Epoch 021 | Batch 330/407 | Current Loss: 0.100326\n[Training] Epoch 021 | Batch 340/407 | Current Loss: 0.100341\n[Training] Epoch 021 | Batch 350/407 | Current Loss: 0.100346\n[Training] Epoch 021 | Batch 360/407 | Current Loss: 0.100511\n[Training] Epoch 021 | Batch 370/407 | Current Loss: 0.100305\n[Training] Epoch 021 | Batch 380/407 | Current Loss: 0.100431\n[Training] Epoch 021 | Batch 390/407 | Current Loss: 0.100316\n[Training] Epoch 021 | Batch 400/407 | Current Loss: 0.100337\n\n[Training] Epoch 021 Summary:\n  Avg Loss: 0.100923\n  Current LR: 9.24e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 022 | Batch 000/407 | Current Loss: 0.100217\n[Training] Epoch 022 | Batch 010/407 | Current Loss: 0.100382\n[Training] Epoch 022 | Batch 020/407 | Current Loss: 0.100502\n[Training] Epoch 022 | Batch 030/407 | Current Loss: 0.100236\n[Training] Epoch 022 | Batch 040/407 | Current Loss: 0.100400\n[Training] Epoch 022 | Batch 050/407 | Current Loss: 0.100596\n[Training] Epoch 022 | Batch 060/407 | Current Loss: 0.100225\n[Training] Epoch 022 | Batch 070/407 | Current Loss: 0.100260\n[Training] Epoch 022 | Batch 080/407 | Current Loss: 0.100302\n[Training] Epoch 022 | Batch 090/407 | Current Loss: 0.100262\n[Training] Epoch 022 | Batch 100/407 | Current Loss: 0.100153\n[Training] Epoch 022 | Batch 110/407 | Current Loss: 0.100356\n[Training] Epoch 022 | Batch 120/407 | Current Loss: 0.100133\n[Training] Epoch 022 | Batch 130/407 | Current Loss: 0.100287\n[Training] Epoch 022 | Batch 140/407 | Current Loss: 0.100360\n[Training] Epoch 022 | Batch 150/407 | Current Loss: 0.100226\n[Training] Epoch 022 | Batch 160/407 | Current Loss: 0.100322\n[Training] Epoch 022 | Batch 170/407 | Current Loss: 0.100169\n[Training] Epoch 022 | Batch 180/407 | Current Loss: 0.100234\n[Training] Epoch 022 | Batch 190/407 | Current Loss: 0.100300\n[Training] Epoch 022 | Batch 200/407 | Current Loss: 0.100325\n[Training] Epoch 022 | Batch 210/407 | Current Loss: 0.100167\n[Training] Epoch 022 | Batch 220/407 | Current Loss: 0.100142\n[Training] Epoch 022 | Batch 230/407 | Current Loss: 0.100355\n[Training] Epoch 022 | Batch 240/407 | Current Loss: 0.100237\n[Training] Epoch 022 | Batch 250/407 | Current Loss: 0.100179\n[Training] Epoch 022 | Batch 260/407 | Current Loss: 0.100188\n[Training] Epoch 022 | Batch 270/407 | Current Loss: 0.100170\n[Training] Epoch 022 | Batch 280/407 | Current Loss: 0.100246\n[Training] Epoch 022 | Batch 290/407 | Current Loss: 0.100317\n[Training] Epoch 022 | Batch 300/407 | Current Loss: 0.100234\n[Training] Epoch 022 | Batch 310/407 | Current Loss: 0.100185\n[Training] Epoch 022 | Batch 320/407 | Current Loss: 0.100202\n[Training] Epoch 022 | Batch 330/407 | Current Loss: 0.100155\n[Training] Epoch 022 | Batch 340/407 | Current Loss: 0.100190\n[Training] Epoch 022 | Batch 350/407 | Current Loss: 0.100126\n[Training] Epoch 022 | Batch 360/407 | Current Loss: 0.100188\n[Training] Epoch 022 | Batch 370/407 | Current Loss: 0.100224\n[Training] Epoch 022 | Batch 380/407 | Current Loss: 0.100305\n[Training] Epoch 022 | Batch 390/407 | Current Loss: 0.100083\n[Training] Epoch 022 | Batch 400/407 | Current Loss: 0.100261\n\n[Training] Epoch 022 Summary:\n  Avg Loss: 0.100251\n  Current LR: 8.56e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 023 | Batch 000/407 | Current Loss: 0.100114\n[Training] Epoch 023 | Batch 010/407 | Current Loss: 0.100255\n[Training] Epoch 023 | Batch 020/407 | Current Loss: 0.100247\n[Training] Epoch 023 | Batch 030/407 | Current Loss: 0.100255\n[Training] Epoch 023 | Batch 040/407 | Current Loss: 0.100476\n[Training] Epoch 023 | Batch 050/407 | Current Loss: 0.100124\n[Training] Epoch 023 | Batch 060/407 | Current Loss: 0.100197\n[Training] Epoch 023 | Batch 070/407 | Current Loss: 0.100238\n[Training] Epoch 023 | Batch 080/407 | Current Loss: 0.100133\n[Training] Epoch 023 | Batch 090/407 | Current Loss: 0.100169\n[Training] Epoch 023 | Batch 100/407 | Current Loss: 0.100135\n[Training] Epoch 023 | Batch 110/407 | Current Loss: 0.100203\n[Training] Epoch 023 | Batch 120/407 | Current Loss: 0.100289\n[Training] Epoch 023 | Batch 130/407 | Current Loss: 0.100141\n[Training] Epoch 023 | Batch 140/407 | Current Loss: 0.100263\n[Training] Epoch 023 | Batch 150/407 | Current Loss: 0.100191\n[Training] Epoch 023 | Batch 160/407 | Current Loss: 0.100351\n[Training] Epoch 023 | Batch 170/407 | Current Loss: 0.100222\n[Training] Epoch 023 | Batch 180/407 | Current Loss: 0.100205\n[Training] Epoch 023 | Batch 190/407 | Current Loss: 0.100200\n[Training] Epoch 023 | Batch 200/407 | Current Loss: 0.100121\n[Training] Epoch 023 | Batch 210/407 | Current Loss: 0.100258\n[Training] Epoch 023 | Batch 220/407 | Current Loss: 0.100162\n[Training] Epoch 023 | Batch 230/407 | Current Loss: 0.100126\n[Training] Epoch 023 | Batch 240/407 | Current Loss: 0.100133\n[Training] Epoch 023 | Batch 250/407 | Current Loss: 0.100346\n[Training] Epoch 023 | Batch 260/407 | Current Loss: 0.100177\n[Training] Epoch 023 | Batch 270/407 | Current Loss: 0.100052\n[Training] Epoch 023 | Batch 280/407 | Current Loss: 0.100106\n[Training] Epoch 023 | Batch 290/407 | Current Loss: 0.100065\n[Training] Epoch 023 | Batch 300/407 | Current Loss: 0.100210\n[Training] Epoch 023 | Batch 310/407 | Current Loss: 0.100484\n[Training] Epoch 023 | Batch 320/407 | Current Loss: 0.100209\n[Training] Epoch 023 | Batch 330/407 | Current Loss: 0.100182\n[Training] Epoch 023 | Batch 340/407 | Current Loss: 0.100145\n[Training] Epoch 023 | Batch 350/407 | Current Loss: 0.100179\n[Training] Epoch 023 | Batch 360/407 | Current Loss: 0.100225\n[Training] Epoch 023 | Batch 370/407 | Current Loss: 0.100054\n[Training] Epoch 023 | Batch 380/407 | Current Loss: 0.100261\n[Training] Epoch 023 | Batch 390/407 | Current Loss: 0.100212\n[Training] Epoch 023 | Batch 400/407 | Current Loss: 0.100227\n\n[Training] Epoch 023 Summary:\n  Avg Loss: 0.100208\n  Current LR: 7.52e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 024 | Batch 000/407 | Current Loss: 0.100137\n[Training] Epoch 024 | Batch 010/407 | Current Loss: 0.100323\n[Training] Epoch 024 | Batch 020/407 | Current Loss: 0.100220\n[Training] Epoch 024 | Batch 030/407 | Current Loss: 0.100259\n[Training] Epoch 024 | Batch 040/407 | Current Loss: 0.100130\n[Training] Epoch 024 | Batch 050/407 | Current Loss: 0.100280\n[Training] Epoch 024 | Batch 060/407 | Current Loss: 0.100291\n[Training] Epoch 024 | Batch 070/407 | Current Loss: 0.100148\n[Training] Epoch 024 | Batch 080/407 | Current Loss: 0.100107\n[Training] Epoch 024 | Batch 090/407 | Current Loss: 0.100067\n[Training] Epoch 024 | Batch 100/407 | Current Loss: 0.100085\n[Training] Epoch 024 | Batch 110/407 | Current Loss: 0.100037\n[Training] Epoch 024 | Batch 120/407 | Current Loss: 0.100220\n[Training] Epoch 024 | Batch 130/407 | Current Loss: 0.100347\n[Training] Epoch 024 | Batch 140/407 | Current Loss: 0.100305\n[Training] Epoch 024 | Batch 150/407 | Current Loss: 0.100178\n[Training] Epoch 024 | Batch 160/407 | Current Loss: 0.100015\n[Training] Epoch 024 | Batch 170/407 | Current Loss: 0.100110\n[Training] Epoch 024 | Batch 180/407 | Current Loss: 0.100113\n[Training] Epoch 024 | Batch 190/407 | Current Loss: 0.100481\n[Training] Epoch 024 | Batch 200/407 | Current Loss: 0.100181\n[Training] Epoch 024 | Batch 210/407 | Current Loss: 0.100144\n[Training] Epoch 024 | Batch 220/407 | Current Loss: 0.100210\n[Training] Epoch 024 | Batch 230/407 | Current Loss: 0.100281\n[Training] Epoch 024 | Batch 240/407 | Current Loss: 0.100273\n[Training] Epoch 024 | Batch 250/407 | Current Loss: 0.100212\n[Training] Epoch 024 | Batch 260/407 | Current Loss: 0.100307\n[Training] Epoch 024 | Batch 270/407 | Current Loss: 0.100304\n[Training] Epoch 024 | Batch 280/407 | Current Loss: 0.100341\n[Training] Epoch 024 | Batch 290/407 | Current Loss: 0.100144\n[Training] Epoch 024 | Batch 300/407 | Current Loss: 0.100201\n[Training] Epoch 024 | Batch 310/407 | Current Loss: 0.100313\n[Training] Epoch 024 | Batch 320/407 | Current Loss: 0.100237\n[Training] Epoch 024 | Batch 330/407 | Current Loss: 0.100157\n[Training] Epoch 024 | Batch 340/407 | Current Loss: 0.100129\n[Training] Epoch 024 | Batch 350/407 | Current Loss: 0.100400\n[Training] Epoch 024 | Batch 360/407 | Current Loss: 0.100249\n[Training] Epoch 024 | Batch 370/407 | Current Loss: 0.100222\n[Training] Epoch 024 | Batch 380/407 | Current Loss: 0.100180\n[Training] Epoch 024 | Batch 390/407 | Current Loss: 0.100031\n[Training] Epoch 024 | Batch 400/407 | Current Loss: 0.100267\n\n[Training] Epoch 024 Summary:\n  Avg Loss: 0.100184\n  Current LR: 6.20e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 025 | Batch 000/407 | Current Loss: 0.100185\n[Training] Epoch 025 | Batch 010/407 | Current Loss: 0.100215\n[Training] Epoch 025 | Batch 020/407 | Current Loss: 0.100025\n[Training] Epoch 025 | Batch 030/407 | Current Loss: 0.100162\n[Training] Epoch 025 | Batch 040/407 | Current Loss: 0.100020\n[Training] Epoch 025 | Batch 050/407 | Current Loss: 0.100229\n[Training] Epoch 025 | Batch 060/407 | Current Loss: 0.100056\n[Training] Epoch 025 | Batch 070/407 | Current Loss: 0.100161\n[Training] Epoch 025 | Batch 080/407 | Current Loss: 0.100207\n[Training] Epoch 025 | Batch 090/407 | Current Loss: 0.100111\n[Training] Epoch 025 | Batch 100/407 | Current Loss: 0.100178\n[Training] Epoch 025 | Batch 110/407 | Current Loss: 0.100089\n[Training] Epoch 025 | Batch 120/407 | Current Loss: 0.100161\n[Training] Epoch 025 | Batch 130/407 | Current Loss: 0.100128\n[Training] Epoch 025 | Batch 140/407 | Current Loss: 0.100251\n[Training] Epoch 025 | Batch 150/407 | Current Loss: 0.100275\n[Training] Epoch 025 | Batch 160/407 | Current Loss: 0.100274\n[Training] Epoch 025 | Batch 170/407 | Current Loss: 0.100289\n[Training] Epoch 025 | Batch 180/407 | Current Loss: 0.100083\n[Training] Epoch 025 | Batch 190/407 | Current Loss: 0.100139\n[Training] Epoch 025 | Batch 200/407 | Current Loss: 0.100144\n[Training] Epoch 025 | Batch 210/407 | Current Loss: 0.100212\n[Training] Epoch 025 | Batch 220/407 | Current Loss: 0.100126\n[Training] Epoch 025 | Batch 230/407 | Current Loss: 0.100224\n[Training] Epoch 025 | Batch 240/407 | Current Loss: 0.100130\n[Training] Epoch 025 | Batch 250/407 | Current Loss: 0.100198\n[Training] Epoch 025 | Batch 260/407 | Current Loss: 0.100212\n[Training] Epoch 025 | Batch 270/407 | Current Loss: 0.100240\n[Training] Epoch 025 | Batch 280/407 | Current Loss: 0.100156\n[Training] Epoch 025 | Batch 290/407 | Current Loss: 0.100064\n[Training] Epoch 025 | Batch 300/407 | Current Loss: 0.100210\n[Training] Epoch 025 | Batch 310/407 | Current Loss: 0.100136\n[Training] Epoch 025 | Batch 320/407 | Current Loss: 0.100391\n[Training] Epoch 025 | Batch 330/407 | Current Loss: 0.100065\n[Training] Epoch 025 | Batch 340/407 | Current Loss: 0.100055\n[Training] Epoch 025 | Batch 350/407 | Current Loss: 0.100269\n[Training] Epoch 025 | Batch 360/407 | Current Loss: 0.100195\n[Training] Epoch 025 | Batch 370/407 | Current Loss: 0.100204\n[Training] Epoch 025 | Batch 380/407 | Current Loss: 0.100096\n[Training] Epoch 025 | Batch 390/407 | Current Loss: 0.100140\n[Training] Epoch 025 | Batch 400/407 | Current Loss: 0.100035\n\n[Training] Epoch 025 Summary:\n  Avg Loss: 0.100170\n  Current LR: 4.74e-04\n[EarlyStopping] Counter: 5/5\n\n[Training] Early stopping triggered\n\n[Optuna] Trial 0 completed with best loss: 0.100122\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 14:47:26,593] Trial 0 finished with value: 0.10012221861749668 and parameters: {'lr': 0.0009467904939391108, 'batch_size': 32, 'bottleneck_width': 512, 'dropout_rate': 0.3979044543867316, 'alpha': 0.5586241729095667, 'weight_decay': 1.0133275210700108e-06}. Best is trial 0 with value: 0.10012221861749668.\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nStarting Optuna Trial 1\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 5.2595132291187947e-05\n  batch_size: 64\n  bottleneck_width: 1024\n  dropout_rate: 0.1789694089827214\n  alpha: 0.5576093850912043\n  weight_decay: 4.483978503838243e-06\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 64, Total batches: 204\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 1024\n  - Dropout rate: 0.1789694089827214\n[Model] Architecture initialized successfully\n[Model] Total parameters: 2,396,992\n[Optimizer] Initialized with lr=5.26e-05, weight_decay=4.48e-06\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.5576093850912043\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/204 | Current Loss: 1.352335\n[Training] Epoch 001 | Batch 010/204 | Current Loss: 0.559313\n[Training] Epoch 001 | Batch 020/204 | Current Loss: 0.309324\n[Training] Epoch 001 | Batch 030/204 | Current Loss: 0.217953\n[Training] Epoch 001 | Batch 040/204 | Current Loss: 0.184614\n[Training] Epoch 001 | Batch 050/204 | Current Loss: 0.163666\n[Training] Epoch 001 | Batch 060/204 | Current Loss: 0.152594\n[Training] Epoch 001 | Batch 070/204 | Current Loss: 0.145386\n[Training] Epoch 001 | Batch 080/204 | Current Loss: 0.139659\n[Training] Epoch 001 | Batch 090/204 | Current Loss: 0.135760\n[Training] Epoch 001 | Batch 100/204 | Current Loss: 0.129566\n[Training] Epoch 001 | Batch 110/204 | Current Loss: 0.126756\n[Training] Epoch 001 | Batch 120/204 | Current Loss: 0.124354\n[Training] Epoch 001 | Batch 130/204 | Current Loss: 0.125780\n[Training] Epoch 001 | Batch 140/204 | Current Loss: 0.121960\n[Training] Epoch 001 | Batch 150/204 | Current Loss: 0.120518\n[Training] Epoch 001 | Batch 160/204 | Current Loss: 0.119683\n[Training] Epoch 001 | Batch 170/204 | Current Loss: 0.117694\n[Training] Epoch 001 | Batch 180/204 | Current Loss: 0.117165\n[Training] Epoch 001 | Batch 190/204 | Current Loss: 0.115698\n[Training] Epoch 001 | Batch 200/204 | Current Loss: 0.115341\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.194894\n  Current LR: 5.13e-05\n[EarlyStopping] Loss improved (inf → 0.194894). Saving model...\n[Training] Epoch 002 | Batch 000/204 | Current Loss: 0.116484\n[Training] Epoch 002 | Batch 010/204 | Current Loss: 0.114867\n[Training] Epoch 002 | Batch 020/204 | Current Loss: 0.114668\n[Training] Epoch 002 | Batch 030/204 | Current Loss: 0.115492\n[Training] Epoch 002 | Batch 040/204 | Current Loss: 0.115587\n[Training] Epoch 002 | Batch 050/204 | Current Loss: 0.112463\n[Training] Epoch 002 | Batch 060/204 | Current Loss: 0.114450\n[Training] Epoch 002 | Batch 070/204 | Current Loss: 0.111965\n[Training] Epoch 002 | Batch 080/204 | Current Loss: 0.111409\n[Training] Epoch 002 | Batch 090/204 | Current Loss: 0.110620\n[Training] Epoch 002 | Batch 100/204 | Current Loss: 0.111120\n[Training] Epoch 002 | Batch 110/204 | Current Loss: 0.113180\n[Training] Epoch 002 | Batch 120/204 | Current Loss: 0.110010\n[Training] Epoch 002 | Batch 130/204 | Current Loss: 0.109736\n[Training] Epoch 002 | Batch 140/204 | Current Loss: 0.109928\n[Training] Epoch 002 | Batch 150/204 | Current Loss: 0.109075\n[Training] Epoch 002 | Batch 160/204 | Current Loss: 0.109010\n[Training] Epoch 002 | Batch 170/204 | Current Loss: 0.108872\n[Training] Epoch 002 | Batch 180/204 | Current Loss: 0.110088\n[Training] Epoch 002 | Batch 190/204 | Current Loss: 0.108904\n[Training] Epoch 002 | Batch 200/204 | Current Loss: 0.109066\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.111642\n  Current LR: 4.77e-05\n[EarlyStopping] Loss improved (0.194894 → 0.111642). Saving model...\n[Training] Epoch 003 | Batch 000/204 | Current Loss: 0.108104\n[Training] Epoch 003 | Batch 010/204 | Current Loss: 0.108290\n[Training] Epoch 003 | Batch 020/204 | Current Loss: 0.108566\n[Training] Epoch 003 | Batch 030/204 | Current Loss: 0.107360\n[Training] Epoch 003 | Batch 040/204 | Current Loss: 0.108059\n[Training] Epoch 003 | Batch 050/204 | Current Loss: 0.107762\n[Training] Epoch 003 | Batch 060/204 | Current Loss: 0.107933\n[Training] Epoch 003 | Batch 070/204 | Current Loss: 0.106832\n[Training] Epoch 003 | Batch 080/204 | Current Loss: 0.107458\n[Training] Epoch 003 | Batch 090/204 | Current Loss: 0.107020\n[Training] Epoch 003 | Batch 100/204 | Current Loss: 0.107371\n[Training] Epoch 003 | Batch 110/204 | Current Loss: 0.106897\n[Training] Epoch 003 | Batch 120/204 | Current Loss: 0.107411\n[Training] Epoch 003 | Batch 130/204 | Current Loss: 0.107517\n[Training] Epoch 003 | Batch 140/204 | Current Loss: 0.106553\n[Training] Epoch 003 | Batch 150/204 | Current Loss: 0.107086\n[Training] Epoch 003 | Batch 160/204 | Current Loss: 0.106281\n[Training] Epoch 003 | Batch 170/204 | Current Loss: 0.106533\n[Training] Epoch 003 | Batch 180/204 | Current Loss: 0.106135\n[Training] Epoch 003 | Batch 190/204 | Current Loss: 0.106637\n[Training] Epoch 003 | Batch 200/204 | Current Loss: 0.106315\n\n[Training] Epoch 003 Summary:\n  Avg Loss: 0.107592\n  Current LR: 4.20e-05\n[EarlyStopping] Loss improved (0.111642 → 0.107592). Saving model...\n[Training] Epoch 004 | Batch 000/204 | Current Loss: 0.106638\n[Training] Epoch 004 | Batch 010/204 | Current Loss: 0.108179\n[Training] Epoch 004 | Batch 020/204 | Current Loss: 0.106419\n[Training] Epoch 004 | Batch 030/204 | Current Loss: 0.108652\n[Training] Epoch 004 | Batch 040/204 | Current Loss: 0.105912\n[Training] Epoch 004 | Batch 050/204 | Current Loss: 0.105803\n[Training] Epoch 004 | Batch 060/204 | Current Loss: 0.106723\n[Training] Epoch 004 | Batch 070/204 | Current Loss: 0.106036\n[Training] Epoch 004 | Batch 080/204 | Current Loss: 0.105649\n[Training] Epoch 004 | Batch 090/204 | Current Loss: 0.105230\n[Training] Epoch 004 | Batch 100/204 | Current Loss: 0.105036\n[Training] Epoch 004 | Batch 110/204 | Current Loss: 0.105592\n[Training] Epoch 004 | Batch 120/204 | Current Loss: 0.105708\n[Training] Epoch 004 | Batch 130/204 | Current Loss: 0.105085\n[Training] Epoch 004 | Batch 140/204 | Current Loss: 0.105418\n[Training] Epoch 004 | Batch 150/204 | Current Loss: 0.105102\n[Training] Epoch 004 | Batch 160/204 | Current Loss: 0.106012\n[Training] Epoch 004 | Batch 170/204 | Current Loss: 0.105677\n[Training] Epoch 004 | Batch 180/204 | Current Loss: 0.104905\n[Training] Epoch 004 | Batch 190/204 | Current Loss: 0.104599\n[Training] Epoch 004 | Batch 200/204 | Current Loss: 0.105446\n\n[Training] Epoch 004 Summary:\n  Avg Loss: 0.105717\n  Current LR: 3.48e-05\n[EarlyStopping] Loss improved (0.107592 → 0.105717). Saving model...\n[Training] Epoch 005 | Batch 000/204 | Current Loss: 0.104757\n[Training] Epoch 005 | Batch 010/204 | Current Loss: 0.105877\n[Training] Epoch 005 | Batch 020/204 | Current Loss: 0.104731\n[Training] Epoch 005 | Batch 030/204 | Current Loss: 0.104884\n[Training] Epoch 005 | Batch 040/204 | Current Loss: 0.104361\n[Training] Epoch 005 | Batch 050/204 | Current Loss: 0.104432\n[Training] Epoch 005 | Batch 060/204 | Current Loss: 0.104604\n[Training] Epoch 005 | Batch 070/204 | Current Loss: 0.104933\n[Training] Epoch 005 | Batch 080/204 | Current Loss: 0.106399\n[Training] Epoch 005 | Batch 090/204 | Current Loss: 0.104416\n[Training] Epoch 005 | Batch 100/204 | Current Loss: 0.104625\n[Training] Epoch 005 | Batch 110/204 | Current Loss: 0.104293\n[Training] Epoch 005 | Batch 120/204 | Current Loss: 0.104374\n[Training] Epoch 005 | Batch 130/204 | Current Loss: 0.104952\n[Training] Epoch 005 | Batch 140/204 | Current Loss: 0.104501\n[Training] Epoch 005 | Batch 150/204 | Current Loss: 0.104465\n[Training] Epoch 005 | Batch 160/204 | Current Loss: 0.104132\n[Training] Epoch 005 | Batch 170/204 | Current Loss: 0.104449\n[Training] Epoch 005 | Batch 180/204 | Current Loss: 0.104545\n[Training] Epoch 005 | Batch 190/204 | Current Loss: 0.103876\n[Training] Epoch 005 | Batch 200/204 | Current Loss: 0.104117\n\n[Training] Epoch 005 Summary:\n  Avg Loss: 0.104688\n  Current LR: 2.68e-05\n[EarlyStopping] Loss improved (0.105717 → 0.104688). Saving model...\n[Training] Epoch 006 | Batch 000/204 | Current Loss: 0.104305\n[Training] Epoch 006 | Batch 010/204 | Current Loss: 0.104177\n[Training] Epoch 006 | Batch 020/204 | Current Loss: 0.105496\n[Training] Epoch 006 | Batch 030/204 | Current Loss: 0.104351\n[Training] Epoch 006 | Batch 040/204 | Current Loss: 0.103947\n[Training] Epoch 006 | Batch 050/204 | Current Loss: 0.104602\n[Training] Epoch 006 | Batch 060/204 | Current Loss: 0.104068\n[Training] Epoch 006 | Batch 070/204 | Current Loss: 0.103748\n[Training] Epoch 006 | Batch 080/204 | Current Loss: 0.103897\n[Training] Epoch 006 | Batch 090/204 | Current Loss: 0.103406\n[Training] Epoch 006 | Batch 100/204 | Current Loss: 0.104522\n[Training] Epoch 006 | Batch 110/204 | Current Loss: 0.104104\n[Training] Epoch 006 | Batch 120/204 | Current Loss: 0.103859\n[Training] Epoch 006 | Batch 130/204 | Current Loss: 0.103834\n[Training] Epoch 006 | Batch 140/204 | Current Loss: 0.104131\n[Training] Epoch 006 | Batch 150/204 | Current Loss: 0.103543\n[Training] Epoch 006 | Batch 160/204 | Current Loss: 0.103872\n[Training] Epoch 006 | Batch 170/204 | Current Loss: 0.103822\n[Training] Epoch 006 | Batch 180/204 | Current Loss: 0.103482\n[Training] Epoch 006 | Batch 190/204 | Current Loss: 0.103580\n[Training] Epoch 006 | Batch 200/204 | Current Loss: 0.103305\n\n[Training] Epoch 006 Summary:\n  Avg Loss: 0.104125\n  Current LR: 1.88e-05\n[EarlyStopping] Loss improved (0.104688 → 0.104125). Saving model...\n[Training] Epoch 007 | Batch 000/204 | Current Loss: 0.103781\n[Training] Epoch 007 | Batch 010/204 | Current Loss: 0.103822\n[Training] Epoch 007 | Batch 020/204 | Current Loss: 0.104535\n[Training] Epoch 007 | Batch 030/204 | Current Loss: 0.103718\n[Training] Epoch 007 | Batch 040/204 | Current Loss: 0.103793\n[Training] Epoch 007 | Batch 050/204 | Current Loss: 0.103663\n[Training] Epoch 007 | Batch 060/204 | Current Loss: 0.104723\n[Training] Epoch 007 | Batch 070/204 | Current Loss: 0.103631\n[Training] Epoch 007 | Batch 080/204 | Current Loss: 0.103983\n[Training] Epoch 007 | Batch 090/204 | Current Loss: 0.103591\n[Training] Epoch 007 | Batch 100/204 | Current Loss: 0.103819\n[Training] Epoch 007 | Batch 110/204 | Current Loss: 0.103487\n[Training] Epoch 007 | Batch 120/204 | Current Loss: 0.103621\n[Training] Epoch 007 | Batch 130/204 | Current Loss: 0.103209\n[Training] Epoch 007 | Batch 140/204 | Current Loss: 0.103407\n[Training] Epoch 007 | Batch 150/204 | Current Loss: 0.103468\n[Training] Epoch 007 | Batch 160/204 | Current Loss: 0.103818\n[Training] Epoch 007 | Batch 170/204 | Current Loss: 0.103709\n[Training] Epoch 007 | Batch 180/204 | Current Loss: 0.103748\n[Training] Epoch 007 | Batch 190/204 | Current Loss: 0.103617\n[Training] Epoch 007 | Batch 200/204 | Current Loss: 0.103315\n\n[Training] Epoch 007 Summary:\n  Avg Loss: 0.103795\n  Current LR: 1.16e-05\n[EarlyStopping] Loss improved (0.104125 → 0.103795). Saving model...\n[Training] Epoch 008 | Batch 000/204 | Current Loss: 0.103569\n[Training] Epoch 008 | Batch 010/204 | Current Loss: 0.103629\n[Training] Epoch 008 | Batch 020/204 | Current Loss: 0.103692\n[Training] Epoch 008 | Batch 030/204 | Current Loss: 0.103702\n[Training] Epoch 008 | Batch 040/204 | Current Loss: 0.103443\n[Training] Epoch 008 | Batch 050/204 | Current Loss: 0.103870\n[Training] Epoch 008 | Batch 060/204 | Current Loss: 0.103575\n[Training] Epoch 008 | Batch 070/204 | Current Loss: 0.103420\n[Training] Epoch 008 | Batch 080/204 | Current Loss: 0.103977\n[Training] Epoch 008 | Batch 090/204 | Current Loss: 0.103197\n[Training] Epoch 008 | Batch 100/204 | Current Loss: 0.103246\n[Training] Epoch 008 | Batch 110/204 | Current Loss: 0.103238\n[Training] Epoch 008 | Batch 120/204 | Current Loss: 0.103609\n[Training] Epoch 008 | Batch 130/204 | Current Loss: 0.103244\n[Training] Epoch 008 | Batch 140/204 | Current Loss: 0.103369\n[Training] Epoch 008 | Batch 150/204 | Current Loss: 0.103119\n[Training] Epoch 008 | Batch 160/204 | Current Loss: 0.103522\n[Training] Epoch 008 | Batch 170/204 | Current Loss: 0.103792\n[Training] Epoch 008 | Batch 180/204 | Current Loss: 0.103334\n[Training] Epoch 008 | Batch 190/204 | Current Loss: 0.103130\n[Training] Epoch 008 | Batch 200/204 | Current Loss: 0.103204\n\n[Training] Epoch 008 Summary:\n  Avg Loss: 0.103553\n  Current LR: 5.93e-06\n[EarlyStopping] Loss improved (0.103795 → 0.103553). Saving model...\n[Training] Epoch 009 | Batch 000/204 | Current Loss: 0.103131\n[Training] Epoch 009 | Batch 010/204 | Current Loss: 0.103942\n[Training] Epoch 009 | Batch 020/204 | Current Loss: 0.103156\n[Training] Epoch 009 | Batch 030/204 | Current Loss: 0.102993\n[Training] Epoch 009 | Batch 040/204 | Current Loss: 0.103500\n[Training] Epoch 009 | Batch 050/204 | Current Loss: 0.103213\n[Training] Epoch 009 | Batch 060/204 | Current Loss: 0.103687\n[Training] Epoch 009 | Batch 070/204 | Current Loss: 0.104217\n[Training] Epoch 009 | Batch 080/204 | Current Loss: 0.103397\n[Training] Epoch 009 | Batch 090/204 | Current Loss: 0.103405\n[Training] Epoch 009 | Batch 100/204 | Current Loss: 0.103480\n[Training] Epoch 009 | Batch 110/204 | Current Loss: 0.104164\n[Training] Epoch 009 | Batch 120/204 | Current Loss: 0.103377\n[Training] Epoch 009 | Batch 130/204 | Current Loss: 0.103478\n[Training] Epoch 009 | Batch 140/204 | Current Loss: 0.103434\n[Training] Epoch 009 | Batch 150/204 | Current Loss: 0.103707\n[Training] Epoch 009 | Batch 160/204 | Current Loss: 0.103205\n[Training] Epoch 009 | Batch 170/204 | Current Loss: 0.103086\n[Training] Epoch 009 | Batch 180/204 | Current Loss: 0.103100\n[Training] Epoch 009 | Batch 190/204 | Current Loss: 0.103150\n[Training] Epoch 009 | Batch 200/204 | Current Loss: 0.103303\n\n[Training] Epoch 009 Summary:\n  Avg Loss: 0.103358\n  Current LR: 2.26e-06\n[EarlyStopping] Loss improved (0.103553 → 0.103358). Saving model...\n[Training] Epoch 010 | Batch 000/204 | Current Loss: 0.105215\n[Training] Epoch 010 | Batch 010/204 | Current Loss: 0.103419\n[Training] Epoch 010 | Batch 020/204 | Current Loss: 0.103343\n[Training] Epoch 010 | Batch 030/204 | Current Loss: 0.102978\n[Training] Epoch 010 | Batch 040/204 | Current Loss: 0.103144\n[Training] Epoch 010 | Batch 050/204 | Current Loss: 0.102989\n[Training] Epoch 010 | Batch 060/204 | Current Loss: 0.103428\n[Training] Epoch 010 | Batch 070/204 | Current Loss: 0.103195\n[Training] Epoch 010 | Batch 080/204 | Current Loss: 0.103647\n[Training] Epoch 010 | Batch 090/204 | Current Loss: 0.103442\n[Training] Epoch 010 | Batch 100/204 | Current Loss: 0.103158\n[Training] Epoch 010 | Batch 110/204 | Current Loss: 0.103092\n[Training] Epoch 010 | Batch 120/204 | Current Loss: 0.104130\n[Training] Epoch 010 | Batch 130/204 | Current Loss: 0.102980\n[Training] Epoch 010 | Batch 140/204 | Current Loss: 0.102827\n[Training] Epoch 010 | Batch 150/204 | Current Loss: 0.103123\n[Training] Epoch 010 | Batch 160/204 | Current Loss: 0.102922\n[Training] Epoch 010 | Batch 170/204 | Current Loss: 0.103014\n[Training] Epoch 010 | Batch 180/204 | Current Loss: 0.103214\n[Training] Epoch 010 | Batch 190/204 | Current Loss: 0.103054\n[Training] Epoch 010 | Batch 200/204 | Current Loss: 0.102825\n\n[Training] Epoch 010 Summary:\n  Avg Loss: 0.103336\n  Current LR: 5.26e-05\n[EarlyStopping] Loss improved (0.103358 → 0.103336). Saving model...\n[Training] Epoch 011 | Batch 000/204 | Current Loss: 0.104841\n[Training] Epoch 011 | Batch 010/204 | Current Loss: 0.103634\n[Training] Epoch 011 | Batch 020/204 | Current Loss: 0.103902\n[Training] Epoch 011 | Batch 030/204 | Current Loss: 0.102975\n[Training] Epoch 011 | Batch 040/204 | Current Loss: 0.103259\n[Training] Epoch 011 | Batch 050/204 | Current Loss: 0.103000\n[Training] Epoch 011 | Batch 060/204 | Current Loss: 0.103187\n[Training] Epoch 011 | Batch 070/204 | Current Loss: 0.102902\n[Training] Epoch 011 | Batch 080/204 | Current Loss: 0.102963\n[Training] Epoch 011 | Batch 090/204 | Current Loss: 0.102923\n[Training] Epoch 011 | Batch 100/204 | Current Loss: 0.103098\n[Training] Epoch 011 | Batch 110/204 | Current Loss: 0.102678\n[Training] Epoch 011 | Batch 120/204 | Current Loss: 0.103008\n[Training] Epoch 011 | Batch 130/204 | Current Loss: 0.103034\n[Training] Epoch 011 | Batch 140/204 | Current Loss: 0.102693\n[Training] Epoch 011 | Batch 150/204 | Current Loss: 0.102630\n[Training] Epoch 011 | Batch 160/204 | Current Loss: 0.102907\n[Training] Epoch 011 | Batch 170/204 | Current Loss: 0.102499\n[Training] Epoch 011 | Batch 180/204 | Current Loss: 0.102690\n[Training] Epoch 011 | Batch 190/204 | Current Loss: 0.102789\n[Training] Epoch 011 | Batch 200/204 | Current Loss: 0.102729\n\n[Training] Epoch 011 Summary:\n  Avg Loss: 0.103079\n  Current LR: 5.13e-05\n[EarlyStopping] Loss improved (0.103336 → 0.103079). Saving model...\n[Training] Epoch 012 | Batch 000/204 | Current Loss: 0.102601\n[Training] Epoch 012 | Batch 010/204 | Current Loss: 0.102842\n[Training] Epoch 012 | Batch 020/204 | Current Loss: 0.103232\n[Training] Epoch 012 | Batch 030/204 | Current Loss: 0.103012\n[Training] Epoch 012 | Batch 040/204 | Current Loss: 0.102675\n[Training] Epoch 012 | Batch 050/204 | Current Loss: 0.102673\n[Training] Epoch 012 | Batch 060/204 | Current Loss: 0.102700\n[Training] Epoch 012 | Batch 070/204 | Current Loss: 0.102468\n[Training] Epoch 012 | Batch 080/204 | Current Loss: 0.102302\n[Training] Epoch 012 | Batch 090/204 | Current Loss: 0.102585\n[Training] Epoch 012 | Batch 100/204 | Current Loss: 0.102434\n[Training] Epoch 012 | Batch 110/204 | Current Loss: 0.102842\n[Training] Epoch 012 | Batch 120/204 | Current Loss: 0.102502\n[Training] Epoch 012 | Batch 130/204 | Current Loss: 0.102609\n[Training] Epoch 012 | Batch 140/204 | Current Loss: 0.102590\n[Training] Epoch 012 | Batch 150/204 | Current Loss: 0.103243\n[Training] Epoch 012 | Batch 160/204 | Current Loss: 0.102280\n[Training] Epoch 012 | Batch 170/204 | Current Loss: 0.102546\n[Training] Epoch 012 | Batch 180/204 | Current Loss: 0.102551\n[Training] Epoch 012 | Batch 190/204 | Current Loss: 0.102436\n[Training] Epoch 012 | Batch 200/204 | Current Loss: 0.102353\n\n[Training] Epoch 012 Summary:\n  Avg Loss: 0.102700\n  Current LR: 4.77e-05\n[EarlyStopping] Loss improved (0.103079 → 0.102700). Saving model...\n[Training] Epoch 013 | Batch 000/204 | Current Loss: 0.102694\n[Training] Epoch 013 | Batch 010/204 | Current Loss: 0.104287\n[Training] Epoch 013 | Batch 020/204 | Current Loss: 0.102395\n[Training] Epoch 013 | Batch 030/204 | Current Loss: 0.102122\n[Training] Epoch 013 | Batch 040/204 | Current Loss: 0.102180\n[Training] Epoch 013 | Batch 050/204 | Current Loss: 0.102056\n[Training] Epoch 013 | Batch 060/204 | Current Loss: 0.102837\n[Training] Epoch 013 | Batch 070/204 | Current Loss: 0.102255\n[Training] Epoch 013 | Batch 080/204 | Current Loss: 0.102094\n[Training] Epoch 013 | Batch 090/204 | Current Loss: 0.101890\n[Training] Epoch 013 | Batch 100/204 | Current Loss: 0.102182\n[Training] Epoch 013 | Batch 110/204 | Current Loss: 0.102267\n[Training] Epoch 013 | Batch 120/204 | Current Loss: 0.101686\n[Training] Epoch 013 | Batch 130/204 | Current Loss: 0.102092\n[Training] Epoch 013 | Batch 140/204 | Current Loss: 0.101947\n[Training] Epoch 013 | Batch 150/204 | Current Loss: 0.102123\n[Training] Epoch 013 | Batch 160/204 | Current Loss: 0.101774\n[Training] Epoch 013 | Batch 170/204 | Current Loss: 0.102043\n[Training] Epoch 013 | Batch 180/204 | Current Loss: 0.102197\n[Training] Epoch 013 | Batch 190/204 | Current Loss: 0.101857\n[Training] Epoch 013 | Batch 200/204 | Current Loss: 0.102129\n\n[Training] Epoch 013 Summary:\n  Avg Loss: 0.102239\n  Current LR: 4.20e-05\n[EarlyStopping] Loss improved (0.102700 → 0.102239). Saving model...\n[Training] Epoch 014 | Batch 000/204 | Current Loss: 0.102748\n[Training] Epoch 014 | Batch 010/204 | Current Loss: 0.101876\n[Training] Epoch 014 | Batch 020/204 | Current Loss: 0.102228\n[Training] Epoch 014 | Batch 030/204 | Current Loss: 0.101853\n[Training] Epoch 014 | Batch 040/204 | Current Loss: 0.101808\n[Training] Epoch 014 | Batch 050/204 | Current Loss: 0.102067\n[Training] Epoch 014 | Batch 060/204 | Current Loss: 0.102134\n[Training] Epoch 014 | Batch 070/204 | Current Loss: 0.102156\n[Training] Epoch 014 | Batch 080/204 | Current Loss: 0.101742\n[Training] Epoch 014 | Batch 090/204 | Current Loss: 0.102462\n[Training] Epoch 014 | Batch 100/204 | Current Loss: 0.101997\n[Training] Epoch 014 | Batch 110/204 | Current Loss: 0.102212\n[Training] Epoch 014 | Batch 120/204 | Current Loss: 0.101829\n[Training] Epoch 014 | Batch 130/204 | Current Loss: 0.101905\n[Training] Epoch 014 | Batch 140/204 | Current Loss: 0.102247\n[Training] Epoch 014 | Batch 150/204 | Current Loss: 0.102085\n[Training] Epoch 014 | Batch 160/204 | Current Loss: 0.101897\n[Training] Epoch 014 | Batch 170/204 | Current Loss: 0.102146\n[Training] Epoch 014 | Batch 180/204 | Current Loss: 0.102772\n[Training] Epoch 014 | Batch 190/204 | Current Loss: 0.102133\n[Training] Epoch 014 | Batch 200/204 | Current Loss: 0.101993\n\n[Training] Epoch 014 Summary:\n  Avg Loss: 0.102008\n  Current LR: 3.48e-05\n[EarlyStopping] Loss improved (0.102239 → 0.102008). Saving model...\n[Training] Epoch 015 | Batch 000/204 | Current Loss: 0.101965\n[Training] Epoch 015 | Batch 010/204 | Current Loss: 0.101857\n[Training] Epoch 015 | Batch 020/204 | Current Loss: 0.101850\n[Training] Epoch 015 | Batch 030/204 | Current Loss: 0.101616\n[Training] Epoch 015 | Batch 040/204 | Current Loss: 0.101771\n[Training] Epoch 015 | Batch 050/204 | Current Loss: 0.101531\n[Training] Epoch 015 | Batch 060/204 | Current Loss: 0.101689\n[Training] Epoch 015 | Batch 070/204 | Current Loss: 0.101677\n[Training] Epoch 015 | Batch 080/204 | Current Loss: 0.101846\n[Training] Epoch 015 | Batch 090/204 | Current Loss: 0.101698\n[Training] Epoch 015 | Batch 100/204 | Current Loss: 0.101766\n[Training] Epoch 015 | Batch 110/204 | Current Loss: 0.101820\n[Training] Epoch 015 | Batch 120/204 | Current Loss: 0.101629\n[Training] Epoch 015 | Batch 130/204 | Current Loss: 0.101677\n[Training] Epoch 015 | Batch 140/204 | Current Loss: 0.101791\n[Training] Epoch 015 | Batch 150/204 | Current Loss: 0.101885\n[Training] Epoch 015 | Batch 160/204 | Current Loss: 0.101656\n[Training] Epoch 015 | Batch 170/204 | Current Loss: 0.101571\n[Training] Epoch 015 | Batch 180/204 | Current Loss: 0.101430\n[Training] Epoch 015 | Batch 190/204 | Current Loss: 0.101603\n[Training] Epoch 015 | Batch 200/204 | Current Loss: 0.101743\n\n[Training] Epoch 015 Summary:\n  Avg Loss: 0.101794\n  Current LR: 2.68e-05\n[EarlyStopping] Loss improved (0.102008 → 0.101794). Saving model...\n[Training] Epoch 016 | Batch 000/204 | Current Loss: 0.101896\n[Training] Epoch 016 | Batch 010/204 | Current Loss: 0.101717\n[Training] Epoch 016 | Batch 020/204 | Current Loss: 0.101708\n[Training] Epoch 016 | Batch 030/204 | Current Loss: 0.101668\n[Training] Epoch 016 | Batch 040/204 | Current Loss: 0.101723\n[Training] Epoch 016 | Batch 050/204 | Current Loss: 0.101551\n[Training] Epoch 016 | Batch 060/204 | Current Loss: 0.101602\n[Training] Epoch 016 | Batch 070/204 | Current Loss: 0.101497\n[Training] Epoch 016 | Batch 080/204 | Current Loss: 0.101713\n[Training] Epoch 016 | Batch 090/204 | Current Loss: 0.101497\n[Training] Epoch 016 | Batch 100/204 | Current Loss: 0.101628\n[Training] Epoch 016 | Batch 110/204 | Current Loss: 0.101385\n[Training] Epoch 016 | Batch 120/204 | Current Loss: 0.101570\n[Training] Epoch 016 | Batch 130/204 | Current Loss: 0.101493\n[Training] Epoch 016 | Batch 140/204 | Current Loss: 0.101605\n[Training] Epoch 016 | Batch 150/204 | Current Loss: 0.101669\n[Training] Epoch 016 | Batch 160/204 | Current Loss: 0.101379\n[Training] Epoch 016 | Batch 170/204 | Current Loss: 0.101746\n[Training] Epoch 016 | Batch 180/204 | Current Loss: 0.101927\n[Training] Epoch 016 | Batch 190/204 | Current Loss: 0.101647\n[Training] Epoch 016 | Batch 200/204 | Current Loss: 0.101708\n\n[Training] Epoch 016 Summary:\n  Avg Loss: 0.101642\n  Current LR: 1.88e-05\n[EarlyStopping] Loss improved (0.101794 → 0.101642). Saving model...\n[Training] Epoch 017 | Batch 000/204 | Current Loss: 0.101641\n[Training] Epoch 017 | Batch 010/204 | Current Loss: 0.101534\n[Training] Epoch 017 | Batch 020/204 | Current Loss: 0.101616\n[Training] Epoch 017 | Batch 030/204 | Current Loss: 0.101533\n[Training] Epoch 017 | Batch 040/204 | Current Loss: 0.101468\n[Training] Epoch 017 | Batch 050/204 | Current Loss: 0.101872\n[Training] Epoch 017 | Batch 060/204 | Current Loss: 0.101343\n[Training] Epoch 017 | Batch 070/204 | Current Loss: 0.101808\n[Training] Epoch 017 | Batch 080/204 | Current Loss: 0.101764\n[Training] Epoch 017 | Batch 090/204 | Current Loss: 0.101797\n[Training] Epoch 017 | Batch 100/204 | Current Loss: 0.101508\n[Training] Epoch 017 | Batch 110/204 | Current Loss: 0.102018\n[Training] Epoch 017 | Batch 120/204 | Current Loss: 0.101917\n[Training] Epoch 017 | Batch 130/204 | Current Loss: 0.101695\n[Training] Epoch 017 | Batch 140/204 | Current Loss: 0.101505\n[Training] Epoch 017 | Batch 150/204 | Current Loss: 0.101389\n[Training] Epoch 017 | Batch 160/204 | Current Loss: 0.101864\n[Training] Epoch 017 | Batch 170/204 | Current Loss: 0.101641\n[Training] Epoch 017 | Batch 180/204 | Current Loss: 0.101590\n[Training] Epoch 017 | Batch 190/204 | Current Loss: 0.101523\n[Training] Epoch 017 | Batch 200/204 | Current Loss: 0.101459\n\n[Training] Epoch 017 Summary:\n  Avg Loss: 0.101590\n  Current LR: 1.16e-05\n[EarlyStopping] Loss improved (0.101642 → 0.101590). Saving model...\n[Training] Epoch 018 | Batch 000/204 | Current Loss: 0.101412\n[Training] Epoch 018 | Batch 010/204 | Current Loss: 0.101501\n[Training] Epoch 018 | Batch 020/204 | Current Loss: 0.101436\n[Training] Epoch 018 | Batch 030/204 | Current Loss: 0.101348\n[Training] Epoch 018 | Batch 040/204 | Current Loss: 0.101781\n[Training] Epoch 018 | Batch 050/204 | Current Loss: 0.101349\n[Training] Epoch 018 | Batch 060/204 | Current Loss: 0.101541\n[Training] Epoch 018 | Batch 070/204 | Current Loss: 0.101429\n[Training] Epoch 018 | Batch 080/204 | Current Loss: 0.101486\n[Training] Epoch 018 | Batch 090/204 | Current Loss: 0.101472\n[Training] Epoch 018 | Batch 100/204 | Current Loss: 0.101215\n[Training] Epoch 018 | Batch 110/204 | Current Loss: 0.101637\n[Training] Epoch 018 | Batch 120/204 | Current Loss: 0.101303\n[Training] Epoch 018 | Batch 130/204 | Current Loss: 0.101426\n[Training] Epoch 018 | Batch 140/204 | Current Loss: 0.101265\n[Training] Epoch 018 | Batch 150/204 | Current Loss: 0.101380\n[Training] Epoch 018 | Batch 160/204 | Current Loss: 0.101605\n[Training] Epoch 018 | Batch 170/204 | Current Loss: 0.102291\n[Training] Epoch 018 | Batch 180/204 | Current Loss: 0.101459\n[Training] Epoch 018 | Batch 190/204 | Current Loss: 0.101463\n[Training] Epoch 018 | Batch 200/204 | Current Loss: 0.101377\n\n[Training] Epoch 018 Summary:\n  Avg Loss: 0.101491\n  Current LR: 5.93e-06\n[EarlyStopping] Loss improved (0.101590 → 0.101491). Saving model...\n[Training] Epoch 019 | Batch 000/204 | Current Loss: 0.101685\n[Training] Epoch 019 | Batch 010/204 | Current Loss: 0.101305\n[Training] Epoch 019 | Batch 020/204 | Current Loss: 0.101417\n[Training] Epoch 019 | Batch 030/204 | Current Loss: 0.101207\n[Training] Epoch 019 | Batch 040/204 | Current Loss: 0.101408\n[Training] Epoch 019 | Batch 050/204 | Current Loss: 0.101442\n[Training] Epoch 019 | Batch 060/204 | Current Loss: 0.101342\n[Training] Epoch 019 | Batch 070/204 | Current Loss: 0.101429\n[Training] Epoch 019 | Batch 080/204 | Current Loss: 0.101616\n[Training] Epoch 019 | Batch 090/204 | Current Loss: 0.101502\n[Training] Epoch 019 | Batch 100/204 | Current Loss: 0.101652\n[Training] Epoch 019 | Batch 110/204 | Current Loss: 0.101442\n[Training] Epoch 019 | Batch 120/204 | Current Loss: 0.101591\n[Training] Epoch 019 | Batch 130/204 | Current Loss: 0.101452\n[Training] Epoch 019 | Batch 140/204 | Current Loss: 0.101347\n[Training] Epoch 019 | Batch 150/204 | Current Loss: 0.101274\n[Training] Epoch 019 | Batch 160/204 | Current Loss: 0.101459\n[Training] Epoch 019 | Batch 170/204 | Current Loss: 0.101224\n[Training] Epoch 019 | Batch 180/204 | Current Loss: 0.101379\n[Training] Epoch 019 | Batch 190/204 | Current Loss: 0.101237\n[Training] Epoch 019 | Batch 200/204 | Current Loss: 0.101290\n\n[Training] Epoch 019 Summary:\n  Avg Loss: 0.101454\n  Current LR: 2.26e-06\n[EarlyStopping] Loss improved (0.101491 → 0.101454). Saving model...\n[Training] Epoch 020 | Batch 000/204 | Current Loss: 0.101462\n[Training] Epoch 020 | Batch 010/204 | Current Loss: 0.101728\n[Training] Epoch 020 | Batch 020/204 | Current Loss: 0.101348\n[Training] Epoch 020 | Batch 030/204 | Current Loss: 0.101241\n[Training] Epoch 020 | Batch 040/204 | Current Loss: 0.101397\n[Training] Epoch 020 | Batch 050/204 | Current Loss: 0.101347\n[Training] Epoch 020 | Batch 060/204 | Current Loss: 0.101321\n[Training] Epoch 020 | Batch 070/204 | Current Loss: 0.101337\n[Training] Epoch 020 | Batch 080/204 | Current Loss: 0.101404\n[Training] Epoch 020 | Batch 090/204 | Current Loss: 0.101454\n[Training] Epoch 020 | Batch 100/204 | Current Loss: 0.101346\n[Training] Epoch 020 | Batch 110/204 | Current Loss: 0.101273\n[Training] Epoch 020 | Batch 120/204 | Current Loss: 0.101433\n[Training] Epoch 020 | Batch 130/204 | Current Loss: 0.101218\n[Training] Epoch 020 | Batch 140/204 | Current Loss: 0.101314\n[Training] Epoch 020 | Batch 150/204 | Current Loss: 0.101419\n[Training] Epoch 020 | Batch 160/204 | Current Loss: 0.101450\n[Training] Epoch 020 | Batch 170/204 | Current Loss: 0.101455\n[Training] Epoch 020 | Batch 180/204 | Current Loss: 0.101528\n[Training] Epoch 020 | Batch 190/204 | Current Loss: 0.101490\n[Training] Epoch 020 | Batch 200/204 | Current Loss: 0.101439\n\n[Training] Epoch 020 Summary:\n  Avg Loss: 0.101415\n  Current LR: 5.26e-05\n[EarlyStopping] Loss improved (0.101454 → 0.101415). Saving model...\n[Training] Epoch 021 | Batch 000/204 | Current Loss: 0.101972\n[Training] Epoch 021 | Batch 010/204 | Current Loss: 0.101354\n[Training] Epoch 021 | Batch 020/204 | Current Loss: 0.101474\n[Training] Epoch 021 | Batch 030/204 | Current Loss: 0.101284\n[Training] Epoch 021 | Batch 040/204 | Current Loss: 0.101429\n[Training] Epoch 021 | Batch 050/204 | Current Loss: 0.101536\n[Training] Epoch 021 | Batch 060/204 | Current Loss: 0.101337\n[Training] Epoch 021 | Batch 070/204 | Current Loss: 0.101370\n[Training] Epoch 021 | Batch 080/204 | Current Loss: 0.101318\n[Training] Epoch 021 | Batch 090/204 | Current Loss: 0.101907\n[Training] Epoch 021 | Batch 100/204 | Current Loss: 0.101222\n[Training] Epoch 021 | Batch 110/204 | Current Loss: 0.101512\n[Training] Epoch 021 | Batch 120/204 | Current Loss: 0.101254\n[Training] Epoch 021 | Batch 130/204 | Current Loss: 0.101274\n[Training] Epoch 021 | Batch 140/204 | Current Loss: 0.101103\n[Training] Epoch 021 | Batch 150/204 | Current Loss: 0.101772\n[Training] Epoch 021 | Batch 160/204 | Current Loss: 0.101388\n[Training] Epoch 021 | Batch 170/204 | Current Loss: 0.101212\n[Training] Epoch 021 | Batch 180/204 | Current Loss: 0.101292\n[Training] Epoch 021 | Batch 190/204 | Current Loss: 0.101211\n[Training] Epoch 021 | Batch 200/204 | Current Loss: 0.101318\n\n[Training] Epoch 021 Summary:\n  Avg Loss: 0.101411\n  Current LR: 5.13e-05\n[EarlyStopping] Loss improved (0.101415 → 0.101411). Saving model...\n[Training] Epoch 022 | Batch 000/204 | Current Loss: 0.101371\n[Training] Epoch 022 | Batch 010/204 | Current Loss: 0.101237\n[Training] Epoch 022 | Batch 020/204 | Current Loss: 0.101824\n[Training] Epoch 022 | Batch 030/204 | Current Loss: 0.101212\n[Training] Epoch 022 | Batch 040/204 | Current Loss: 0.101239\n[Training] Epoch 022 | Batch 050/204 | Current Loss: 0.101124\n[Training] Epoch 022 | Batch 060/204 | Current Loss: 0.101128\n[Training] Epoch 022 | Batch 070/204 | Current Loss: 0.101184\n[Training] Epoch 022 | Batch 080/204 | Current Loss: 0.101343\n[Training] Epoch 022 | Batch 090/204 | Current Loss: 0.101108\n[Training] Epoch 022 | Batch 100/204 | Current Loss: 0.101325\n[Training] Epoch 022 | Batch 110/204 | Current Loss: 0.101235\n[Training] Epoch 022 | Batch 120/204 | Current Loss: 0.101065\n[Training] Epoch 022 | Batch 130/204 | Current Loss: 0.101036\n[Training] Epoch 022 | Batch 140/204 | Current Loss: 0.101109\n[Training] Epoch 022 | Batch 150/204 | Current Loss: 0.100920\n[Training] Epoch 022 | Batch 160/204 | Current Loss: 0.101273\n[Training] Epoch 022 | Batch 170/204 | Current Loss: 0.101453\n[Training] Epoch 022 | Batch 180/204 | Current Loss: 0.101374\n[Training] Epoch 022 | Batch 190/204 | Current Loss: 0.101112\n[Training] Epoch 022 | Batch 200/204 | Current Loss: 0.101332\n\n[Training] Epoch 022 Summary:\n  Avg Loss: 0.101243\n  Current LR: 4.77e-05\n[EarlyStopping] Loss improved (0.101411 → 0.101243). Saving model...\n[Training] Epoch 023 | Batch 000/204 | Current Loss: 0.101175\n[Training] Epoch 023 | Batch 010/204 | Current Loss: 0.101131\n[Training] Epoch 023 | Batch 020/204 | Current Loss: 0.101195\n[Training] Epoch 023 | Batch 030/204 | Current Loss: 0.101119\n[Training] Epoch 023 | Batch 040/204 | Current Loss: 0.101185\n[Training] Epoch 023 | Batch 050/204 | Current Loss: 0.100943\n[Training] Epoch 023 | Batch 060/204 | Current Loss: 0.101057\n[Training] Epoch 023 | Batch 070/204 | Current Loss: 0.100976\n[Training] Epoch 023 | Batch 080/204 | Current Loss: 0.101004\n[Training] Epoch 023 | Batch 090/204 | Current Loss: 0.101025\n[Training] Epoch 023 | Batch 100/204 | Current Loss: 0.100982\n[Training] Epoch 023 | Batch 110/204 | Current Loss: 0.100937\n[Training] Epoch 023 | Batch 120/204 | Current Loss: 0.101054\n[Training] Epoch 023 | Batch 130/204 | Current Loss: 0.100904\n[Training] Epoch 023 | Batch 140/204 | Current Loss: 0.100952\n[Training] Epoch 023 | Batch 150/204 | Current Loss: 0.100886\n[Training] Epoch 023 | Batch 160/204 | Current Loss: 0.101104\n[Training] Epoch 023 | Batch 170/204 | Current Loss: 0.101202\n[Training] Epoch 023 | Batch 180/204 | Current Loss: 0.100838\n[Training] Epoch 023 | Batch 190/204 | Current Loss: 0.101058\n[Training] Epoch 023 | Batch 200/204 | Current Loss: 0.100967\n\n[Training] Epoch 023 Summary:\n  Avg Loss: 0.101105\n  Current LR: 4.20e-05\n[EarlyStopping] Loss improved (0.101243 → 0.101105). Saving model...\n[Training] Epoch 024 | Batch 000/204 | Current Loss: 0.101008\n[Training] Epoch 024 | Batch 010/204 | Current Loss: 0.100999\n[Training] Epoch 024 | Batch 020/204 | Current Loss: 0.101194\n[Training] Epoch 024 | Batch 030/204 | Current Loss: 0.100898\n[Training] Epoch 024 | Batch 040/204 | Current Loss: 0.101019\n[Training] Epoch 024 | Batch 050/204 | Current Loss: 0.101127\n[Training] Epoch 024 | Batch 060/204 | Current Loss: 0.100892\n[Training] Epoch 024 | Batch 070/204 | Current Loss: 0.101121\n[Training] Epoch 024 | Batch 080/204 | Current Loss: 0.100919\n[Training] Epoch 024 | Batch 090/204 | Current Loss: 0.101011\n[Training] Epoch 024 | Batch 100/204 | Current Loss: 0.100996\n[Training] Epoch 024 | Batch 110/204 | Current Loss: 0.101130\n[Training] Epoch 024 | Batch 120/204 | Current Loss: 0.100964\n[Training] Epoch 024 | Batch 130/204 | Current Loss: 0.100998\n[Training] Epoch 024 | Batch 140/204 | Current Loss: 0.100899\n[Training] Epoch 024 | Batch 150/204 | Current Loss: 0.100977\n[Training] Epoch 024 | Batch 160/204 | Current Loss: 0.100909\n[Training] Epoch 024 | Batch 170/204 | Current Loss: 0.100856\n[Training] Epoch 024 | Batch 180/204 | Current Loss: 0.101138\n[Training] Epoch 024 | Batch 190/204 | Current Loss: 0.101169\n[Training] Epoch 024 | Batch 200/204 | Current Loss: 0.100904\n\n[Training] Epoch 024 Summary:\n  Avg Loss: 0.101033\n  Current LR: 3.48e-05\n[EarlyStopping] Loss improved (0.101105 → 0.101033). Saving model...\n[Training] Epoch 025 | Batch 000/204 | Current Loss: 0.100998\n[Training] Epoch 025 | Batch 010/204 | Current Loss: 0.101067\n[Training] Epoch 025 | Batch 020/204 | Current Loss: 0.100890\n[Training] Epoch 025 | Batch 030/204 | Current Loss: 0.100843\n[Training] Epoch 025 | Batch 040/204 | Current Loss: 0.100814\n[Training] Epoch 025 | Batch 050/204 | Current Loss: 0.100806\n[Training] Epoch 025 | Batch 060/204 | Current Loss: 0.100954\n[Training] Epoch 025 | Batch 070/204 | Current Loss: 0.100807\n[Training] Epoch 025 | Batch 080/204 | Current Loss: 0.100940\n[Training] Epoch 025 | Batch 090/204 | Current Loss: 0.100938\n[Training] Epoch 025 | Batch 100/204 | Current Loss: 0.100861\n[Training] Epoch 025 | Batch 110/204 | Current Loss: 0.100989\n[Training] Epoch 025 | Batch 120/204 | Current Loss: 0.100935\n[Training] Epoch 025 | Batch 130/204 | Current Loss: 0.100909\n[Training] Epoch 025 | Batch 140/204 | Current Loss: 0.100771\n[Training] Epoch 025 | Batch 150/204 | Current Loss: 0.100849\n[Training] Epoch 025 | Batch 160/204 | Current Loss: 0.101087\n[Training] Epoch 025 | Batch 170/204 | Current Loss: 0.100905\n[Training] Epoch 025 | Batch 180/204 | Current Loss: 0.100959\n[Training] Epoch 025 | Batch 190/204 | Current Loss: 0.100817\n[Training] Epoch 025 | Batch 200/204 | Current Loss: 0.100823\n\n[Training] Epoch 025 Summary:\n  Avg Loss: 0.100943\n  Current LR: 2.68e-05\n[EarlyStopping] Loss improved (0.101033 → 0.100943). Saving model...\n[Training] Epoch 026 | Batch 000/204 | Current Loss: 0.100952\n[Training] Epoch 026 | Batch 010/204 | Current Loss: 0.100791\n[Training] Epoch 026 | Batch 020/204 | Current Loss: 0.100823\n[Training] Epoch 026 | Batch 030/204 | Current Loss: 0.100838\n[Training] Epoch 026 | Batch 040/204 | Current Loss: 0.100969\n[Training] Epoch 026 | Batch 050/204 | Current Loss: 0.100786\n[Training] Epoch 026 | Batch 060/204 | Current Loss: 0.100689\n[Training] Epoch 026 | Batch 070/204 | Current Loss: 0.100796\n[Training] Epoch 026 | Batch 080/204 | Current Loss: 0.100847\n[Training] Epoch 026 | Batch 090/204 | Current Loss: 0.100881\n[Training] Epoch 026 | Batch 100/204 | Current Loss: 0.100859\n[Training] Epoch 026 | Batch 110/204 | Current Loss: 0.100730\n[Training] Epoch 026 | Batch 120/204 | Current Loss: 0.100881\n[Training] Epoch 026 | Batch 130/204 | Current Loss: 0.100934\n[Training] Epoch 026 | Batch 140/204 | Current Loss: 0.101060\n[Training] Epoch 026 | Batch 150/204 | Current Loss: 0.100908\n[Training] Epoch 026 | Batch 160/204 | Current Loss: 0.100845\n[Training] Epoch 026 | Batch 170/204 | Current Loss: 0.100906\n[Training] Epoch 026 | Batch 180/204 | Current Loss: 0.100851\n[Training] Epoch 026 | Batch 190/204 | Current Loss: 0.100840\n[Training] Epoch 026 | Batch 200/204 | Current Loss: 0.100741\n\n[Training] Epoch 026 Summary:\n  Avg Loss: 0.100866\n  Current LR: 1.88e-05\n[EarlyStopping] Loss improved (0.100943 → 0.100866). Saving model...\n[Training] Epoch 027 | Batch 000/204 | Current Loss: 0.100836\n[Training] Epoch 027 | Batch 010/204 | Current Loss: 0.100891\n[Training] Epoch 027 | Batch 020/204 | Current Loss: 0.100886\n[Training] Epoch 027 | Batch 030/204 | Current Loss: 0.100716\n[Training] Epoch 027 | Batch 040/204 | Current Loss: 0.100881\n[Training] Epoch 027 | Batch 050/204 | Current Loss: 0.100668\n[Training] Epoch 027 | Batch 060/204 | Current Loss: 0.100807\n[Training] Epoch 027 | Batch 070/204 | Current Loss: 0.100792\n[Training] Epoch 027 | Batch 080/204 | Current Loss: 0.100721\n[Training] Epoch 027 | Batch 090/204 | Current Loss: 0.100726\n[Training] Epoch 027 | Batch 100/204 | Current Loss: 0.100876\n[Training] Epoch 027 | Batch 110/204 | Current Loss: 0.100818\n[Training] Epoch 027 | Batch 120/204 | Current Loss: 0.100830\n[Training] Epoch 027 | Batch 130/204 | Current Loss: 0.100760\n[Training] Epoch 027 | Batch 140/204 | Current Loss: 0.100810\n[Training] Epoch 027 | Batch 150/204 | Current Loss: 0.100894\n[Training] Epoch 027 | Batch 160/204 | Current Loss: 0.100914\n[Training] Epoch 027 | Batch 170/204 | Current Loss: 0.100731\n[Training] Epoch 027 | Batch 180/204 | Current Loss: 0.100831\n[Training] Epoch 027 | Batch 190/204 | Current Loss: 0.100789\n[Training] Epoch 027 | Batch 200/204 | Current Loss: 0.100800\n\n[Training] Epoch 027 Summary:\n  Avg Loss: 0.100818\n  Current LR: 1.16e-05\n[EarlyStopping] Loss improved (0.100866 → 0.100818). Saving model...\n[Training] Epoch 028 | Batch 000/204 | Current Loss: 0.100894\n[Training] Epoch 028 | Batch 010/204 | Current Loss: 0.100677\n[Training] Epoch 028 | Batch 020/204 | Current Loss: 0.100810\n[Training] Epoch 028 | Batch 030/204 | Current Loss: 0.100722\n[Training] Epoch 028 | Batch 040/204 | Current Loss: 0.100880\n[Training] Epoch 028 | Batch 050/204 | Current Loss: 0.100910\n[Training] Epoch 028 | Batch 060/204 | Current Loss: 0.100673\n[Training] Epoch 028 | Batch 070/204 | Current Loss: 0.100822\n[Training] Epoch 028 | Batch 080/204 | Current Loss: 0.100947\n[Training] Epoch 028 | Batch 090/204 | Current Loss: 0.100726\n[Training] Epoch 028 | Batch 100/204 | Current Loss: 0.100866\n[Training] Epoch 028 | Batch 110/204 | Current Loss: 0.100781\n[Training] Epoch 028 | Batch 120/204 | Current Loss: 0.100664\n[Training] Epoch 028 | Batch 130/204 | Current Loss: 0.100838\n[Training] Epoch 028 | Batch 140/204 | Current Loss: 0.100918\n[Training] Epoch 028 | Batch 150/204 | Current Loss: 0.100668\n[Training] Epoch 028 | Batch 160/204 | Current Loss: 0.100660\n[Training] Epoch 028 | Batch 170/204 | Current Loss: 0.100857\n[Training] Epoch 028 | Batch 180/204 | Current Loss: 0.100785\n[Training] Epoch 028 | Batch 190/204 | Current Loss: 0.100866\n[Training] Epoch 028 | Batch 200/204 | Current Loss: 0.100767\n\n[Training] Epoch 028 Summary:\n  Avg Loss: 0.100783\n  Current LR: 5.93e-06\n[EarlyStopping] Loss improved (0.100818 → 0.100783). Saving model...\n[Training] Epoch 029 | Batch 000/204 | Current Loss: 0.100648\n[Training] Epoch 029 | Batch 010/204 | Current Loss: 0.100765\n[Training] Epoch 029 | Batch 020/204 | Current Loss: 0.100796\n[Training] Epoch 029 | Batch 030/204 | Current Loss: 0.100853\n[Training] Epoch 029 | Batch 040/204 | Current Loss: 0.101088\n[Training] Epoch 029 | Batch 050/204 | Current Loss: 0.100736\n[Training] Epoch 029 | Batch 060/204 | Current Loss: 0.100715\n[Training] Epoch 029 | Batch 070/204 | Current Loss: 0.100600\n[Training] Epoch 029 | Batch 080/204 | Current Loss: 0.100704\n[Training] Epoch 029 | Batch 090/204 | Current Loss: 0.100631\n[Training] Epoch 029 | Batch 100/204 | Current Loss: 0.100722\n[Training] Epoch 029 | Batch 110/204 | Current Loss: 0.100781\n[Training] Epoch 029 | Batch 120/204 | Current Loss: 0.100708\n[Training] Epoch 029 | Batch 130/204 | Current Loss: 0.100701\n[Training] Epoch 029 | Batch 140/204 | Current Loss: 0.100771\n[Training] Epoch 029 | Batch 150/204 | Current Loss: 0.100709\n[Training] Epoch 029 | Batch 160/204 | Current Loss: 0.100803\n[Training] Epoch 029 | Batch 170/204 | Current Loss: 0.100858\n[Training] Epoch 029 | Batch 180/204 | Current Loss: 0.100700\n[Training] Epoch 029 | Batch 190/204 | Current Loss: 0.100759\n[Training] Epoch 029 | Batch 200/204 | Current Loss: 0.100789\n\n[Training] Epoch 029 Summary:\n  Avg Loss: 0.100773\n  Current LR: 2.26e-06\n[EarlyStopping] Loss improved (0.100783 → 0.100773). Saving model...\n[Training] Epoch 030 | Batch 000/204 | Current Loss: 0.100780\n[Training] Epoch 030 | Batch 010/204 | Current Loss: 0.100739\n[Training] Epoch 030 | Batch 020/204 | Current Loss: 0.100742\n[Training] Epoch 030 | Batch 030/204 | Current Loss: 0.100783\n[Training] Epoch 030 | Batch 040/204 | Current Loss: 0.100753\n[Training] Epoch 030 | Batch 050/204 | Current Loss: 0.100891\n[Training] Epoch 030 | Batch 060/204 | Current Loss: 0.100886\n[Training] Epoch 030 | Batch 070/204 | Current Loss: 0.100658\n[Training] Epoch 030 | Batch 080/204 | Current Loss: 0.100753\n[Training] Epoch 030 | Batch 090/204 | Current Loss: 0.100689\n[Training] Epoch 030 | Batch 100/204 | Current Loss: 0.100709\n[Training] Epoch 030 | Batch 110/204 | Current Loss: 0.100703\n[Training] Epoch 030 | Batch 120/204 | Current Loss: 0.100720\n[Training] Epoch 030 | Batch 130/204 | Current Loss: 0.100970\n[Training] Epoch 030 | Batch 140/204 | Current Loss: 0.100683\n[Training] Epoch 030 | Batch 150/204 | Current Loss: 0.100687\n[Training] Epoch 030 | Batch 160/204 | Current Loss: 0.100627\n[Training] Epoch 030 | Batch 170/204 | Current Loss: 0.100817\n[Training] Epoch 030 | Batch 180/204 | Current Loss: 0.100719\n[Training] Epoch 030 | Batch 190/204 | Current Loss: 0.100832\n[Training] Epoch 030 | Batch 200/204 | Current Loss: 0.100691\n\n[Training] Epoch 030 Summary:\n  Avg Loss: 0.100755\n  Current LR: 5.26e-05\n[EarlyStopping] Loss improved (0.100773 → 0.100755). Saving model...\n[Training] Epoch 031 | Batch 000/204 | Current Loss: 0.100781\n[Training] Epoch 031 | Batch 010/204 | Current Loss: 0.100825\n[Training] Epoch 031 | Batch 020/204 | Current Loss: 0.100936\n[Training] Epoch 031 | Batch 030/204 | Current Loss: 0.100774\n[Training] Epoch 031 | Batch 040/204 | Current Loss: 0.100736\n[Training] Epoch 031 | Batch 050/204 | Current Loss: 0.100692\n[Training] Epoch 031 | Batch 060/204 | Current Loss: 0.100765\n[Training] Epoch 031 | Batch 070/204 | Current Loss: 0.100756\n[Training] Epoch 031 | Batch 080/204 | Current Loss: 0.100757\n[Training] Epoch 031 | Batch 090/204 | Current Loss: 0.100803\n[Training] Epoch 031 | Batch 100/204 | Current Loss: 0.100612\n[Training] Epoch 031 | Batch 110/204 | Current Loss: 0.100676\n[Training] Epoch 031 | Batch 120/204 | Current Loss: 0.100648\n[Training] Epoch 031 | Batch 130/204 | Current Loss: 0.100839\n[Training] Epoch 031 | Batch 140/204 | Current Loss: 0.100631\n[Training] Epoch 031 | Batch 150/204 | Current Loss: 0.100865\n[Training] Epoch 031 | Batch 160/204 | Current Loss: 0.100645\n[Training] Epoch 031 | Batch 170/204 | Current Loss: 0.100730\n[Training] Epoch 031 | Batch 180/204 | Current Loss: 0.100801\n[Training] Epoch 031 | Batch 190/204 | Current Loss: 0.100879\n[Training] Epoch 031 | Batch 200/204 | Current Loss: 0.100609\n\n[Training] Epoch 031 Summary:\n  Avg Loss: 0.100753\n  Current LR: 5.13e-05\n[EarlyStopping] Loss improved (0.100755 → 0.100753). Saving model...\n[Training] Epoch 032 | Batch 000/204 | Current Loss: 0.100875\n[Training] Epoch 032 | Batch 010/204 | Current Loss: 0.100641\n[Training] Epoch 032 | Batch 020/204 | Current Loss: 0.100617\n[Training] Epoch 032 | Batch 030/204 | Current Loss: 0.100820\n[Training] Epoch 032 | Batch 040/204 | Current Loss: 0.100596\n[Training] Epoch 032 | Batch 050/204 | Current Loss: 0.100755\n[Training] Epoch 032 | Batch 060/204 | Current Loss: 0.100544\n[Training] Epoch 032 | Batch 070/204 | Current Loss: 0.100615\n[Training] Epoch 032 | Batch 080/204 | Current Loss: 0.100597\n[Training] Epoch 032 | Batch 090/204 | Current Loss: 0.100701\n[Training] Epoch 032 | Batch 100/204 | Current Loss: 0.100614\n[Training] Epoch 032 | Batch 110/204 | Current Loss: 0.100660\n[Training] Epoch 032 | Batch 120/204 | Current Loss: 0.100585\n[Training] Epoch 032 | Batch 130/204 | Current Loss: 0.100713\n[Training] Epoch 032 | Batch 140/204 | Current Loss: 0.100672\n[Training] Epoch 032 | Batch 150/204 | Current Loss: 0.100551\n[Training] Epoch 032 | Batch 160/204 | Current Loss: 0.100669\n[Training] Epoch 032 | Batch 170/204 | Current Loss: 0.100642\n[Training] Epoch 032 | Batch 180/204 | Current Loss: 0.100555\n[Training] Epoch 032 | Batch 190/204 | Current Loss: 0.100502\n[Training] Epoch 032 | Batch 200/204 | Current Loss: 0.100642\n\n[Training] Epoch 032 Summary:\n  Avg Loss: 0.100672\n  Current LR: 4.77e-05\n[EarlyStopping] Loss improved (0.100753 → 0.100672). Saving model...\n[Training] Epoch 033 | Batch 000/204 | Current Loss: 0.100575\n[Training] Epoch 033 | Batch 010/204 | Current Loss: 0.100540\n[Training] Epoch 033 | Batch 020/204 | Current Loss: 0.100745\n[Training] Epoch 033 | Batch 030/204 | Current Loss: 0.100562\n[Training] Epoch 033 | Batch 040/204 | Current Loss: 0.100446\n[Training] Epoch 033 | Batch 050/204 | Current Loss: 0.100618\n[Training] Epoch 033 | Batch 060/204 | Current Loss: 0.100753\n[Training] Epoch 033 | Batch 070/204 | Current Loss: 0.100593\n[Training] Epoch 033 | Batch 080/204 | Current Loss: 0.100741\n[Training] Epoch 033 | Batch 090/204 | Current Loss: 0.100621\n[Training] Epoch 033 | Batch 100/204 | Current Loss: 0.100655\n[Training] Epoch 033 | Batch 110/204 | Current Loss: 0.100766\n[Training] Epoch 033 | Batch 120/204 | Current Loss: 0.100734\n[Training] Epoch 033 | Batch 130/204 | Current Loss: 0.100574\n[Training] Epoch 033 | Batch 140/204 | Current Loss: 0.100667\n[Training] Epoch 033 | Batch 150/204 | Current Loss: 0.100579\n[Training] Epoch 033 | Batch 160/204 | Current Loss: 0.100737\n[Training] Epoch 033 | Batch 170/204 | Current Loss: 0.100644\n[Training] Epoch 033 | Batch 180/204 | Current Loss: 0.100598\n[Training] Epoch 033 | Batch 190/204 | Current Loss: 0.100471\n[Training] Epoch 033 | Batch 200/204 | Current Loss: 0.100568\n\n[Training] Epoch 033 Summary:\n  Avg Loss: 0.100729\n  Current LR: 4.20e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 034 | Batch 000/204 | Current Loss: 0.101736\n[Training] Epoch 034 | Batch 010/204 | Current Loss: 0.101560\n[Training] Epoch 034 | Batch 020/204 | Current Loss: 0.100853\n[Training] Epoch 034 | Batch 030/204 | Current Loss: 0.100758\n[Training] Epoch 034 | Batch 040/204 | Current Loss: 0.100632\n[Training] Epoch 034 | Batch 050/204 | Current Loss: 0.100485\n[Training] Epoch 034 | Batch 060/204 | Current Loss: 0.100619\n[Training] Epoch 034 | Batch 070/204 | Current Loss: 0.100610\n[Training] Epoch 034 | Batch 080/204 | Current Loss: 0.100578\n[Training] Epoch 034 | Batch 090/204 | Current Loss: 0.100583\n[Training] Epoch 034 | Batch 100/204 | Current Loss: 0.100639\n[Training] Epoch 034 | Batch 110/204 | Current Loss: 0.100609\n[Training] Epoch 034 | Batch 120/204 | Current Loss: 0.100649\n[Training] Epoch 034 | Batch 130/204 | Current Loss: 0.100717\n[Training] Epoch 034 | Batch 140/204 | Current Loss: 0.100577\n[Training] Epoch 034 | Batch 150/204 | Current Loss: 0.100609\n[Training] Epoch 034 | Batch 160/204 | Current Loss: 0.100727\n[Training] Epoch 034 | Batch 170/204 | Current Loss: 0.100455\n[Training] Epoch 034 | Batch 180/204 | Current Loss: 0.100551\n[Training] Epoch 034 | Batch 190/204 | Current Loss: 0.100627\n[Training] Epoch 034 | Batch 200/204 | Current Loss: 0.100710\n\n[Training] Epoch 034 Summary:\n  Avg Loss: 0.100718\n  Current LR: 3.48e-05\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 035 | Batch 000/204 | Current Loss: 0.100568\n[Training] Epoch 035 | Batch 010/204 | Current Loss: 0.100511\n[Training] Epoch 035 | Batch 020/204 | Current Loss: 0.100499\n[Training] Epoch 035 | Batch 030/204 | Current Loss: 0.100593\n[Training] Epoch 035 | Batch 040/204 | Current Loss: 0.100564\n[Training] Epoch 035 | Batch 050/204 | Current Loss: 0.100504\n[Training] Epoch 035 | Batch 060/204 | Current Loss: 0.100472\n[Training] Epoch 035 | Batch 070/204 | Current Loss: 0.100690\n[Training] Epoch 035 | Batch 080/204 | Current Loss: 0.100572\n[Training] Epoch 035 | Batch 090/204 | Current Loss: 0.100618\n[Training] Epoch 035 | Batch 100/204 | Current Loss: 0.100497\n[Training] Epoch 035 | Batch 110/204 | Current Loss: 0.100560\n[Training] Epoch 035 | Batch 120/204 | Current Loss: 0.100476\n[Training] Epoch 035 | Batch 130/204 | Current Loss: 0.100586\n[Training] Epoch 035 | Batch 140/204 | Current Loss: 0.100488\n[Training] Epoch 035 | Batch 150/204 | Current Loss: 0.100514\n[Training] Epoch 035 | Batch 160/204 | Current Loss: 0.100425\n[Training] Epoch 035 | Batch 170/204 | Current Loss: 0.100452\n[Training] Epoch 035 | Batch 180/204 | Current Loss: 0.100436\n[Training] Epoch 035 | Batch 190/204 | Current Loss: 0.100490\n[Training] Epoch 035 | Batch 200/204 | Current Loss: 0.100536\n\n[Training] Epoch 035 Summary:\n  Avg Loss: 0.100548\n  Current LR: 2.68e-05\n[EarlyStopping] Loss improved (0.100672 → 0.100548). Saving model...\n[Training] Epoch 036 | Batch 000/204 | Current Loss: 0.100598\n[Training] Epoch 036 | Batch 010/204 | Current Loss: 0.100482\n[Training] Epoch 036 | Batch 020/204 | Current Loss: 0.100498\n[Training] Epoch 036 | Batch 030/204 | Current Loss: 0.100474\n[Training] Epoch 036 | Batch 040/204 | Current Loss: 0.100476\n[Training] Epoch 036 | Batch 050/204 | Current Loss: 0.100622\n[Training] Epoch 036 | Batch 060/204 | Current Loss: 0.100443\n[Training] Epoch 036 | Batch 070/204 | Current Loss: 0.100448\n[Training] Epoch 036 | Batch 080/204 | Current Loss: 0.100514\n[Training] Epoch 036 | Batch 090/204 | Current Loss: 0.100469\n[Training] Epoch 036 | Batch 100/204 | Current Loss: 0.100666\n[Training] Epoch 036 | Batch 110/204 | Current Loss: 0.100436\n[Training] Epoch 036 | Batch 120/204 | Current Loss: 0.100424\n[Training] Epoch 036 | Batch 130/204 | Current Loss: 0.100494\n[Training] Epoch 036 | Batch 140/204 | Current Loss: 0.100510\n[Training] Epoch 036 | Batch 150/204 | Current Loss: 0.100534\n[Training] Epoch 036 | Batch 160/204 | Current Loss: 0.100492\n[Training] Epoch 036 | Batch 170/204 | Current Loss: 0.100439\n[Training] Epoch 036 | Batch 180/204 | Current Loss: 0.100520\n[Training] Epoch 036 | Batch 190/204 | Current Loss: 0.100392\n[Training] Epoch 036 | Batch 200/204 | Current Loss: 0.100503\n\n[Training] Epoch 036 Summary:\n  Avg Loss: 0.100526\n  Current LR: 1.88e-05\n[EarlyStopping] Loss improved (0.100548 → 0.100526). Saving model...\n[Training] Epoch 037 | Batch 000/204 | Current Loss: 0.100549\n[Training] Epoch 037 | Batch 010/204 | Current Loss: 0.100376\n[Training] Epoch 037 | Batch 020/204 | Current Loss: 0.100547\n[Training] Epoch 037 | Batch 030/204 | Current Loss: 0.100566\n[Training] Epoch 037 | Batch 040/204 | Current Loss: 0.100538\n[Training] Epoch 037 | Batch 050/204 | Current Loss: 0.100522\n[Training] Epoch 037 | Batch 060/204 | Current Loss: 0.100595\n[Training] Epoch 037 | Batch 070/204 | Current Loss: 0.100633\n[Training] Epoch 037 | Batch 080/204 | Current Loss: 0.100470\n[Training] Epoch 037 | Batch 090/204 | Current Loss: 0.100461\n[Training] Epoch 037 | Batch 100/204 | Current Loss: 0.100486\n[Training] Epoch 037 | Batch 110/204 | Current Loss: 0.100458\n[Training] Epoch 037 | Batch 120/204 | Current Loss: 0.100402\n[Training] Epoch 037 | Batch 130/204 | Current Loss: 0.100546\n[Training] Epoch 037 | Batch 140/204 | Current Loss: 0.100447\n[Training] Epoch 037 | Batch 150/204 | Current Loss: 0.100402\n[Training] Epoch 037 | Batch 160/204 | Current Loss: 0.100507\n[Training] Epoch 037 | Batch 170/204 | Current Loss: 0.100518\n[Training] Epoch 037 | Batch 180/204 | Current Loss: 0.100545\n[Training] Epoch 037 | Batch 190/204 | Current Loss: 0.100533\n[Training] Epoch 037 | Batch 200/204 | Current Loss: 0.100417\n\n[Training] Epoch 037 Summary:\n  Avg Loss: 0.100498\n  Current LR: 1.16e-05\n[EarlyStopping] Loss improved (0.100526 → 0.100498). Saving model...\n[Training] Epoch 038 | Batch 000/204 | Current Loss: 0.100438\n[Training] Epoch 038 | Batch 010/204 | Current Loss: 0.100616\n[Training] Epoch 038 | Batch 020/204 | Current Loss: 0.100415\n[Training] Epoch 038 | Batch 030/204 | Current Loss: 0.100505\n[Training] Epoch 038 | Batch 040/204 | Current Loss: 0.100477\n[Training] Epoch 038 | Batch 050/204 | Current Loss: 0.100400\n[Training] Epoch 038 | Batch 060/204 | Current Loss: 0.100516\n[Training] Epoch 038 | Batch 070/204 | Current Loss: 0.100518\n[Training] Epoch 038 | Batch 080/204 | Current Loss: 0.100560\n[Training] Epoch 038 | Batch 090/204 | Current Loss: 0.100427\n[Training] Epoch 038 | Batch 100/204 | Current Loss: 0.100585\n[Training] Epoch 038 | Batch 110/204 | Current Loss: 0.100615\n[Training] Epoch 038 | Batch 120/204 | Current Loss: 0.100434\n[Training] Epoch 038 | Batch 130/204 | Current Loss: 0.100754\n[Training] Epoch 038 | Batch 140/204 | Current Loss: 0.100493\n[Training] Epoch 038 | Batch 150/204 | Current Loss: 0.100527\n[Training] Epoch 038 | Batch 160/204 | Current Loss: 0.100498\n[Training] Epoch 038 | Batch 170/204 | Current Loss: 0.100456\n[Training] Epoch 038 | Batch 180/204 | Current Loss: 0.100462\n[Training] Epoch 038 | Batch 190/204 | Current Loss: 0.100509\n[Training] Epoch 038 | Batch 200/204 | Current Loss: 0.100537\n\n[Training] Epoch 038 Summary:\n  Avg Loss: 0.100481\n  Current LR: 5.93e-06\n[EarlyStopping] Loss improved (0.100498 → 0.100481). Saving model...\n[Training] Epoch 039 | Batch 000/204 | Current Loss: 0.100457\n[Training] Epoch 039 | Batch 010/204 | Current Loss: 0.100611\n[Training] Epoch 039 | Batch 020/204 | Current Loss: 0.100458\n[Training] Epoch 039 | Batch 030/204 | Current Loss: 0.100490\n[Training] Epoch 039 | Batch 040/204 | Current Loss: 0.100495\n[Training] Epoch 039 | Batch 050/204 | Current Loss: 0.100386\n[Training] Epoch 039 | Batch 060/204 | Current Loss: 0.100415\n[Training] Epoch 039 | Batch 070/204 | Current Loss: 0.100331\n[Training] Epoch 039 | Batch 080/204 | Current Loss: 0.100512\n[Training] Epoch 039 | Batch 090/204 | Current Loss: 0.100383\n[Training] Epoch 039 | Batch 100/204 | Current Loss: 0.100577\n[Training] Epoch 039 | Batch 110/204 | Current Loss: 0.100523\n[Training] Epoch 039 | Batch 120/204 | Current Loss: 0.100467\n[Training] Epoch 039 | Batch 130/204 | Current Loss: 0.100452\n[Training] Epoch 039 | Batch 140/204 | Current Loss: 0.100424\n[Training] Epoch 039 | Batch 150/204 | Current Loss: 0.100506\n[Training] Epoch 039 | Batch 160/204 | Current Loss: 0.100416\n[Training] Epoch 039 | Batch 170/204 | Current Loss: 0.100316\n[Training] Epoch 039 | Batch 180/204 | Current Loss: 0.100448\n[Training] Epoch 039 | Batch 190/204 | Current Loss: 0.100366\n[Training] Epoch 039 | Batch 200/204 | Current Loss: 0.100399\n\n[Training] Epoch 039 Summary:\n  Avg Loss: 0.100467\n  Current LR: 2.26e-06\n[EarlyStopping] Loss improved (0.100481 → 0.100467). Saving model...\n[Training] Epoch 040 | Batch 000/204 | Current Loss: 0.100475\n[Training] Epoch 040 | Batch 010/204 | Current Loss: 0.100496\n[Training] Epoch 040 | Batch 020/204 | Current Loss: 0.100412\n[Training] Epoch 040 | Batch 030/204 | Current Loss: 0.100461\n[Training] Epoch 040 | Batch 040/204 | Current Loss: 0.100448\n[Training] Epoch 040 | Batch 050/204 | Current Loss: 0.100550\n[Training] Epoch 040 | Batch 060/204 | Current Loss: 0.100500\n[Training] Epoch 040 | Batch 070/204 | Current Loss: 0.100494\n[Training] Epoch 040 | Batch 080/204 | Current Loss: 0.100499\n[Training] Epoch 040 | Batch 090/204 | Current Loss: 0.100379\n[Training] Epoch 040 | Batch 100/204 | Current Loss: 0.100587\n[Training] Epoch 040 | Batch 110/204 | Current Loss: 0.100351\n[Training] Epoch 040 | Batch 120/204 | Current Loss: 0.100426\n[Training] Epoch 040 | Batch 130/204 | Current Loss: 0.100582\n[Training] Epoch 040 | Batch 140/204 | Current Loss: 0.100394\n[Training] Epoch 040 | Batch 150/204 | Current Loss: 0.100509\n[Training] Epoch 040 | Batch 160/204 | Current Loss: 0.100512\n[Training] Epoch 040 | Batch 170/204 | Current Loss: 0.100573\n[Training] Epoch 040 | Batch 180/204 | Current Loss: 0.100560\n[Training] Epoch 040 | Batch 190/204 | Current Loss: 0.100475\n[Training] Epoch 040 | Batch 200/204 | Current Loss: 0.100304\n\n[Training] Epoch 040 Summary:\n  Avg Loss: 0.100469\n  Current LR: 5.26e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 041 | Batch 000/204 | Current Loss: 0.100470\n[Training] Epoch 041 | Batch 010/204 | Current Loss: 0.100458\n[Training] Epoch 041 | Batch 020/204 | Current Loss: 0.100626\n[Training] Epoch 041 | Batch 030/204 | Current Loss: 0.100485\n[Training] Epoch 041 | Batch 040/204 | Current Loss: 0.100524\n[Training] Epoch 041 | Batch 050/204 | Current Loss: 0.100435\n[Training] Epoch 041 | Batch 060/204 | Current Loss: 0.100448\n[Training] Epoch 041 | Batch 070/204 | Current Loss: 0.100553\n[Training] Epoch 041 | Batch 080/204 | Current Loss: 0.100616\n[Training] Epoch 041 | Batch 090/204 | Current Loss: 0.100649\n[Training] Epoch 041 | Batch 100/204 | Current Loss: 0.100528\n[Training] Epoch 041 | Batch 110/204 | Current Loss: 0.100518\n[Training] Epoch 041 | Batch 120/204 | Current Loss: 0.100371\n[Training] Epoch 041 | Batch 130/204 | Current Loss: 0.100374\n[Training] Epoch 041 | Batch 140/204 | Current Loss: 0.100413\n[Training] Epoch 041 | Batch 150/204 | Current Loss: 0.100471\n[Training] Epoch 041 | Batch 160/204 | Current Loss: 0.100374\n[Training] Epoch 041 | Batch 170/204 | Current Loss: 0.100450\n[Training] Epoch 041 | Batch 180/204 | Current Loss: 0.100680\n[Training] Epoch 041 | Batch 190/204 | Current Loss: 0.100441\n[Training] Epoch 041 | Batch 200/204 | Current Loss: 0.100482\n\n[Training] Epoch 041 Summary:\n  Avg Loss: 0.100472\n  Current LR: 5.13e-05\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 042 | Batch 000/204 | Current Loss: 0.100537\n[Training] Epoch 042 | Batch 010/204 | Current Loss: 0.100392\n[Training] Epoch 042 | Batch 020/204 | Current Loss: 0.100487\n[Training] Epoch 042 | Batch 030/204 | Current Loss: 0.100494\n[Training] Epoch 042 | Batch 040/204 | Current Loss: 0.100378\n[Training] Epoch 042 | Batch 050/204 | Current Loss: 0.100427\n[Training] Epoch 042 | Batch 060/204 | Current Loss: 0.100479\n[Training] Epoch 042 | Batch 070/204 | Current Loss: 0.100472\n[Training] Epoch 042 | Batch 080/204 | Current Loss: 0.100522\n[Training] Epoch 042 | Batch 090/204 | Current Loss: 0.100320\n[Training] Epoch 042 | Batch 100/204 | Current Loss: 0.100323\n[Training] Epoch 042 | Batch 110/204 | Current Loss: 0.100384\n[Training] Epoch 042 | Batch 120/204 | Current Loss: 0.100391\n[Training] Epoch 042 | Batch 130/204 | Current Loss: 0.100394\n[Training] Epoch 042 | Batch 140/204 | Current Loss: 0.100231\n[Training] Epoch 042 | Batch 150/204 | Current Loss: 0.100441\n[Training] Epoch 042 | Batch 160/204 | Current Loss: 0.100391\n[Training] Epoch 042 | Batch 170/204 | Current Loss: 0.100453\n[Training] Epoch 042 | Batch 180/204 | Current Loss: 0.100403\n[Training] Epoch 042 | Batch 190/204 | Current Loss: 0.100340\n[Training] Epoch 042 | Batch 200/204 | Current Loss: 0.100421\n\n[Training] Epoch 042 Summary:\n  Avg Loss: 0.100436\n  Current LR: 4.77e-05\n[EarlyStopping] Loss improved (0.100467 → 0.100436). Saving model...\n[Training] Epoch 043 | Batch 000/204 | Current Loss: 0.100497\n[Training] Epoch 043 | Batch 010/204 | Current Loss: 0.100511\n[Training] Epoch 043 | Batch 020/204 | Current Loss: 0.100428\n[Training] Epoch 043 | Batch 030/204 | Current Loss: 0.100487\n[Training] Epoch 043 | Batch 040/204 | Current Loss: 0.100421\n[Training] Epoch 043 | Batch 050/204 | Current Loss: 0.100327\n[Training] Epoch 043 | Batch 060/204 | Current Loss: 0.100362\n[Training] Epoch 043 | Batch 070/204 | Current Loss: 0.100344\n[Training] Epoch 043 | Batch 080/204 | Current Loss: 0.100395\n[Training] Epoch 043 | Batch 090/204 | Current Loss: 0.100379\n[Training] Epoch 043 | Batch 100/204 | Current Loss: 0.100568\n[Training] Epoch 043 | Batch 110/204 | Current Loss: 0.100375\n[Training] Epoch 043 | Batch 120/204 | Current Loss: 0.100308\n[Training] Epoch 043 | Batch 130/204 | Current Loss: 0.100475\n[Training] Epoch 043 | Batch 140/204 | Current Loss: 0.100370\n[Training] Epoch 043 | Batch 150/204 | Current Loss: 0.100423\n[Training] Epoch 043 | Batch 160/204 | Current Loss: 0.100340\n[Training] Epoch 043 | Batch 170/204 | Current Loss: 0.100476\n[Training] Epoch 043 | Batch 180/204 | Current Loss: 0.100343\n[Training] Epoch 043 | Batch 190/204 | Current Loss: 0.100282\n[Training] Epoch 043 | Batch 200/204 | Current Loss: 0.100427\n\n[Training] Epoch 043 Summary:\n  Avg Loss: 0.100403\n  Current LR: 4.20e-05\n[EarlyStopping] Loss improved (0.100436 → 0.100403). Saving model...\n[Training] Epoch 044 | Batch 000/204 | Current Loss: 0.100422\n[Training] Epoch 044 | Batch 010/204 | Current Loss: 0.100279\n[Training] Epoch 044 | Batch 020/204 | Current Loss: 0.100414\n[Training] Epoch 044 | Batch 030/204 | Current Loss: 0.100280\n[Training] Epoch 044 | Batch 040/204 | Current Loss: 0.100497\n[Training] Epoch 044 | Batch 050/204 | Current Loss: 0.100332\n[Training] Epoch 044 | Batch 060/204 | Current Loss: 0.100366\n[Training] Epoch 044 | Batch 070/204 | Current Loss: 0.100362\n[Training] Epoch 044 | Batch 080/204 | Current Loss: 0.100464\n[Training] Epoch 044 | Batch 090/204 | Current Loss: 0.100412\n[Training] Epoch 044 | Batch 100/204 | Current Loss: 0.100276\n[Training] Epoch 044 | Batch 110/204 | Current Loss: 0.100437\n[Training] Epoch 044 | Batch 120/204 | Current Loss: 0.100515\n[Training] Epoch 044 | Batch 130/204 | Current Loss: 0.100375\n[Training] Epoch 044 | Batch 140/204 | Current Loss: 0.100360\n[Training] Epoch 044 | Batch 150/204 | Current Loss: 0.100406\n[Training] Epoch 044 | Batch 160/204 | Current Loss: 0.100351\n[Training] Epoch 044 | Batch 170/204 | Current Loss: 0.100426\n[Training] Epoch 044 | Batch 180/204 | Current Loss: 0.100268\n[Training] Epoch 044 | Batch 190/204 | Current Loss: 0.100357\n[Training] Epoch 044 | Batch 200/204 | Current Loss: 0.100454\n\n[Training] Epoch 044 Summary:\n  Avg Loss: 0.100379\n  Current LR: 3.48e-05\n[EarlyStopping] Loss improved (0.100403 → 0.100379). Saving model...\n[Training] Epoch 045 | Batch 000/204 | Current Loss: 0.100395\n[Training] Epoch 045 | Batch 010/204 | Current Loss: 0.100292\n[Training] Epoch 045 | Batch 020/204 | Current Loss: 0.100346\n[Training] Epoch 045 | Batch 030/204 | Current Loss: 0.100349\n[Training] Epoch 045 | Batch 040/204 | Current Loss: 0.100443\n[Training] Epoch 045 | Batch 050/204 | Current Loss: 0.100352\n[Training] Epoch 045 | Batch 060/204 | Current Loss: 0.100352\n[Training] Epoch 045 | Batch 070/204 | Current Loss: 0.100378\n[Training] Epoch 045 | Batch 080/204 | Current Loss: 0.100399\n[Training] Epoch 045 | Batch 090/204 | Current Loss: 0.100360\n[Training] Epoch 045 | Batch 100/204 | Current Loss: 0.100344\n[Training] Epoch 045 | Batch 110/204 | Current Loss: 0.100352\n[Training] Epoch 045 | Batch 120/204 | Current Loss: 0.100255\n[Training] Epoch 045 | Batch 130/204 | Current Loss: 0.100365\n[Training] Epoch 045 | Batch 140/204 | Current Loss: 0.100380\n[Training] Epoch 045 | Batch 150/204 | Current Loss: 0.100351\n[Training] Epoch 045 | Batch 160/204 | Current Loss: 0.100414\n[Training] Epoch 045 | Batch 170/204 | Current Loss: 0.100396\n[Training] Epoch 045 | Batch 180/204 | Current Loss: 0.100460\n[Training] Epoch 045 | Batch 190/204 | Current Loss: 0.100506\n[Training] Epoch 045 | Batch 200/204 | Current Loss: 0.100234\n\n[Training] Epoch 045 Summary:\n  Avg Loss: 0.100360\n  Current LR: 2.68e-05\n[EarlyStopping] Loss improved (0.100379 → 0.100360). Saving model...\n[Training] Epoch 046 | Batch 000/204 | Current Loss: 0.100365\n[Training] Epoch 046 | Batch 010/204 | Current Loss: 0.100246\n[Training] Epoch 046 | Batch 020/204 | Current Loss: 0.100374\n[Training] Epoch 046 | Batch 030/204 | Current Loss: 0.100400\n[Training] Epoch 046 | Batch 040/204 | Current Loss: 0.100313\n[Training] Epoch 046 | Batch 050/204 | Current Loss: 0.100254\n[Training] Epoch 046 | Batch 060/204 | Current Loss: 0.100294\n[Training] Epoch 046 | Batch 070/204 | Current Loss: 0.100251\n[Training] Epoch 046 | Batch 080/204 | Current Loss: 0.100233\n[Training] Epoch 046 | Batch 090/204 | Current Loss: 0.100326\n[Training] Epoch 046 | Batch 100/204 | Current Loss: 0.100324\n[Training] Epoch 046 | Batch 110/204 | Current Loss: 0.100272\n[Training] Epoch 046 | Batch 120/204 | Current Loss: 0.100357\n[Training] Epoch 046 | Batch 130/204 | Current Loss: 0.100255\n[Training] Epoch 046 | Batch 140/204 | Current Loss: 0.100379\n[Training] Epoch 046 | Batch 150/204 | Current Loss: 0.100352\n[Training] Epoch 046 | Batch 160/204 | Current Loss: 0.100329\n[Training] Epoch 046 | Batch 170/204 | Current Loss: 0.100415\n[Training] Epoch 046 | Batch 180/204 | Current Loss: 0.100257\n[Training] Epoch 046 | Batch 190/204 | Current Loss: 0.100365\n[Training] Epoch 046 | Batch 200/204 | Current Loss: 0.100267\n\n[Training] Epoch 046 Summary:\n  Avg Loss: 0.100340\n  Current LR: 1.88e-05\n[EarlyStopping] Loss improved (0.100360 → 0.100340). Saving model...\n[Training] Epoch 047 | Batch 000/204 | Current Loss: 0.100415\n[Training] Epoch 047 | Batch 010/204 | Current Loss: 0.100336\n[Training] Epoch 047 | Batch 020/204 | Current Loss: 0.100284\n[Training] Epoch 047 | Batch 030/204 | Current Loss: 0.100419\n[Training] Epoch 047 | Batch 040/204 | Current Loss: 0.100230\n[Training] Epoch 047 | Batch 050/204 | Current Loss: 0.100364\n[Training] Epoch 047 | Batch 060/204 | Current Loss: 0.100354\n[Training] Epoch 047 | Batch 070/204 | Current Loss: 0.100326\n[Training] Epoch 047 | Batch 080/204 | Current Loss: 0.100371\n[Training] Epoch 047 | Batch 090/204 | Current Loss: 0.100221\n[Training] Epoch 047 | Batch 100/204 | Current Loss: 0.100340\n[Training] Epoch 047 | Batch 110/204 | Current Loss: 0.100287\n[Training] Epoch 047 | Batch 120/204 | Current Loss: 0.100386\n[Training] Epoch 047 | Batch 130/204 | Current Loss: 0.100234\n[Training] Epoch 047 | Batch 140/204 | Current Loss: 0.100282\n[Training] Epoch 047 | Batch 150/204 | Current Loss: 0.100248\n[Training] Epoch 047 | Batch 160/204 | Current Loss: 0.100286\n[Training] Epoch 047 | Batch 170/204 | Current Loss: 0.100377\n[Training] Epoch 047 | Batch 180/204 | Current Loss: 0.100292\n[Training] Epoch 047 | Batch 190/204 | Current Loss: 0.100359\n[Training] Epoch 047 | Batch 200/204 | Current Loss: 0.100182\n\n[Training] Epoch 047 Summary:\n  Avg Loss: 0.100321\n  Current LR: 1.16e-05\n[EarlyStopping] Loss improved (0.100340 → 0.100321). Saving model...\n[Training] Epoch 048 | Batch 000/204 | Current Loss: 0.100280\n[Training] Epoch 048 | Batch 010/204 | Current Loss: 0.100307\n[Training] Epoch 048 | Batch 020/204 | Current Loss: 0.100415\n[Training] Epoch 048 | Batch 030/204 | Current Loss: 0.100395\n[Training] Epoch 048 | Batch 040/204 | Current Loss: 0.100283\n[Training] Epoch 048 | Batch 050/204 | Current Loss: 0.100335\n[Training] Epoch 048 | Batch 060/204 | Current Loss: 0.100333\n[Training] Epoch 048 | Batch 070/204 | Current Loss: 0.100272\n[Training] Epoch 048 | Batch 080/204 | Current Loss: 0.100400\n[Training] Epoch 048 | Batch 090/204 | Current Loss: 0.100293\n[Training] Epoch 048 | Batch 100/204 | Current Loss: 0.100382\n[Training] Epoch 048 | Batch 110/204 | Current Loss: 0.100423\n[Training] Epoch 048 | Batch 120/204 | Current Loss: 0.100335\n[Training] Epoch 048 | Batch 130/204 | Current Loss: 0.100249\n[Training] Epoch 048 | Batch 140/204 | Current Loss: 0.100336\n[Training] Epoch 048 | Batch 150/204 | Current Loss: 0.100217\n[Training] Epoch 048 | Batch 160/204 | Current Loss: 0.100382\n[Training] Epoch 048 | Batch 170/204 | Current Loss: 0.100309\n[Training] Epoch 048 | Batch 180/204 | Current Loss: 0.100385\n[Training] Epoch 048 | Batch 190/204 | Current Loss: 0.100300\n[Training] Epoch 048 | Batch 200/204 | Current Loss: 0.100304\n\n[Training] Epoch 048 Summary:\n  Avg Loss: 0.100320\n  Current LR: 5.93e-06\n[EarlyStopping] Loss improved (0.100321 → 0.100320). Saving model...\n[Training] Epoch 049 | Batch 000/204 | Current Loss: 0.100332\n[Training] Epoch 049 | Batch 010/204 | Current Loss: 0.100283\n[Training] Epoch 049 | Batch 020/204 | Current Loss: 0.100230\n[Training] Epoch 049 | Batch 030/204 | Current Loss: 0.100314\n[Training] Epoch 049 | Batch 040/204 | Current Loss: 0.100231\n[Training] Epoch 049 | Batch 050/204 | Current Loss: 0.100364\n[Training] Epoch 049 | Batch 060/204 | Current Loss: 0.100309\n[Training] Epoch 049 | Batch 070/204 | Current Loss: 0.100248\n[Training] Epoch 049 | Batch 080/204 | Current Loss: 0.100286\n[Training] Epoch 049 | Batch 090/204 | Current Loss: 0.100380\n[Training] Epoch 049 | Batch 100/204 | Current Loss: 0.100271\n[Training] Epoch 049 | Batch 110/204 | Current Loss: 0.100316\n[Training] Epoch 049 | Batch 120/204 | Current Loss: 0.100252\n[Training] Epoch 049 | Batch 130/204 | Current Loss: 0.100231\n[Training] Epoch 049 | Batch 140/204 | Current Loss: 0.100350\n[Training] Epoch 049 | Batch 150/204 | Current Loss: 0.100251\n[Training] Epoch 049 | Batch 160/204 | Current Loss: 0.100332\n[Training] Epoch 049 | Batch 170/204 | Current Loss: 0.100361\n[Training] Epoch 049 | Batch 180/204 | Current Loss: 0.100283\n[Training] Epoch 049 | Batch 190/204 | Current Loss: 0.100339\n[Training] Epoch 049 | Batch 200/204 | Current Loss: 0.100185\n\n[Training] Epoch 049 Summary:\n  Avg Loss: 0.100307\n  Current LR: 2.26e-06\n[EarlyStopping] Loss improved (0.100320 → 0.100307). Saving model...\n[Training] Epoch 050 | Batch 000/204 | Current Loss: 0.100451\n[Training] Epoch 050 | Batch 010/204 | Current Loss: 0.100248\n[Training] Epoch 050 | Batch 020/204 | Current Loss: 0.100260\n[Training] Epoch 050 | Batch 030/204 | Current Loss: 0.100434\n[Training] Epoch 050 | Batch 040/204 | Current Loss: 0.100325\n[Training] Epoch 050 | Batch 050/204 | Current Loss: 0.100336\n[Training] Epoch 050 | Batch 060/204 | Current Loss: 0.100277\n[Training] Epoch 050 | Batch 070/204 | Current Loss: 0.100217\n[Training] Epoch 050 | Batch 080/204 | Current Loss: 0.100339\n[Training] Epoch 050 | Batch 090/204 | Current Loss: 0.100278\n[Training] Epoch 050 | Batch 100/204 | Current Loss: 0.100245\n[Training] Epoch 050 | Batch 110/204 | Current Loss: 0.100310\n[Training] Epoch 050 | Batch 120/204 | Current Loss: 0.100274\n[Training] Epoch 050 | Batch 130/204 | Current Loss: 0.100242\n[Training] Epoch 050 | Batch 140/204 | Current Loss: 0.100276\n[Training] Epoch 050 | Batch 150/204 | Current Loss: 0.100325\n[Training] Epoch 050 | Batch 160/204 | Current Loss: 0.100482\n[Training] Epoch 050 | Batch 170/204 | Current Loss: 0.100381\n[Training] Epoch 050 | Batch 180/204 | Current Loss: 0.100296\n[Training] Epoch 050 | Batch 190/204 | Current Loss: 0.100272\n[Training] Epoch 050 | Batch 200/204 | Current Loss: 0.100344\n\n[Training] Epoch 050 Summary:\n  Avg Loss: 0.100309\n  Current LR: 5.26e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 051 | Batch 000/204 | Current Loss: 0.100248\n[Training] Epoch 051 | Batch 010/204 | Current Loss: 0.100463\n[Training] Epoch 051 | Batch 020/204 | Current Loss: 0.100450\n[Training] Epoch 051 | Batch 030/204 | Current Loss: 0.100304\n[Training] Epoch 051 | Batch 040/204 | Current Loss: 0.100254\n[Training] Epoch 051 | Batch 050/204 | Current Loss: 0.100314\n[Training] Epoch 051 | Batch 060/204 | Current Loss: 0.100336\n[Training] Epoch 051 | Batch 070/204 | Current Loss: 0.100344\n[Training] Epoch 051 | Batch 080/204 | Current Loss: 0.100346\n[Training] Epoch 051 | Batch 090/204 | Current Loss: 0.100403\n[Training] Epoch 051 | Batch 100/204 | Current Loss: 0.100347\n[Training] Epoch 051 | Batch 110/204 | Current Loss: 0.100263\n[Training] Epoch 051 | Batch 120/204 | Current Loss: 0.100300\n[Training] Epoch 051 | Batch 130/204 | Current Loss: 0.100260\n[Training] Epoch 051 | Batch 140/204 | Current Loss: 0.100498\n[Training] Epoch 051 | Batch 150/204 | Current Loss: 0.100349\n[Training] Epoch 051 | Batch 160/204 | Current Loss: 0.100315\n[Training] Epoch 051 | Batch 170/204 | Current Loss: 0.100269\n[Training] Epoch 051 | Batch 180/204 | Current Loss: 0.100284\n[Training] Epoch 051 | Batch 190/204 | Current Loss: 0.100378\n[Training] Epoch 051 | Batch 200/204 | Current Loss: 0.100281\n\n[Training] Epoch 051 Summary:\n  Avg Loss: 0.100331\n  Current LR: 5.13e-05\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 052 | Batch 000/204 | Current Loss: 0.100567\n[Training] Epoch 052 | Batch 010/204 | Current Loss: 0.100406\n[Training] Epoch 052 | Batch 020/204 | Current Loss: 0.100286\n[Training] Epoch 052 | Batch 030/204 | Current Loss: 0.100388\n[Training] Epoch 052 | Batch 040/204 | Current Loss: 0.100246\n[Training] Epoch 052 | Batch 050/204 | Current Loss: 0.100314\n[Training] Epoch 052 | Batch 060/204 | Current Loss: 0.100251\n[Training] Epoch 052 | Batch 070/204 | Current Loss: 0.100320\n[Training] Epoch 052 | Batch 080/204 | Current Loss: 0.100453\n[Training] Epoch 052 | Batch 090/204 | Current Loss: 0.100329\n[Training] Epoch 052 | Batch 100/204 | Current Loss: 0.100305\n[Training] Epoch 052 | Batch 110/204 | Current Loss: 0.100360\n[Training] Epoch 052 | Batch 120/204 | Current Loss: 0.100307\n[Training] Epoch 052 | Batch 130/204 | Current Loss: 0.100424\n[Training] Epoch 052 | Batch 140/204 | Current Loss: 0.100307\n[Training] Epoch 052 | Batch 150/204 | Current Loss: 0.100131\n[Training] Epoch 052 | Batch 160/204 | Current Loss: 0.100269\n[Training] Epoch 052 | Batch 170/204 | Current Loss: 0.100212\n[Training] Epoch 052 | Batch 180/204 | Current Loss: 0.100319\n[Training] Epoch 052 | Batch 190/204 | Current Loss: 0.100340\n[Training] Epoch 052 | Batch 200/204 | Current Loss: 0.100336\n\n[Training] Epoch 052 Summary:\n  Avg Loss: 0.100320\n  Current LR: 4.77e-05\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 053 | Batch 000/204 | Current Loss: 0.100253\n[Training] Epoch 053 | Batch 010/204 | Current Loss: 0.100288\n[Training] Epoch 053 | Batch 020/204 | Current Loss: 0.100189\n[Training] Epoch 053 | Batch 030/204 | Current Loss: 0.100292\n[Training] Epoch 053 | Batch 040/204 | Current Loss: 0.100308\n[Training] Epoch 053 | Batch 050/204 | Current Loss: 0.100308\n[Training] Epoch 053 | Batch 060/204 | Current Loss: 0.100182\n[Training] Epoch 053 | Batch 070/204 | Current Loss: 0.100409\n[Training] Epoch 053 | Batch 080/204 | Current Loss: 0.100217\n[Training] Epoch 053 | Batch 090/204 | Current Loss: 0.100398\n[Training] Epoch 053 | Batch 100/204 | Current Loss: 0.100280\n[Training] Epoch 053 | Batch 110/204 | Current Loss: 0.100116\n[Training] Epoch 053 | Batch 120/204 | Current Loss: 0.100215\n[Training] Epoch 053 | Batch 130/204 | Current Loss: 0.100219\n[Training] Epoch 053 | Batch 140/204 | Current Loss: 0.100379\n[Training] Epoch 053 | Batch 150/204 | Current Loss: 0.100355\n[Training] Epoch 053 | Batch 160/204 | Current Loss: 0.100274\n[Training] Epoch 053 | Batch 170/204 | Current Loss: 0.100333\n[Training] Epoch 053 | Batch 180/204 | Current Loss: 0.100187\n[Training] Epoch 053 | Batch 190/204 | Current Loss: 0.100282\n[Training] Epoch 053 | Batch 200/204 | Current Loss: 0.100237\n\n[Training] Epoch 053 Summary:\n  Avg Loss: 0.100281\n  Current LR: 4.20e-05\n[EarlyStopping] Loss improved (0.100307 → 0.100281). Saving model...\n[Training] Epoch 054 | Batch 000/204 | Current Loss: 0.100330\n[Training] Epoch 054 | Batch 010/204 | Current Loss: 0.100305\n[Training] Epoch 054 | Batch 020/204 | Current Loss: 0.100343\n[Training] Epoch 054 | Batch 030/204 | Current Loss: 0.100208\n[Training] Epoch 054 | Batch 040/204 | Current Loss: 0.100249\n[Training] Epoch 054 | Batch 050/204 | Current Loss: 0.100405\n[Training] Epoch 054 | Batch 060/204 | Current Loss: 0.100274\n[Training] Epoch 054 | Batch 070/204 | Current Loss: 0.100173\n[Training] Epoch 054 | Batch 080/204 | Current Loss: 0.100242\n[Training] Epoch 054 | Batch 090/204 | Current Loss: 0.100340\n[Training] Epoch 054 | Batch 100/204 | Current Loss: 0.100198\n[Training] Epoch 054 | Batch 110/204 | Current Loss: 0.100254\n[Training] Epoch 054 | Batch 120/204 | Current Loss: 0.100209\n[Training] Epoch 054 | Batch 130/204 | Current Loss: 0.100244\n[Training] Epoch 054 | Batch 140/204 | Current Loss: 0.100292\n[Training] Epoch 054 | Batch 150/204 | Current Loss: 0.100421\n[Training] Epoch 054 | Batch 160/204 | Current Loss: 0.100300\n[Training] Epoch 054 | Batch 170/204 | Current Loss: 0.100324\n[Training] Epoch 054 | Batch 180/204 | Current Loss: 0.100244\n[Training] Epoch 054 | Batch 190/204 | Current Loss: 0.100292\n[Training] Epoch 054 | Batch 200/204 | Current Loss: 0.100172\n\n[Training] Epoch 054 Summary:\n  Avg Loss: 0.100273\n  Current LR: 3.48e-05\n[EarlyStopping] Loss improved (0.100281 → 0.100273). Saving model...\n[Training] Epoch 055 | Batch 000/204 | Current Loss: 0.100416\n[Training] Epoch 055 | Batch 010/204 | Current Loss: 0.100312\n[Training] Epoch 055 | Batch 020/204 | Current Loss: 0.100193\n[Training] Epoch 055 | Batch 030/204 | Current Loss: 0.100206\n[Training] Epoch 055 | Batch 040/204 | Current Loss: 0.100235\n[Training] Epoch 055 | Batch 050/204 | Current Loss: 0.100338\n[Training] Epoch 055 | Batch 060/204 | Current Loss: 0.100272\n[Training] Epoch 055 | Batch 070/204 | Current Loss: 0.100295\n[Training] Epoch 055 | Batch 080/204 | Current Loss: 0.100316\n[Training] Epoch 055 | Batch 090/204 | Current Loss: 0.100243\n[Training] Epoch 055 | Batch 100/204 | Current Loss: 0.100225\n[Training] Epoch 055 | Batch 110/204 | Current Loss: 0.100167\n[Training] Epoch 055 | Batch 120/204 | Current Loss: 0.100229\n[Training] Epoch 055 | Batch 130/204 | Current Loss: 0.100150\n[Training] Epoch 055 | Batch 140/204 | Current Loss: 0.100173\n[Training] Epoch 055 | Batch 150/204 | Current Loss: 0.100230\n[Training] Epoch 055 | Batch 160/204 | Current Loss: 0.100145\n[Training] Epoch 055 | Batch 170/204 | Current Loss: 0.100243\n[Training] Epoch 055 | Batch 180/204 | Current Loss: 0.100290\n[Training] Epoch 055 | Batch 190/204 | Current Loss: 0.100147\n[Training] Epoch 055 | Batch 200/204 | Current Loss: 0.100257\n\n[Training] Epoch 055 Summary:\n  Avg Loss: 0.100251\n  Current LR: 2.68e-05\n[EarlyStopping] Loss improved (0.100273 → 0.100251). Saving model...\n[Training] Epoch 056 | Batch 000/204 | Current Loss: 0.100287\n[Training] Epoch 056 | Batch 010/204 | Current Loss: 0.100188\n[Training] Epoch 056 | Batch 020/204 | Current Loss: 0.100211\n[Training] Epoch 056 | Batch 030/204 | Current Loss: 0.100210\n[Training] Epoch 056 | Batch 040/204 | Current Loss: 0.100239\n[Training] Epoch 056 | Batch 050/204 | Current Loss: 0.100247\n[Training] Epoch 056 | Batch 060/204 | Current Loss: 0.100194\n[Training] Epoch 056 | Batch 070/204 | Current Loss: 0.100310\n[Training] Epoch 056 | Batch 080/204 | Current Loss: 0.100201\n[Training] Epoch 056 | Batch 090/204 | Current Loss: 0.100278\n[Training] Epoch 056 | Batch 100/204 | Current Loss: 0.100233\n[Training] Epoch 056 | Batch 110/204 | Current Loss: 0.100201\n[Training] Epoch 056 | Batch 120/204 | Current Loss: 0.100216\n[Training] Epoch 056 | Batch 130/204 | Current Loss: 0.100215\n[Training] Epoch 056 | Batch 140/204 | Current Loss: 0.100263\n[Training] Epoch 056 | Batch 150/204 | Current Loss: 0.100239\n[Training] Epoch 056 | Batch 160/204 | Current Loss: 0.100225\n[Training] Epoch 056 | Batch 170/204 | Current Loss: 0.100155\n[Training] Epoch 056 | Batch 180/204 | Current Loss: 0.100291\n[Training] Epoch 056 | Batch 190/204 | Current Loss: 0.100340\n[Training] Epoch 056 | Batch 200/204 | Current Loss: 0.100253\n\n[Training] Epoch 056 Summary:\n  Avg Loss: 0.100237\n  Current LR: 1.88e-05\n[EarlyStopping] Loss improved (0.100251 → 0.100237). Saving model...\n[Training] Epoch 057 | Batch 000/204 | Current Loss: 0.100221\n[Training] Epoch 057 | Batch 010/204 | Current Loss: 0.100215\n[Training] Epoch 057 | Batch 020/204 | Current Loss: 0.100278\n[Training] Epoch 057 | Batch 030/204 | Current Loss: 0.100297\n[Training] Epoch 057 | Batch 040/204 | Current Loss: 0.100228\n[Training] Epoch 057 | Batch 050/204 | Current Loss: 0.100244\n[Training] Epoch 057 | Batch 060/204 | Current Loss: 0.100242\n[Training] Epoch 057 | Batch 070/204 | Current Loss: 0.100248\n[Training] Epoch 057 | Batch 080/204 | Current Loss: 0.100373\n[Training] Epoch 057 | Batch 090/204 | Current Loss: 0.100149\n[Training] Epoch 057 | Batch 100/204 | Current Loss: 0.100241\n[Training] Epoch 057 | Batch 110/204 | Current Loss: 0.100219\n[Training] Epoch 057 | Batch 120/204 | Current Loss: 0.100154\n[Training] Epoch 057 | Batch 130/204 | Current Loss: 0.100212\n[Training] Epoch 057 | Batch 140/204 | Current Loss: 0.100148\n[Training] Epoch 057 | Batch 150/204 | Current Loss: 0.100213\n[Training] Epoch 057 | Batch 160/204 | Current Loss: 0.100189\n[Training] Epoch 057 | Batch 170/204 | Current Loss: 0.100331\n[Training] Epoch 057 | Batch 180/204 | Current Loss: 0.100224\n[Training] Epoch 057 | Batch 190/204 | Current Loss: 0.100231\n[Training] Epoch 057 | Batch 200/204 | Current Loss: 0.100289\n\n[Training] Epoch 057 Summary:\n  Avg Loss: 0.100243\n  Current LR: 1.16e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 058 | Batch 000/204 | Current Loss: 0.100353\n[Training] Epoch 058 | Batch 010/204 | Current Loss: 0.100322\n[Training] Epoch 058 | Batch 020/204 | Current Loss: 0.100162\n[Training] Epoch 058 | Batch 030/204 | Current Loss: 0.100222\n[Training] Epoch 058 | Batch 040/204 | Current Loss: 0.100263\n[Training] Epoch 058 | Batch 050/204 | Current Loss: 0.100258\n[Training] Epoch 058 | Batch 060/204 | Current Loss: 0.100172\n[Training] Epoch 058 | Batch 070/204 | Current Loss: 0.100153\n[Training] Epoch 058 | Batch 080/204 | Current Loss: 0.100160\n[Training] Epoch 058 | Batch 090/204 | Current Loss: 0.100198\n[Training] Epoch 058 | Batch 100/204 | Current Loss: 0.100203\n[Training] Epoch 058 | Batch 110/204 | Current Loss: 0.100211\n[Training] Epoch 058 | Batch 120/204 | Current Loss: 0.100339\n[Training] Epoch 058 | Batch 130/204 | Current Loss: 0.100122\n[Training] Epoch 058 | Batch 140/204 | Current Loss: 0.100177\n[Training] Epoch 058 | Batch 150/204 | Current Loss: 0.100216\n[Training] Epoch 058 | Batch 160/204 | Current Loss: 0.100129\n[Training] Epoch 058 | Batch 170/204 | Current Loss: 0.100230\n[Training] Epoch 058 | Batch 180/204 | Current Loss: 0.100140\n[Training] Epoch 058 | Batch 190/204 | Current Loss: 0.100293\n[Training] Epoch 058 | Batch 200/204 | Current Loss: 0.100332\n\n[Training] Epoch 058 Summary:\n  Avg Loss: 0.100226\n  Current LR: 5.93e-06\n[EarlyStopping] Loss improved (0.100237 → 0.100226). Saving model...\n[Training] Epoch 059 | Batch 000/204 | Current Loss: 0.100312\n[Training] Epoch 059 | Batch 010/204 | Current Loss: 0.100182\n[Training] Epoch 059 | Batch 020/204 | Current Loss: 0.100137\n[Training] Epoch 059 | Batch 030/204 | Current Loss: 0.100149\n[Training] Epoch 059 | Batch 040/204 | Current Loss: 0.100140\n[Training] Epoch 059 | Batch 050/204 | Current Loss: 0.100192\n[Training] Epoch 059 | Batch 060/204 | Current Loss: 0.100226\n[Training] Epoch 059 | Batch 070/204 | Current Loss: 0.100104\n[Training] Epoch 059 | Batch 080/204 | Current Loss: 0.100255\n[Training] Epoch 059 | Batch 090/204 | Current Loss: 0.100131\n[Training] Epoch 059 | Batch 100/204 | Current Loss: 0.100226\n[Training] Epoch 059 | Batch 110/204 | Current Loss: 0.100309\n[Training] Epoch 059 | Batch 120/204 | Current Loss: 0.100240\n[Training] Epoch 059 | Batch 130/204 | Current Loss: 0.100256\n[Training] Epoch 059 | Batch 140/204 | Current Loss: 0.100251\n[Training] Epoch 059 | Batch 150/204 | Current Loss: 0.100185\n[Training] Epoch 059 | Batch 160/204 | Current Loss: 0.100112\n[Training] Epoch 059 | Batch 170/204 | Current Loss: 0.100155\n[Training] Epoch 059 | Batch 180/204 | Current Loss: 0.100225\n[Training] Epoch 059 | Batch 190/204 | Current Loss: 0.100283\n[Training] Epoch 059 | Batch 200/204 | Current Loss: 0.100305\n\n[Training] Epoch 059 Summary:\n  Avg Loss: 0.100223\n  Current LR: 2.26e-06\n[EarlyStopping] Loss improved (0.100226 → 0.100223). Saving model...\n[Training] Epoch 060 | Batch 000/204 | Current Loss: 0.100267\n[Training] Epoch 060 | Batch 010/204 | Current Loss: 0.100188\n[Training] Epoch 060 | Batch 020/204 | Current Loss: 0.100163\n[Training] Epoch 060 | Batch 030/204 | Current Loss: 0.100136\n[Training] Epoch 060 | Batch 040/204 | Current Loss: 0.100233\n[Training] Epoch 060 | Batch 050/204 | Current Loss: 0.100243\n[Training] Epoch 060 | Batch 060/204 | Current Loss: 0.100206\n[Training] Epoch 060 | Batch 070/204 | Current Loss: 0.100171\n[Training] Epoch 060 | Batch 080/204 | Current Loss: 0.100232\n[Training] Epoch 060 | Batch 090/204 | Current Loss: 0.100248\n[Training] Epoch 060 | Batch 100/204 | Current Loss: 0.100159\n[Training] Epoch 060 | Batch 110/204 | Current Loss: 0.100155\n[Training] Epoch 060 | Batch 120/204 | Current Loss: 0.100355\n[Training] Epoch 060 | Batch 130/204 | Current Loss: 0.100152\n[Training] Epoch 060 | Batch 140/204 | Current Loss: 0.100316\n[Training] Epoch 060 | Batch 150/204 | Current Loss: 0.100189\n[Training] Epoch 060 | Batch 160/204 | Current Loss: 0.100226\n[Training] Epoch 060 | Batch 170/204 | Current Loss: 0.100169\n[Training] Epoch 060 | Batch 180/204 | Current Loss: 0.100268\n[Training] Epoch 060 | Batch 190/204 | Current Loss: 0.100169\n[Training] Epoch 060 | Batch 200/204 | Current Loss: 0.100193\n\n[Training] Epoch 060 Summary:\n  Avg Loss: 0.100217\n  Current LR: 5.26e-05\n[EarlyStopping] Loss improved (0.100223 → 0.100217). Saving model...\n[Training] Epoch 061 | Batch 000/204 | Current Loss: 0.100200\n[Training] Epoch 061 | Batch 010/204 | Current Loss: 0.100217\n[Training] Epoch 061 | Batch 020/204 | Current Loss: 0.100149\n[Training] Epoch 061 | Batch 030/204 | Current Loss: 0.100262\n[Training] Epoch 061 | Batch 040/204 | Current Loss: 0.100295\n[Training] Epoch 061 | Batch 050/204 | Current Loss: 0.100266\n[Training] Epoch 061 | Batch 060/204 | Current Loss: 0.100181\n[Training] Epoch 061 | Batch 070/204 | Current Loss: 0.100156\n[Training] Epoch 061 | Batch 080/204 | Current Loss: 0.100262\n[Training] Epoch 061 | Batch 090/204 | Current Loss: 0.100210\n[Training] Epoch 061 | Batch 100/204 | Current Loss: 0.100369\n[Training] Epoch 061 | Batch 110/204 | Current Loss: 0.100190\n[Training] Epoch 061 | Batch 120/204 | Current Loss: 0.100319\n[Training] Epoch 061 | Batch 130/204 | Current Loss: 0.100276\n[Training] Epoch 061 | Batch 140/204 | Current Loss: 0.100328\n[Training] Epoch 061 | Batch 150/204 | Current Loss: 0.100176\n[Training] Epoch 061 | Batch 160/204 | Current Loss: 0.100201\n[Training] Epoch 061 | Batch 170/204 | Current Loss: 0.100290\n[Training] Epoch 061 | Batch 180/204 | Current Loss: 0.100174\n[Training] Epoch 061 | Batch 190/204 | Current Loss: 0.100267\n[Training] Epoch 061 | Batch 200/204 | Current Loss: 0.100253\n\n[Training] Epoch 061 Summary:\n  Avg Loss: 0.100235\n  Current LR: 5.13e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 062 | Batch 000/204 | Current Loss: 0.100173\n[Training] Epoch 062 | Batch 010/204 | Current Loss: 0.100316\n[Training] Epoch 062 | Batch 020/204 | Current Loss: 0.100331\n[Training] Epoch 062 | Batch 030/204 | Current Loss: 0.100234\n[Training] Epoch 062 | Batch 040/204 | Current Loss: 0.100114\n[Training] Epoch 062 | Batch 050/204 | Current Loss: 0.100245\n[Training] Epoch 062 | Batch 060/204 | Current Loss: 0.100205\n[Training] Epoch 062 | Batch 070/204 | Current Loss: 0.100326\n[Training] Epoch 062 | Batch 080/204 | Current Loss: 0.100323\n[Training] Epoch 062 | Batch 090/204 | Current Loss: 0.100217\n[Training] Epoch 062 | Batch 100/204 | Current Loss: 0.100238\n[Training] Epoch 062 | Batch 110/204 | Current Loss: 0.100258\n[Training] Epoch 062 | Batch 120/204 | Current Loss: 0.100245\n[Training] Epoch 062 | Batch 130/204 | Current Loss: 0.100336\n[Training] Epoch 062 | Batch 140/204 | Current Loss: 0.100131\n[Training] Epoch 062 | Batch 150/204 | Current Loss: 0.100265\n[Training] Epoch 062 | Batch 160/204 | Current Loss: 0.100261\n[Training] Epoch 062 | Batch 170/204 | Current Loss: 0.100254\n[Training] Epoch 062 | Batch 180/204 | Current Loss: 0.100236\n[Training] Epoch 062 | Batch 190/204 | Current Loss: 0.100216\n[Training] Epoch 062 | Batch 200/204 | Current Loss: 0.100265\n\n[Training] Epoch 062 Summary:\n  Avg Loss: 0.100226\n  Current LR: 4.77e-05\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 063 | Batch 000/204 | Current Loss: 0.100203\n[Training] Epoch 063 | Batch 010/204 | Current Loss: 0.100146\n[Training] Epoch 063 | Batch 020/204 | Current Loss: 0.100304\n[Training] Epoch 063 | Batch 030/204 | Current Loss: 0.100238\n[Training] Epoch 063 | Batch 040/204 | Current Loss: 0.100177\n[Training] Epoch 063 | Batch 050/204 | Current Loss: 0.100227\n[Training] Epoch 063 | Batch 060/204 | Current Loss: 0.100190\n[Training] Epoch 063 | Batch 070/204 | Current Loss: 0.100351\n[Training] Epoch 063 | Batch 080/204 | Current Loss: 0.100351\n[Training] Epoch 063 | Batch 090/204 | Current Loss: 0.100285\n[Training] Epoch 063 | Batch 100/204 | Current Loss: 0.100255\n[Training] Epoch 063 | Batch 110/204 | Current Loss: 0.100252\n[Training] Epoch 063 | Batch 120/204 | Current Loss: 0.100282\n[Training] Epoch 063 | Batch 130/204 | Current Loss: 0.100156\n[Training] Epoch 063 | Batch 140/204 | Current Loss: 0.100220\n[Training] Epoch 063 | Batch 150/204 | Current Loss: 0.100209\n[Training] Epoch 063 | Batch 160/204 | Current Loss: 0.100245\n[Training] Epoch 063 | Batch 170/204 | Current Loss: 0.100354\n[Training] Epoch 063 | Batch 180/204 | Current Loss: 0.100256\n[Training] Epoch 063 | Batch 190/204 | Current Loss: 0.100365\n[Training] Epoch 063 | Batch 200/204 | Current Loss: 0.100213\n\n[Training] Epoch 063 Summary:\n  Avg Loss: 0.100215\n  Current LR: 4.20e-05\n[EarlyStopping] Loss improved (0.100217 → 0.100215). Saving model...\n[Training] Epoch 064 | Batch 000/204 | Current Loss: 0.100245\n[Training] Epoch 064 | Batch 010/204 | Current Loss: 0.100254\n[Training] Epoch 064 | Batch 020/204 | Current Loss: 0.100167\n[Training] Epoch 064 | Batch 030/204 | Current Loss: 0.100171\n[Training] Epoch 064 | Batch 040/204 | Current Loss: 0.100089\n[Training] Epoch 064 | Batch 050/204 | Current Loss: 0.100203\n[Training] Epoch 064 | Batch 060/204 | Current Loss: 0.100300\n[Training] Epoch 064 | Batch 070/204 | Current Loss: 0.100214\n[Training] Epoch 064 | Batch 080/204 | Current Loss: 0.100162\n[Training] Epoch 064 | Batch 090/204 | Current Loss: 0.100200\n[Training] Epoch 064 | Batch 100/204 | Current Loss: 0.100180\n[Training] Epoch 064 | Batch 110/204 | Current Loss: 0.100168\n[Training] Epoch 064 | Batch 120/204 | Current Loss: 0.100123\n[Training] Epoch 064 | Batch 130/204 | Current Loss: 0.100173\n[Training] Epoch 064 | Batch 140/204 | Current Loss: 0.100219\n[Training] Epoch 064 | Batch 150/204 | Current Loss: 0.100244\n[Training] Epoch 064 | Batch 160/204 | Current Loss: 0.100253\n[Training] Epoch 064 | Batch 170/204 | Current Loss: 0.100249\n[Training] Epoch 064 | Batch 180/204 | Current Loss: 0.100162\n[Training] Epoch 064 | Batch 190/204 | Current Loss: 0.100177\n[Training] Epoch 064 | Batch 200/204 | Current Loss: 0.100168\n\n[Training] Epoch 064 Summary:\n  Avg Loss: 0.100202\n  Current LR: 3.48e-05\n[EarlyStopping] Loss improved (0.100215 → 0.100202). Saving model...\n[Training] Epoch 065 | Batch 000/204 | Current Loss: 0.100239\n[Training] Epoch 065 | Batch 010/204 | Current Loss: 0.100316\n[Training] Epoch 065 | Batch 020/204 | Current Loss: 0.100114\n[Training] Epoch 065 | Batch 030/204 | Current Loss: 0.100206\n[Training] Epoch 065 | Batch 040/204 | Current Loss: 0.100114\n[Training] Epoch 065 | Batch 050/204 | Current Loss: 0.100176\n[Training] Epoch 065 | Batch 060/204 | Current Loss: 0.100123\n[Training] Epoch 065 | Batch 070/204 | Current Loss: 0.100098\n[Training] Epoch 065 | Batch 080/204 | Current Loss: 0.100099\n[Training] Epoch 065 | Batch 090/204 | Current Loss: 0.100161\n[Training] Epoch 065 | Batch 100/204 | Current Loss: 0.100217\n[Training] Epoch 065 | Batch 110/204 | Current Loss: 0.100137\n[Training] Epoch 065 | Batch 120/204 | Current Loss: 0.100212\n[Training] Epoch 065 | Batch 130/204 | Current Loss: 0.100197\n[Training] Epoch 065 | Batch 140/204 | Current Loss: 0.100179\n[Training] Epoch 065 | Batch 150/204 | Current Loss: 0.100190\n[Training] Epoch 065 | Batch 160/204 | Current Loss: 0.100207\n[Training] Epoch 065 | Batch 170/204 | Current Loss: 0.100230\n[Training] Epoch 065 | Batch 180/204 | Current Loss: 0.100065\n[Training] Epoch 065 | Batch 190/204 | Current Loss: 0.100202\n[Training] Epoch 065 | Batch 200/204 | Current Loss: 0.100166\n\n[Training] Epoch 065 Summary:\n  Avg Loss: 0.100196\n  Current LR: 2.68e-05\n[EarlyStopping] Loss improved (0.100202 → 0.100196). Saving model...\n[Training] Epoch 066 | Batch 000/204 | Current Loss: 0.100296\n[Training] Epoch 066 | Batch 010/204 | Current Loss: 0.100209\n[Training] Epoch 066 | Batch 020/204 | Current Loss: 0.100182\n[Training] Epoch 066 | Batch 030/204 | Current Loss: 0.100201\n[Training] Epoch 066 | Batch 040/204 | Current Loss: 0.100168\n[Training] Epoch 066 | Batch 050/204 | Current Loss: 0.100231\n[Training] Epoch 066 | Batch 060/204 | Current Loss: 0.100170\n[Training] Epoch 066 | Batch 070/204 | Current Loss: 0.100295\n[Training] Epoch 066 | Batch 080/204 | Current Loss: 0.100094\n[Training] Epoch 066 | Batch 090/204 | Current Loss: 0.100161\n[Training] Epoch 066 | Batch 100/204 | Current Loss: 0.100282\n[Training] Epoch 066 | Batch 110/204 | Current Loss: 0.100196\n[Training] Epoch 066 | Batch 120/204 | Current Loss: 0.100092\n[Training] Epoch 066 | Batch 130/204 | Current Loss: 0.100252\n[Training] Epoch 066 | Batch 140/204 | Current Loss: 0.100235\n[Training] Epoch 066 | Batch 150/204 | Current Loss: 0.100232\n[Training] Epoch 066 | Batch 160/204 | Current Loss: 0.100267\n[Training] Epoch 066 | Batch 170/204 | Current Loss: 0.100224\n[Training] Epoch 066 | Batch 180/204 | Current Loss: 0.100167\n[Training] Epoch 066 | Batch 190/204 | Current Loss: 0.100174\n[Training] Epoch 066 | Batch 200/204 | Current Loss: 0.100201\n\n[Training] Epoch 066 Summary:\n  Avg Loss: 0.100195\n  Current LR: 1.88e-05\n[EarlyStopping] Loss improved (0.100196 → 0.100195). Saving model...\n[Training] Epoch 067 | Batch 000/204 | Current Loss: 0.100379\n[Training] Epoch 067 | Batch 010/204 | Current Loss: 0.100166\n[Training] Epoch 067 | Batch 020/204 | Current Loss: 0.100235\n[Training] Epoch 067 | Batch 030/204 | Current Loss: 0.100228\n[Training] Epoch 067 | Batch 040/204 | Current Loss: 0.100196\n[Training] Epoch 067 | Batch 050/204 | Current Loss: 0.100136\n[Training] Epoch 067 | Batch 060/204 | Current Loss: 0.100146\n[Training] Epoch 067 | Batch 070/204 | Current Loss: 0.100125\n[Training] Epoch 067 | Batch 080/204 | Current Loss: 0.100149\n[Training] Epoch 067 | Batch 090/204 | Current Loss: 0.100143\n[Training] Epoch 067 | Batch 100/204 | Current Loss: 0.100122\n[Training] Epoch 067 | Batch 110/204 | Current Loss: 0.100126\n[Training] Epoch 067 | Batch 120/204 | Current Loss: 0.100265\n[Training] Epoch 067 | Batch 130/204 | Current Loss: 0.100089\n[Training] Epoch 067 | Batch 140/204 | Current Loss: 0.100225\n[Training] Epoch 067 | Batch 150/204 | Current Loss: 0.100129\n[Training] Epoch 067 | Batch 160/204 | Current Loss: 0.100168\n[Training] Epoch 067 | Batch 170/204 | Current Loss: 0.100185\n[Training] Epoch 067 | Batch 180/204 | Current Loss: 0.100111\n[Training] Epoch 067 | Batch 190/204 | Current Loss: 0.100131\n[Training] Epoch 067 | Batch 200/204 | Current Loss: 0.100264\n\n[Training] Epoch 067 Summary:\n  Avg Loss: 0.100187\n  Current LR: 1.16e-05\n[EarlyStopping] Loss improved (0.100195 → 0.100187). Saving model...\n[Training] Epoch 068 | Batch 000/204 | Current Loss: 0.100159\n[Training] Epoch 068 | Batch 010/204 | Current Loss: 0.100106\n[Training] Epoch 068 | Batch 020/204 | Current Loss: 0.100125\n[Training] Epoch 068 | Batch 030/204 | Current Loss: 0.100192\n[Training] Epoch 068 | Batch 040/204 | Current Loss: 0.100099\n[Training] Epoch 068 | Batch 050/204 | Current Loss: 0.100114\n[Training] Epoch 068 | Batch 060/204 | Current Loss: 0.100238\n[Training] Epoch 068 | Batch 070/204 | Current Loss: 0.100105\n[Training] Epoch 068 | Batch 080/204 | Current Loss: 0.100144\n[Training] Epoch 068 | Batch 090/204 | Current Loss: 0.100199\n[Training] Epoch 068 | Batch 100/204 | Current Loss: 0.100212\n[Training] Epoch 068 | Batch 110/204 | Current Loss: 0.100205\n[Training] Epoch 068 | Batch 120/204 | Current Loss: 0.100102\n[Training] Epoch 068 | Batch 130/204 | Current Loss: 0.100194\n[Training] Epoch 068 | Batch 140/204 | Current Loss: 0.100189\n[Training] Epoch 068 | Batch 150/204 | Current Loss: 0.100167\n[Training] Epoch 068 | Batch 160/204 | Current Loss: 0.100138\n[Training] Epoch 068 | Batch 170/204 | Current Loss: 0.100095\n[Training] Epoch 068 | Batch 180/204 | Current Loss: 0.100181\n[Training] Epoch 068 | Batch 190/204 | Current Loss: 0.100287\n[Training] Epoch 068 | Batch 200/204 | Current Loss: 0.100182\n\n[Training] Epoch 068 Summary:\n  Avg Loss: 0.100182\n  Current LR: 5.93e-06\n[EarlyStopping] Loss improved (0.100187 → 0.100182). Saving model...\n[Training] Epoch 069 | Batch 000/204 | Current Loss: 0.100187\n[Training] Epoch 069 | Batch 010/204 | Current Loss: 0.100188\n[Training] Epoch 069 | Batch 020/204 | Current Loss: 0.100123\n[Training] Epoch 069 | Batch 030/204 | Current Loss: 0.100221\n[Training] Epoch 069 | Batch 040/204 | Current Loss: 0.100185\n[Training] Epoch 069 | Batch 050/204 | Current Loss: 0.100381\n[Training] Epoch 069 | Batch 060/204 | Current Loss: 0.100106\n[Training] Epoch 069 | Batch 070/204 | Current Loss: 0.100179\n[Training] Epoch 069 | Batch 080/204 | Current Loss: 0.100111\n[Training] Epoch 069 | Batch 090/204 | Current Loss: 0.100125\n[Training] Epoch 069 | Batch 100/204 | Current Loss: 0.100208\n[Training] Epoch 069 | Batch 110/204 | Current Loss: 0.100220\n[Training] Epoch 069 | Batch 120/204 | Current Loss: 0.100109\n[Training] Epoch 069 | Batch 130/204 | Current Loss: 0.100122\n[Training] Epoch 069 | Batch 140/204 | Current Loss: 0.100059\n[Training] Epoch 069 | Batch 150/204 | Current Loss: 0.100155\n[Training] Epoch 069 | Batch 160/204 | Current Loss: 0.100180\n[Training] Epoch 069 | Batch 170/204 | Current Loss: 0.100127\n[Training] Epoch 069 | Batch 180/204 | Current Loss: 0.100047\n[Training] Epoch 069 | Batch 190/204 | Current Loss: 0.100130\n[Training] Epoch 069 | Batch 200/204 | Current Loss: 0.100121\n\n[Training] Epoch 069 Summary:\n  Avg Loss: 0.100175\n  Current LR: 2.26e-06\n[EarlyStopping] Loss improved (0.100182 → 0.100175). Saving model...\n[Training] Epoch 070 | Batch 000/204 | Current Loss: 0.100132\n[Training] Epoch 070 | Batch 010/204 | Current Loss: 0.100175\n[Training] Epoch 070 | Batch 020/204 | Current Loss: 0.100110\n[Training] Epoch 070 | Batch 030/204 | Current Loss: 0.100130\n[Training] Epoch 070 | Batch 040/204 | Current Loss: 0.100119\n[Training] Epoch 070 | Batch 050/204 | Current Loss: 0.100210\n[Training] Epoch 070 | Batch 060/204 | Current Loss: 0.100165\n[Training] Epoch 070 | Batch 070/204 | Current Loss: 0.100145\n[Training] Epoch 070 | Batch 080/204 | Current Loss: 0.100203\n[Training] Epoch 070 | Batch 090/204 | Current Loss: 0.100187\n[Training] Epoch 070 | Batch 100/204 | Current Loss: 0.100152\n[Training] Epoch 070 | Batch 110/204 | Current Loss: 0.100177\n[Training] Epoch 070 | Batch 120/204 | Current Loss: 0.100131\n[Training] Epoch 070 | Batch 130/204 | Current Loss: 0.100184\n[Training] Epoch 070 | Batch 140/204 | Current Loss: 0.100094\n[Training] Epoch 070 | Batch 150/204 | Current Loss: 0.100249\n[Training] Epoch 070 | Batch 160/204 | Current Loss: 0.100200\n[Training] Epoch 070 | Batch 170/204 | Current Loss: 0.100080\n[Training] Epoch 070 | Batch 180/204 | Current Loss: 0.100151\n[Training] Epoch 070 | Batch 190/204 | Current Loss: 0.100163\n[Training] Epoch 070 | Batch 200/204 | Current Loss: 0.100027\n\n[Training] Epoch 070 Summary:\n  Avg Loss: 0.100164\n  Current LR: 5.26e-05\n[EarlyStopping] Loss improved (0.100175 → 0.100164). Saving model...\n[Training] Epoch 071 | Batch 000/204 | Current Loss: 0.100135\n[Training] Epoch 071 | Batch 010/204 | Current Loss: 0.100209\n[Training] Epoch 071 | Batch 020/204 | Current Loss: 0.100161\n[Training] Epoch 071 | Batch 030/204 | Current Loss: 0.100213\n[Training] Epoch 071 | Batch 040/204 | Current Loss: 0.100108\n[Training] Epoch 071 | Batch 050/204 | Current Loss: 0.100204\n[Training] Epoch 071 | Batch 060/204 | Current Loss: 0.100175\n[Training] Epoch 071 | Batch 070/204 | Current Loss: 0.100182\n[Training] Epoch 071 | Batch 080/204 | Current Loss: 0.100237\n[Training] Epoch 071 | Batch 090/204 | Current Loss: 0.100162\n[Training] Epoch 071 | Batch 100/204 | Current Loss: 0.100157\n[Training] Epoch 071 | Batch 110/204 | Current Loss: 0.100154\n[Training] Epoch 071 | Batch 120/204 | Current Loss: 0.100192\n[Training] Epoch 071 | Batch 130/204 | Current Loss: 0.100202\n[Training] Epoch 071 | Batch 140/204 | Current Loss: 0.100280\n[Training] Epoch 071 | Batch 150/204 | Current Loss: 0.100181\n[Training] Epoch 071 | Batch 160/204 | Current Loss: 0.100098\n[Training] Epoch 071 | Batch 170/204 | Current Loss: 0.100090\n[Training] Epoch 071 | Batch 180/204 | Current Loss: 0.100225\n[Training] Epoch 071 | Batch 190/204 | Current Loss: 0.100258\n[Training] Epoch 071 | Batch 200/204 | Current Loss: 0.100296\n\n[Training] Epoch 071 Summary:\n  Avg Loss: 0.100193\n  Current LR: 5.13e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 072 | Batch 000/204 | Current Loss: 0.100311\n[Training] Epoch 072 | Batch 010/204 | Current Loss: 0.100198\n[Training] Epoch 072 | Batch 020/204 | Current Loss: 0.100106\n[Training] Epoch 072 | Batch 030/204 | Current Loss: 0.100177\n[Training] Epoch 072 | Batch 040/204 | Current Loss: 0.100206\n[Training] Epoch 072 | Batch 050/204 | Current Loss: 0.100178\n[Training] Epoch 072 | Batch 060/204 | Current Loss: 0.100157\n[Training] Epoch 072 | Batch 070/204 | Current Loss: 0.100187\n[Training] Epoch 072 | Batch 080/204 | Current Loss: 0.100143\n[Training] Epoch 072 | Batch 090/204 | Current Loss: 0.100203\n[Training] Epoch 072 | Batch 100/204 | Current Loss: 0.100181\n[Training] Epoch 072 | Batch 110/204 | Current Loss: 0.100135\n[Training] Epoch 072 | Batch 120/204 | Current Loss: 0.100111\n[Training] Epoch 072 | Batch 130/204 | Current Loss: 0.100151\n[Training] Epoch 072 | Batch 140/204 | Current Loss: 0.100223\n[Training] Epoch 072 | Batch 150/204 | Current Loss: 0.100192\n[Training] Epoch 072 | Batch 160/204 | Current Loss: 0.100088\n[Training] Epoch 072 | Batch 170/204 | Current Loss: 0.100163\n[Training] Epoch 072 | Batch 180/204 | Current Loss: 0.100074\n[Training] Epoch 072 | Batch 190/204 | Current Loss: 0.100240\n[Training] Epoch 072 | Batch 200/204 | Current Loss: 0.100239\n\n[Training] Epoch 072 Summary:\n  Avg Loss: 0.100193\n  Current LR: 4.77e-05\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 073 | Batch 000/204 | Current Loss: 0.100171\n[Training] Epoch 073 | Batch 010/204 | Current Loss: 0.100176\n[Training] Epoch 073 | Batch 020/204 | Current Loss: 0.100205\n[Training] Epoch 073 | Batch 030/204 | Current Loss: 0.100203\n[Training] Epoch 073 | Batch 040/204 | Current Loss: 0.100049\n[Training] Epoch 073 | Batch 050/204 | Current Loss: 0.100241\n[Training] Epoch 073 | Batch 060/204 | Current Loss: 0.100135\n[Training] Epoch 073 | Batch 070/204 | Current Loss: 0.100165\n[Training] Epoch 073 | Batch 080/204 | Current Loss: 0.100197\n[Training] Epoch 073 | Batch 090/204 | Current Loss: 0.100194\n[Training] Epoch 073 | Batch 100/204 | Current Loss: 0.100148\n[Training] Epoch 073 | Batch 110/204 | Current Loss: 0.100150\n[Training] Epoch 073 | Batch 120/204 | Current Loss: 0.100178\n[Training] Epoch 073 | Batch 130/204 | Current Loss: 0.100149\n[Training] Epoch 073 | Batch 140/204 | Current Loss: 0.100084\n[Training] Epoch 073 | Batch 150/204 | Current Loss: 0.100191\n[Training] Epoch 073 | Batch 160/204 | Current Loss: 0.100217\n[Training] Epoch 073 | Batch 170/204 | Current Loss: 0.100178\n[Training] Epoch 073 | Batch 180/204 | Current Loss: 0.100218\n[Training] Epoch 073 | Batch 190/204 | Current Loss: 0.100163\n[Training] Epoch 073 | Batch 200/204 | Current Loss: 0.100105\n\n[Training] Epoch 073 Summary:\n  Avg Loss: 0.100177\n  Current LR: 4.20e-05\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 074 | Batch 000/204 | Current Loss: 0.100166\n[Training] Epoch 074 | Batch 010/204 | Current Loss: 0.100195\n[Training] Epoch 074 | Batch 020/204 | Current Loss: 0.100128\n[Training] Epoch 074 | Batch 030/204 | Current Loss: 0.100203\n[Training] Epoch 074 | Batch 040/204 | Current Loss: 0.100125\n[Training] Epoch 074 | Batch 050/204 | Current Loss: 0.100223\n[Training] Epoch 074 | Batch 060/204 | Current Loss: 0.100335\n[Training] Epoch 074 | Batch 070/204 | Current Loss: 0.100167\n[Training] Epoch 074 | Batch 080/204 | Current Loss: 0.100116\n[Training] Epoch 074 | Batch 090/204 | Current Loss: 0.100143\n[Training] Epoch 074 | Batch 100/204 | Current Loss: 0.100219\n[Training] Epoch 074 | Batch 110/204 | Current Loss: 0.100071\n[Training] Epoch 074 | Batch 120/204 | Current Loss: 0.100224\n[Training] Epoch 074 | Batch 130/204 | Current Loss: 0.100105\n[Training] Epoch 074 | Batch 140/204 | Current Loss: 0.100264\n[Training] Epoch 074 | Batch 150/204 | Current Loss: 0.100251\n[Training] Epoch 074 | Batch 160/204 | Current Loss: 0.100122\n[Training] Epoch 074 | Batch 170/204 | Current Loss: 0.100110\n[Training] Epoch 074 | Batch 180/204 | Current Loss: 0.100211\n[Training] Epoch 074 | Batch 190/204 | Current Loss: 0.100224\n[Training] Epoch 074 | Batch 200/204 | Current Loss: 0.100177\n\n[Training] Epoch 074 Summary:\n  Avg Loss: 0.100165\n  Current LR: 3.48e-05\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 075 | Batch 000/204 | Current Loss: 0.100200\n[Training] Epoch 075 | Batch 010/204 | Current Loss: 0.100107\n[Training] Epoch 075 | Batch 020/204 | Current Loss: 0.100162\n[Training] Epoch 075 | Batch 030/204 | Current Loss: 0.100167\n[Training] Epoch 075 | Batch 040/204 | Current Loss: 0.100112\n[Training] Epoch 075 | Batch 050/204 | Current Loss: 0.100099\n[Training] Epoch 075 | Batch 060/204 | Current Loss: 0.100092\n[Training] Epoch 075 | Batch 070/204 | Current Loss: 0.100172\n[Training] Epoch 075 | Batch 080/204 | Current Loss: 0.100268\n[Training] Epoch 075 | Batch 090/204 | Current Loss: 0.100178\n[Training] Epoch 075 | Batch 100/204 | Current Loss: 0.100249\n[Training] Epoch 075 | Batch 110/204 | Current Loss: 0.100187\n[Training] Epoch 075 | Batch 120/204 | Current Loss: 0.100142\n[Training] Epoch 075 | Batch 130/204 | Current Loss: 0.100170\n[Training] Epoch 075 | Batch 140/204 | Current Loss: 0.100113\n[Training] Epoch 075 | Batch 150/204 | Current Loss: 0.100213\n[Training] Epoch 075 | Batch 160/204 | Current Loss: 0.100243\n[Training] Epoch 075 | Batch 170/204 | Current Loss: 0.100153\n[Training] Epoch 075 | Batch 180/204 | Current Loss: 0.100226\n[Training] Epoch 075 | Batch 190/204 | Current Loss: 0.100093\n[Training] Epoch 075 | Batch 200/204 | Current Loss: 0.100148\n\n[Training] Epoch 075 Summary:\n  Avg Loss: 0.100162\n  Current LR: 2.68e-05\n[EarlyStopping] Loss improved (0.100164 → 0.100162). Saving model...\n[Training] Epoch 076 | Batch 000/204 | Current Loss: 0.100203\n[Training] Epoch 076 | Batch 010/204 | Current Loss: 0.100181\n[Training] Epoch 076 | Batch 020/204 | Current Loss: 0.100256\n[Training] Epoch 076 | Batch 030/204 | Current Loss: 0.100129\n[Training] Epoch 076 | Batch 040/204 | Current Loss: 0.100004\n[Training] Epoch 076 | Batch 050/204 | Current Loss: 0.100172\n[Training] Epoch 076 | Batch 060/204 | Current Loss: 0.100171\n[Training] Epoch 076 | Batch 070/204 | Current Loss: 0.100157\n[Training] Epoch 076 | Batch 080/204 | Current Loss: 0.100108\n[Training] Epoch 076 | Batch 090/204 | Current Loss: 0.100147\n[Training] Epoch 076 | Batch 100/204 | Current Loss: 0.100198\n[Training] Epoch 076 | Batch 110/204 | Current Loss: 0.100153\n[Training] Epoch 076 | Batch 120/204 | Current Loss: 0.100156\n[Training] Epoch 076 | Batch 130/204 | Current Loss: 0.100209\n[Training] Epoch 076 | Batch 140/204 | Current Loss: 0.100013\n[Training] Epoch 076 | Batch 150/204 | Current Loss: 0.100168\n[Training] Epoch 076 | Batch 160/204 | Current Loss: 0.100232\n[Training] Epoch 076 | Batch 170/204 | Current Loss: 0.100084\n[Training] Epoch 076 | Batch 180/204 | Current Loss: 0.100122\n[Training] Epoch 076 | Batch 190/204 | Current Loss: 0.100016\n[Training] Epoch 076 | Batch 200/204 | Current Loss: 0.100205\n\n[Training] Epoch 076 Summary:\n  Avg Loss: 0.100158\n  Current LR: 1.88e-05\n[EarlyStopping] Loss improved (0.100162 → 0.100158). Saving model...\n[Training] Epoch 077 | Batch 000/204 | Current Loss: 0.100192\n[Training] Epoch 077 | Batch 010/204 | Current Loss: 0.100069\n[Training] Epoch 077 | Batch 020/204 | Current Loss: 0.100134\n[Training] Epoch 077 | Batch 030/204 | Current Loss: 0.100187\n[Training] Epoch 077 | Batch 040/204 | Current Loss: 0.100089\n[Training] Epoch 077 | Batch 050/204 | Current Loss: 0.100061\n[Training] Epoch 077 | Batch 060/204 | Current Loss: 0.100170\n[Training] Epoch 077 | Batch 070/204 | Current Loss: 0.100141\n[Training] Epoch 077 | Batch 080/204 | Current Loss: 0.100088\n[Training] Epoch 077 | Batch 090/204 | Current Loss: 0.100095\n[Training] Epoch 077 | Batch 100/204 | Current Loss: 0.100082\n[Training] Epoch 077 | Batch 110/204 | Current Loss: 0.100182\n[Training] Epoch 077 | Batch 120/204 | Current Loss: 0.100239\n[Training] Epoch 077 | Batch 130/204 | Current Loss: 0.100161\n[Training] Epoch 077 | Batch 140/204 | Current Loss: 0.100143\n[Training] Epoch 077 | Batch 150/204 | Current Loss: 0.100140\n[Training] Epoch 077 | Batch 160/204 | Current Loss: 0.100181\n[Training] Epoch 077 | Batch 170/204 | Current Loss: 0.100129\n[Training] Epoch 077 | Batch 180/204 | Current Loss: 0.100144\n[Training] Epoch 077 | Batch 190/204 | Current Loss: 0.100097\n[Training] Epoch 077 | Batch 200/204 | Current Loss: 0.100164\n\n[Training] Epoch 077 Summary:\n  Avg Loss: 0.100151\n  Current LR: 1.16e-05\n[EarlyStopping] Loss improved (0.100158 → 0.100151). Saving model...\n[Training] Epoch 078 | Batch 000/204 | Current Loss: 0.100156\n[Training] Epoch 078 | Batch 010/204 | Current Loss: 0.100229\n[Training] Epoch 078 | Batch 020/204 | Current Loss: 0.100052\n[Training] Epoch 078 | Batch 030/204 | Current Loss: 0.100186\n[Training] Epoch 078 | Batch 040/204 | Current Loss: 0.100048\n[Training] Epoch 078 | Batch 050/204 | Current Loss: 0.100125\n[Training] Epoch 078 | Batch 060/204 | Current Loss: 0.100187\n[Training] Epoch 078 | Batch 070/204 | Current Loss: 0.100131\n[Training] Epoch 078 | Batch 080/204 | Current Loss: 0.100122\n[Training] Epoch 078 | Batch 090/204 | Current Loss: 0.100166\n[Training] Epoch 078 | Batch 100/204 | Current Loss: 0.100128\n[Training] Epoch 078 | Batch 110/204 | Current Loss: 0.100154\n[Training] Epoch 078 | Batch 120/204 | Current Loss: 0.100114\n[Training] Epoch 078 | Batch 130/204 | Current Loss: 0.100170\n[Training] Epoch 078 | Batch 140/204 | Current Loss: 0.100235\n[Training] Epoch 078 | Batch 150/204 | Current Loss: 0.100218\n[Training] Epoch 078 | Batch 160/204 | Current Loss: 0.100181\n[Training] Epoch 078 | Batch 170/204 | Current Loss: 0.100193\n[Training] Epoch 078 | Batch 180/204 | Current Loss: 0.100185\n[Training] Epoch 078 | Batch 190/204 | Current Loss: 0.100154\n[Training] Epoch 078 | Batch 200/204 | Current Loss: 0.100173\n\n[Training] Epoch 078 Summary:\n  Avg Loss: 0.100143\n  Current LR: 5.93e-06\n[EarlyStopping] Loss improved (0.100151 → 0.100143). Saving model...\n[Training] Epoch 079 | Batch 000/204 | Current Loss: 0.100106\n[Training] Epoch 079 | Batch 010/204 | Current Loss: 0.100005\n[Training] Epoch 079 | Batch 020/204 | Current Loss: 0.100157\n[Training] Epoch 079 | Batch 030/204 | Current Loss: 0.100084\n[Training] Epoch 079 | Batch 040/204 | Current Loss: 0.100114\n[Training] Epoch 079 | Batch 050/204 | Current Loss: 0.100193\n[Training] Epoch 079 | Batch 060/204 | Current Loss: 0.100122\n[Training] Epoch 079 | Batch 070/204 | Current Loss: 0.100223\n[Training] Epoch 079 | Batch 080/204 | Current Loss: 0.100122\n[Training] Epoch 079 | Batch 090/204 | Current Loss: 0.100231\n[Training] Epoch 079 | Batch 100/204 | Current Loss: 0.100201\n[Training] Epoch 079 | Batch 110/204 | Current Loss: 0.100156\n[Training] Epoch 079 | Batch 120/204 | Current Loss: 0.100056\n[Training] Epoch 079 | Batch 130/204 | Current Loss: 0.100177\n[Training] Epoch 079 | Batch 140/204 | Current Loss: 0.100203\n[Training] Epoch 079 | Batch 150/204 | Current Loss: 0.100094\n[Training] Epoch 079 | Batch 160/204 | Current Loss: 0.100080\n[Training] Epoch 079 | Batch 170/204 | Current Loss: 0.100298\n[Training] Epoch 079 | Batch 180/204 | Current Loss: 0.100082\n[Training] Epoch 079 | Batch 190/204 | Current Loss: 0.100175\n[Training] Epoch 079 | Batch 200/204 | Current Loss: 0.100085\n\n[Training] Epoch 079 Summary:\n  Avg Loss: 0.100149\n  Current LR: 2.26e-06\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 080 | Batch 000/204 | Current Loss: 0.100226\n[Training] Epoch 080 | Batch 010/204 | Current Loss: 0.100164\n[Training] Epoch 080 | Batch 020/204 | Current Loss: 0.100151\n[Training] Epoch 080 | Batch 030/204 | Current Loss: 0.100016\n[Training] Epoch 080 | Batch 040/204 | Current Loss: 0.100145\n[Training] Epoch 080 | Batch 050/204 | Current Loss: 0.100180\n[Training] Epoch 080 | Batch 060/204 | Current Loss: 0.100184\n[Training] Epoch 080 | Batch 070/204 | Current Loss: 0.100236\n[Training] Epoch 080 | Batch 080/204 | Current Loss: 0.099988\n[Training] Epoch 080 | Batch 090/204 | Current Loss: 0.100200\n[Training] Epoch 080 | Batch 100/204 | Current Loss: 0.100110\n[Training] Epoch 080 | Batch 110/204 | Current Loss: 0.100108\n[Training] Epoch 080 | Batch 120/204 | Current Loss: 0.100154\n[Training] Epoch 080 | Batch 130/204 | Current Loss: 0.100181\n[Training] Epoch 080 | Batch 140/204 | Current Loss: 0.100036\n[Training] Epoch 080 | Batch 150/204 | Current Loss: 0.100050\n[Training] Epoch 080 | Batch 160/204 | Current Loss: 0.100115\n[Training] Epoch 080 | Batch 170/204 | Current Loss: 0.100213\n[Training] Epoch 080 | Batch 180/204 | Current Loss: 0.100147\n[Training] Epoch 080 | Batch 190/204 | Current Loss: 0.100117\n[Training] Epoch 080 | Batch 200/204 | Current Loss: 0.100199\n\n[Training] Epoch 080 Summary:\n  Avg Loss: 0.100140\n  Current LR: 5.26e-05\n[EarlyStopping] Loss improved (0.100143 → 0.100140). Saving model...\n[Training] Epoch 081 | Batch 000/204 | Current Loss: 0.100228\n[Training] Epoch 081 | Batch 010/204 | Current Loss: 0.100217\n[Training] Epoch 081 | Batch 020/204 | Current Loss: 0.100142\n[Training] Epoch 081 | Batch 030/204 | Current Loss: 0.100194\n[Training] Epoch 081 | Batch 040/204 | Current Loss: 0.100194\n[Training] Epoch 081 | Batch 050/204 | Current Loss: 0.100221\n[Training] Epoch 081 | Batch 060/204 | Current Loss: 0.100118\n[Training] Epoch 081 | Batch 070/204 | Current Loss: 0.100287\n[Training] Epoch 081 | Batch 080/204 | Current Loss: 0.100267\n[Training] Epoch 081 | Batch 090/204 | Current Loss: 0.100152\n[Training] Epoch 081 | Batch 100/204 | Current Loss: 0.100102\n[Training] Epoch 081 | Batch 110/204 | Current Loss: 0.100070\n[Training] Epoch 081 | Batch 120/204 | Current Loss: 0.100141\n[Training] Epoch 081 | Batch 130/204 | Current Loss: 0.100173\n[Training] Epoch 081 | Batch 140/204 | Current Loss: 0.100173\n[Training] Epoch 081 | Batch 150/204 | Current Loss: 0.100244\n[Training] Epoch 081 | Batch 160/204 | Current Loss: 0.100211\n[Training] Epoch 081 | Batch 170/204 | Current Loss: 0.100218\n[Training] Epoch 081 | Batch 180/204 | Current Loss: 0.100160\n[Training] Epoch 081 | Batch 190/204 | Current Loss: 0.100125\n[Training] Epoch 081 | Batch 200/204 | Current Loss: 0.100151\n\n[Training] Epoch 081 Summary:\n  Avg Loss: 0.100159\n  Current LR: 5.13e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 082 | Batch 000/204 | Current Loss: 0.100152\n[Training] Epoch 082 | Batch 010/204 | Current Loss: 0.100083\n[Training] Epoch 082 | Batch 020/204 | Current Loss: 0.100232\n[Training] Epoch 082 | Batch 030/204 | Current Loss: 0.100151\n[Training] Epoch 082 | Batch 040/204 | Current Loss: 0.100168\n[Training] Epoch 082 | Batch 050/204 | Current Loss: 0.100128\n[Training] Epoch 082 | Batch 060/204 | Current Loss: 0.100238\n[Training] Epoch 082 | Batch 070/204 | Current Loss: 0.100174\n[Training] Epoch 082 | Batch 080/204 | Current Loss: 0.100151\n[Training] Epoch 082 | Batch 090/204 | Current Loss: 0.100131\n[Training] Epoch 082 | Batch 100/204 | Current Loss: 0.100133\n[Training] Epoch 082 | Batch 110/204 | Current Loss: 0.100258\n[Training] Epoch 082 | Batch 120/204 | Current Loss: 0.100130\n[Training] Epoch 082 | Batch 130/204 | Current Loss: 0.100164\n[Training] Epoch 082 | Batch 140/204 | Current Loss: 0.100102\n[Training] Epoch 082 | Batch 150/204 | Current Loss: 0.100167\n[Training] Epoch 082 | Batch 160/204 | Current Loss: 0.100185\n[Training] Epoch 082 | Batch 170/204 | Current Loss: 0.100087\n[Training] Epoch 082 | Batch 180/204 | Current Loss: 0.100166\n[Training] Epoch 082 | Batch 190/204 | Current Loss: 0.100135\n[Training] Epoch 082 | Batch 200/204 | Current Loss: 0.100218\n\n[Training] Epoch 082 Summary:\n  Avg Loss: 0.100154\n  Current LR: 4.77e-05\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 083 | Batch 000/204 | Current Loss: 0.100129\n[Training] Epoch 083 | Batch 010/204 | Current Loss: 0.100073\n[Training] Epoch 083 | Batch 020/204 | Current Loss: 0.100045\n[Training] Epoch 083 | Batch 030/204 | Current Loss: 0.100225\n[Training] Epoch 083 | Batch 040/204 | Current Loss: 0.100116\n[Training] Epoch 083 | Batch 050/204 | Current Loss: 0.100156\n[Training] Epoch 083 | Batch 060/204 | Current Loss: 0.100208\n[Training] Epoch 083 | Batch 070/204 | Current Loss: 0.100112\n[Training] Epoch 083 | Batch 080/204 | Current Loss: 0.100005\n[Training] Epoch 083 | Batch 090/204 | Current Loss: 0.100039\n[Training] Epoch 083 | Batch 100/204 | Current Loss: 0.100128\n[Training] Epoch 083 | Batch 110/204 | Current Loss: 0.100174\n[Training] Epoch 083 | Batch 120/204 | Current Loss: 0.100048\n[Training] Epoch 083 | Batch 130/204 | Current Loss: 0.100031\n[Training] Epoch 083 | Batch 140/204 | Current Loss: 0.100069\n[Training] Epoch 083 | Batch 150/204 | Current Loss: 0.100134\n[Training] Epoch 083 | Batch 160/204 | Current Loss: 0.100099\n[Training] Epoch 083 | Batch 170/204 | Current Loss: 0.100236\n[Training] Epoch 083 | Batch 180/204 | Current Loss: 0.100040\n[Training] Epoch 083 | Batch 190/204 | Current Loss: 0.100108\n[Training] Epoch 083 | Batch 200/204 | Current Loss: 0.100188\n\n[Training] Epoch 083 Summary:\n  Avg Loss: 0.100146\n  Current LR: 4.20e-05\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 084 | Batch 000/204 | Current Loss: 0.100116\n[Training] Epoch 084 | Batch 010/204 | Current Loss: 0.100088\n[Training] Epoch 084 | Batch 020/204 | Current Loss: 0.100168\n[Training] Epoch 084 | Batch 030/204 | Current Loss: 0.100135\n[Training] Epoch 084 | Batch 040/204 | Current Loss: 0.100141\n[Training] Epoch 084 | Batch 050/204 | Current Loss: 0.100099\n[Training] Epoch 084 | Batch 060/204 | Current Loss: 0.100158\n[Training] Epoch 084 | Batch 070/204 | Current Loss: 0.100151\n[Training] Epoch 084 | Batch 080/204 | Current Loss: 0.100075\n[Training] Epoch 084 | Batch 090/204 | Current Loss: 0.100155\n[Training] Epoch 084 | Batch 100/204 | Current Loss: 0.100064\n[Training] Epoch 084 | Batch 110/204 | Current Loss: 0.100181\n[Training] Epoch 084 | Batch 120/204 | Current Loss: 0.100098\n[Training] Epoch 084 | Batch 130/204 | Current Loss: 0.100119\n[Training] Epoch 084 | Batch 140/204 | Current Loss: 0.100199\n[Training] Epoch 084 | Batch 150/204 | Current Loss: 0.100177\n[Training] Epoch 084 | Batch 160/204 | Current Loss: 0.100222\n[Training] Epoch 084 | Batch 170/204 | Current Loss: 0.100167\n[Training] Epoch 084 | Batch 180/204 | Current Loss: 0.100159\n[Training] Epoch 084 | Batch 190/204 | Current Loss: 0.100132\n[Training] Epoch 084 | Batch 200/204 | Current Loss: 0.100191\n\n[Training] Epoch 084 Summary:\n  Avg Loss: 0.100141\n  Current LR: 3.48e-05\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 085 | Batch 000/204 | Current Loss: 0.100119\n[Training] Epoch 085 | Batch 010/204 | Current Loss: 0.100114\n[Training] Epoch 085 | Batch 020/204 | Current Loss: 0.100141\n[Training] Epoch 085 | Batch 030/204 | Current Loss: 0.100197\n[Training] Epoch 085 | Batch 040/204 | Current Loss: 0.100087\n[Training] Epoch 085 | Batch 050/204 | Current Loss: 0.100170\n[Training] Epoch 085 | Batch 060/204 | Current Loss: 0.100078\n[Training] Epoch 085 | Batch 070/204 | Current Loss: 0.100138\n[Training] Epoch 085 | Batch 080/204 | Current Loss: 0.100087\n[Training] Epoch 085 | Batch 090/204 | Current Loss: 0.100117\n[Training] Epoch 085 | Batch 100/204 | Current Loss: 0.100086\n[Training] Epoch 085 | Batch 110/204 | Current Loss: 0.100130\n[Training] Epoch 085 | Batch 120/204 | Current Loss: 0.100093\n[Training] Epoch 085 | Batch 130/204 | Current Loss: 0.100189\n[Training] Epoch 085 | Batch 140/204 | Current Loss: 0.100095\n[Training] Epoch 085 | Batch 150/204 | Current Loss: 0.100146\n[Training] Epoch 085 | Batch 160/204 | Current Loss: 0.100085\n[Training] Epoch 085 | Batch 170/204 | Current Loss: 0.100123\n[Training] Epoch 085 | Batch 180/204 | Current Loss: 0.100202\n[Training] Epoch 085 | Batch 190/204 | Current Loss: 0.100182\n[Training] Epoch 085 | Batch 200/204 | Current Loss: 0.100266\n\n[Training] Epoch 085 Summary:\n  Avg Loss: 0.100138\n  Current LR: 2.68e-05\n[EarlyStopping] Loss improved (0.100140 → 0.100138). Saving model...\n[Training] Epoch 086 | Batch 000/204 | Current Loss: 0.100200\n[Training] Epoch 086 | Batch 010/204 | Current Loss: 0.100167\n[Training] Epoch 086 | Batch 020/204 | Current Loss: 0.100236\n[Training] Epoch 086 | Batch 030/204 | Current Loss: 0.100257\n[Training] Epoch 086 | Batch 040/204 | Current Loss: 0.100144\n[Training] Epoch 086 | Batch 050/204 | Current Loss: 0.100089\n[Training] Epoch 086 | Batch 060/204 | Current Loss: 0.100265\n[Training] Epoch 086 | Batch 070/204 | Current Loss: 0.100219\n[Training] Epoch 086 | Batch 080/204 | Current Loss: 0.100141\n[Training] Epoch 086 | Batch 090/204 | Current Loss: 0.100102\n[Training] Epoch 086 | Batch 100/204 | Current Loss: 0.100025\n[Training] Epoch 086 | Batch 110/204 | Current Loss: 0.100221\n[Training] Epoch 086 | Batch 120/204 | Current Loss: 0.100162\n[Training] Epoch 086 | Batch 130/204 | Current Loss: 0.100108\n[Training] Epoch 086 | Batch 140/204 | Current Loss: 0.100222\n[Training] Epoch 086 | Batch 150/204 | Current Loss: 0.100165\n[Training] Epoch 086 | Batch 160/204 | Current Loss: 0.100136\n[Training] Epoch 086 | Batch 170/204 | Current Loss: 0.100080\n[Training] Epoch 086 | Batch 180/204 | Current Loss: 0.100237\n[Training] Epoch 086 | Batch 190/204 | Current Loss: 0.100164\n[Training] Epoch 086 | Batch 200/204 | Current Loss: 0.100018\n\n[Training] Epoch 086 Summary:\n  Avg Loss: 0.100133\n  Current LR: 1.88e-05\n[EarlyStopping] Loss improved (0.100138 → 0.100133). Saving model...\n[Training] Epoch 087 | Batch 000/204 | Current Loss: 0.100245\n[Training] Epoch 087 | Batch 010/204 | Current Loss: 0.100063\n[Training] Epoch 087 | Batch 020/204 | Current Loss: 0.100071\n[Training] Epoch 087 | Batch 030/204 | Current Loss: 0.100105\n[Training] Epoch 087 | Batch 040/204 | Current Loss: 0.100174\n[Training] Epoch 087 | Batch 050/204 | Current Loss: 0.100115\n[Training] Epoch 087 | Batch 060/204 | Current Loss: 0.100202\n[Training] Epoch 087 | Batch 070/204 | Current Loss: 0.100113\n[Training] Epoch 087 | Batch 080/204 | Current Loss: 0.100086\n[Training] Epoch 087 | Batch 090/204 | Current Loss: 0.100052\n[Training] Epoch 087 | Batch 100/204 | Current Loss: 0.099936\n[Training] Epoch 087 | Batch 110/204 | Current Loss: 0.100046\n[Training] Epoch 087 | Batch 120/204 | Current Loss: 0.100123\n[Training] Epoch 087 | Batch 130/204 | Current Loss: 0.100148\n[Training] Epoch 087 | Batch 140/204 | Current Loss: 0.100004\n[Training] Epoch 087 | Batch 150/204 | Current Loss: 0.100107\n[Training] Epoch 087 | Batch 160/204 | Current Loss: 0.100070\n[Training] Epoch 087 | Batch 170/204 | Current Loss: 0.100146\n[Training] Epoch 087 | Batch 180/204 | Current Loss: 0.100116\n[Training] Epoch 087 | Batch 190/204 | Current Loss: 0.100015\n[Training] Epoch 087 | Batch 200/204 | Current Loss: 0.100122\n\n[Training] Epoch 087 Summary:\n  Avg Loss: 0.100127\n  Current LR: 1.16e-05\n[EarlyStopping] Loss improved (0.100133 → 0.100127). Saving model...\n[Training] Epoch 088 | Batch 000/204 | Current Loss: 0.100175\n[Training] Epoch 088 | Batch 010/204 | Current Loss: 0.100025\n[Training] Epoch 088 | Batch 020/204 | Current Loss: 0.100156\n[Training] Epoch 088 | Batch 030/204 | Current Loss: 0.100060\n[Training] Epoch 088 | Batch 040/204 | Current Loss: 0.100086\n[Training] Epoch 088 | Batch 050/204 | Current Loss: 0.100186\n[Training] Epoch 088 | Batch 060/204 | Current Loss: 0.100089\n[Training] Epoch 088 | Batch 070/204 | Current Loss: 0.100075\n[Training] Epoch 088 | Batch 080/204 | Current Loss: 0.100097\n[Training] Epoch 088 | Batch 090/204 | Current Loss: 0.100170\n[Training] Epoch 088 | Batch 100/204 | Current Loss: 0.100153\n[Training] Epoch 088 | Batch 110/204 | Current Loss: 0.100101\n[Training] Epoch 088 | Batch 120/204 | Current Loss: 0.100096\n[Training] Epoch 088 | Batch 130/204 | Current Loss: 0.100062\n[Training] Epoch 088 | Batch 140/204 | Current Loss: 0.100175\n[Training] Epoch 088 | Batch 150/204 | Current Loss: 0.100123\n[Training] Epoch 088 | Batch 160/204 | Current Loss: 0.100099\n[Training] Epoch 088 | Batch 170/204 | Current Loss: 0.100043\n[Training] Epoch 088 | Batch 180/204 | Current Loss: 0.100181\n[Training] Epoch 088 | Batch 190/204 | Current Loss: 0.100124\n[Training] Epoch 088 | Batch 200/204 | Current Loss: 0.100079\n\n[Training] Epoch 088 Summary:\n  Avg Loss: 0.100128\n  Current LR: 5.93e-06\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 089 | Batch 000/204 | Current Loss: 0.100095\n[Training] Epoch 089 | Batch 010/204 | Current Loss: 0.100352\n[Training] Epoch 089 | Batch 020/204 | Current Loss: 0.100170\n[Training] Epoch 089 | Batch 030/204 | Current Loss: 0.100074\n[Training] Epoch 089 | Batch 040/204 | Current Loss: 0.100118\n[Training] Epoch 089 | Batch 050/204 | Current Loss: 0.100085\n[Training] Epoch 089 | Batch 060/204 | Current Loss: 0.100086\n[Training] Epoch 089 | Batch 070/204 | Current Loss: 0.100172\n[Training] Epoch 089 | Batch 080/204 | Current Loss: 0.100022\n[Training] Epoch 089 | Batch 090/204 | Current Loss: 0.100017\n[Training] Epoch 089 | Batch 100/204 | Current Loss: 0.100126\n[Training] Epoch 089 | Batch 110/204 | Current Loss: 0.100119\n[Training] Epoch 089 | Batch 120/204 | Current Loss: 0.100072\n[Training] Epoch 089 | Batch 130/204 | Current Loss: 0.100103\n[Training] Epoch 089 | Batch 140/204 | Current Loss: 0.100186\n[Training] Epoch 089 | Batch 150/204 | Current Loss: 0.100157\n[Training] Epoch 089 | Batch 160/204 | Current Loss: 0.100043\n[Training] Epoch 089 | Batch 170/204 | Current Loss: 0.100171\n[Training] Epoch 089 | Batch 180/204 | Current Loss: 0.100014\n[Training] Epoch 089 | Batch 190/204 | Current Loss: 0.100140\n[Training] Epoch 089 | Batch 200/204 | Current Loss: 0.100075\n\n[Training] Epoch 089 Summary:\n  Avg Loss: 0.100122\n  Current LR: 2.26e-06\n[EarlyStopping] Loss improved (0.100127 → 0.100122). Saving model...\n[Training] Epoch 090 | Batch 000/204 | Current Loss: 0.100014\n[Training] Epoch 090 | Batch 010/204 | Current Loss: 0.100098\n[Training] Epoch 090 | Batch 020/204 | Current Loss: 0.100032\n[Training] Epoch 090 | Batch 030/204 | Current Loss: 0.100095\n[Training] Epoch 090 | Batch 040/204 | Current Loss: 0.100092\n[Training] Epoch 090 | Batch 050/204 | Current Loss: 0.100076\n[Training] Epoch 090 | Batch 060/204 | Current Loss: 0.100207\n[Training] Epoch 090 | Batch 070/204 | Current Loss: 0.100147\n[Training] Epoch 090 | Batch 080/204 | Current Loss: 0.100089\n[Training] Epoch 090 | Batch 090/204 | Current Loss: 0.100115\n[Training] Epoch 090 | Batch 100/204 | Current Loss: 0.100129\n[Training] Epoch 090 | Batch 110/204 | Current Loss: 0.100257\n[Training] Epoch 090 | Batch 120/204 | Current Loss: 0.100122\n[Training] Epoch 090 | Batch 130/204 | Current Loss: 0.100169\n[Training] Epoch 090 | Batch 140/204 | Current Loss: 0.100117\n[Training] Epoch 090 | Batch 150/204 | Current Loss: 0.100037\n[Training] Epoch 090 | Batch 160/204 | Current Loss: 0.100125\n[Training] Epoch 090 | Batch 170/204 | Current Loss: 0.100035\n[Training] Epoch 090 | Batch 180/204 | Current Loss: 0.100147\n[Training] Epoch 090 | Batch 190/204 | Current Loss: 0.100106\n[Training] Epoch 090 | Batch 200/204 | Current Loss: 0.100057\n\n[Training] Epoch 090 Summary:\n  Avg Loss: 0.100117\n  Current LR: 5.26e-05\n[EarlyStopping] Loss improved (0.100122 → 0.100117). Saving model...\n[Training] Epoch 091 | Batch 000/204 | Current Loss: 0.100119\n[Training] Epoch 091 | Batch 010/204 | Current Loss: 0.100118\n[Training] Epoch 091 | Batch 020/204 | Current Loss: 0.100133\n[Training] Epoch 091 | Batch 030/204 | Current Loss: 0.100126\n[Training] Epoch 091 | Batch 040/204 | Current Loss: 0.100119\n[Training] Epoch 091 | Batch 050/204 | Current Loss: 0.100198\n[Training] Epoch 091 | Batch 060/204 | Current Loss: 0.100087\n[Training] Epoch 091 | Batch 070/204 | Current Loss: 0.100086\n[Training] Epoch 091 | Batch 080/204 | Current Loss: 0.100071\n[Training] Epoch 091 | Batch 090/204 | Current Loss: 0.100058\n[Training] Epoch 091 | Batch 100/204 | Current Loss: 0.100090\n[Training] Epoch 091 | Batch 110/204 | Current Loss: 0.100048\n[Training] Epoch 091 | Batch 120/204 | Current Loss: 0.100103\n[Training] Epoch 091 | Batch 130/204 | Current Loss: 0.100089\n[Training] Epoch 091 | Batch 140/204 | Current Loss: 0.100183\n[Training] Epoch 091 | Batch 150/204 | Current Loss: 0.100151\n[Training] Epoch 091 | Batch 160/204 | Current Loss: 0.100196\n[Training] Epoch 091 | Batch 170/204 | Current Loss: 0.100241\n[Training] Epoch 091 | Batch 180/204 | Current Loss: 0.100143\n[Training] Epoch 091 | Batch 190/204 | Current Loss: 0.100079\n[Training] Epoch 091 | Batch 200/204 | Current Loss: 0.100098\n\n[Training] Epoch 091 Summary:\n  Avg Loss: 0.100132\n  Current LR: 5.13e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 092 | Batch 000/204 | Current Loss: 0.100123\n[Training] Epoch 092 | Batch 010/204 | Current Loss: 0.100070\n[Training] Epoch 092 | Batch 020/204 | Current Loss: 0.100115\n[Training] Epoch 092 | Batch 030/204 | Current Loss: 0.100005\n[Training] Epoch 092 | Batch 040/204 | Current Loss: 0.100200\n[Training] Epoch 092 | Batch 050/204 | Current Loss: 0.100159\n[Training] Epoch 092 | Batch 060/204 | Current Loss: 0.100173\n[Training] Epoch 092 | Batch 070/204 | Current Loss: 0.100126\n[Training] Epoch 092 | Batch 080/204 | Current Loss: 0.100149\n[Training] Epoch 092 | Batch 090/204 | Current Loss: 0.100094\n[Training] Epoch 092 | Batch 100/204 | Current Loss: 0.100160\n[Training] Epoch 092 | Batch 110/204 | Current Loss: 0.100117\n[Training] Epoch 092 | Batch 120/204 | Current Loss: 0.100125\n[Training] Epoch 092 | Batch 130/204 | Current Loss: 0.100190\n[Training] Epoch 092 | Batch 140/204 | Current Loss: 0.100041\n[Training] Epoch 092 | Batch 150/204 | Current Loss: 0.100036\n[Training] Epoch 092 | Batch 160/204 | Current Loss: 0.100139\n[Training] Epoch 092 | Batch 170/204 | Current Loss: 0.100074\n[Training] Epoch 092 | Batch 180/204 | Current Loss: 0.100086\n[Training] Epoch 092 | Batch 190/204 | Current Loss: 0.100210\n[Training] Epoch 092 | Batch 200/204 | Current Loss: 0.100182\n\n[Training] Epoch 092 Summary:\n  Avg Loss: 0.100135\n  Current LR: 4.77e-05\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 093 | Batch 000/204 | Current Loss: 0.100125\n[Training] Epoch 093 | Batch 010/204 | Current Loss: 0.100243\n[Training] Epoch 093 | Batch 020/204 | Current Loss: 0.100255\n[Training] Epoch 093 | Batch 030/204 | Current Loss: 0.100103\n[Training] Epoch 093 | Batch 040/204 | Current Loss: 0.100109\n[Training] Epoch 093 | Batch 050/204 | Current Loss: 0.100176\n[Training] Epoch 093 | Batch 060/204 | Current Loss: 0.100147\n[Training] Epoch 093 | Batch 070/204 | Current Loss: 0.100143\n[Training] Epoch 093 | Batch 080/204 | Current Loss: 0.100131\n[Training] Epoch 093 | Batch 090/204 | Current Loss: 0.100091\n[Training] Epoch 093 | Batch 100/204 | Current Loss: 0.100110\n[Training] Epoch 093 | Batch 110/204 | Current Loss: 0.100213\n[Training] Epoch 093 | Batch 120/204 | Current Loss: 0.100144\n[Training] Epoch 093 | Batch 130/204 | Current Loss: 0.100193\n[Training] Epoch 093 | Batch 140/204 | Current Loss: 0.100087\n[Training] Epoch 093 | Batch 150/204 | Current Loss: 0.100128\n[Training] Epoch 093 | Batch 160/204 | Current Loss: 0.100184\n[Training] Epoch 093 | Batch 170/204 | Current Loss: 0.100112\n[Training] Epoch 093 | Batch 180/204 | Current Loss: 0.100073\n[Training] Epoch 093 | Batch 190/204 | Current Loss: 0.100177\n[Training] Epoch 093 | Batch 200/204 | Current Loss: 0.100220\n\n[Training] Epoch 093 Summary:\n  Avg Loss: 0.100137\n  Current LR: 4.20e-05\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 094 | Batch 000/204 | Current Loss: 0.100071\n[Training] Epoch 094 | Batch 010/204 | Current Loss: 0.100202\n[Training] Epoch 094 | Batch 020/204 | Current Loss: 0.100118\n[Training] Epoch 094 | Batch 030/204 | Current Loss: 0.100195\n[Training] Epoch 094 | Batch 040/204 | Current Loss: 0.100171\n[Training] Epoch 094 | Batch 050/204 | Current Loss: 0.100025\n[Training] Epoch 094 | Batch 060/204 | Current Loss: 0.100075\n[Training] Epoch 094 | Batch 070/204 | Current Loss: 0.100173\n[Training] Epoch 094 | Batch 080/204 | Current Loss: 0.100112\n[Training] Epoch 094 | Batch 090/204 | Current Loss: 0.100049\n[Training] Epoch 094 | Batch 100/204 | Current Loss: 0.100281\n[Training] Epoch 094 | Batch 110/204 | Current Loss: 0.100081\n[Training] Epoch 094 | Batch 120/204 | Current Loss: 0.100105\n[Training] Epoch 094 | Batch 130/204 | Current Loss: 0.100101\n[Training] Epoch 094 | Batch 140/204 | Current Loss: 0.100095\n[Training] Epoch 094 | Batch 150/204 | Current Loss: 0.100085\n[Training] Epoch 094 | Batch 160/204 | Current Loss: 0.100179\n[Training] Epoch 094 | Batch 170/204 | Current Loss: 0.100196\n[Training] Epoch 094 | Batch 180/204 | Current Loss: 0.100158\n[Training] Epoch 094 | Batch 190/204 | Current Loss: 0.100113\n[Training] Epoch 094 | Batch 200/204 | Current Loss: 0.100073\n\n[Training] Epoch 094 Summary:\n  Avg Loss: 0.100130\n  Current LR: 3.48e-05\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 095 | Batch 000/204 | Current Loss: 0.100125\n[Training] Epoch 095 | Batch 010/204 | Current Loss: 0.100147\n[Training] Epoch 095 | Batch 020/204 | Current Loss: 0.100120\n[Training] Epoch 095 | Batch 030/204 | Current Loss: 0.100271\n[Training] Epoch 095 | Batch 040/204 | Current Loss: 0.100202\n[Training] Epoch 095 | Batch 050/204 | Current Loss: 0.100099\n[Training] Epoch 095 | Batch 060/204 | Current Loss: 0.099975\n[Training] Epoch 095 | Batch 070/204 | Current Loss: 0.100150\n[Training] Epoch 095 | Batch 080/204 | Current Loss: 0.100163\n[Training] Epoch 095 | Batch 090/204 | Current Loss: 0.100205\n[Training] Epoch 095 | Batch 100/204 | Current Loss: 0.100145\n[Training] Epoch 095 | Batch 110/204 | Current Loss: 0.100118\n[Training] Epoch 095 | Batch 120/204 | Current Loss: 0.100116\n[Training] Epoch 095 | Batch 130/204 | Current Loss: 0.099947\n[Training] Epoch 095 | Batch 140/204 | Current Loss: 0.100110\n[Training] Epoch 095 | Batch 150/204 | Current Loss: 0.100111\n[Training] Epoch 095 | Batch 160/204 | Current Loss: 0.100186\n[Training] Epoch 095 | Batch 170/204 | Current Loss: 0.099986\n[Training] Epoch 095 | Batch 180/204 | Current Loss: 0.100090\n[Training] Epoch 095 | Batch 190/204 | Current Loss: 0.100147\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 15:09:44,465] Trial 1 finished with value: 0.10011662966480442 and parameters: {'lr': 5.2595132291187947e-05, 'batch_size': 64, 'bottleneck_width': 1024, 'dropout_rate': 0.1789694089827214, 'alpha': 0.5576093850912043, 'weight_decay': 4.483978503838243e-06}. Best is trial 1 with value: 0.10011662966480442.\n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 095 | Batch 200/204 | Current Loss: 0.100109\n\n[Training] Epoch 095 Summary:\n  Avg Loss: 0.100119\n  Current LR: 2.68e-05\n[EarlyStopping] Counter: 5/5\n\n[Training] Early stopping triggered\n\n[Optuna] Trial 1 completed with best loss: 0.100117\n\n==================================================\nStarting Optuna Trial 2\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.0005201857972779173\n  batch_size: 256\n  bottleneck_width: 2048\n  dropout_rate: 0.17017258595011986\n  alpha: 0.5489804503264961\n  weight_decay: 8.950090467096336e-06\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 256, Total batches: 51\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 2048\n  - Dropout rate: 0.17017258595011986\n[Model] Architecture initialized successfully\n[Model] Total parameters: 4,495,168\n[Optimizer] Initialized with lr=5.20e-04, weight_decay=8.95e-06\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.5489804503264961\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/051 | Current Loss: 1.357835\n[Training] Epoch 001 | Batch 010/051 | Current Loss: 0.123729\n[Training] Epoch 001 | Batch 020/051 | Current Loss: 0.110834\n[Training] Epoch 001 | Batch 030/051 | Current Loss: 0.107368\n[Training] Epoch 001 | Batch 040/051 | Current Loss: 0.106348\n[Training] Epoch 001 | Batch 050/051 | Current Loss: 0.103899\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.153225\n  Current LR: 5.07e-04\n[EarlyStopping] Loss improved (inf → 0.153225). Saving model...\n[Training] Epoch 002 | Batch 000/051 | Current Loss: 0.103554\n[Training] Epoch 002 | Batch 010/051 | Current Loss: 0.103378\n[Training] Epoch 002 | Batch 020/051 | Current Loss: 0.102810\n[Training] Epoch 002 | Batch 030/051 | Current Loss: 0.105308\n[Training] Epoch 002 | Batch 040/051 | Current Loss: 0.102413\n[Training] Epoch 002 | Batch 050/051 | Current Loss: 0.102649\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.103633\n  Current LR: 4.71e-04\n[EarlyStopping] Loss improved (0.153225 → 0.103633). Saving model...\n[Training] Epoch 003 | Batch 000/051 | Current Loss: 0.103454\n[Training] Epoch 003 | Batch 010/051 | Current Loss: 0.105566\n[Training] Epoch 003 | Batch 020/051 | Current Loss: 0.101988\n[Training] Epoch 003 | Batch 030/051 | Current Loss: 0.102293\n[Training] Epoch 003 | Batch 040/051 | Current Loss: 0.102385\n[Training] Epoch 003 | Batch 050/051 | Current Loss: 0.102404\n\n[Training] Epoch 003 Summary:\n  Avg Loss: 0.102499\n  Current LR: 4.13e-04\n[EarlyStopping] Loss improved (0.103633 → 0.102499). Saving model...\n[Training] Epoch 004 | Batch 000/051 | Current Loss: 0.101759\n[Training] Epoch 004 | Batch 010/051 | Current Loss: 0.101689\n[Training] Epoch 004 | Batch 020/051 | Current Loss: 0.101631\n[Training] Epoch 004 | Batch 030/051 | Current Loss: 0.101619\n[Training] Epoch 004 | Batch 040/051 | Current Loss: 0.101824\n[Training] Epoch 004 | Batch 050/051 | Current Loss: 0.101695\n\n[Training] Epoch 004 Summary:\n  Avg Loss: 0.101768\n  Current LR: 3.41e-04\n[EarlyStopping] Loss improved (0.102499 → 0.101768). Saving model...\n[Training] Epoch 005 | Batch 000/051 | Current Loss: 0.101756\n[Training] Epoch 005 | Batch 010/051 | Current Loss: 0.101343\n[Training] Epoch 005 | Batch 020/051 | Current Loss: 0.101584\n[Training] Epoch 005 | Batch 030/051 | Current Loss: 0.101367\n[Training] Epoch 005 | Batch 040/051 | Current Loss: 0.101305\n[Training] Epoch 005 | Batch 050/051 | Current Loss: 0.101362\n\n[Training] Epoch 005 Summary:\n  Avg Loss: 0.101590\n  Current LR: 2.61e-04\n[EarlyStopping] Loss improved (0.101768 → 0.101590). Saving model...\n[Training] Epoch 006 | Batch 000/051 | Current Loss: 0.101750\n[Training] Epoch 006 | Batch 010/051 | Current Loss: 0.101927\n[Training] Epoch 006 | Batch 020/051 | Current Loss: 0.101306\n[Training] Epoch 006 | Batch 030/051 | Current Loss: 0.101193\n[Training] Epoch 006 | Batch 040/051 | Current Loss: 0.101111\n[Training] Epoch 006 | Batch 050/051 | Current Loss: 0.101187\n\n[Training] Epoch 006 Summary:\n  Avg Loss: 0.101383\n  Current LR: 1.80e-04\n[EarlyStopping] Loss improved (0.101590 → 0.101383). Saving model...\n[Training] Epoch 007 | Batch 000/051 | Current Loss: 0.101188\n[Training] Epoch 007 | Batch 010/051 | Current Loss: 0.101263\n[Training] Epoch 007 | Batch 020/051 | Current Loss: 0.101238\n[Training] Epoch 007 | Batch 030/051 | Current Loss: 0.101654\n[Training] Epoch 007 | Batch 040/051 | Current Loss: 0.101347\n[Training] Epoch 007 | Batch 050/051 | Current Loss: 0.101641\n\n[Training] Epoch 007 Summary:\n  Avg Loss: 0.101273\n  Current LR: 1.08e-04\n[EarlyStopping] Loss improved (0.101383 → 0.101273). Saving model...\n[Training] Epoch 008 | Batch 000/051 | Current Loss: 0.101202\n[Training] Epoch 008 | Batch 010/051 | Current Loss: 0.101335\n[Training] Epoch 008 | Batch 020/051 | Current Loss: 0.101128\n[Training] Epoch 008 | Batch 030/051 | Current Loss: 0.101265\n[Training] Epoch 008 | Batch 040/051 | Current Loss: 0.101081\n[Training] Epoch 008 | Batch 050/051 | Current Loss: 0.101264\n\n[Training] Epoch 008 Summary:\n  Avg Loss: 0.101207\n  Current LR: 5.06e-05\n[EarlyStopping] Loss improved (0.101273 → 0.101207). Saving model...\n[Training] Epoch 009 | Batch 000/051 | Current Loss: 0.101064\n[Training] Epoch 009 | Batch 010/051 | Current Loss: 0.101026\n[Training] Epoch 009 | Batch 020/051 | Current Loss: 0.101012\n[Training] Epoch 009 | Batch 030/051 | Current Loss: 0.101064\n[Training] Epoch 009 | Batch 040/051 | Current Loss: 0.101010\n[Training] Epoch 009 | Batch 050/051 | Current Loss: 0.101211\n\n[Training] Epoch 009 Summary:\n  Avg Loss: 0.101069\n  Current LR: 1.37e-05\n[EarlyStopping] Loss improved (0.101207 → 0.101069). Saving model...\n[Training] Epoch 010 | Batch 000/051 | Current Loss: 0.101027\n[Training] Epoch 010 | Batch 010/051 | Current Loss: 0.101005\n[Training] Epoch 010 | Batch 020/051 | Current Loss: 0.101022\n[Training] Epoch 010 | Batch 030/051 | Current Loss: 0.101043\n[Training] Epoch 010 | Batch 040/051 | Current Loss: 0.100949\n[Training] Epoch 010 | Batch 050/051 | Current Loss: 0.101014\n\n[Training] Epoch 010 Summary:\n  Avg Loss: 0.101053\n  Current LR: 5.20e-04\n[EarlyStopping] Loss improved (0.101069 → 0.101053). Saving model...\n[Training] Epoch 011 | Batch 000/051 | Current Loss: 0.101155\n[Training] Epoch 011 | Batch 010/051 | Current Loss: 0.101043\n[Training] Epoch 011 | Batch 020/051 | Current Loss: 0.101615\n[Training] Epoch 011 | Batch 030/051 | Current Loss: 0.101066\n[Training] Epoch 011 | Batch 040/051 | Current Loss: 0.100926\n[Training] Epoch 011 | Batch 050/051 | Current Loss: 0.101070\n\n[Training] Epoch 011 Summary:\n  Avg Loss: 0.101168\n  Current LR: 5.07e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 012 | Batch 000/051 | Current Loss: 0.100945\n[Training] Epoch 012 | Batch 010/051 | Current Loss: 0.100939\n[Training] Epoch 012 | Batch 020/051 | Current Loss: 0.102250\n[Training] Epoch 012 | Batch 030/051 | Current Loss: 0.101192\n[Training] Epoch 012 | Batch 040/051 | Current Loss: 0.100840\n[Training] Epoch 012 | Batch 050/051 | Current Loss: 0.101299\n\n[Training] Epoch 012 Summary:\n  Avg Loss: 0.101238\n  Current LR: 4.71e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 013 | Batch 000/051 | Current Loss: 0.100819\n[Training] Epoch 013 | Batch 010/051 | Current Loss: 0.101416\n[Training] Epoch 013 | Batch 020/051 | Current Loss: 0.101077\n[Training] Epoch 013 | Batch 030/051 | Current Loss: 0.101277\n[Training] Epoch 013 | Batch 040/051 | Current Loss: 0.101101\n[Training] Epoch 013 | Batch 050/051 | Current Loss: 0.100664\n\n[Training] Epoch 013 Summary:\n  Avg Loss: 0.100988\n  Current LR: 4.13e-04\n[EarlyStopping] Loss improved (0.101053 → 0.100988). Saving model...\n[Training] Epoch 014 | Batch 000/051 | Current Loss: 0.100812\n[Training] Epoch 014 | Batch 010/051 | Current Loss: 0.100692\n[Training] Epoch 014 | Batch 020/051 | Current Loss: 0.100792\n[Training] Epoch 014 | Batch 030/051 | Current Loss: 0.100708\n[Training] Epoch 014 | Batch 040/051 | Current Loss: 0.100723\n[Training] Epoch 014 | Batch 050/051 | Current Loss: 0.100679\n\n[Training] Epoch 014 Summary:\n  Avg Loss: 0.100775\n  Current LR: 3.41e-04\n[EarlyStopping] Loss improved (0.100988 → 0.100775). Saving model...\n[Training] Epoch 015 | Batch 000/051 | Current Loss: 0.100860\n[Training] Epoch 015 | Batch 010/051 | Current Loss: 0.100747\n[Training] Epoch 015 | Batch 020/051 | Current Loss: 0.100577\n[Training] Epoch 015 | Batch 030/051 | Current Loss: 0.100694\n[Training] Epoch 015 | Batch 040/051 | Current Loss: 0.100655\n[Training] Epoch 015 | Batch 050/051 | Current Loss: 0.100595\n\n[Training] Epoch 015 Summary:\n  Avg Loss: 0.100650\n  Current LR: 2.61e-04\n[EarlyStopping] Loss improved (0.100775 → 0.100650). Saving model...\n[Training] Epoch 016 | Batch 000/051 | Current Loss: 0.100623\n[Training] Epoch 016 | Batch 010/051 | Current Loss: 0.100609\n[Training] Epoch 016 | Batch 020/051 | Current Loss: 0.100528\n[Training] Epoch 016 | Batch 030/051 | Current Loss: 0.100610\n[Training] Epoch 016 | Batch 040/051 | Current Loss: 0.100579\n[Training] Epoch 016 | Batch 050/051 | Current Loss: 0.100543\n\n[Training] Epoch 016 Summary:\n  Avg Loss: 0.100584\n  Current LR: 1.80e-04\n[EarlyStopping] Loss improved (0.100650 → 0.100584). Saving model...\n[Training] Epoch 017 | Batch 000/051 | Current Loss: 0.100564\n[Training] Epoch 017 | Batch 010/051 | Current Loss: 0.100543\n[Training] Epoch 017 | Batch 020/051 | Current Loss: 0.100546\n[Training] Epoch 017 | Batch 030/051 | Current Loss: 0.100577\n[Training] Epoch 017 | Batch 040/051 | Current Loss: 0.100568\n[Training] Epoch 017 | Batch 050/051 | Current Loss: 0.100528\n\n[Training] Epoch 017 Summary:\n  Avg Loss: 0.100559\n  Current LR: 1.08e-04\n[EarlyStopping] Loss improved (0.100584 → 0.100559). Saving model...\n[Training] Epoch 018 | Batch 000/051 | Current Loss: 0.100513\n[Training] Epoch 018 | Batch 010/051 | Current Loss: 0.100535\n[Training] Epoch 018 | Batch 020/051 | Current Loss: 0.100582\n[Training] Epoch 018 | Batch 030/051 | Current Loss: 0.100520\n[Training] Epoch 018 | Batch 040/051 | Current Loss: 0.100445\n[Training] Epoch 018 | Batch 050/051 | Current Loss: 0.100489\n\n[Training] Epoch 018 Summary:\n  Avg Loss: 0.100528\n  Current LR: 5.06e-05\n[EarlyStopping] Loss improved (0.100559 → 0.100528). Saving model...\n[Training] Epoch 019 | Batch 000/051 | Current Loss: 0.100504\n[Training] Epoch 019 | Batch 010/051 | Current Loss: 0.100422\n[Training] Epoch 019 | Batch 020/051 | Current Loss: 0.100549\n[Training] Epoch 019 | Batch 030/051 | Current Loss: 0.100483\n[Training] Epoch 019 | Batch 040/051 | Current Loss: 0.100420\n[Training] Epoch 019 | Batch 050/051 | Current Loss: 0.100505\n\n[Training] Epoch 019 Summary:\n  Avg Loss: 0.100500\n  Current LR: 1.37e-05\n[EarlyStopping] Loss improved (0.100528 → 0.100500). Saving model...\n[Training] Epoch 020 | Batch 000/051 | Current Loss: 0.100462\n[Training] Epoch 020 | Batch 010/051 | Current Loss: 0.100547\n[Training] Epoch 020 | Batch 020/051 | Current Loss: 0.100453\n[Training] Epoch 020 | Batch 030/051 | Current Loss: 0.100532\n[Training] Epoch 020 | Batch 040/051 | Current Loss: 0.100537\n[Training] Epoch 020 | Batch 050/051 | Current Loss: 0.100512\n\n[Training] Epoch 020 Summary:\n  Avg Loss: 0.100508\n  Current LR: 5.20e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 021 | Batch 000/051 | Current Loss: 0.100543\n[Training] Epoch 021 | Batch 010/051 | Current Loss: 0.100609\n[Training] Epoch 021 | Batch 020/051 | Current Loss: 0.100598\n[Training] Epoch 021 | Batch 030/051 | Current Loss: 0.100565\n[Training] Epoch 021 | Batch 040/051 | Current Loss: 0.100488\n[Training] Epoch 021 | Batch 050/051 | Current Loss: 0.100496\n\n[Training] Epoch 021 Summary:\n  Avg Loss: 0.100557\n  Current LR: 5.07e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 022 | Batch 000/051 | Current Loss: 0.100485\n[Training] Epoch 022 | Batch 010/051 | Current Loss: 0.100494\n[Training] Epoch 022 | Batch 020/051 | Current Loss: 0.100422\n[Training] Epoch 022 | Batch 030/051 | Current Loss: 0.100436\n[Training] Epoch 022 | Batch 040/051 | Current Loss: 0.100601\n[Training] Epoch 022 | Batch 050/051 | Current Loss: 0.100538\n\n[Training] Epoch 022 Summary:\n  Avg Loss: 0.100562\n  Current LR: 4.71e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 023 | Batch 000/051 | Current Loss: 0.100869\n[Training] Epoch 023 | Batch 010/051 | Current Loss: 0.100420\n[Training] Epoch 023 | Batch 020/051 | Current Loss: 0.100570\n[Training] Epoch 023 | Batch 030/051 | Current Loss: 0.100663\n[Training] Epoch 023 | Batch 040/051 | Current Loss: 0.100575\n[Training] Epoch 023 | Batch 050/051 | Current Loss: 0.100544\n\n[Training] Epoch 023 Summary:\n  Avg Loss: 0.100539\n  Current LR: 4.13e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 024 | Batch 000/051 | Current Loss: 0.100403\n[Training] Epoch 024 | Batch 010/051 | Current Loss: 0.100434\n[Training] Epoch 024 | Batch 020/051 | Current Loss: 0.100449\n[Training] Epoch 024 | Batch 030/051 | Current Loss: 0.100434\n[Training] Epoch 024 | Batch 040/051 | Current Loss: 0.100382\n[Training] Epoch 024 | Batch 050/051 | Current Loss: 0.100389\n\n[Training] Epoch 024 Summary:\n  Avg Loss: 0.100418\n  Current LR: 3.41e-04\n[EarlyStopping] Loss improved (0.100500 → 0.100418). Saving model...\n[Training] Epoch 025 | Batch 000/051 | Current Loss: 0.100421\n[Training] Epoch 025 | Batch 010/051 | Current Loss: 0.100398\n[Training] Epoch 025 | Batch 020/051 | Current Loss: 0.100319\n[Training] Epoch 025 | Batch 030/051 | Current Loss: 0.100385\n[Training] Epoch 025 | Batch 040/051 | Current Loss: 0.100361\n[Training] Epoch 025 | Batch 050/051 | Current Loss: 0.100311\n\n[Training] Epoch 025 Summary:\n  Avg Loss: 0.100375\n  Current LR: 2.61e-04\n[EarlyStopping] Loss improved (0.100418 → 0.100375). Saving model...\n[Training] Epoch 026 | Batch 000/051 | Current Loss: 0.100342\n[Training] Epoch 026 | Batch 010/051 | Current Loss: 0.100354\n[Training] Epoch 026 | Batch 020/051 | Current Loss: 0.100340\n[Training] Epoch 026 | Batch 030/051 | Current Loss: 0.100364\n[Training] Epoch 026 | Batch 040/051 | Current Loss: 0.100346\n[Training] Epoch 026 | Batch 050/051 | Current Loss: 0.100292\n\n[Training] Epoch 026 Summary:\n  Avg Loss: 0.100351\n  Current LR: 1.80e-04\n[EarlyStopping] Loss improved (0.100375 → 0.100351). Saving model...\n[Training] Epoch 027 | Batch 000/051 | Current Loss: 0.100352\n[Training] Epoch 027 | Batch 010/051 | Current Loss: 0.100307\n[Training] Epoch 027 | Batch 020/051 | Current Loss: 0.100341\n[Training] Epoch 027 | Batch 030/051 | Current Loss: 0.100345\n[Training] Epoch 027 | Batch 040/051 | Current Loss: 0.100271\n[Training] Epoch 027 | Batch 050/051 | Current Loss: 0.100355\n\n[Training] Epoch 027 Summary:\n  Avg Loss: 0.100337\n  Current LR: 1.08e-04\n[EarlyStopping] Loss improved (0.100351 → 0.100337). Saving model...\n[Training] Epoch 028 | Batch 000/051 | Current Loss: 0.100344\n[Training] Epoch 028 | Batch 010/051 | Current Loss: 0.100258\n[Training] Epoch 028 | Batch 020/051 | Current Loss: 0.100295\n[Training] Epoch 028 | Batch 030/051 | Current Loss: 0.100326\n[Training] Epoch 028 | Batch 040/051 | Current Loss: 0.100336\n[Training] Epoch 028 | Batch 050/051 | Current Loss: 0.100310\n\n[Training] Epoch 028 Summary:\n  Avg Loss: 0.100325\n  Current LR: 5.06e-05\n[EarlyStopping] Loss improved (0.100337 → 0.100325). Saving model...\n[Training] Epoch 029 | Batch 000/051 | Current Loss: 0.100337\n[Training] Epoch 029 | Batch 010/051 | Current Loss: 0.100281\n[Training] Epoch 029 | Batch 020/051 | Current Loss: 0.100319\n[Training] Epoch 029 | Batch 030/051 | Current Loss: 0.100326\n[Training] Epoch 029 | Batch 040/051 | Current Loss: 0.100364\n[Training] Epoch 029 | Batch 050/051 | Current Loss: 0.100270\n\n[Training] Epoch 029 Summary:\n  Avg Loss: 0.100321\n  Current LR: 1.37e-05\n[EarlyStopping] Loss improved (0.100325 → 0.100321). Saving model...\n[Training] Epoch 030 | Batch 000/051 | Current Loss: 0.100357\n[Training] Epoch 030 | Batch 010/051 | Current Loss: 0.100333\n[Training] Epoch 030 | Batch 020/051 | Current Loss: 0.100386\n[Training] Epoch 030 | Batch 030/051 | Current Loss: 0.100332\n[Training] Epoch 030 | Batch 040/051 | Current Loss: 0.100301\n[Training] Epoch 030 | Batch 050/051 | Current Loss: 0.100369\n\n[Training] Epoch 030 Summary:\n  Avg Loss: 0.100318\n  Current LR: 5.20e-04\n[EarlyStopping] Loss improved (0.100321 → 0.100318). Saving model...\n[Training] Epoch 031 | Batch 000/051 | Current Loss: 0.100301\n[Training] Epoch 031 | Batch 010/051 | Current Loss: 0.100265\n[Training] Epoch 031 | Batch 020/051 | Current Loss: 0.100349\n[Training] Epoch 031 | Batch 030/051 | Current Loss: 0.100320\n[Training] Epoch 031 | Batch 040/051 | Current Loss: 0.100335\n[Training] Epoch 031 | Batch 050/051 | Current Loss: 0.100324\n\n[Training] Epoch 031 Summary:\n  Avg Loss: 0.100339\n  Current LR: 5.07e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 032 | Batch 000/051 | Current Loss: 0.100355\n[Training] Epoch 032 | Batch 010/051 | Current Loss: 0.100358\n[Training] Epoch 032 | Batch 020/051 | Current Loss: 0.100328\n[Training] Epoch 032 | Batch 030/051 | Current Loss: 0.100300\n[Training] Epoch 032 | Batch 040/051 | Current Loss: 0.100316\n[Training] Epoch 032 | Batch 050/051 | Current Loss: 0.100315\n\n[Training] Epoch 032 Summary:\n  Avg Loss: 0.100315\n  Current LR: 4.71e-04\n[EarlyStopping] Loss improved (0.100318 → 0.100315). Saving model...\n[Training] Epoch 033 | Batch 000/051 | Current Loss: 0.100254\n[Training] Epoch 033 | Batch 010/051 | Current Loss: 0.100248\n[Training] Epoch 033 | Batch 020/051 | Current Loss: 0.100281\n[Training] Epoch 033 | Batch 030/051 | Current Loss: 0.100328\n[Training] Epoch 033 | Batch 040/051 | Current Loss: 0.100308\n[Training] Epoch 033 | Batch 050/051 | Current Loss: 0.100294\n\n[Training] Epoch 033 Summary:\n  Avg Loss: 0.100297\n  Current LR: 4.13e-04\n[EarlyStopping] Loss improved (0.100315 → 0.100297). Saving model...\n[Training] Epoch 034 | Batch 000/051 | Current Loss: 0.100257\n[Training] Epoch 034 | Batch 010/051 | Current Loss: 0.100293\n[Training] Epoch 034 | Batch 020/051 | Current Loss: 0.100334\n[Training] Epoch 034 | Batch 030/051 | Current Loss: 0.100221\n[Training] Epoch 034 | Batch 040/051 | Current Loss: 0.100324\n[Training] Epoch 034 | Batch 050/051 | Current Loss: 0.100317\n\n[Training] Epoch 034 Summary:\n  Avg Loss: 0.100284\n  Current LR: 3.41e-04\n[EarlyStopping] Loss improved (0.100297 → 0.100284). Saving model...\n[Training] Epoch 035 | Batch 000/051 | Current Loss: 0.100290\n[Training] Epoch 035 | Batch 010/051 | Current Loss: 0.100318\n[Training] Epoch 035 | Batch 020/051 | Current Loss: 0.100273\n[Training] Epoch 035 | Batch 030/051 | Current Loss: 0.100269\n[Training] Epoch 035 | Batch 040/051 | Current Loss: 0.100247\n[Training] Epoch 035 | Batch 050/051 | Current Loss: 0.100220\n\n[Training] Epoch 035 Summary:\n  Avg Loss: 0.100268\n  Current LR: 2.61e-04\n[EarlyStopping] Loss improved (0.100284 → 0.100268). Saving model...\n[Training] Epoch 036 | Batch 000/051 | Current Loss: 0.100250\n[Training] Epoch 036 | Batch 010/051 | Current Loss: 0.100276\n[Training] Epoch 036 | Batch 020/051 | Current Loss: 0.100296\n[Training] Epoch 036 | Batch 030/051 | Current Loss: 0.100254\n[Training] Epoch 036 | Batch 040/051 | Current Loss: 0.100277\n[Training] Epoch 036 | Batch 050/051 | Current Loss: 0.100238\n\n[Training] Epoch 036 Summary:\n  Avg Loss: 0.100254\n  Current LR: 1.80e-04\n[EarlyStopping] Loss improved (0.100268 → 0.100254). Saving model...\n[Training] Epoch 037 | Batch 000/051 | Current Loss: 0.100226\n[Training] Epoch 037 | Batch 010/051 | Current Loss: 0.100243\n[Training] Epoch 037 | Batch 020/051 | Current Loss: 0.100263\n[Training] Epoch 037 | Batch 030/051 | Current Loss: 0.100242\n[Training] Epoch 037 | Batch 040/051 | Current Loss: 0.100255\n[Training] Epoch 037 | Batch 050/051 | Current Loss: 0.100244\n\n[Training] Epoch 037 Summary:\n  Avg Loss: 0.100246\n  Current LR: 1.08e-04\n[EarlyStopping] Loss improved (0.100254 → 0.100246). Saving model...\n[Training] Epoch 038 | Batch 000/051 | Current Loss: 0.100202\n[Training] Epoch 038 | Batch 010/051 | Current Loss: 0.100235\n[Training] Epoch 038 | Batch 020/051 | Current Loss: 0.100245\n[Training] Epoch 038 | Batch 030/051 | Current Loss: 0.100284\n[Training] Epoch 038 | Batch 040/051 | Current Loss: 0.100222\n[Training] Epoch 038 | Batch 050/051 | Current Loss: 0.100328\n\n[Training] Epoch 038 Summary:\n  Avg Loss: 0.100238\n  Current LR: 5.06e-05\n[EarlyStopping] Loss improved (0.100246 → 0.100238). Saving model...\n[Training] Epoch 039 | Batch 000/051 | Current Loss: 0.100223\n[Training] Epoch 039 | Batch 010/051 | Current Loss: 0.100256\n[Training] Epoch 039 | Batch 020/051 | Current Loss: 0.100227\n[Training] Epoch 039 | Batch 030/051 | Current Loss: 0.100230\n[Training] Epoch 039 | Batch 040/051 | Current Loss: 0.100270\n[Training] Epoch 039 | Batch 050/051 | Current Loss: 0.100293\n\n[Training] Epoch 039 Summary:\n  Avg Loss: 0.100231\n  Current LR: 1.37e-05\n[EarlyStopping] Loss improved (0.100238 → 0.100231). Saving model...\n[Training] Epoch 040 | Batch 000/051 | Current Loss: 0.100231\n[Training] Epoch 040 | Batch 010/051 | Current Loss: 0.100192\n[Training] Epoch 040 | Batch 020/051 | Current Loss: 0.100214\n[Training] Epoch 040 | Batch 030/051 | Current Loss: 0.100231\n[Training] Epoch 040 | Batch 040/051 | Current Loss: 0.100157\n[Training] Epoch 040 | Batch 050/051 | Current Loss: 0.100209\n\n[Training] Epoch 040 Summary:\n  Avg Loss: 0.100234\n  Current LR: 5.20e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 041 | Batch 000/051 | Current Loss: 0.100218\n[Training] Epoch 041 | Batch 010/051 | Current Loss: 0.100305\n[Training] Epoch 041 | Batch 020/051 | Current Loss: 0.100282\n[Training] Epoch 041 | Batch 030/051 | Current Loss: 0.100252\n[Training] Epoch 041 | Batch 040/051 | Current Loss: 0.100324\n[Training] Epoch 041 | Batch 050/051 | Current Loss: 0.100283\n\n[Training] Epoch 041 Summary:\n  Avg Loss: 0.100257\n  Current LR: 5.07e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 042 | Batch 000/051 | Current Loss: 0.100214\n[Training] Epoch 042 | Batch 010/051 | Current Loss: 0.100231\n[Training] Epoch 042 | Batch 020/051 | Current Loss: 0.100268\n[Training] Epoch 042 | Batch 030/051 | Current Loss: 0.100274\n[Training] Epoch 042 | Batch 040/051 | Current Loss: 0.100230\n[Training] Epoch 042 | Batch 050/051 | Current Loss: 0.100212\n\n[Training] Epoch 042 Summary:\n  Avg Loss: 0.100249\n  Current LR: 4.71e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 043 | Batch 000/051 | Current Loss: 0.100216\n[Training] Epoch 043 | Batch 010/051 | Current Loss: 0.100226\n[Training] Epoch 043 | Batch 020/051 | Current Loss: 0.100233\n[Training] Epoch 043 | Batch 030/051 | Current Loss: 0.100257\n[Training] Epoch 043 | Batch 040/051 | Current Loss: 0.100214\n[Training] Epoch 043 | Batch 050/051 | Current Loss: 0.100220\n\n[Training] Epoch 043 Summary:\n  Avg Loss: 0.100230\n  Current LR: 4.13e-04\n[EarlyStopping] Loss improved (0.100231 → 0.100230). Saving model...\n[Training] Epoch 044 | Batch 000/051 | Current Loss: 0.100260\n[Training] Epoch 044 | Batch 010/051 | Current Loss: 0.100190\n[Training] Epoch 044 | Batch 020/051 | Current Loss: 0.100216\n[Training] Epoch 044 | Batch 030/051 | Current Loss: 0.100202\n[Training] Epoch 044 | Batch 040/051 | Current Loss: 0.100237\n[Training] Epoch 044 | Batch 050/051 | Current Loss: 0.100285\n\n[Training] Epoch 044 Summary:\n  Avg Loss: 0.100224\n  Current LR: 3.41e-04\n[EarlyStopping] Loss improved (0.100230 → 0.100224). Saving model...\n[Training] Epoch 045 | Batch 000/051 | Current Loss: 0.100207\n[Training] Epoch 045 | Batch 010/051 | Current Loss: 0.100202\n[Training] Epoch 045 | Batch 020/051 | Current Loss: 0.100199\n[Training] Epoch 045 | Batch 030/051 | Current Loss: 0.100219\n[Training] Epoch 045 | Batch 040/051 | Current Loss: 0.100208\n[Training] Epoch 045 | Batch 050/051 | Current Loss: 0.100127\n\n[Training] Epoch 045 Summary:\n  Avg Loss: 0.100214\n  Current LR: 2.61e-04\n[EarlyStopping] Loss improved (0.100224 → 0.100214). Saving model...\n[Training] Epoch 046 | Batch 000/051 | Current Loss: 0.100147\n[Training] Epoch 046 | Batch 010/051 | Current Loss: 0.100225\n[Training] Epoch 046 | Batch 020/051 | Current Loss: 0.100171\n[Training] Epoch 046 | Batch 030/051 | Current Loss: 0.100176\n[Training] Epoch 046 | Batch 040/051 | Current Loss: 0.100240\n[Training] Epoch 046 | Batch 050/051 | Current Loss: 0.100214\n\n[Training] Epoch 046 Summary:\n  Avg Loss: 0.100197\n  Current LR: 1.80e-04\n[EarlyStopping] Loss improved (0.100214 → 0.100197). Saving model...\n[Training] Epoch 047 | Batch 000/051 | Current Loss: 0.100202\n[Training] Epoch 047 | Batch 010/051 | Current Loss: 0.100180\n[Training] Epoch 047 | Batch 020/051 | Current Loss: 0.100205\n[Training] Epoch 047 | Batch 030/051 | Current Loss: 0.100212\n[Training] Epoch 047 | Batch 040/051 | Current Loss: 0.100251\n[Training] Epoch 047 | Batch 050/051 | Current Loss: 0.100189\n\n[Training] Epoch 047 Summary:\n  Avg Loss: 0.100196\n  Current LR: 1.08e-04\n[EarlyStopping] Loss improved (0.100197 → 0.100196). Saving model...\n[Training] Epoch 048 | Batch 000/051 | Current Loss: 0.100193\n[Training] Epoch 048 | Batch 010/051 | Current Loss: 0.100198\n[Training] Epoch 048 | Batch 020/051 | Current Loss: 0.100266\n[Training] Epoch 048 | Batch 030/051 | Current Loss: 0.100238\n[Training] Epoch 048 | Batch 040/051 | Current Loss: 0.100186\n[Training] Epoch 048 | Batch 050/051 | Current Loss: 0.100189\n\n[Training] Epoch 048 Summary:\n  Avg Loss: 0.100193\n  Current LR: 5.06e-05\n[EarlyStopping] Loss improved (0.100196 → 0.100193). Saving model...\n[Training] Epoch 049 | Batch 000/051 | Current Loss: 0.100171\n[Training] Epoch 049 | Batch 010/051 | Current Loss: 0.100161\n[Training] Epoch 049 | Batch 020/051 | Current Loss: 0.100206\n[Training] Epoch 049 | Batch 030/051 | Current Loss: 0.100180\n[Training] Epoch 049 | Batch 040/051 | Current Loss: 0.100196\n[Training] Epoch 049 | Batch 050/051 | Current Loss: 0.100205\n\n[Training] Epoch 049 Summary:\n  Avg Loss: 0.100179\n  Current LR: 1.37e-05\n[EarlyStopping] Loss improved (0.100193 → 0.100179). Saving model...\n[Training] Epoch 050 | Batch 000/051 | Current Loss: 0.100134\n[Training] Epoch 050 | Batch 010/051 | Current Loss: 0.100228\n[Training] Epoch 050 | Batch 020/051 | Current Loss: 0.100104\n[Training] Epoch 050 | Batch 030/051 | Current Loss: 0.100162\n[Training] Epoch 050 | Batch 040/051 | Current Loss: 0.100227\n[Training] Epoch 050 | Batch 050/051 | Current Loss: 0.100163\n\n[Training] Epoch 050 Summary:\n  Avg Loss: 0.100176\n  Current LR: 5.20e-04\n[EarlyStopping] Loss improved (0.100179 → 0.100176). Saving model...\n[Training] Epoch 051 | Batch 000/051 | Current Loss: 0.100196\n[Training] Epoch 051 | Batch 010/051 | Current Loss: 0.100176\n[Training] Epoch 051 | Batch 020/051 | Current Loss: 0.100230\n[Training] Epoch 051 | Batch 030/051 | Current Loss: 0.100207\n[Training] Epoch 051 | Batch 040/051 | Current Loss: 0.100207\n[Training] Epoch 051 | Batch 050/051 | Current Loss: 0.100238\n\n[Training] Epoch 051 Summary:\n  Avg Loss: 0.100200\n  Current LR: 5.07e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 052 | Batch 000/051 | Current Loss: 0.100220\n[Training] Epoch 052 | Batch 010/051 | Current Loss: 0.100214\n[Training] Epoch 052 | Batch 020/051 | Current Loss: 0.100205\n[Training] Epoch 052 | Batch 030/051 | Current Loss: 0.100196\n[Training] Epoch 052 | Batch 040/051 | Current Loss: 0.100263\n[Training] Epoch 052 | Batch 050/051 | Current Loss: 0.100168\n\n[Training] Epoch 052 Summary:\n  Avg Loss: 0.100202\n  Current LR: 4.71e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 053 | Batch 000/051 | Current Loss: 0.100168\n[Training] Epoch 053 | Batch 010/051 | Current Loss: 0.100139\n[Training] Epoch 053 | Batch 020/051 | Current Loss: 0.100181\n[Training] Epoch 053 | Batch 030/051 | Current Loss: 0.100209\n[Training] Epoch 053 | Batch 040/051 | Current Loss: 0.100248\n[Training] Epoch 053 | Batch 050/051 | Current Loss: 0.100213\n\n[Training] Epoch 053 Summary:\n  Avg Loss: 0.100195\n  Current LR: 4.13e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 054 | Batch 000/051 | Current Loss: 0.100203\n[Training] Epoch 054 | Batch 010/051 | Current Loss: 0.100221\n[Training] Epoch 054 | Batch 020/051 | Current Loss: 0.100194\n[Training] Epoch 054 | Batch 030/051 | Current Loss: 0.100162\n[Training] Epoch 054 | Batch 040/051 | Current Loss: 0.100178\n[Training] Epoch 054 | Batch 050/051 | Current Loss: 0.100156\n\n[Training] Epoch 054 Summary:\n  Avg Loss: 0.100185\n  Current LR: 3.41e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 055 | Batch 000/051 | Current Loss: 0.100156\n[Training] Epoch 055 | Batch 010/051 | Current Loss: 0.100173\n[Training] Epoch 055 | Batch 020/051 | Current Loss: 0.100114\n[Training] Epoch 055 | Batch 030/051 | Current Loss: 0.100172\n[Training] Epoch 055 | Batch 040/051 | Current Loss: 0.100174\n[Training] Epoch 055 | Batch 050/051 | Current Loss: 0.100188\n\n[Training] Epoch 055 Summary:\n  Avg Loss: 0.100172\n  Current LR: 2.61e-04\n[EarlyStopping] Loss improved (0.100176 → 0.100172). Saving model...\n[Training] Epoch 056 | Batch 000/051 | Current Loss: 0.100139\n[Training] Epoch 056 | Batch 010/051 | Current Loss: 0.100165\n[Training] Epoch 056 | Batch 020/051 | Current Loss: 0.100171\n[Training] Epoch 056 | Batch 030/051 | Current Loss: 0.100116\n[Training] Epoch 056 | Batch 040/051 | Current Loss: 0.100136\n[Training] Epoch 056 | Batch 050/051 | Current Loss: 0.100209\n\n[Training] Epoch 056 Summary:\n  Avg Loss: 0.100164\n  Current LR: 1.80e-04\n[EarlyStopping] Loss improved (0.100172 → 0.100164). Saving model...\n[Training] Epoch 057 | Batch 000/051 | Current Loss: 0.100140\n[Training] Epoch 057 | Batch 010/051 | Current Loss: 0.100172\n[Training] Epoch 057 | Batch 020/051 | Current Loss: 0.100121\n[Training] Epoch 057 | Batch 030/051 | Current Loss: 0.100139\n[Training] Epoch 057 | Batch 040/051 | Current Loss: 0.100140\n[Training] Epoch 057 | Batch 050/051 | Current Loss: 0.100087\n\n[Training] Epoch 057 Summary:\n  Avg Loss: 0.100153\n  Current LR: 1.08e-04\n[EarlyStopping] Loss improved (0.100164 → 0.100153). Saving model...\n[Training] Epoch 058 | Batch 000/051 | Current Loss: 0.100166\n[Training] Epoch 058 | Batch 010/051 | Current Loss: 0.100136\n[Training] Epoch 058 | Batch 020/051 | Current Loss: 0.100124\n[Training] Epoch 058 | Batch 030/051 | Current Loss: 0.100120\n[Training] Epoch 058 | Batch 040/051 | Current Loss: 0.100138\n[Training] Epoch 058 | Batch 050/051 | Current Loss: 0.100146\n\n[Training] Epoch 058 Summary:\n  Avg Loss: 0.100147\n  Current LR: 5.06e-05\n[EarlyStopping] Loss improved (0.100153 → 0.100147). Saving model...\n[Training] Epoch 059 | Batch 000/051 | Current Loss: 0.100127\n[Training] Epoch 059 | Batch 010/051 | Current Loss: 0.100155\n[Training] Epoch 059 | Batch 020/051 | Current Loss: 0.100134\n[Training] Epoch 059 | Batch 030/051 | Current Loss: 0.100193\n[Training] Epoch 059 | Batch 040/051 | Current Loss: 0.100135\n[Training] Epoch 059 | Batch 050/051 | Current Loss: 0.100135\n\n[Training] Epoch 059 Summary:\n  Avg Loss: 0.100144\n  Current LR: 1.37e-05\n[EarlyStopping] Loss improved (0.100147 → 0.100144). Saving model...\n[Training] Epoch 060 | Batch 000/051 | Current Loss: 0.100151\n[Training] Epoch 060 | Batch 010/051 | Current Loss: 0.100124\n[Training] Epoch 060 | Batch 020/051 | Current Loss: 0.100201\n[Training] Epoch 060 | Batch 030/051 | Current Loss: 0.100123\n[Training] Epoch 060 | Batch 040/051 | Current Loss: 0.100139\n[Training] Epoch 060 | Batch 050/051 | Current Loss: 0.100186\n\n[Training] Epoch 060 Summary:\n  Avg Loss: 0.100144\n  Current LR: 5.20e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 061 | Batch 000/051 | Current Loss: 0.100147\n[Training] Epoch 061 | Batch 010/051 | Current Loss: 0.100167\n[Training] Epoch 061 | Batch 020/051 | Current Loss: 0.100220\n[Training] Epoch 061 | Batch 030/051 | Current Loss: 0.100168\n[Training] Epoch 061 | Batch 040/051 | Current Loss: 0.100167\n[Training] Epoch 061 | Batch 050/051 | Current Loss: 0.100227\n\n[Training] Epoch 061 Summary:\n  Avg Loss: 0.100179\n  Current LR: 5.07e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 062 | Batch 000/051 | Current Loss: 0.100261\n[Training] Epoch 062 | Batch 010/051 | Current Loss: 0.100187\n[Training] Epoch 062 | Batch 020/051 | Current Loss: 0.100232\n[Training] Epoch 062 | Batch 030/051 | Current Loss: 0.100164\n[Training] Epoch 062 | Batch 040/051 | Current Loss: 0.100127\n[Training] Epoch 062 | Batch 050/051 | Current Loss: 0.100138\n\n[Training] Epoch 062 Summary:\n  Avg Loss: 0.100195\n  Current LR: 4.71e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 063 | Batch 000/051 | Current Loss: 0.100139\n[Training] Epoch 063 | Batch 010/051 | Current Loss: 0.100173\n[Training] Epoch 063 | Batch 020/051 | Current Loss: 0.100202\n[Training] Epoch 063 | Batch 030/051 | Current Loss: 0.100202\n[Training] Epoch 063 | Batch 040/051 | Current Loss: 0.100126\n[Training] Epoch 063 | Batch 050/051 | Current Loss: 0.100095\n\n[Training] Epoch 063 Summary:\n  Avg Loss: 0.100176\n  Current LR: 4.13e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 064 | Batch 000/051 | Current Loss: 0.100170\n[Training] Epoch 064 | Batch 010/051 | Current Loss: 0.100201\n[Training] Epoch 064 | Batch 020/051 | Current Loss: 0.100203\n[Training] Epoch 064 | Batch 030/051 | Current Loss: 0.100170\n[Training] Epoch 064 | Batch 040/051 | Current Loss: 0.100195\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 15:23:35,025] Trial 2 finished with value: 0.10014381741776186 and parameters: {'lr': 0.0005201857972779173, 'batch_size': 256, 'bottleneck_width': 2048, 'dropout_rate': 0.17017258595011986, 'alpha': 0.5489804503264961, 'weight_decay': 8.950090467096336e-06}. Best is trial 1 with value: 0.10011662966480442.\n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 064 | Batch 050/051 | Current Loss: 0.100160\n\n[Training] Epoch 064 Summary:\n  Avg Loss: 0.100160\n  Current LR: 3.41e-04\n[EarlyStopping] Counter: 5/5\n\n[Training] Early stopping triggered\n\n[Optuna] Trial 2 completed with best loss: 0.100144\n\n==================================================\nStarting Optuna Trial 3\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.0001215802890237896\n  batch_size: 256\n  bottleneck_width: 1024\n  dropout_rate: 0.3438623105865585\n  alpha: 0.8858620925213587\n  weight_decay: 6.498883072157955e-06\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 256, Total batches: 51\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 1024\n  - Dropout rate: 0.3438623105865585\n[Model] Architecture initialized successfully\n[Model] Total parameters: 2,396,992\n[Optimizer] Initialized with lr=1.22e-04, weight_decay=6.50e-06\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.8858620925213587\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/051 | Current Loss: 1.723972\n[Training] Epoch 001 | Batch 010/051 | Current Loss: 0.376595\n[Training] Epoch 001 | Batch 020/051 | Current Loss: 0.209581\n[Training] Epoch 001 | Batch 030/051 | Current Loss: 0.163076\n[Training] Epoch 001 | Batch 040/051 | Current Loss: 0.141636\n[Training] Epoch 001 | Batch 050/051 | Current Loss: 0.130702\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.310568\n  Current LR: 1.19e-04\n[EarlyStopping] Loss improved (inf → 0.310568). Saving model...\n[Training] Epoch 002 | Batch 000/051 | Current Loss: 0.130807\n[Training] Epoch 002 | Batch 010/051 | Current Loss: 0.125457\n[Training] Epoch 002 | Batch 020/051 | Current Loss: 0.121476\n[Training] Epoch 002 | Batch 030/051 | Current Loss: 0.120314\n[Training] Epoch 002 | Batch 040/051 | Current Loss: 0.115829\n[Training] Epoch 002 | Batch 050/051 | Current Loss: 0.119529\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.121426\n  Current LR: 1.10e-04\n[EarlyStopping] Loss improved (0.310568 → 0.121426). Saving model...\n[Training] Epoch 003 | Batch 000/051 | Current Loss: 0.114496\n[Training] Epoch 003 | Batch 010/051 | Current Loss: 0.115854\n[Training] Epoch 003 | Batch 020/051 | Current Loss: 0.112235\n[Training] Epoch 003 | Batch 030/051 | Current Loss: 0.111897\n[Training] Epoch 003 | Batch 040/051 | Current Loss: 0.110605\n[Training] Epoch 003 | Batch 050/051 | Current Loss: 0.110831\n\n[Training] Epoch 003 Summary:\n  Avg Loss: 0.112933\n  Current LR: 9.67e-05\n[EarlyStopping] Loss improved (0.121426 → 0.112933). Saving model...\n[Training] Epoch 004 | Batch 000/051 | Current Loss: 0.112081\n[Training] Epoch 004 | Batch 010/051 | Current Loss: 0.110498\n[Training] Epoch 004 | Batch 020/051 | Current Loss: 0.110550\n[Training] Epoch 004 | Batch 030/051 | Current Loss: 0.108990\n[Training] Epoch 004 | Batch 040/051 | Current Loss: 0.108850\n[Training] Epoch 004 | Batch 050/051 | Current Loss: 0.110645\n\n[Training] Epoch 004 Summary:\n  Avg Loss: 0.109808\n  Current LR: 7.99e-05\n[EarlyStopping] Loss improved (0.112933 → 0.109808). Saving model...\n[Training] Epoch 005 | Batch 000/051 | Current Loss: 0.108193\n[Training] Epoch 005 | Batch 010/051 | Current Loss: 0.108240\n[Training] Epoch 005 | Batch 020/051 | Current Loss: 0.107945\n[Training] Epoch 005 | Batch 030/051 | Current Loss: 0.107917\n[Training] Epoch 005 | Batch 040/051 | Current Loss: 0.107219\n[Training] Epoch 005 | Batch 050/051 | Current Loss: 0.107333\n\n[Training] Epoch 005 Summary:\n  Avg Loss: 0.108111\n  Current LR: 6.13e-05\n[EarlyStopping] Loss improved (0.109808 → 0.108111). Saving model...\n[Training] Epoch 006 | Batch 000/051 | Current Loss: 0.107894\n[Training] Epoch 006 | Batch 010/051 | Current Loss: 0.106659\n[Training] Epoch 006 | Batch 020/051 | Current Loss: 0.106743\n[Training] Epoch 006 | Batch 030/051 | Current Loss: 0.106847\n[Training] Epoch 006 | Batch 040/051 | Current Loss: 0.107720\n[Training] Epoch 006 | Batch 050/051 | Current Loss: 0.106692\n\n[Training] Epoch 006 Summary:\n  Avg Loss: 0.107037\n  Current LR: 4.27e-05\n[EarlyStopping] Loss improved (0.108111 → 0.107037). Saving model...\n[Training] Epoch 007 | Batch 000/051 | Current Loss: 0.106275\n[Training] Epoch 007 | Batch 010/051 | Current Loss: 0.106660\n[Training] Epoch 007 | Batch 020/051 | Current Loss: 0.107142\n[Training] Epoch 007 | Batch 030/051 | Current Loss: 0.106382\n[Training] Epoch 007 | Batch 040/051 | Current Loss: 0.106228\n[Training] Epoch 007 | Batch 050/051 | Current Loss: 0.106586\n\n[Training] Epoch 007 Summary:\n  Avg Loss: 0.106534\n  Current LR: 2.59e-05\n[EarlyStopping] Loss improved (0.107037 → 0.106534). Saving model...\n[Training] Epoch 008 | Batch 000/051 | Current Loss: 0.105861\n[Training] Epoch 008 | Batch 010/051 | Current Loss: 0.106078\n[Training] Epoch 008 | Batch 020/051 | Current Loss: 0.106719\n[Training] Epoch 008 | Batch 030/051 | Current Loss: 0.105667\n[Training] Epoch 008 | Batch 040/051 | Current Loss: 0.106480\n[Training] Epoch 008 | Batch 050/051 | Current Loss: 0.106441\n\n[Training] Epoch 008 Summary:\n  Avg Loss: 0.106234\n  Current LR: 1.25e-05\n[EarlyStopping] Loss improved (0.106534 → 0.106234). Saving model...\n[Training] Epoch 009 | Batch 000/051 | Current Loss: 0.106845\n[Training] Epoch 009 | Batch 010/051 | Current Loss: 0.105862\n[Training] Epoch 009 | Batch 020/051 | Current Loss: 0.106122\n[Training] Epoch 009 | Batch 030/051 | Current Loss: 0.106320\n[Training] Epoch 009 | Batch 040/051 | Current Loss: 0.105954\n[Training] Epoch 009 | Batch 050/051 | Current Loss: 0.105719\n\n[Training] Epoch 009 Summary:\n  Avg Loss: 0.106066\n  Current LR: 3.95e-06\n[EarlyStopping] Loss improved (0.106234 → 0.106066). Saving model...\n[Training] Epoch 010 | Batch 000/051 | Current Loss: 0.105547\n[Training] Epoch 010 | Batch 010/051 | Current Loss: 0.105621\n[Training] Epoch 010 | Batch 020/051 | Current Loss: 0.106049\n[Training] Epoch 010 | Batch 030/051 | Current Loss: 0.106709\n[Training] Epoch 010 | Batch 040/051 | Current Loss: 0.105809\n[Training] Epoch 010 | Batch 050/051 | Current Loss: 0.105751\n\n[Training] Epoch 010 Summary:\n  Avg Loss: 0.105912\n  Current LR: 1.22e-04\n[EarlyStopping] Loss improved (0.106066 → 0.105912). Saving model...\n[Training] Epoch 011 | Batch 000/051 | Current Loss: 0.105865\n[Training] Epoch 011 | Batch 010/051 | Current Loss: 0.107083\n[Training] Epoch 011 | Batch 020/051 | Current Loss: 0.106692\n[Training] Epoch 011 | Batch 030/051 | Current Loss: 0.105235\n[Training] Epoch 011 | Batch 040/051 | Current Loss: 0.105238\n[Training] Epoch 011 | Batch 050/051 | Current Loss: 0.104639\n\n[Training] Epoch 011 Summary:\n  Avg Loss: 0.105757\n  Current LR: 1.19e-04\n[EarlyStopping] Loss improved (0.105912 → 0.105757). Saving model...\n[Training] Epoch 012 | Batch 000/051 | Current Loss: 0.104748\n[Training] Epoch 012 | Batch 010/051 | Current Loss: 0.106872\n[Training] Epoch 012 | Batch 020/051 | Current Loss: 0.105480\n[Training] Epoch 012 | Batch 030/051 | Current Loss: 0.104414\n[Training] Epoch 012 | Batch 040/051 | Current Loss: 0.105428\n[Training] Epoch 012 | Batch 050/051 | Current Loss: 0.105238\n\n[Training] Epoch 012 Summary:\n  Avg Loss: 0.104920\n  Current LR: 1.10e-04\n[EarlyStopping] Loss improved (0.105757 → 0.104920). Saving model...\n[Training] Epoch 013 | Batch 000/051 | Current Loss: 0.104321\n[Training] Epoch 013 | Batch 010/051 | Current Loss: 0.105037\n[Training] Epoch 013 | Batch 020/051 | Current Loss: 0.103956\n[Training] Epoch 013 | Batch 030/051 | Current Loss: 0.104038\n[Training] Epoch 013 | Batch 040/051 | Current Loss: 0.103695\n[Training] Epoch 013 | Batch 050/051 | Current Loss: 0.104056\n\n[Training] Epoch 013 Summary:\n  Avg Loss: 0.104137\n  Current LR: 9.67e-05\n[EarlyStopping] Loss improved (0.104920 → 0.104137). Saving model...\n[Training] Epoch 014 | Batch 000/051 | Current Loss: 0.104088\n[Training] Epoch 014 | Batch 010/051 | Current Loss: 0.103516\n[Training] Epoch 014 | Batch 020/051 | Current Loss: 0.103682\n[Training] Epoch 014 | Batch 030/051 | Current Loss: 0.103534\n[Training] Epoch 014 | Batch 040/051 | Current Loss: 0.103653\n[Training] Epoch 014 | Batch 050/051 | Current Loss: 0.103596\n\n[Training] Epoch 014 Summary:\n  Avg Loss: 0.103786\n  Current LR: 7.99e-05\n[EarlyStopping] Loss improved (0.104137 → 0.103786). Saving model...\n[Training] Epoch 015 | Batch 000/051 | Current Loss: 0.103695\n[Training] Epoch 015 | Batch 010/051 | Current Loss: 0.103591\n[Training] Epoch 015 | Batch 020/051 | Current Loss: 0.103184\n[Training] Epoch 015 | Batch 030/051 | Current Loss: 0.103428\n[Training] Epoch 015 | Batch 040/051 | Current Loss: 0.103205\n[Training] Epoch 015 | Batch 050/051 | Current Loss: 0.103954\n\n[Training] Epoch 015 Summary:\n  Avg Loss: 0.103450\n  Current LR: 6.13e-05\n[EarlyStopping] Loss improved (0.103786 → 0.103450). Saving model...\n[Training] Epoch 016 | Batch 000/051 | Current Loss: 0.103126\n[Training] Epoch 016 | Batch 010/051 | Current Loss: 0.103472\n[Training] Epoch 016 | Batch 020/051 | Current Loss: 0.103057\n[Training] Epoch 016 | Batch 030/051 | Current Loss: 0.103295\n[Training] Epoch 016 | Batch 040/051 | Current Loss: 0.103026\n[Training] Epoch 016 | Batch 050/051 | Current Loss: 0.103119\n\n[Training] Epoch 016 Summary:\n  Avg Loss: 0.103322\n  Current LR: 4.27e-05\n[EarlyStopping] Loss improved (0.103450 → 0.103322). Saving model...\n[Training] Epoch 017 | Batch 000/051 | Current Loss: 0.103021\n[Training] Epoch 017 | Batch 010/051 | Current Loss: 0.103599\n[Training] Epoch 017 | Batch 020/051 | Current Loss: 0.103141\n[Training] Epoch 017 | Batch 030/051 | Current Loss: 0.103400\n[Training] Epoch 017 | Batch 040/051 | Current Loss: 0.102866\n[Training] Epoch 017 | Batch 050/051 | Current Loss: 0.103088\n\n[Training] Epoch 017 Summary:\n  Avg Loss: 0.103110\n  Current LR: 2.59e-05\n[EarlyStopping] Loss improved (0.103322 → 0.103110). Saving model...\n[Training] Epoch 018 | Batch 000/051 | Current Loss: 0.102804\n[Training] Epoch 018 | Batch 010/051 | Current Loss: 0.102883\n[Training] Epoch 018 | Batch 020/051 | Current Loss: 0.103397\n[Training] Epoch 018 | Batch 030/051 | Current Loss: 0.102797\n[Training] Epoch 018 | Batch 040/051 | Current Loss: 0.103006\n[Training] Epoch 018 | Batch 050/051 | Current Loss: 0.102968\n\n[Training] Epoch 018 Summary:\n  Avg Loss: 0.103053\n  Current LR: 1.25e-05\n[EarlyStopping] Loss improved (0.103110 → 0.103053). Saving model...\n[Training] Epoch 019 | Batch 000/051 | Current Loss: 0.102845\n[Training] Epoch 019 | Batch 010/051 | Current Loss: 0.102770\n[Training] Epoch 019 | Batch 020/051 | Current Loss: 0.102736\n[Training] Epoch 019 | Batch 030/051 | Current Loss: 0.102994\n[Training] Epoch 019 | Batch 040/051 | Current Loss: 0.102945\n[Training] Epoch 019 | Batch 050/051 | Current Loss: 0.102972\n\n[Training] Epoch 019 Summary:\n  Avg Loss: 0.102924\n  Current LR: 3.95e-06\n[EarlyStopping] Loss improved (0.103053 → 0.102924). Saving model...\n[Training] Epoch 020 | Batch 000/051 | Current Loss: 0.102647\n[Training] Epoch 020 | Batch 010/051 | Current Loss: 0.102773\n[Training] Epoch 020 | Batch 020/051 | Current Loss: 0.102758\n[Training] Epoch 020 | Batch 030/051 | Current Loss: 0.104277\n[Training] Epoch 020 | Batch 040/051 | Current Loss: 0.103047\n[Training] Epoch 020 | Batch 050/051 | Current Loss: 0.102754\n\n[Training] Epoch 020 Summary:\n  Avg Loss: 0.102879\n  Current LR: 1.22e-04\n[EarlyStopping] Loss improved (0.102924 → 0.102879). Saving model...\n[Training] Epoch 021 | Batch 000/051 | Current Loss: 0.103002\n[Training] Epoch 021 | Batch 010/051 | Current Loss: 0.103462\n[Training] Epoch 021 | Batch 020/051 | Current Loss: 0.102897\n[Training] Epoch 021 | Batch 030/051 | Current Loss: 0.103045\n[Training] Epoch 021 | Batch 040/051 | Current Loss: 0.102791\n[Training] Epoch 021 | Batch 050/051 | Current Loss: 0.102673\n\n[Training] Epoch 021 Summary:\n  Avg Loss: 0.102880\n  Current LR: 1.19e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 022 | Batch 000/051 | Current Loss: 0.102604\n[Training] Epoch 022 | Batch 010/051 | Current Loss: 0.103447\n[Training] Epoch 022 | Batch 020/051 | Current Loss: 0.103044\n[Training] Epoch 022 | Batch 030/051 | Current Loss: 0.102505\n[Training] Epoch 022 | Batch 040/051 | Current Loss: 0.103381\n[Training] Epoch 022 | Batch 050/051 | Current Loss: 0.102364\n\n[Training] Epoch 022 Summary:\n  Avg Loss: 0.102691\n  Current LR: 1.10e-04\n[EarlyStopping] Loss improved (0.102879 → 0.102691). Saving model...\n[Training] Epoch 023 | Batch 000/051 | Current Loss: 0.103574\n[Training] Epoch 023 | Batch 010/051 | Current Loss: 0.102282\n[Training] Epoch 023 | Batch 020/051 | Current Loss: 0.102352\n[Training] Epoch 023 | Batch 030/051 | Current Loss: 0.103150\n[Training] Epoch 023 | Batch 040/051 | Current Loss: 0.102291\n[Training] Epoch 023 | Batch 050/051 | Current Loss: 0.102408\n\n[Training] Epoch 023 Summary:\n  Avg Loss: 0.102507\n  Current LR: 9.67e-05\n[EarlyStopping] Loss improved (0.102691 → 0.102507). Saving model...\n[Training] Epoch 024 | Batch 000/051 | Current Loss: 0.102328\n[Training] Epoch 024 | Batch 010/051 | Current Loss: 0.102778\n[Training] Epoch 024 | Batch 020/051 | Current Loss: 0.102214\n[Training] Epoch 024 | Batch 030/051 | Current Loss: 0.102044\n[Training] Epoch 024 | Batch 040/051 | Current Loss: 0.102003\n[Training] Epoch 024 | Batch 050/051 | Current Loss: 0.102047\n\n[Training] Epoch 024 Summary:\n  Avg Loss: 0.102248\n  Current LR: 7.99e-05\n[EarlyStopping] Loss improved (0.102507 → 0.102248). Saving model...\n[Training] Epoch 025 | Batch 000/051 | Current Loss: 0.102080\n[Training] Epoch 025 | Batch 010/051 | Current Loss: 0.102246\n[Training] Epoch 025 | Batch 020/051 | Current Loss: 0.102137\n[Training] Epoch 025 | Batch 030/051 | Current Loss: 0.102314\n[Training] Epoch 025 | Batch 040/051 | Current Loss: 0.101951\n[Training] Epoch 025 | Batch 050/051 | Current Loss: 0.102213\n\n[Training] Epoch 025 Summary:\n  Avg Loss: 0.102123\n  Current LR: 6.13e-05\n[EarlyStopping] Loss improved (0.102248 → 0.102123). Saving model...\n[Training] Epoch 026 | Batch 000/051 | Current Loss: 0.102269\n[Training] Epoch 026 | Batch 010/051 | Current Loss: 0.101921\n[Training] Epoch 026 | Batch 020/051 | Current Loss: 0.101956\n[Training] Epoch 026 | Batch 030/051 | Current Loss: 0.101990\n[Training] Epoch 026 | Batch 040/051 | Current Loss: 0.102143\n[Training] Epoch 026 | Batch 050/051 | Current Loss: 0.101994\n\n[Training] Epoch 026 Summary:\n  Avg Loss: 0.102022\n  Current LR: 4.27e-05\n[EarlyStopping] Loss improved (0.102123 → 0.102022). Saving model...\n[Training] Epoch 027 | Batch 000/051 | Current Loss: 0.101893\n[Training] Epoch 027 | Batch 010/051 | Current Loss: 0.101911\n[Training] Epoch 027 | Batch 020/051 | Current Loss: 0.101833\n[Training] Epoch 027 | Batch 030/051 | Current Loss: 0.101956\n[Training] Epoch 027 | Batch 040/051 | Current Loss: 0.101869\n[Training] Epoch 027 | Batch 050/051 | Current Loss: 0.101906\n\n[Training] Epoch 027 Summary:\n  Avg Loss: 0.101886\n  Current LR: 2.59e-05\n[EarlyStopping] Loss improved (0.102022 → 0.101886). Saving model...\n[Training] Epoch 028 | Batch 000/051 | Current Loss: 0.101941\n[Training] Epoch 028 | Batch 010/051 | Current Loss: 0.101812\n[Training] Epoch 028 | Batch 020/051 | Current Loss: 0.102061\n[Training] Epoch 028 | Batch 030/051 | Current Loss: 0.101835\n[Training] Epoch 028 | Batch 040/051 | Current Loss: 0.101783\n[Training] Epoch 028 | Batch 050/051 | Current Loss: 0.101952\n\n[Training] Epoch 028 Summary:\n  Avg Loss: 0.101865\n  Current LR: 1.25e-05\n[EarlyStopping] Loss improved (0.101886 → 0.101865). Saving model...\n[Training] Epoch 029 | Batch 000/051 | Current Loss: 0.101993\n[Training] Epoch 029 | Batch 010/051 | Current Loss: 0.101784\n[Training] Epoch 029 | Batch 020/051 | Current Loss: 0.101747\n[Training] Epoch 029 | Batch 030/051 | Current Loss: 0.101849\n[Training] Epoch 029 | Batch 040/051 | Current Loss: 0.101726\n[Training] Epoch 029 | Batch 050/051 | Current Loss: 0.102076\n\n[Training] Epoch 029 Summary:\n  Avg Loss: 0.101853\n  Current LR: 3.95e-06\n[EarlyStopping] Loss improved (0.101865 → 0.101853). Saving model...\n[Training] Epoch 030 | Batch 000/051 | Current Loss: 0.101910\n[Training] Epoch 030 | Batch 010/051 | Current Loss: 0.102084\n[Training] Epoch 030 | Batch 020/051 | Current Loss: 0.101728\n[Training] Epoch 030 | Batch 030/051 | Current Loss: 0.101846\n[Training] Epoch 030 | Batch 040/051 | Current Loss: 0.101821\n[Training] Epoch 030 | Batch 050/051 | Current Loss: 0.101830\n\n[Training] Epoch 030 Summary:\n  Avg Loss: 0.101795\n  Current LR: 1.22e-04\n[EarlyStopping] Loss improved (0.101853 → 0.101795). Saving model...\n[Training] Epoch 031 | Batch 000/051 | Current Loss: 0.101690\n[Training] Epoch 031 | Batch 010/051 | Current Loss: 0.101764\n[Training] Epoch 031 | Batch 020/051 | Current Loss: 0.102109\n[Training] Epoch 031 | Batch 030/051 | Current Loss: 0.101843\n[Training] Epoch 031 | Batch 040/051 | Current Loss: 0.101701\n[Training] Epoch 031 | Batch 050/051 | Current Loss: 0.102062\n\n[Training] Epoch 031 Summary:\n  Avg Loss: 0.101811\n  Current LR: 1.19e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 032 | Batch 000/051 | Current Loss: 0.101599\n[Training] Epoch 032 | Batch 010/051 | Current Loss: 0.102121\n[Training] Epoch 032 | Batch 020/051 | Current Loss: 0.102355\n[Training] Epoch 032 | Batch 030/051 | Current Loss: 0.101837\n[Training] Epoch 032 | Batch 040/051 | Current Loss: 0.101710\n[Training] Epoch 032 | Batch 050/051 | Current Loss: 0.101621\n\n[Training] Epoch 032 Summary:\n  Avg Loss: 0.101702\n  Current LR: 1.10e-04\n[EarlyStopping] Loss improved (0.101795 → 0.101702). Saving model...\n[Training] Epoch 033 | Batch 000/051 | Current Loss: 0.101625\n[Training] Epoch 033 | Batch 010/051 | Current Loss: 0.101677\n[Training] Epoch 033 | Batch 020/051 | Current Loss: 0.101559\n[Training] Epoch 033 | Batch 030/051 | Current Loss: 0.101762\n[Training] Epoch 033 | Batch 040/051 | Current Loss: 0.101760\n[Training] Epoch 033 | Batch 050/051 | Current Loss: 0.101617\n\n[Training] Epoch 033 Summary:\n  Avg Loss: 0.101594\n  Current LR: 9.67e-05\n[EarlyStopping] Loss improved (0.101702 → 0.101594). Saving model...\n[Training] Epoch 034 | Batch 000/051 | Current Loss: 0.101457\n[Training] Epoch 034 | Batch 010/051 | Current Loss: 0.101600\n[Training] Epoch 034 | Batch 020/051 | Current Loss: 0.101425\n[Training] Epoch 034 | Batch 030/051 | Current Loss: 0.101410\n[Training] Epoch 034 | Batch 040/051 | Current Loss: 0.101398\n[Training] Epoch 034 | Batch 050/051 | Current Loss: 0.101375\n\n[Training] Epoch 034 Summary:\n  Avg Loss: 0.101508\n  Current LR: 7.99e-05\n[EarlyStopping] Loss improved (0.101594 → 0.101508). Saving model...\n[Training] Epoch 035 | Batch 000/051 | Current Loss: 0.101452\n[Training] Epoch 035 | Batch 010/051 | Current Loss: 0.101624\n[Training] Epoch 035 | Batch 020/051 | Current Loss: 0.101379\n[Training] Epoch 035 | Batch 030/051 | Current Loss: 0.101464\n[Training] Epoch 035 | Batch 040/051 | Current Loss: 0.101370\n[Training] Epoch 035 | Batch 050/051 | Current Loss: 0.101375\n\n[Training] Epoch 035 Summary:\n  Avg Loss: 0.101429\n  Current LR: 6.13e-05\n[EarlyStopping] Loss improved (0.101508 → 0.101429). Saving model...\n[Training] Epoch 036 | Batch 000/051 | Current Loss: 0.101347\n[Training] Epoch 036 | Batch 010/051 | Current Loss: 0.101547\n[Training] Epoch 036 | Batch 020/051 | Current Loss: 0.101287\n[Training] Epoch 036 | Batch 030/051 | Current Loss: 0.101426\n[Training] Epoch 036 | Batch 040/051 | Current Loss: 0.101306\n[Training] Epoch 036 | Batch 050/051 | Current Loss: 0.101291\n\n[Training] Epoch 036 Summary:\n  Avg Loss: 0.101366\n  Current LR: 4.27e-05\n[EarlyStopping] Loss improved (0.101429 → 0.101366). Saving model...\n[Training] Epoch 037 | Batch 000/051 | Current Loss: 0.101404\n[Training] Epoch 037 | Batch 010/051 | Current Loss: 0.101392\n[Training] Epoch 037 | Batch 020/051 | Current Loss: 0.101425\n[Training] Epoch 037 | Batch 030/051 | Current Loss: 0.101355\n[Training] Epoch 037 | Batch 040/051 | Current Loss: 0.101366\n[Training] Epoch 037 | Batch 050/051 | Current Loss: 0.101308\n\n[Training] Epoch 037 Summary:\n  Avg Loss: 0.101340\n  Current LR: 2.59e-05\n[EarlyStopping] Loss improved (0.101366 → 0.101340). Saving model...\n[Training] Epoch 038 | Batch 000/051 | Current Loss: 0.101235\n[Training] Epoch 038 | Batch 010/051 | Current Loss: 0.101316\n[Training] Epoch 038 | Batch 020/051 | Current Loss: 0.101226\n[Training] Epoch 038 | Batch 030/051 | Current Loss: 0.101267\n[Training] Epoch 038 | Batch 040/051 | Current Loss: 0.101272\n[Training] Epoch 038 | Batch 050/051 | Current Loss: 0.101292\n\n[Training] Epoch 038 Summary:\n  Avg Loss: 0.101299\n  Current LR: 1.25e-05\n[EarlyStopping] Loss improved (0.101340 → 0.101299). Saving model...\n[Training] Epoch 039 | Batch 000/051 | Current Loss: 0.101286\n[Training] Epoch 039 | Batch 010/051 | Current Loss: 0.101212\n[Training] Epoch 039 | Batch 020/051 | Current Loss: 0.101288\n[Training] Epoch 039 | Batch 030/051 | Current Loss: 0.101351\n[Training] Epoch 039 | Batch 040/051 | Current Loss: 0.101239\n[Training] Epoch 039 | Batch 050/051 | Current Loss: 0.101378\n\n[Training] Epoch 039 Summary:\n  Avg Loss: 0.101291\n  Current LR: 3.95e-06\n[EarlyStopping] Loss improved (0.101299 → 0.101291). Saving model...\n[Training] Epoch 040 | Batch 000/051 | Current Loss: 0.101366\n[Training] Epoch 040 | Batch 010/051 | Current Loss: 0.101227\n[Training] Epoch 040 | Batch 020/051 | Current Loss: 0.101391\n[Training] Epoch 040 | Batch 030/051 | Current Loss: 0.101225\n[Training] Epoch 040 | Batch 040/051 | Current Loss: 0.101247\n[Training] Epoch 040 | Batch 050/051 | Current Loss: 0.101177\n\n[Training] Epoch 040 Summary:\n  Avg Loss: 0.101292\n  Current LR: 1.22e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 041 | Batch 000/051 | Current Loss: 0.101269\n[Training] Epoch 041 | Batch 010/051 | Current Loss: 0.101204\n[Training] Epoch 041 | Batch 020/051 | Current Loss: 0.101330\n[Training] Epoch 041 | Batch 030/051 | Current Loss: 0.101219\n[Training] Epoch 041 | Batch 040/051 | Current Loss: 0.101228\n[Training] Epoch 041 | Batch 050/051 | Current Loss: 0.101358\n\n[Training] Epoch 041 Summary:\n  Avg Loss: 0.101294\n  Current LR: 1.19e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 042 | Batch 000/051 | Current Loss: 0.101307\n[Training] Epoch 042 | Batch 010/051 | Current Loss: 0.101317\n[Training] Epoch 042 | Batch 020/051 | Current Loss: 0.101157\n[Training] Epoch 042 | Batch 030/051 | Current Loss: 0.101346\n[Training] Epoch 042 | Batch 040/051 | Current Loss: 0.101255\n[Training] Epoch 042 | Batch 050/051 | Current Loss: 0.101238\n\n[Training] Epoch 042 Summary:\n  Avg Loss: 0.101226\n  Current LR: 1.10e-04\n[EarlyStopping] Loss improved (0.101291 → 0.101226). Saving model...\n[Training] Epoch 043 | Batch 000/051 | Current Loss: 0.101162\n[Training] Epoch 043 | Batch 010/051 | Current Loss: 0.101123\n[Training] Epoch 043 | Batch 020/051 | Current Loss: 0.101167\n[Training] Epoch 043 | Batch 030/051 | Current Loss: 0.101201\n[Training] Epoch 043 | Batch 040/051 | Current Loss: 0.101161\n[Training] Epoch 043 | Batch 050/051 | Current Loss: 0.101056\n\n[Training] Epoch 043 Summary:\n  Avg Loss: 0.101146\n  Current LR: 9.67e-05\n[EarlyStopping] Loss improved (0.101226 → 0.101146). Saving model...\n[Training] Epoch 044 | Batch 000/051 | Current Loss: 0.101080\n[Training] Epoch 044 | Batch 010/051 | Current Loss: 0.101065\n[Training] Epoch 044 | Batch 020/051 | Current Loss: 0.101041\n[Training] Epoch 044 | Batch 030/051 | Current Loss: 0.101135\n[Training] Epoch 044 | Batch 040/051 | Current Loss: 0.101017\n[Training] Epoch 044 | Batch 050/051 | Current Loss: 0.101146\n\n[Training] Epoch 044 Summary:\n  Avg Loss: 0.101107\n  Current LR: 7.99e-05\n[EarlyStopping] Loss improved (0.101146 → 0.101107). Saving model...\n[Training] Epoch 045 | Batch 000/051 | Current Loss: 0.100967\n[Training] Epoch 045 | Batch 010/051 | Current Loss: 0.101123\n[Training] Epoch 045 | Batch 020/051 | Current Loss: 0.101092\n[Training] Epoch 045 | Batch 030/051 | Current Loss: 0.100988\n[Training] Epoch 045 | Batch 040/051 | Current Loss: 0.101051\n[Training] Epoch 045 | Batch 050/051 | Current Loss: 0.101064\n\n[Training] Epoch 045 Summary:\n  Avg Loss: 0.101053\n  Current LR: 6.13e-05\n[EarlyStopping] Loss improved (0.101107 → 0.101053). Saving model...\n[Training] Epoch 046 | Batch 000/051 | Current Loss: 0.101045\n[Training] Epoch 046 | Batch 010/051 | Current Loss: 0.100985\n[Training] Epoch 046 | Batch 020/051 | Current Loss: 0.100991\n[Training] Epoch 046 | Batch 030/051 | Current Loss: 0.100966\n[Training] Epoch 046 | Batch 040/051 | Current Loss: 0.101074\n[Training] Epoch 046 | Batch 050/051 | Current Loss: 0.101008\n\n[Training] Epoch 046 Summary:\n  Avg Loss: 0.101016\n  Current LR: 4.27e-05\n[EarlyStopping] Loss improved (0.101053 → 0.101016). Saving model...\n[Training] Epoch 047 | Batch 000/051 | Current Loss: 0.100985\n[Training] Epoch 047 | Batch 010/051 | Current Loss: 0.100968\n[Training] Epoch 047 | Batch 020/051 | Current Loss: 0.101038\n[Training] Epoch 047 | Batch 030/051 | Current Loss: 0.100969\n[Training] Epoch 047 | Batch 040/051 | Current Loss: 0.101020\n[Training] Epoch 047 | Batch 050/051 | Current Loss: 0.101101\n\n[Training] Epoch 047 Summary:\n  Avg Loss: 0.100998\n  Current LR: 2.59e-05\n[EarlyStopping] Loss improved (0.101016 → 0.100998). Saving model...\n[Training] Epoch 048 | Batch 000/051 | Current Loss: 0.101069\n[Training] Epoch 048 | Batch 010/051 | Current Loss: 0.101026\n[Training] Epoch 048 | Batch 020/051 | Current Loss: 0.101191\n[Training] Epoch 048 | Batch 030/051 | Current Loss: 0.101020\n[Training] Epoch 048 | Batch 040/051 | Current Loss: 0.100935\n[Training] Epoch 048 | Batch 050/051 | Current Loss: 0.100888\n\n[Training] Epoch 048 Summary:\n  Avg Loss: 0.100976\n  Current LR: 1.25e-05\n[EarlyStopping] Loss improved (0.100998 → 0.100976). Saving model...\n[Training] Epoch 049 | Batch 000/051 | Current Loss: 0.101063\n[Training] Epoch 049 | Batch 010/051 | Current Loss: 0.101035\n[Training] Epoch 049 | Batch 020/051 | Current Loss: 0.100908\n[Training] Epoch 049 | Batch 030/051 | Current Loss: 0.101024\n[Training] Epoch 049 | Batch 040/051 | Current Loss: 0.101002\n[Training] Epoch 049 | Batch 050/051 | Current Loss: 0.100893\n\n[Training] Epoch 049 Summary:\n  Avg Loss: 0.100972\n  Current LR: 3.95e-06\n[EarlyStopping] Loss improved (0.100976 → 0.100972). Saving model...\n[Training] Epoch 050 | Batch 000/051 | Current Loss: 0.101027\n[Training] Epoch 050 | Batch 010/051 | Current Loss: 0.100907\n[Training] Epoch 050 | Batch 020/051 | Current Loss: 0.100912\n[Training] Epoch 050 | Batch 030/051 | Current Loss: 0.100959\n[Training] Epoch 050 | Batch 040/051 | Current Loss: 0.101025\n[Training] Epoch 050 | Batch 050/051 | Current Loss: 0.100984\n\n[Training] Epoch 050 Summary:\n  Avg Loss: 0.100966\n  Current LR: 1.22e-04\n[EarlyStopping] Loss improved (0.100972 → 0.100966). Saving model...\n[Training] Epoch 051 | Batch 000/051 | Current Loss: 0.100861\n[Training] Epoch 051 | Batch 010/051 | Current Loss: 0.101009\n[Training] Epoch 051 | Batch 020/051 | Current Loss: 0.100912\n[Training] Epoch 051 | Batch 030/051 | Current Loss: 0.100949\n[Training] Epoch 051 | Batch 040/051 | Current Loss: 0.101019\n[Training] Epoch 051 | Batch 050/051 | Current Loss: 0.101374\n\n[Training] Epoch 051 Summary:\n  Avg Loss: 0.100973\n  Current LR: 1.19e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 052 | Batch 000/051 | Current Loss: 0.101041\n[Training] Epoch 052 | Batch 010/051 | Current Loss: 0.100928\n[Training] Epoch 052 | Batch 020/051 | Current Loss: 0.101007\n[Training] Epoch 052 | Batch 030/051 | Current Loss: 0.100970\n[Training] Epoch 052 | Batch 040/051 | Current Loss: 0.100999\n[Training] Epoch 052 | Batch 050/051 | Current Loss: 0.100813\n\n[Training] Epoch 052 Summary:\n  Avg Loss: 0.100947\n  Current LR: 1.10e-04\n[EarlyStopping] Loss improved (0.100966 → 0.100947). Saving model...\n[Training] Epoch 053 | Batch 000/051 | Current Loss: 0.100960\n[Training] Epoch 053 | Batch 010/051 | Current Loss: 0.100847\n[Training] Epoch 053 | Batch 020/051 | Current Loss: 0.100806\n[Training] Epoch 053 | Batch 030/051 | Current Loss: 0.100873\n[Training] Epoch 053 | Batch 040/051 | Current Loss: 0.100808\n[Training] Epoch 053 | Batch 050/051 | Current Loss: 0.100898\n\n[Training] Epoch 053 Summary:\n  Avg Loss: 0.100876\n  Current LR: 9.67e-05\n[EarlyStopping] Loss improved (0.100947 → 0.100876). Saving model...\n[Training] Epoch 054 | Batch 000/051 | Current Loss: 0.100752\n[Training] Epoch 054 | Batch 010/051 | Current Loss: 0.100831\n[Training] Epoch 054 | Batch 020/051 | Current Loss: 0.100898\n[Training] Epoch 054 | Batch 030/051 | Current Loss: 0.100881\n[Training] Epoch 054 | Batch 040/051 | Current Loss: 0.100889\n[Training] Epoch 054 | Batch 050/051 | Current Loss: 0.100785\n\n[Training] Epoch 054 Summary:\n  Avg Loss: 0.100846\n  Current LR: 7.99e-05\n[EarlyStopping] Loss improved (0.100876 → 0.100846). Saving model...\n[Training] Epoch 055 | Batch 000/051 | Current Loss: 0.100757\n[Training] Epoch 055 | Batch 010/051 | Current Loss: 0.100817\n[Training] Epoch 055 | Batch 020/051 | Current Loss: 0.100876\n[Training] Epoch 055 | Batch 030/051 | Current Loss: 0.100793\n[Training] Epoch 055 | Batch 040/051 | Current Loss: 0.100829\n[Training] Epoch 055 | Batch 050/051 | Current Loss: 0.100828\n\n[Training] Epoch 055 Summary:\n  Avg Loss: 0.100824\n  Current LR: 6.13e-05\n[EarlyStopping] Loss improved (0.100846 → 0.100824). Saving model...\n[Training] Epoch 056 | Batch 000/051 | Current Loss: 0.100880\n[Training] Epoch 056 | Batch 010/051 | Current Loss: 0.100795\n[Training] Epoch 056 | Batch 020/051 | Current Loss: 0.100792\n[Training] Epoch 056 | Batch 030/051 | Current Loss: 0.100836\n[Training] Epoch 056 | Batch 040/051 | Current Loss: 0.100903\n[Training] Epoch 056 | Batch 050/051 | Current Loss: 0.100799\n\n[Training] Epoch 056 Summary:\n  Avg Loss: 0.100801\n  Current LR: 4.27e-05\n[EarlyStopping] Loss improved (0.100824 → 0.100801). Saving model...\n[Training] Epoch 057 | Batch 000/051 | Current Loss: 0.100784\n[Training] Epoch 057 | Batch 010/051 | Current Loss: 0.100767\n[Training] Epoch 057 | Batch 020/051 | Current Loss: 0.100746\n[Training] Epoch 057 | Batch 030/051 | Current Loss: 0.100868\n[Training] Epoch 057 | Batch 040/051 | Current Loss: 0.100745\n[Training] Epoch 057 | Batch 050/051 | Current Loss: 0.100911\n\n[Training] Epoch 057 Summary:\n  Avg Loss: 0.100783\n  Current LR: 2.59e-05\n[EarlyStopping] Loss improved (0.100801 → 0.100783). Saving model...\n[Training] Epoch 058 | Batch 000/051 | Current Loss: 0.100736\n[Training] Epoch 058 | Batch 010/051 | Current Loss: 0.100762\n[Training] Epoch 058 | Batch 020/051 | Current Loss: 0.100694\n[Training] Epoch 058 | Batch 030/051 | Current Loss: 0.100694\n[Training] Epoch 058 | Batch 040/051 | Current Loss: 0.100764\n[Training] Epoch 058 | Batch 050/051 | Current Loss: 0.100798\n\n[Training] Epoch 058 Summary:\n  Avg Loss: 0.100765\n  Current LR: 1.25e-05\n[EarlyStopping] Loss improved (0.100783 → 0.100765). Saving model...\n[Training] Epoch 059 | Batch 000/051 | Current Loss: 0.100803\n[Training] Epoch 059 | Batch 010/051 | Current Loss: 0.100731\n[Training] Epoch 059 | Batch 020/051 | Current Loss: 0.100819\n[Training] Epoch 059 | Batch 030/051 | Current Loss: 0.100777\n[Training] Epoch 059 | Batch 040/051 | Current Loss: 0.100786\n[Training] Epoch 059 | Batch 050/051 | Current Loss: 0.100704\n\n[Training] Epoch 059 Summary:\n  Avg Loss: 0.100766\n  Current LR: 3.95e-06\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 060 | Batch 000/051 | Current Loss: 0.100781\n[Training] Epoch 060 | Batch 010/051 | Current Loss: 0.100727\n[Training] Epoch 060 | Batch 020/051 | Current Loss: 0.100725\n[Training] Epoch 060 | Batch 030/051 | Current Loss: 0.100916\n[Training] Epoch 060 | Batch 040/051 | Current Loss: 0.100795\n[Training] Epoch 060 | Batch 050/051 | Current Loss: 0.100722\n\n[Training] Epoch 060 Summary:\n  Avg Loss: 0.100759\n  Current LR: 1.22e-04\n[EarlyStopping] Loss improved (0.100765 → 0.100759). Saving model...\n[Training] Epoch 061 | Batch 000/051 | Current Loss: 0.100844\n[Training] Epoch 061 | Batch 010/051 | Current Loss: 0.100809\n[Training] Epoch 061 | Batch 020/051 | Current Loss: 0.100761\n[Training] Epoch 061 | Batch 030/051 | Current Loss: 0.100832\n[Training] Epoch 061 | Batch 040/051 | Current Loss: 0.100734\n[Training] Epoch 061 | Batch 050/051 | Current Loss: 0.100775\n\n[Training] Epoch 061 Summary:\n  Avg Loss: 0.100763\n  Current LR: 1.19e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 062 | Batch 000/051 | Current Loss: 0.100682\n[Training] Epoch 062 | Batch 010/051 | Current Loss: 0.100759\n[Training] Epoch 062 | Batch 020/051 | Current Loss: 0.100691\n[Training] Epoch 062 | Batch 030/051 | Current Loss: 0.100750\n[Training] Epoch 062 | Batch 040/051 | Current Loss: 0.100654\n[Training] Epoch 062 | Batch 050/051 | Current Loss: 0.100670\n\n[Training] Epoch 062 Summary:\n  Avg Loss: 0.100739\n  Current LR: 1.10e-04\n[EarlyStopping] Loss improved (0.100759 → 0.100739). Saving model...\n[Training] Epoch 063 | Batch 000/051 | Current Loss: 0.100723\n[Training] Epoch 063 | Batch 010/051 | Current Loss: 0.100692\n[Training] Epoch 063 | Batch 020/051 | Current Loss: 0.100766\n[Training] Epoch 063 | Batch 030/051 | Current Loss: 0.100775\n[Training] Epoch 063 | Batch 040/051 | Current Loss: 0.100645\n[Training] Epoch 063 | Batch 050/051 | Current Loss: 0.100644\n\n[Training] Epoch 063 Summary:\n  Avg Loss: 0.100697\n  Current LR: 9.67e-05\n[EarlyStopping] Loss improved (0.100739 → 0.100697). Saving model...\n[Training] Epoch 064 | Batch 000/051 | Current Loss: 0.100697\n[Training] Epoch 064 | Batch 010/051 | Current Loss: 0.100732\n[Training] Epoch 064 | Batch 020/051 | Current Loss: 0.100682\n[Training] Epoch 064 | Batch 030/051 | Current Loss: 0.100738\n[Training] Epoch 064 | Batch 040/051 | Current Loss: 0.100706\n[Training] Epoch 064 | Batch 050/051 | Current Loss: 0.100638\n\n[Training] Epoch 064 Summary:\n  Avg Loss: 0.100685\n  Current LR: 7.99e-05\n[EarlyStopping] Loss improved (0.100697 → 0.100685). Saving model...\n[Training] Epoch 065 | Batch 000/051 | Current Loss: 0.100628\n[Training] Epoch 065 | Batch 010/051 | Current Loss: 0.100622\n[Training] Epoch 065 | Batch 020/051 | Current Loss: 0.100872\n[Training] Epoch 065 | Batch 030/051 | Current Loss: 0.100679\n[Training] Epoch 065 | Batch 040/051 | Current Loss: 0.100684\n[Training] Epoch 065 | Batch 050/051 | Current Loss: 0.100619\n\n[Training] Epoch 065 Summary:\n  Avg Loss: 0.100671\n  Current LR: 6.13e-05\n[EarlyStopping] Loss improved (0.100685 → 0.100671). Saving model...\n[Training] Epoch 066 | Batch 000/051 | Current Loss: 0.100744\n[Training] Epoch 066 | Batch 010/051 | Current Loss: 0.100712\n[Training] Epoch 066 | Batch 020/051 | Current Loss: 0.100638\n[Training] Epoch 066 | Batch 030/051 | Current Loss: 0.100575\n[Training] Epoch 066 | Batch 040/051 | Current Loss: 0.100620\n[Training] Epoch 066 | Batch 050/051 | Current Loss: 0.100691\n\n[Training] Epoch 066 Summary:\n  Avg Loss: 0.100653\n  Current LR: 4.27e-05\n[EarlyStopping] Loss improved (0.100671 → 0.100653). Saving model...\n[Training] Epoch 067 | Batch 000/051 | Current Loss: 0.100678\n[Training] Epoch 067 | Batch 010/051 | Current Loss: 0.100796\n[Training] Epoch 067 | Batch 020/051 | Current Loss: 0.100652\n[Training] Epoch 067 | Batch 030/051 | Current Loss: 0.100659\n[Training] Epoch 067 | Batch 040/051 | Current Loss: 0.100622\n[Training] Epoch 067 | Batch 050/051 | Current Loss: 0.100632\n\n[Training] Epoch 067 Summary:\n  Avg Loss: 0.100640\n  Current LR: 2.59e-05\n[EarlyStopping] Loss improved (0.100653 → 0.100640). Saving model...\n[Training] Epoch 068 | Batch 000/051 | Current Loss: 0.100626\n[Training] Epoch 068 | Batch 010/051 | Current Loss: 0.100559\n[Training] Epoch 068 | Batch 020/051 | Current Loss: 0.100666\n[Training] Epoch 068 | Batch 030/051 | Current Loss: 0.100623\n[Training] Epoch 068 | Batch 040/051 | Current Loss: 0.100631\n[Training] Epoch 068 | Batch 050/051 | Current Loss: 0.100542\n\n[Training] Epoch 068 Summary:\n  Avg Loss: 0.100628\n  Current LR: 1.25e-05\n[EarlyStopping] Loss improved (0.100640 → 0.100628). Saving model...\n[Training] Epoch 069 | Batch 000/051 | Current Loss: 0.100643\n[Training] Epoch 069 | Batch 010/051 | Current Loss: 0.100555\n[Training] Epoch 069 | Batch 020/051 | Current Loss: 0.100637\n[Training] Epoch 069 | Batch 030/051 | Current Loss: 0.100535\n[Training] Epoch 069 | Batch 040/051 | Current Loss: 0.100588\n[Training] Epoch 069 | Batch 050/051 | Current Loss: 0.100569\n\n[Training] Epoch 069 Summary:\n  Avg Loss: 0.100615\n  Current LR: 3.95e-06\n[EarlyStopping] Loss improved (0.100628 → 0.100615). Saving model...\n[Training] Epoch 070 | Batch 000/051 | Current Loss: 0.100690\n[Training] Epoch 070 | Batch 010/051 | Current Loss: 0.100664\n[Training] Epoch 070 | Batch 020/051 | Current Loss: 0.100701\n[Training] Epoch 070 | Batch 030/051 | Current Loss: 0.100649\n[Training] Epoch 070 | Batch 040/051 | Current Loss: 0.100621\n[Training] Epoch 070 | Batch 050/051 | Current Loss: 0.100686\n\n[Training] Epoch 070 Summary:\n  Avg Loss: 0.100626\n  Current LR: 1.22e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 071 | Batch 000/051 | Current Loss: 0.100646\n[Training] Epoch 071 | Batch 010/051 | Current Loss: 0.100687\n[Training] Epoch 071 | Batch 020/051 | Current Loss: 0.100628\n[Training] Epoch 071 | Batch 030/051 | Current Loss: 0.100640\n[Training] Epoch 071 | Batch 040/051 | Current Loss: 0.100596\n[Training] Epoch 071 | Batch 050/051 | Current Loss: 0.100533\n\n[Training] Epoch 071 Summary:\n  Avg Loss: 0.100633\n  Current LR: 1.19e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 072 | Batch 000/051 | Current Loss: 0.100689\n[Training] Epoch 072 | Batch 010/051 | Current Loss: 0.100579\n[Training] Epoch 072 | Batch 020/051 | Current Loss: 0.100608\n[Training] Epoch 072 | Batch 030/051 | Current Loss: 0.100588\n[Training] Epoch 072 | Batch 040/051 | Current Loss: 0.100673\n[Training] Epoch 072 | Batch 050/051 | Current Loss: 0.100618\n\n[Training] Epoch 072 Summary:\n  Avg Loss: 0.100610\n  Current LR: 1.10e-04\n[EarlyStopping] Loss improved (0.100615 → 0.100610). Saving model...\n[Training] Epoch 073 | Batch 000/051 | Current Loss: 0.100546\n[Training] Epoch 073 | Batch 010/051 | Current Loss: 0.100554\n[Training] Epoch 073 | Batch 020/051 | Current Loss: 0.100562\n[Training] Epoch 073 | Batch 030/051 | Current Loss: 0.100631\n[Training] Epoch 073 | Batch 040/051 | Current Loss: 0.100598\n[Training] Epoch 073 | Batch 050/051 | Current Loss: 0.100667\n\n[Training] Epoch 073 Summary:\n  Avg Loss: 0.100595\n  Current LR: 9.67e-05\n[EarlyStopping] Loss improved (0.100610 → 0.100595). Saving model...\n[Training] Epoch 074 | Batch 000/051 | Current Loss: 0.100615\n[Training] Epoch 074 | Batch 010/051 | Current Loss: 0.100574\n[Training] Epoch 074 | Batch 020/051 | Current Loss: 0.100535\n[Training] Epoch 074 | Batch 030/051 | Current Loss: 0.100588\n[Training] Epoch 074 | Batch 040/051 | Current Loss: 0.100509\n[Training] Epoch 074 | Batch 050/051 | Current Loss: 0.100537\n\n[Training] Epoch 074 Summary:\n  Avg Loss: 0.100567\n  Current LR: 7.99e-05\n[EarlyStopping] Loss improved (0.100595 → 0.100567). Saving model...\n[Training] Epoch 075 | Batch 000/051 | Current Loss: 0.100553\n[Training] Epoch 075 | Batch 010/051 | Current Loss: 0.100511\n[Training] Epoch 075 | Batch 020/051 | Current Loss: 0.100511\n[Training] Epoch 075 | Batch 030/051 | Current Loss: 0.100532\n[Training] Epoch 075 | Batch 040/051 | Current Loss: 0.100523\n[Training] Epoch 075 | Batch 050/051 | Current Loss: 0.100539\n\n[Training] Epoch 075 Summary:\n  Avg Loss: 0.100551\n  Current LR: 6.13e-05\n[EarlyStopping] Loss improved (0.100567 → 0.100551). Saving model...\n[Training] Epoch 076 | Batch 000/051 | Current Loss: 0.100543\n[Training] Epoch 076 | Batch 010/051 | Current Loss: 0.100541\n[Training] Epoch 076 | Batch 020/051 | Current Loss: 0.100597\n[Training] Epoch 076 | Batch 030/051 | Current Loss: 0.100501\n[Training] Epoch 076 | Batch 040/051 | Current Loss: 0.100595\n[Training] Epoch 076 | Batch 050/051 | Current Loss: 0.100584\n\n[Training] Epoch 076 Summary:\n  Avg Loss: 0.100541\n  Current LR: 4.27e-05\n[EarlyStopping] Loss improved (0.100551 → 0.100541). Saving model...\n[Training] Epoch 077 | Batch 000/051 | Current Loss: 0.100566\n[Training] Epoch 077 | Batch 010/051 | Current Loss: 0.100480\n[Training] Epoch 077 | Batch 020/051 | Current Loss: 0.100497\n[Training] Epoch 077 | Batch 030/051 | Current Loss: 0.100543\n[Training] Epoch 077 | Batch 040/051 | Current Loss: 0.100524\n[Training] Epoch 077 | Batch 050/051 | Current Loss: 0.100527\n\n[Training] Epoch 077 Summary:\n  Avg Loss: 0.100541\n  Current LR: 2.59e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 078 | Batch 000/051 | Current Loss: 0.100512\n[Training] Epoch 078 | Batch 010/051 | Current Loss: 0.100541\n[Training] Epoch 078 | Batch 020/051 | Current Loss: 0.100534\n[Training] Epoch 078 | Batch 030/051 | Current Loss: 0.100577\n[Training] Epoch 078 | Batch 040/051 | Current Loss: 0.100509\n[Training] Epoch 078 | Batch 050/051 | Current Loss: 0.100527\n\n[Training] Epoch 078 Summary:\n  Avg Loss: 0.100528\n  Current LR: 1.25e-05\n[EarlyStopping] Loss improved (0.100541 → 0.100528). Saving model...\n[Training] Epoch 079 | Batch 000/051 | Current Loss: 0.100477\n[Training] Epoch 079 | Batch 010/051 | Current Loss: 0.100505\n[Training] Epoch 079 | Batch 020/051 | Current Loss: 0.100482\n[Training] Epoch 079 | Batch 030/051 | Current Loss: 0.100536\n[Training] Epoch 079 | Batch 040/051 | Current Loss: 0.100502\n[Training] Epoch 079 | Batch 050/051 | Current Loss: 0.100532\n\n[Training] Epoch 079 Summary:\n  Avg Loss: 0.100528\n  Current LR: 3.95e-06\n[EarlyStopping] Loss improved (0.100528 → 0.100528). Saving model...\n[Training] Epoch 080 | Batch 000/051 | Current Loss: 0.100468\n[Training] Epoch 080 | Batch 010/051 | Current Loss: 0.100456\n[Training] Epoch 080 | Batch 020/051 | Current Loss: 0.100511\n[Training] Epoch 080 | Batch 030/051 | Current Loss: 0.100509\n[Training] Epoch 080 | Batch 040/051 | Current Loss: 0.100495\n[Training] Epoch 080 | Batch 050/051 | Current Loss: 0.100484\n\n[Training] Epoch 080 Summary:\n  Avg Loss: 0.100523\n  Current LR: 1.22e-04\n[EarlyStopping] Loss improved (0.100528 → 0.100523). Saving model...\n[Training] Epoch 081 | Batch 000/051 | Current Loss: 0.100587\n[Training] Epoch 081 | Batch 010/051 | Current Loss: 0.100508\n[Training] Epoch 081 | Batch 020/051 | Current Loss: 0.100552\n[Training] Epoch 081 | Batch 030/051 | Current Loss: 0.100492\n[Training] Epoch 081 | Batch 040/051 | Current Loss: 0.100504\n[Training] Epoch 081 | Batch 050/051 | Current Loss: 0.100477\n\n[Training] Epoch 081 Summary:\n  Avg Loss: 0.100530\n  Current LR: 1.19e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 082 | Batch 000/051 | Current Loss: 0.100490\n[Training] Epoch 082 | Batch 010/051 | Current Loss: 0.100502\n[Training] Epoch 082 | Batch 020/051 | Current Loss: 0.100541\n[Training] Epoch 082 | Batch 030/051 | Current Loss: 0.100598\n[Training] Epoch 082 | Batch 040/051 | Current Loss: 0.100444\n[Training] Epoch 082 | Batch 050/051 | Current Loss: 0.100555\n\n[Training] Epoch 082 Summary:\n  Avg Loss: 0.100515\n  Current LR: 1.10e-04\n[EarlyStopping] Loss improved (0.100523 → 0.100515). Saving model...\n[Training] Epoch 083 | Batch 000/051 | Current Loss: 0.100467\n[Training] Epoch 083 | Batch 010/051 | Current Loss: 0.100558\n[Training] Epoch 083 | Batch 020/051 | Current Loss: 0.100457\n[Training] Epoch 083 | Batch 030/051 | Current Loss: 0.100533\n[Training] Epoch 083 | Batch 040/051 | Current Loss: 0.100542\n[Training] Epoch 083 | Batch 050/051 | Current Loss: 0.100460\n\n[Training] Epoch 083 Summary:\n  Avg Loss: 0.100500\n  Current LR: 9.67e-05\n[EarlyStopping] Loss improved (0.100515 → 0.100500). Saving model...\n[Training] Epoch 084 | Batch 000/051 | Current Loss: 0.100432\n[Training] Epoch 084 | Batch 010/051 | Current Loss: 0.100454\n[Training] Epoch 084 | Batch 020/051 | Current Loss: 0.100491\n[Training] Epoch 084 | Batch 030/051 | Current Loss: 0.100464\n[Training] Epoch 084 | Batch 040/051 | Current Loss: 0.100492\n[Training] Epoch 084 | Batch 050/051 | Current Loss: 0.100446\n\n[Training] Epoch 084 Summary:\n  Avg Loss: 0.100492\n  Current LR: 7.99e-05\n[EarlyStopping] Loss improved (0.100500 → 0.100492). Saving model...\n[Training] Epoch 085 | Batch 000/051 | Current Loss: 0.100422\n[Training] Epoch 085 | Batch 010/051 | Current Loss: 0.100419\n[Training] Epoch 085 | Batch 020/051 | Current Loss: 0.100431\n[Training] Epoch 085 | Batch 030/051 | Current Loss: 0.100510\n[Training] Epoch 085 | Batch 040/051 | Current Loss: 0.100521\n[Training] Epoch 085 | Batch 050/051 | Current Loss: 0.100493\n\n[Training] Epoch 085 Summary:\n  Avg Loss: 0.100474\n  Current LR: 6.13e-05\n[EarlyStopping] Loss improved (0.100492 → 0.100474). Saving model...\n[Training] Epoch 086 | Batch 000/051 | Current Loss: 0.100522\n[Training] Epoch 086 | Batch 010/051 | Current Loss: 0.100483\n[Training] Epoch 086 | Batch 020/051 | Current Loss: 0.100491\n[Training] Epoch 086 | Batch 030/051 | Current Loss: 0.100420\n[Training] Epoch 086 | Batch 040/051 | Current Loss: 0.100482\n[Training] Epoch 086 | Batch 050/051 | Current Loss: 0.100504\n\n[Training] Epoch 086 Summary:\n  Avg Loss: 0.100463\n  Current LR: 4.27e-05\n[EarlyStopping] Loss improved (0.100474 → 0.100463). Saving model...\n[Training] Epoch 087 | Batch 000/051 | Current Loss: 0.100518\n[Training] Epoch 087 | Batch 010/051 | Current Loss: 0.100458\n[Training] Epoch 087 | Batch 020/051 | Current Loss: 0.100417\n[Training] Epoch 087 | Batch 030/051 | Current Loss: 0.100485\n[Training] Epoch 087 | Batch 040/051 | Current Loss: 0.100404\n[Training] Epoch 087 | Batch 050/051 | Current Loss: 0.100427\n\n[Training] Epoch 087 Summary:\n  Avg Loss: 0.100457\n  Current LR: 2.59e-05\n[EarlyStopping] Loss improved (0.100463 → 0.100457). Saving model...\n[Training] Epoch 088 | Batch 000/051 | Current Loss: 0.100410\n[Training] Epoch 088 | Batch 010/051 | Current Loss: 0.100533\n[Training] Epoch 088 | Batch 020/051 | Current Loss: 0.100463\n[Training] Epoch 088 | Batch 030/051 | Current Loss: 0.100495\n[Training] Epoch 088 | Batch 040/051 | Current Loss: 0.100448\n[Training] Epoch 088 | Batch 050/051 | Current Loss: 0.100402\n\n[Training] Epoch 088 Summary:\n  Avg Loss: 0.100451\n  Current LR: 1.25e-05\n[EarlyStopping] Loss improved (0.100457 → 0.100451). Saving model...\n[Training] Epoch 089 | Batch 000/051 | Current Loss: 0.100426\n[Training] Epoch 089 | Batch 010/051 | Current Loss: 0.100425\n[Training] Epoch 089 | Batch 020/051 | Current Loss: 0.100450\n[Training] Epoch 089 | Batch 030/051 | Current Loss: 0.100405\n[Training] Epoch 089 | Batch 040/051 | Current Loss: 0.100398\n[Training] Epoch 089 | Batch 050/051 | Current Loss: 0.100356\n\n[Training] Epoch 089 Summary:\n  Avg Loss: 0.100444\n  Current LR: 3.95e-06\n[EarlyStopping] Loss improved (0.100451 → 0.100444). Saving model...\n[Training] Epoch 090 | Batch 000/051 | Current Loss: 0.100435\n[Training] Epoch 090 | Batch 010/051 | Current Loss: 0.100421\n[Training] Epoch 090 | Batch 020/051 | Current Loss: 0.100477\n[Training] Epoch 090 | Batch 030/051 | Current Loss: 0.100423\n[Training] Epoch 090 | Batch 040/051 | Current Loss: 0.100446\n[Training] Epoch 090 | Batch 050/051 | Current Loss: 0.100382\n\n[Training] Epoch 090 Summary:\n  Avg Loss: 0.100447\n  Current LR: 1.22e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 091 | Batch 000/051 | Current Loss: 0.100495\n[Training] Epoch 091 | Batch 010/051 | Current Loss: 0.100492\n[Training] Epoch 091 | Batch 020/051 | Current Loss: 0.100447\n[Training] Epoch 091 | Batch 030/051 | Current Loss: 0.100414\n[Training] Epoch 091 | Batch 040/051 | Current Loss: 0.100388\n[Training] Epoch 091 | Batch 050/051 | Current Loss: 0.100437\n\n[Training] Epoch 091 Summary:\n  Avg Loss: 0.100462\n  Current LR: 1.19e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 092 | Batch 000/051 | Current Loss: 0.100469\n[Training] Epoch 092 | Batch 010/051 | Current Loss: 0.100456\n[Training] Epoch 092 | Batch 020/051 | Current Loss: 0.100473\n[Training] Epoch 092 | Batch 030/051 | Current Loss: 0.100497\n[Training] Epoch 092 | Batch 040/051 | Current Loss: 0.100492\n[Training] Epoch 092 | Batch 050/051 | Current Loss: 0.100466\n\n[Training] Epoch 092 Summary:\n  Avg Loss: 0.100446\n  Current LR: 1.10e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 093 | Batch 000/051 | Current Loss: 0.100558\n[Training] Epoch 093 | Batch 010/051 | Current Loss: 0.100413\n[Training] Epoch 093 | Batch 020/051 | Current Loss: 0.100476\n[Training] Epoch 093 | Batch 030/051 | Current Loss: 0.100404\n[Training] Epoch 093 | Batch 040/051 | Current Loss: 0.100392\n[Training] Epoch 093 | Batch 050/051 | Current Loss: 0.100505\n\n[Training] Epoch 093 Summary:\n  Avg Loss: 0.100428\n  Current LR: 9.67e-05\n[EarlyStopping] Loss improved (0.100444 → 0.100428). Saving model...\n[Training] Epoch 094 | Batch 000/051 | Current Loss: 0.100350\n[Training] Epoch 094 | Batch 010/051 | Current Loss: 0.100379\n[Training] Epoch 094 | Batch 020/051 | Current Loss: 0.100442\n[Training] Epoch 094 | Batch 030/051 | Current Loss: 0.100439\n[Training] Epoch 094 | Batch 040/051 | Current Loss: 0.100448\n[Training] Epoch 094 | Batch 050/051 | Current Loss: 0.100340\n\n[Training] Epoch 094 Summary:\n  Avg Loss: 0.100416\n  Current LR: 7.99e-05\n[EarlyStopping] Loss improved (0.100428 → 0.100416). Saving model...\n[Training] Epoch 095 | Batch 000/051 | Current Loss: 0.100447\n[Training] Epoch 095 | Batch 010/051 | Current Loss: 0.100423\n[Training] Epoch 095 | Batch 020/051 | Current Loss: 0.100432\n[Training] Epoch 095 | Batch 030/051 | Current Loss: 0.100404\n[Training] Epoch 095 | Batch 040/051 | Current Loss: 0.100418\n[Training] Epoch 095 | Batch 050/051 | Current Loss: 0.100432\n\n[Training] Epoch 095 Summary:\n  Avg Loss: 0.100416\n  Current LR: 6.13e-05\n[EarlyStopping] Loss improved (0.100416 → 0.100416). Saving model...\n[Training] Epoch 096 | Batch 000/051 | Current Loss: 0.100407\n[Training] Epoch 096 | Batch 010/051 | Current Loss: 0.100419\n[Training] Epoch 096 | Batch 020/051 | Current Loss: 0.100364\n[Training] Epoch 096 | Batch 030/051 | Current Loss: 0.100419\n[Training] Epoch 096 | Batch 040/051 | Current Loss: 0.100426\n[Training] Epoch 096 | Batch 050/051 | Current Loss: 0.100400\n\n[Training] Epoch 096 Summary:\n  Avg Loss: 0.100401\n  Current LR: 4.27e-05\n[EarlyStopping] Loss improved (0.100416 → 0.100401). Saving model...\n[Training] Epoch 097 | Batch 000/051 | Current Loss: 0.100444\n[Training] Epoch 097 | Batch 010/051 | Current Loss: 0.100415\n[Training] Epoch 097 | Batch 020/051 | Current Loss: 0.100339\n[Training] Epoch 097 | Batch 030/051 | Current Loss: 0.100412\n[Training] Epoch 097 | Batch 040/051 | Current Loss: 0.100329\n[Training] Epoch 097 | Batch 050/051 | Current Loss: 0.100364\n\n[Training] Epoch 097 Summary:\n  Avg Loss: 0.100392\n  Current LR: 2.59e-05\n[EarlyStopping] Loss improved (0.100401 → 0.100392). Saving model...\n[Training] Epoch 098 | Batch 000/051 | Current Loss: 0.100413\n[Training] Epoch 098 | Batch 010/051 | Current Loss: 0.100372\n[Training] Epoch 098 | Batch 020/051 | Current Loss: 0.100375\n[Training] Epoch 098 | Batch 030/051 | Current Loss: 0.100353\n[Training] Epoch 098 | Batch 040/051 | Current Loss: 0.100404\n[Training] Epoch 098 | Batch 050/051 | Current Loss: 0.100429\n\n[Training] Epoch 098 Summary:\n  Avg Loss: 0.100387\n  Current LR: 1.25e-05\n[EarlyStopping] Loss improved (0.100392 → 0.100387). Saving model...\n[Training] Epoch 099 | Batch 000/051 | Current Loss: 0.100373\n[Training] Epoch 099 | Batch 010/051 | Current Loss: 0.100347\n[Training] Epoch 099 | Batch 020/051 | Current Loss: 0.100344\n[Training] Epoch 099 | Batch 030/051 | Current Loss: 0.100353\n[Training] Epoch 099 | Batch 040/051 | Current Loss: 0.100367\n[Training] Epoch 099 | Batch 050/051 | Current Loss: 0.100390\n\n[Training] Epoch 099 Summary:\n  Avg Loss: 0.100385\n  Current LR: 3.95e-06\n[EarlyStopping] Loss improved (0.100387 → 0.100385). Saving model...\n[Training] Epoch 100 | Batch 000/051 | Current Loss: 0.100377\n[Training] Epoch 100 | Batch 010/051 | Current Loss: 0.100379\n[Training] Epoch 100 | Batch 020/051 | Current Loss: 0.100344\n[Training] Epoch 100 | Batch 030/051 | Current Loss: 0.100396\n[Training] Epoch 100 | Batch 040/051 | Current Loss: 0.100348\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 15:45:19,861] Trial 3 finished with value: 0.10038027824724421 and parameters: {'lr': 0.0001215802890237896, 'batch_size': 256, 'bottleneck_width': 1024, 'dropout_rate': 0.3438623105865585, 'alpha': 0.8858620925213587, 'weight_decay': 6.498883072157955e-06}. Best is trial 1 with value: 0.10011662966480442.\n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 100 | Batch 050/051 | Current Loss: 0.100441\n\n[Training] Epoch 100 Summary:\n  Avg Loss: 0.100380\n  Current LR: 1.22e-04\n[EarlyStopping] Loss improved (0.100385 → 0.100380). Saving model...\n\n[Optuna] Trial 3 completed with best loss: 0.100380\n\n==================================================\nStarting Optuna Trial 4\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 9.496433119337225e-05\n  batch_size: 128\n  bottleneck_width: 512\n  dropout_rate: 0.20473365624673523\n  alpha: 0.6536122610703325\n  weight_decay: 4.61096933635227e-06\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 128, Total batches: 102\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 512\n  - Dropout rate: 0.20473365624673523\n[Model] Architecture initialized successfully\n[Model] Total parameters: 1,347,904\n[Optimizer] Initialized with lr=9.50e-05, weight_decay=4.61e-06\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.6536122610703325\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/102 | Current Loss: 1.355298\n[Training] Epoch 001 | Batch 010/102 | Current Loss: 0.597070\n[Training] Epoch 001 | Batch 020/102 | Current Loss: 0.381259\n[Training] Epoch 001 | Batch 030/102 | Current Loss: 0.244686\n[Training] Epoch 001 | Batch 040/102 | Current Loss: 0.180169\n[Training] Epoch 001 | Batch 050/102 | Current Loss: 0.149857\n[Training] Epoch 001 | Batch 060/102 | Current Loss: 0.138792\n[Training] Epoch 001 | Batch 070/102 | Current Loss: 0.135218\n[Training] Epoch 001 | Batch 080/102 | Current Loss: 0.125975\n[Training] Epoch 001 | Batch 090/102 | Current Loss: 0.122856\n[Training] Epoch 001 | Batch 100/102 | Current Loss: 0.123105\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.269285\n  Current LR: 9.27e-05\n[EarlyStopping] Loss improved (inf → 0.269285). Saving model...\n[Training] Epoch 002 | Batch 000/102 | Current Loss: 0.118878\n[Training] Epoch 002 | Batch 010/102 | Current Loss: 0.121066\n[Training] Epoch 002 | Batch 020/102 | Current Loss: 0.118490\n[Training] Epoch 002 | Batch 030/102 | Current Loss: 0.118238\n[Training] Epoch 002 | Batch 040/102 | Current Loss: 0.115091\n[Training] Epoch 002 | Batch 050/102 | Current Loss: 0.115362\n[Training] Epoch 002 | Batch 060/102 | Current Loss: 0.113484\n[Training] Epoch 002 | Batch 070/102 | Current Loss: 0.112782\n[Training] Epoch 002 | Batch 080/102 | Current Loss: 0.111838\n[Training] Epoch 002 | Batch 090/102 | Current Loss: 0.111941\n[Training] Epoch 002 | Batch 100/102 | Current Loss: 0.110811\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.115509\n  Current LR: 8.60e-05\n[EarlyStopping] Loss improved (0.269285 → 0.115509). Saving model...\n[Training] Epoch 003 | Batch 000/102 | Current Loss: 0.111226\n[Training] Epoch 003 | Batch 010/102 | Current Loss: 0.111674\n[Training] Epoch 003 | Batch 020/102 | Current Loss: 0.110763\n[Training] Epoch 003 | Batch 030/102 | Current Loss: 0.110038\n[Training] Epoch 003 | Batch 040/102 | Current Loss: 0.109754\n[Training] Epoch 003 | Batch 050/102 | Current Loss: 0.108980\n[Training] Epoch 003 | Batch 060/102 | Current Loss: 0.108639\n[Training] Epoch 003 | Batch 070/102 | Current Loss: 0.109164\n[Training] Epoch 003 | Batch 080/102 | Current Loss: 0.115264\n[Training] Epoch 003 | Batch 090/102 | Current Loss: 0.108313\n[Training] Epoch 003 | Batch 100/102 | Current Loss: 0.107909\n\n[Training] Epoch 003 Summary:\n  Avg Loss: 0.109937\n  Current LR: 7.56e-05\n[EarlyStopping] Loss improved (0.115509 → 0.109937). Saving model...\n[Training] Epoch 004 | Batch 000/102 | Current Loss: 0.107890\n[Training] Epoch 004 | Batch 010/102 | Current Loss: 0.107921\n[Training] Epoch 004 | Batch 020/102 | Current Loss: 0.108642\n[Training] Epoch 004 | Batch 030/102 | Current Loss: 0.107408\n[Training] Epoch 004 | Batch 040/102 | Current Loss: 0.107168\n[Training] Epoch 004 | Batch 050/102 | Current Loss: 0.107096\n[Training] Epoch 004 | Batch 060/102 | Current Loss: 0.107391\n[Training] Epoch 004 | Batch 070/102 | Current Loss: 0.107008\n[Training] Epoch 004 | Batch 080/102 | Current Loss: 0.107542\n[Training] Epoch 004 | Batch 090/102 | Current Loss: 0.106831\n[Training] Epoch 004 | Batch 100/102 | Current Loss: 0.106769\n\n[Training] Epoch 004 Summary:\n  Avg Loss: 0.107560\n  Current LR: 6.25e-05\n[EarlyStopping] Loss improved (0.109937 → 0.107560). Saving model...\n[Training] Epoch 005 | Batch 000/102 | Current Loss: 0.107030\n[Training] Epoch 005 | Batch 010/102 | Current Loss: 0.106008\n[Training] Epoch 005 | Batch 020/102 | Current Loss: 0.105769\n[Training] Epoch 005 | Batch 030/102 | Current Loss: 0.107474\n[Training] Epoch 005 | Batch 040/102 | Current Loss: 0.106416\n[Training] Epoch 005 | Batch 050/102 | Current Loss: 0.105898\n[Training] Epoch 005 | Batch 060/102 | Current Loss: 0.106026\n[Training] Epoch 005 | Batch 070/102 | Current Loss: 0.105771\n[Training] Epoch 005 | Batch 080/102 | Current Loss: 0.106023\n[Training] Epoch 005 | Batch 090/102 | Current Loss: 0.105636\n[Training] Epoch 005 | Batch 100/102 | Current Loss: 0.105538\n\n[Training] Epoch 005 Summary:\n  Avg Loss: 0.106384\n  Current LR: 4.80e-05\n[EarlyStopping] Loss improved (0.107560 → 0.106384). Saving model...\n[Training] Epoch 006 | Batch 000/102 | Current Loss: 0.105436\n[Training] Epoch 006 | Batch 010/102 | Current Loss: 0.105638\n[Training] Epoch 006 | Batch 020/102 | Current Loss: 0.105721\n[Training] Epoch 006 | Batch 030/102 | Current Loss: 0.105377\n[Training] Epoch 006 | Batch 040/102 | Current Loss: 0.105234\n[Training] Epoch 006 | Batch 050/102 | Current Loss: 0.105520\n[Training] Epoch 006 | Batch 060/102 | Current Loss: 0.106164\n[Training] Epoch 006 | Batch 070/102 | Current Loss: 0.105091\n[Training] Epoch 006 | Batch 080/102 | Current Loss: 0.104885\n[Training] Epoch 006 | Batch 090/102 | Current Loss: 0.105653\n[Training] Epoch 006 | Batch 100/102 | Current Loss: 0.105365\n\n[Training] Epoch 006 Summary:\n  Avg Loss: 0.105470\n  Current LR: 3.35e-05\n[EarlyStopping] Loss improved (0.106384 → 0.105470). Saving model...\n[Training] Epoch 007 | Batch 000/102 | Current Loss: 0.105169\n[Training] Epoch 007 | Batch 010/102 | Current Loss: 0.105085\n[Training] Epoch 007 | Batch 020/102 | Current Loss: 0.104743\n[Training] Epoch 007 | Batch 030/102 | Current Loss: 0.104800\n[Training] Epoch 007 | Batch 040/102 | Current Loss: 0.104623\n[Training] Epoch 007 | Batch 050/102 | Current Loss: 0.104692\n[Training] Epoch 007 | Batch 060/102 | Current Loss: 0.104753\n[Training] Epoch 007 | Batch 070/102 | Current Loss: 0.104666\n[Training] Epoch 007 | Batch 080/102 | Current Loss: 0.104828\n[Training] Epoch 007 | Batch 090/102 | Current Loss: 0.104814\n[Training] Epoch 007 | Batch 100/102 | Current Loss: 0.105209\n\n[Training] Epoch 007 Summary:\n  Avg Loss: 0.105003\n  Current LR: 2.04e-05\n[EarlyStopping] Loss improved (0.105470 → 0.105003). Saving model...\n[Training] Epoch 008 | Batch 000/102 | Current Loss: 0.104916\n[Training] Epoch 008 | Batch 010/102 | Current Loss: 0.104933\n[Training] Epoch 008 | Batch 020/102 | Current Loss: 0.104691\n[Training] Epoch 008 | Batch 030/102 | Current Loss: 0.104455\n[Training] Epoch 008 | Batch 040/102 | Current Loss: 0.104946\n[Training] Epoch 008 | Batch 050/102 | Current Loss: 0.104339\n[Training] Epoch 008 | Batch 060/102 | Current Loss: 0.106056\n[Training] Epoch 008 | Batch 070/102 | Current Loss: 0.104579\n[Training] Epoch 008 | Batch 080/102 | Current Loss: 0.104830\n[Training] Epoch 008 | Batch 090/102 | Current Loss: 0.104498\n[Training] Epoch 008 | Batch 100/102 | Current Loss: 0.104301\n\n[Training] Epoch 008 Summary:\n  Avg Loss: 0.104735\n  Current LR: 9.97e-06\n[EarlyStopping] Loss improved (0.105003 → 0.104735). Saving model...\n[Training] Epoch 009 | Batch 000/102 | Current Loss: 0.104267\n[Training] Epoch 009 | Batch 010/102 | Current Loss: 0.106005\n[Training] Epoch 009 | Batch 020/102 | Current Loss: 0.104258\n[Training] Epoch 009 | Batch 030/102 | Current Loss: 0.104655\n[Training] Epoch 009 | Batch 040/102 | Current Loss: 0.104790\n[Training] Epoch 009 | Batch 050/102 | Current Loss: 0.104497\n[Training] Epoch 009 | Batch 060/102 | Current Loss: 0.104658\n[Training] Epoch 009 | Batch 070/102 | Current Loss: 0.104373\n[Training] Epoch 009 | Batch 080/102 | Current Loss: 0.104252\n[Training] Epoch 009 | Batch 090/102 | Current Loss: 0.104346\n[Training] Epoch 009 | Batch 100/102 | Current Loss: 0.104277\n\n[Training] Epoch 009 Summary:\n  Avg Loss: 0.104574\n  Current LR: 3.30e-06\n[EarlyStopping] Loss improved (0.104735 → 0.104574). Saving model...\n[Training] Epoch 010 | Batch 000/102 | Current Loss: 0.104127\n[Training] Epoch 010 | Batch 010/102 | Current Loss: 0.104182\n[Training] Epoch 010 | Batch 020/102 | Current Loss: 0.105077\n[Training] Epoch 010 | Batch 030/102 | Current Loss: 0.104346\n[Training] Epoch 010 | Batch 040/102 | Current Loss: 0.104499\n[Training] Epoch 010 | Batch 050/102 | Current Loss: 0.103883\n[Training] Epoch 010 | Batch 060/102 | Current Loss: 0.104246\n[Training] Epoch 010 | Batch 070/102 | Current Loss: 0.104556\n[Training] Epoch 010 | Batch 080/102 | Current Loss: 0.104265\n[Training] Epoch 010 | Batch 090/102 | Current Loss: 0.104598\n[Training] Epoch 010 | Batch 100/102 | Current Loss: 0.105216\n\n[Training] Epoch 010 Summary:\n  Avg Loss: 0.104451\n  Current LR: 9.50e-05\n[EarlyStopping] Loss improved (0.104574 → 0.104451). Saving model...\n[Training] Epoch 011 | Batch 000/102 | Current Loss: 0.104886\n[Training] Epoch 011 | Batch 010/102 | Current Loss: 0.104730\n[Training] Epoch 011 | Batch 020/102 | Current Loss: 0.104721\n[Training] Epoch 011 | Batch 030/102 | Current Loss: 0.104007\n[Training] Epoch 011 | Batch 040/102 | Current Loss: 0.104140\n[Training] Epoch 011 | Batch 050/102 | Current Loss: 0.104049\n[Training] Epoch 011 | Batch 060/102 | Current Loss: 0.103589\n[Training] Epoch 011 | Batch 070/102 | Current Loss: 0.104098\n[Training] Epoch 011 | Batch 080/102 | Current Loss: 0.103901\n[Training] Epoch 011 | Batch 090/102 | Current Loss: 0.103678\n[Training] Epoch 011 | Batch 100/102 | Current Loss: 0.103517\n\n[Training] Epoch 011 Summary:\n  Avg Loss: 0.104344\n  Current LR: 9.27e-05\n[EarlyStopping] Loss improved (0.104451 → 0.104344). Saving model...\n[Training] Epoch 012 | Batch 000/102 | Current Loss: 0.103802\n[Training] Epoch 012 | Batch 010/102 | Current Loss: 0.103933\n[Training] Epoch 012 | Batch 020/102 | Current Loss: 0.103656\n[Training] Epoch 012 | Batch 030/102 | Current Loss: 0.103327\n[Training] Epoch 012 | Batch 040/102 | Current Loss: 0.103544\n[Training] Epoch 012 | Batch 050/102 | Current Loss: 0.103171\n[Training] Epoch 012 | Batch 060/102 | Current Loss: 0.103507\n[Training] Epoch 012 | Batch 070/102 | Current Loss: 0.103121\n[Training] Epoch 012 | Batch 080/102 | Current Loss: 0.103196\n[Training] Epoch 012 | Batch 090/102 | Current Loss: 0.103200\n[Training] Epoch 012 | Batch 100/102 | Current Loss: 0.103329\n\n[Training] Epoch 012 Summary:\n  Avg Loss: 0.103666\n  Current LR: 8.60e-05\n[EarlyStopping] Loss improved (0.104344 → 0.103666). Saving model...\n[Training] Epoch 013 | Batch 000/102 | Current Loss: 0.103252\n[Training] Epoch 013 | Batch 010/102 | Current Loss: 0.103444\n[Training] Epoch 013 | Batch 020/102 | Current Loss: 0.103141\n[Training] Epoch 013 | Batch 030/102 | Current Loss: 0.102922\n[Training] Epoch 013 | Batch 040/102 | Current Loss: 0.103048\n[Training] Epoch 013 | Batch 050/102 | Current Loss: 0.103191\n[Training] Epoch 013 | Batch 060/102 | Current Loss: 0.103224\n[Training] Epoch 013 | Batch 070/102 | Current Loss: 0.102894\n[Training] Epoch 013 | Batch 080/102 | Current Loss: 0.102634\n[Training] Epoch 013 | Batch 090/102 | Current Loss: 0.103789\n[Training] Epoch 013 | Batch 100/102 | Current Loss: 0.102774\n\n[Training] Epoch 013 Summary:\n  Avg Loss: 0.103111\n  Current LR: 7.56e-05\n[EarlyStopping] Loss improved (0.103666 → 0.103111). Saving model...\n[Training] Epoch 014 | Batch 000/102 | Current Loss: 0.102904\n[Training] Epoch 014 | Batch 010/102 | Current Loss: 0.103179\n[Training] Epoch 014 | Batch 020/102 | Current Loss: 0.102617\n[Training] Epoch 014 | Batch 030/102 | Current Loss: 0.102687\n[Training] Epoch 014 | Batch 040/102 | Current Loss: 0.103470\n[Training] Epoch 014 | Batch 050/102 | Current Loss: 0.102652\n[Training] Epoch 014 | Batch 060/102 | Current Loss: 0.102449\n[Training] Epoch 014 | Batch 070/102 | Current Loss: 0.103644\n[Training] Epoch 014 | Batch 080/102 | Current Loss: 0.102724\n[Training] Epoch 014 | Batch 090/102 | Current Loss: 0.102540\n[Training] Epoch 014 | Batch 100/102 | Current Loss: 0.103182\n\n[Training] Epoch 014 Summary:\n  Avg Loss: 0.102831\n  Current LR: 6.25e-05\n[EarlyStopping] Loss improved (0.103111 → 0.102831). Saving model...\n[Training] Epoch 015 | Batch 000/102 | Current Loss: 0.102475\n[Training] Epoch 015 | Batch 010/102 | Current Loss: 0.102836\n[Training] Epoch 015 | Batch 020/102 | Current Loss: 0.102416\n[Training] Epoch 015 | Batch 030/102 | Current Loss: 0.103024\n[Training] Epoch 015 | Batch 040/102 | Current Loss: 0.102459\n[Training] Epoch 015 | Batch 050/102 | Current Loss: 0.102998\n[Training] Epoch 015 | Batch 060/102 | Current Loss: 0.102554\n[Training] Epoch 015 | Batch 070/102 | Current Loss: 0.102412\n[Training] Epoch 015 | Batch 080/102 | Current Loss: 0.102470\n[Training] Epoch 015 | Batch 090/102 | Current Loss: 0.102786\n[Training] Epoch 015 | Batch 100/102 | Current Loss: 0.102638\n\n[Training] Epoch 015 Summary:\n  Avg Loss: 0.102598\n  Current LR: 4.80e-05\n[EarlyStopping] Loss improved (0.102831 → 0.102598). Saving model...\n[Training] Epoch 016 | Batch 000/102 | Current Loss: 0.102341\n[Training] Epoch 016 | Batch 010/102 | Current Loss: 0.102880\n[Training] Epoch 016 | Batch 020/102 | Current Loss: 0.102490\n[Training] Epoch 016 | Batch 030/102 | Current Loss: 0.102538\n[Training] Epoch 016 | Batch 040/102 | Current Loss: 0.102502\n[Training] Epoch 016 | Batch 050/102 | Current Loss: 0.102451\n[Training] Epoch 016 | Batch 060/102 | Current Loss: 0.102426\n[Training] Epoch 016 | Batch 070/102 | Current Loss: 0.102251\n[Training] Epoch 016 | Batch 080/102 | Current Loss: 0.102210\n[Training] Epoch 016 | Batch 090/102 | Current Loss: 0.102307\n[Training] Epoch 016 | Batch 100/102 | Current Loss: 0.102444\n\n[Training] Epoch 016 Summary:\n  Avg Loss: 0.102477\n  Current LR: 3.35e-05\n[EarlyStopping] Loss improved (0.102598 → 0.102477). Saving model...\n[Training] Epoch 017 | Batch 000/102 | Current Loss: 0.102352\n[Training] Epoch 017 | Batch 010/102 | Current Loss: 0.102158\n[Training] Epoch 017 | Batch 020/102 | Current Loss: 0.102178\n[Training] Epoch 017 | Batch 030/102 | Current Loss: 0.102489\n[Training] Epoch 017 | Batch 040/102 | Current Loss: 0.102587\n[Training] Epoch 017 | Batch 050/102 | Current Loss: 0.102051\n[Training] Epoch 017 | Batch 060/102 | Current Loss: 0.102057\n[Training] Epoch 017 | Batch 070/102 | Current Loss: 0.102198\n[Training] Epoch 017 | Batch 080/102 | Current Loss: 0.102040\n[Training] Epoch 017 | Batch 090/102 | Current Loss: 0.102073\n[Training] Epoch 017 | Batch 100/102 | Current Loss: 0.102039\n\n[Training] Epoch 017 Summary:\n  Avg Loss: 0.102325\n  Current LR: 2.04e-05\n[EarlyStopping] Loss improved (0.102477 → 0.102325). Saving model...\n[Training] Epoch 018 | Batch 000/102 | Current Loss: 0.102170\n[Training] Epoch 018 | Batch 010/102 | Current Loss: 0.102574\n[Training] Epoch 018 | Batch 020/102 | Current Loss: 0.102418\n[Training] Epoch 018 | Batch 030/102 | Current Loss: 0.102721\n[Training] Epoch 018 | Batch 040/102 | Current Loss: 0.102381\n[Training] Epoch 018 | Batch 050/102 | Current Loss: 0.102649\n[Training] Epoch 018 | Batch 060/102 | Current Loss: 0.102072\n[Training] Epoch 018 | Batch 070/102 | Current Loss: 0.102098\n[Training] Epoch 018 | Batch 080/102 | Current Loss: 0.102160\n[Training] Epoch 018 | Batch 090/102 | Current Loss: 0.102250\n[Training] Epoch 018 | Batch 100/102 | Current Loss: 0.102261\n\n[Training] Epoch 018 Summary:\n  Avg Loss: 0.102245\n  Current LR: 9.97e-06\n[EarlyStopping] Loss improved (0.102325 → 0.102245). Saving model...\n[Training] Epoch 019 | Batch 000/102 | Current Loss: 0.102083\n[Training] Epoch 019 | Batch 010/102 | Current Loss: 0.102184\n[Training] Epoch 019 | Batch 020/102 | Current Loss: 0.102137\n[Training] Epoch 019 | Batch 030/102 | Current Loss: 0.102233\n[Training] Epoch 019 | Batch 040/102 | Current Loss: 0.102047\n[Training] Epoch 019 | Batch 050/102 | Current Loss: 0.102165\n[Training] Epoch 019 | Batch 060/102 | Current Loss: 0.102113\n[Training] Epoch 019 | Batch 070/102 | Current Loss: 0.102212\n[Training] Epoch 019 | Batch 080/102 | Current Loss: 0.102063\n[Training] Epoch 019 | Batch 090/102 | Current Loss: 0.102363\n[Training] Epoch 019 | Batch 100/102 | Current Loss: 0.102977\n\n[Training] Epoch 019 Summary:\n  Avg Loss: 0.102234\n  Current LR: 3.30e-06\n[EarlyStopping] Loss improved (0.102245 → 0.102234). Saving model...\n[Training] Epoch 020 | Batch 000/102 | Current Loss: 0.102016\n[Training] Epoch 020 | Batch 010/102 | Current Loss: 0.102113\n[Training] Epoch 020 | Batch 020/102 | Current Loss: 0.101893\n[Training] Epoch 020 | Batch 030/102 | Current Loss: 0.102185\n[Training] Epoch 020 | Batch 040/102 | Current Loss: 0.101939\n[Training] Epoch 020 | Batch 050/102 | Current Loss: 0.102183\n[Training] Epoch 020 | Batch 060/102 | Current Loss: 0.102853\n[Training] Epoch 020 | Batch 070/102 | Current Loss: 0.102238\n[Training] Epoch 020 | Batch 080/102 | Current Loss: 0.102388\n[Training] Epoch 020 | Batch 090/102 | Current Loss: 0.102041\n[Training] Epoch 020 | Batch 100/102 | Current Loss: 0.101942\n\n[Training] Epoch 020 Summary:\n  Avg Loss: 0.102188\n  Current LR: 9.50e-05\n[EarlyStopping] Loss improved (0.102234 → 0.102188). Saving model...\n[Training] Epoch 021 | Batch 000/102 | Current Loss: 0.102246\n[Training] Epoch 021 | Batch 010/102 | Current Loss: 0.102076\n[Training] Epoch 021 | Batch 020/102 | Current Loss: 0.102294\n[Training] Epoch 021 | Batch 030/102 | Current Loss: 0.102251\n[Training] Epoch 021 | Batch 040/102 | Current Loss: 0.102050\n[Training] Epoch 021 | Batch 050/102 | Current Loss: 0.102265\n[Training] Epoch 021 | Batch 060/102 | Current Loss: 0.102396\n[Training] Epoch 021 | Batch 070/102 | Current Loss: 0.102036\n[Training] Epoch 021 | Batch 080/102 | Current Loss: 0.102632\n[Training] Epoch 021 | Batch 090/102 | Current Loss: 0.101758\n[Training] Epoch 021 | Batch 100/102 | Current Loss: 0.102014\n\n[Training] Epoch 021 Summary:\n  Avg Loss: 0.102167\n  Current LR: 9.27e-05\n[EarlyStopping] Loss improved (0.102188 → 0.102167). Saving model...\n[Training] Epoch 022 | Batch 000/102 | Current Loss: 0.101895\n[Training] Epoch 022 | Batch 010/102 | Current Loss: 0.102610\n[Training] Epoch 022 | Batch 020/102 | Current Loss: 0.102611\n[Training] Epoch 022 | Batch 030/102 | Current Loss: 0.101782\n[Training] Epoch 022 | Batch 040/102 | Current Loss: 0.102284\n[Training] Epoch 022 | Batch 050/102 | Current Loss: 0.101995\n[Training] Epoch 022 | Batch 060/102 | Current Loss: 0.101921\n[Training] Epoch 022 | Batch 070/102 | Current Loss: 0.102091\n[Training] Epoch 022 | Batch 080/102 | Current Loss: 0.102174\n[Training] Epoch 022 | Batch 090/102 | Current Loss: 0.101872\n[Training] Epoch 022 | Batch 100/102 | Current Loss: 0.101857\n\n[Training] Epoch 022 Summary:\n  Avg Loss: 0.102065\n  Current LR: 8.60e-05\n[EarlyStopping] Loss improved (0.102167 → 0.102065). Saving model...\n[Training] Epoch 023 | Batch 000/102 | Current Loss: 0.102125\n[Training] Epoch 023 | Batch 010/102 | Current Loss: 0.101690\n[Training] Epoch 023 | Batch 020/102 | Current Loss: 0.101898\n[Training] Epoch 023 | Batch 030/102 | Current Loss: 0.101913\n[Training] Epoch 023 | Batch 040/102 | Current Loss: 0.101542\n[Training] Epoch 023 | Batch 050/102 | Current Loss: 0.101646\n[Training] Epoch 023 | Batch 060/102 | Current Loss: 0.101846\n[Training] Epoch 023 | Batch 070/102 | Current Loss: 0.102075\n[Training] Epoch 023 | Batch 080/102 | Current Loss: 0.101773\n[Training] Epoch 023 | Batch 090/102 | Current Loss: 0.101821\n[Training] Epoch 023 | Batch 100/102 | Current Loss: 0.101908\n\n[Training] Epoch 023 Summary:\n  Avg Loss: 0.101863\n  Current LR: 7.56e-05\n[EarlyStopping] Loss improved (0.102065 → 0.101863). Saving model...\n[Training] Epoch 024 | Batch 000/102 | Current Loss: 0.102106\n[Training] Epoch 024 | Batch 010/102 | Current Loss: 0.101966\n[Training] Epoch 024 | Batch 020/102 | Current Loss: 0.101534\n[Training] Epoch 024 | Batch 030/102 | Current Loss: 0.101550\n[Training] Epoch 024 | Batch 040/102 | Current Loss: 0.101768\n[Training] Epoch 024 | Batch 050/102 | Current Loss: 0.101746\n[Training] Epoch 024 | Batch 060/102 | Current Loss: 0.101644\n[Training] Epoch 024 | Batch 070/102 | Current Loss: 0.101612\n[Training] Epoch 024 | Batch 080/102 | Current Loss: 0.101823\n[Training] Epoch 024 | Batch 090/102 | Current Loss: 0.101363\n[Training] Epoch 024 | Batch 100/102 | Current Loss: 0.101520\n\n[Training] Epoch 024 Summary:\n  Avg Loss: 0.101713\n  Current LR: 6.25e-05\n[EarlyStopping] Loss improved (0.101863 → 0.101713). Saving model...\n[Training] Epoch 025 | Batch 000/102 | Current Loss: 0.101590\n[Training] Epoch 025 | Batch 010/102 | Current Loss: 0.101743\n[Training] Epoch 025 | Batch 020/102 | Current Loss: 0.101933\n[Training] Epoch 025 | Batch 030/102 | Current Loss: 0.101486\n[Training] Epoch 025 | Batch 040/102 | Current Loss: 0.101464\n[Training] Epoch 025 | Batch 050/102 | Current Loss: 0.101463\n[Training] Epoch 025 | Batch 060/102 | Current Loss: 0.101585\n[Training] Epoch 025 | Batch 070/102 | Current Loss: 0.102289\n[Training] Epoch 025 | Batch 080/102 | Current Loss: 0.101389\n[Training] Epoch 025 | Batch 090/102 | Current Loss: 0.101573\n[Training] Epoch 025 | Batch 100/102 | Current Loss: 0.101778\n\n[Training] Epoch 025 Summary:\n  Avg Loss: 0.101617\n  Current LR: 4.80e-05\n[EarlyStopping] Loss improved (0.101713 → 0.101617). Saving model...\n[Training] Epoch 026 | Batch 000/102 | Current Loss: 0.101772\n[Training] Epoch 026 | Batch 010/102 | Current Loss: 0.101675\n[Training] Epoch 026 | Batch 020/102 | Current Loss: 0.101522\n[Training] Epoch 026 | Batch 030/102 | Current Loss: 0.101615\n[Training] Epoch 026 | Batch 040/102 | Current Loss: 0.101446\n[Training] Epoch 026 | Batch 050/102 | Current Loss: 0.101698\n[Training] Epoch 026 | Batch 060/102 | Current Loss: 0.101525\n[Training] Epoch 026 | Batch 070/102 | Current Loss: 0.101497\n[Training] Epoch 026 | Batch 080/102 | Current Loss: 0.101389\n[Training] Epoch 026 | Batch 090/102 | Current Loss: 0.101387\n[Training] Epoch 026 | Batch 100/102 | Current Loss: 0.101666\n\n[Training] Epoch 026 Summary:\n  Avg Loss: 0.101574\n  Current LR: 3.35e-05\n[EarlyStopping] Loss improved (0.101617 → 0.101574). Saving model...\n[Training] Epoch 027 | Batch 000/102 | Current Loss: 0.101700\n[Training] Epoch 027 | Batch 010/102 | Current Loss: 0.101406\n[Training] Epoch 027 | Batch 020/102 | Current Loss: 0.101753\n[Training] Epoch 027 | Batch 030/102 | Current Loss: 0.101347\n[Training] Epoch 027 | Batch 040/102 | Current Loss: 0.101480\n[Training] Epoch 027 | Batch 050/102 | Current Loss: 0.101424\n[Training] Epoch 027 | Batch 060/102 | Current Loss: 0.101272\n[Training] Epoch 027 | Batch 070/102 | Current Loss: 0.101458\n[Training] Epoch 027 | Batch 080/102 | Current Loss: 0.101406\n[Training] Epoch 027 | Batch 090/102 | Current Loss: 0.101373\n[Training] Epoch 027 | Batch 100/102 | Current Loss: 0.101285\n\n[Training] Epoch 027 Summary:\n  Avg Loss: 0.101422\n  Current LR: 2.04e-05\n[EarlyStopping] Loss improved (0.101574 → 0.101422). Saving model...\n[Training] Epoch 028 | Batch 000/102 | Current Loss: 0.101389\n[Training] Epoch 028 | Batch 010/102 | Current Loss: 0.101293\n[Training] Epoch 028 | Batch 020/102 | Current Loss: 0.101337\n[Training] Epoch 028 | Batch 030/102 | Current Loss: 0.101432\n[Training] Epoch 028 | Batch 040/102 | Current Loss: 0.101453\n[Training] Epoch 028 | Batch 050/102 | Current Loss: 0.101304\n[Training] Epoch 028 | Batch 060/102 | Current Loss: 0.101253\n[Training] Epoch 028 | Batch 070/102 | Current Loss: 0.101276\n[Training] Epoch 028 | Batch 080/102 | Current Loss: 0.101531\n[Training] Epoch 028 | Batch 090/102 | Current Loss: 0.101411\n[Training] Epoch 028 | Batch 100/102 | Current Loss: 0.101227\n\n[Training] Epoch 028 Summary:\n  Avg Loss: 0.101396\n  Current LR: 9.97e-06\n[EarlyStopping] Loss improved (0.101422 → 0.101396). Saving model...\n[Training] Epoch 029 | Batch 000/102 | Current Loss: 0.101406\n[Training] Epoch 029 | Batch 010/102 | Current Loss: 0.101570\n[Training] Epoch 029 | Batch 020/102 | Current Loss: 0.101284\n[Training] Epoch 029 | Batch 030/102 | Current Loss: 0.101176\n[Training] Epoch 029 | Batch 040/102 | Current Loss: 0.101473\n[Training] Epoch 029 | Batch 050/102 | Current Loss: 0.101403\n[Training] Epoch 029 | Batch 060/102 | Current Loss: 0.102265\n[Training] Epoch 029 | Batch 070/102 | Current Loss: 0.101378\n[Training] Epoch 029 | Batch 080/102 | Current Loss: 0.101306\n[Training] Epoch 029 | Batch 090/102 | Current Loss: 0.101442\n[Training] Epoch 029 | Batch 100/102 | Current Loss: 0.101240\n\n[Training] Epoch 029 Summary:\n  Avg Loss: 0.101387\n  Current LR: 3.30e-06\n[EarlyStopping] Loss improved (0.101396 → 0.101387). Saving model...\n[Training] Epoch 030 | Batch 000/102 | Current Loss: 0.101271\n[Training] Epoch 030 | Batch 010/102 | Current Loss: 0.101370\n[Training] Epoch 030 | Batch 020/102 | Current Loss: 0.101716\n[Training] Epoch 030 | Batch 030/102 | Current Loss: 0.101291\n[Training] Epoch 030 | Batch 040/102 | Current Loss: 0.101430\n[Training] Epoch 030 | Batch 050/102 | Current Loss: 0.101277\n[Training] Epoch 030 | Batch 060/102 | Current Loss: 0.101375\n[Training] Epoch 030 | Batch 070/102 | Current Loss: 0.101357\n[Training] Epoch 030 | Batch 080/102 | Current Loss: 0.101293\n[Training] Epoch 030 | Batch 090/102 | Current Loss: 0.101252\n[Training] Epoch 030 | Batch 100/102 | Current Loss: 0.101251\n\n[Training] Epoch 030 Summary:\n  Avg Loss: 0.101363\n  Current LR: 9.50e-05\n[EarlyStopping] Loss improved (0.101387 → 0.101363). Saving model...\n[Training] Epoch 031 | Batch 000/102 | Current Loss: 0.101648\n[Training] Epoch 031 | Batch 010/102 | Current Loss: 0.101411\n[Training] Epoch 031 | Batch 020/102 | Current Loss: 0.101454\n[Training] Epoch 031 | Batch 030/102 | Current Loss: 0.101472\n[Training] Epoch 031 | Batch 040/102 | Current Loss: 0.101229\n[Training] Epoch 031 | Batch 050/102 | Current Loss: 0.101556\n[Training] Epoch 031 | Batch 060/102 | Current Loss: 0.101383\n[Training] Epoch 031 | Batch 070/102 | Current Loss: 0.101365\n[Training] Epoch 031 | Batch 080/102 | Current Loss: 0.101291\n[Training] Epoch 031 | Batch 090/102 | Current Loss: 0.101583\n[Training] Epoch 031 | Batch 100/102 | Current Loss: 0.101702\n\n[Training] Epoch 031 Summary:\n  Avg Loss: 0.101434\n  Current LR: 9.27e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 032 | Batch 000/102 | Current Loss: 0.101230\n[Training] Epoch 032 | Batch 010/102 | Current Loss: 0.101214\n[Training] Epoch 032 | Batch 020/102 | Current Loss: 0.101162\n[Training] Epoch 032 | Batch 030/102 | Current Loss: 0.101318\n[Training] Epoch 032 | Batch 040/102 | Current Loss: 0.101444\n[Training] Epoch 032 | Batch 050/102 | Current Loss: 0.101217\n[Training] Epoch 032 | Batch 060/102 | Current Loss: 0.101331\n[Training] Epoch 032 | Batch 070/102 | Current Loss: 0.101336\n[Training] Epoch 032 | Batch 080/102 | Current Loss: 0.101246\n[Training] Epoch 032 | Batch 090/102 | Current Loss: 0.101181\n[Training] Epoch 032 | Batch 100/102 | Current Loss: 0.101240\n\n[Training] Epoch 032 Summary:\n  Avg Loss: 0.101335\n  Current LR: 8.60e-05\n[EarlyStopping] Loss improved (0.101363 → 0.101335). Saving model...\n[Training] Epoch 033 | Batch 000/102 | Current Loss: 0.101248\n[Training] Epoch 033 | Batch 010/102 | Current Loss: 0.101184\n[Training] Epoch 033 | Batch 020/102 | Current Loss: 0.101396\n[Training] Epoch 033 | Batch 030/102 | Current Loss: 0.101127\n[Training] Epoch 033 | Batch 040/102 | Current Loss: 0.101568\n[Training] Epoch 033 | Batch 050/102 | Current Loss: 0.101414\n[Training] Epoch 033 | Batch 060/102 | Current Loss: 0.101115\n[Training] Epoch 033 | Batch 070/102 | Current Loss: 0.101134\n[Training] Epoch 033 | Batch 080/102 | Current Loss: 0.101093\n[Training] Epoch 033 | Batch 090/102 | Current Loss: 0.101148\n[Training] Epoch 033 | Batch 100/102 | Current Loss: 0.101282\n\n[Training] Epoch 033 Summary:\n  Avg Loss: 0.101208\n  Current LR: 7.56e-05\n[EarlyStopping] Loss improved (0.101335 → 0.101208). Saving model...\n[Training] Epoch 034 | Batch 000/102 | Current Loss: 0.101054\n[Training] Epoch 034 | Batch 010/102 | Current Loss: 0.101211\n[Training] Epoch 034 | Batch 020/102 | Current Loss: 0.101244\n[Training] Epoch 034 | Batch 030/102 | Current Loss: 0.100994\n[Training] Epoch 034 | Batch 040/102 | Current Loss: 0.101207\n[Training] Epoch 034 | Batch 050/102 | Current Loss: 0.101055\n[Training] Epoch 034 | Batch 060/102 | Current Loss: 0.101036\n[Training] Epoch 034 | Batch 070/102 | Current Loss: 0.101080\n[Training] Epoch 034 | Batch 080/102 | Current Loss: 0.100984\n[Training] Epoch 034 | Batch 090/102 | Current Loss: 0.101183\n[Training] Epoch 034 | Batch 100/102 | Current Loss: 0.101073\n\n[Training] Epoch 034 Summary:\n  Avg Loss: 0.101140\n  Current LR: 6.25e-05\n[EarlyStopping] Loss improved (0.101208 → 0.101140). Saving model...\n[Training] Epoch 035 | Batch 000/102 | Current Loss: 0.100976\n[Training] Epoch 035 | Batch 010/102 | Current Loss: 0.101190\n[Training] Epoch 035 | Batch 020/102 | Current Loss: 0.101367\n[Training] Epoch 035 | Batch 030/102 | Current Loss: 0.101107\n[Training] Epoch 035 | Batch 040/102 | Current Loss: 0.101029\n[Training] Epoch 035 | Batch 050/102 | Current Loss: 0.101148\n[Training] Epoch 035 | Batch 060/102 | Current Loss: 0.101159\n[Training] Epoch 035 | Batch 070/102 | Current Loss: 0.101308\n[Training] Epoch 035 | Batch 080/102 | Current Loss: 0.101214\n[Training] Epoch 035 | Batch 090/102 | Current Loss: 0.100932\n[Training] Epoch 035 | Batch 100/102 | Current Loss: 0.101076\n\n[Training] Epoch 035 Summary:\n  Avg Loss: 0.101082\n  Current LR: 4.80e-05\n[EarlyStopping] Loss improved (0.101140 → 0.101082). Saving model...\n[Training] Epoch 036 | Batch 000/102 | Current Loss: 0.101077\n[Training] Epoch 036 | Batch 010/102 | Current Loss: 0.100973\n[Training] Epoch 036 | Batch 020/102 | Current Loss: 0.100954\n[Training] Epoch 036 | Batch 030/102 | Current Loss: 0.100968\n[Training] Epoch 036 | Batch 040/102 | Current Loss: 0.101019\n[Training] Epoch 036 | Batch 050/102 | Current Loss: 0.100931\n[Training] Epoch 036 | Batch 060/102 | Current Loss: 0.100890\n[Training] Epoch 036 | Batch 070/102 | Current Loss: 0.100887\n[Training] Epoch 036 | Batch 080/102 | Current Loss: 0.101122\n[Training] Epoch 036 | Batch 090/102 | Current Loss: 0.101062\n[Training] Epoch 036 | Batch 100/102 | Current Loss: 0.100880\n\n[Training] Epoch 036 Summary:\n  Avg Loss: 0.101010\n  Current LR: 3.35e-05\n[EarlyStopping] Loss improved (0.101082 → 0.101010). Saving model...\n[Training] Epoch 037 | Batch 000/102 | Current Loss: 0.100885\n[Training] Epoch 037 | Batch 010/102 | Current Loss: 0.100907\n[Training] Epoch 037 | Batch 020/102 | Current Loss: 0.101003\n[Training] Epoch 037 | Batch 030/102 | Current Loss: 0.101016\n[Training] Epoch 037 | Batch 040/102 | Current Loss: 0.100950\n[Training] Epoch 037 | Batch 050/102 | Current Loss: 0.100888\n[Training] Epoch 037 | Batch 060/102 | Current Loss: 0.101068\n[Training] Epoch 037 | Batch 070/102 | Current Loss: 0.101093\n[Training] Epoch 037 | Batch 080/102 | Current Loss: 0.100974\n[Training] Epoch 037 | Batch 090/102 | Current Loss: 0.100860\n[Training] Epoch 037 | Batch 100/102 | Current Loss: 0.100938\n\n[Training] Epoch 037 Summary:\n  Avg Loss: 0.100977\n  Current LR: 2.04e-05\n[EarlyStopping] Loss improved (0.101010 → 0.100977). Saving model...\n[Training] Epoch 038 | Batch 000/102 | Current Loss: 0.100860\n[Training] Epoch 038 | Batch 010/102 | Current Loss: 0.100899\n[Training] Epoch 038 | Batch 020/102 | Current Loss: 0.100852\n[Training] Epoch 038 | Batch 030/102 | Current Loss: 0.100858\n[Training] Epoch 038 | Batch 040/102 | Current Loss: 0.100926\n[Training] Epoch 038 | Batch 050/102 | Current Loss: 0.100940\n[Training] Epoch 038 | Batch 060/102 | Current Loss: 0.100942\n[Training] Epoch 038 | Batch 070/102 | Current Loss: 0.100844\n[Training] Epoch 038 | Batch 080/102 | Current Loss: 0.100946\n[Training] Epoch 038 | Batch 090/102 | Current Loss: 0.100895\n[Training] Epoch 038 | Batch 100/102 | Current Loss: 0.100940\n\n[Training] Epoch 038 Summary:\n  Avg Loss: 0.100937\n  Current LR: 9.97e-06\n[EarlyStopping] Loss improved (0.100977 → 0.100937). Saving model...\n[Training] Epoch 039 | Batch 000/102 | Current Loss: 0.101008\n[Training] Epoch 039 | Batch 010/102 | Current Loss: 0.100915\n[Training] Epoch 039 | Batch 020/102 | Current Loss: 0.101003\n[Training] Epoch 039 | Batch 030/102 | Current Loss: 0.100909\n[Training] Epoch 039 | Batch 040/102 | Current Loss: 0.100987\n[Training] Epoch 039 | Batch 050/102 | Current Loss: 0.100864\n[Training] Epoch 039 | Batch 060/102 | Current Loss: 0.100942\n[Training] Epoch 039 | Batch 070/102 | Current Loss: 0.101148\n[Training] Epoch 039 | Batch 080/102 | Current Loss: 0.101115\n[Training] Epoch 039 | Batch 090/102 | Current Loss: 0.101085\n[Training] Epoch 039 | Batch 100/102 | Current Loss: 0.100879\n\n[Training] Epoch 039 Summary:\n  Avg Loss: 0.100954\n  Current LR: 3.30e-06\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 040 | Batch 000/102 | Current Loss: 0.100960\n[Training] Epoch 040 | Batch 010/102 | Current Loss: 0.100854\n[Training] Epoch 040 | Batch 020/102 | Current Loss: 0.100820\n[Training] Epoch 040 | Batch 030/102 | Current Loss: 0.100880\n[Training] Epoch 040 | Batch 040/102 | Current Loss: 0.100910\n[Training] Epoch 040 | Batch 050/102 | Current Loss: 0.100953\n[Training] Epoch 040 | Batch 060/102 | Current Loss: 0.100844\n[Training] Epoch 040 | Batch 070/102 | Current Loss: 0.101019\n[Training] Epoch 040 | Batch 080/102 | Current Loss: 0.100865\n[Training] Epoch 040 | Batch 090/102 | Current Loss: 0.100961\n[Training] Epoch 040 | Batch 100/102 | Current Loss: 0.100806\n\n[Training] Epoch 040 Summary:\n  Avg Loss: 0.100928\n  Current LR: 9.50e-05\n[EarlyStopping] Loss improved (0.100937 → 0.100928). Saving model...\n[Training] Epoch 041 | Batch 000/102 | Current Loss: 0.100982\n[Training] Epoch 041 | Batch 010/102 | Current Loss: 0.100930\n[Training] Epoch 041 | Batch 020/102 | Current Loss: 0.101032\n[Training] Epoch 041 | Batch 030/102 | Current Loss: 0.101004\n[Training] Epoch 041 | Batch 040/102 | Current Loss: 0.100879\n[Training] Epoch 041 | Batch 050/102 | Current Loss: 0.100951\n[Training] Epoch 041 | Batch 060/102 | Current Loss: 0.100770\n[Training] Epoch 041 | Batch 070/102 | Current Loss: 0.100920\n[Training] Epoch 041 | Batch 080/102 | Current Loss: 0.101101\n[Training] Epoch 041 | Batch 090/102 | Current Loss: 0.100813\n[Training] Epoch 041 | Batch 100/102 | Current Loss: 0.100841\n\n[Training] Epoch 041 Summary:\n  Avg Loss: 0.100952\n  Current LR: 9.27e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 042 | Batch 000/102 | Current Loss: 0.100951\n[Training] Epoch 042 | Batch 010/102 | Current Loss: 0.100894\n[Training] Epoch 042 | Batch 020/102 | Current Loss: 0.100841\n[Training] Epoch 042 | Batch 030/102 | Current Loss: 0.100892\n[Training] Epoch 042 | Batch 040/102 | Current Loss: 0.100845\n[Training] Epoch 042 | Batch 050/102 | Current Loss: 0.100939\n[Training] Epoch 042 | Batch 060/102 | Current Loss: 0.101143\n[Training] Epoch 042 | Batch 070/102 | Current Loss: 0.101122\n[Training] Epoch 042 | Batch 080/102 | Current Loss: 0.100832\n[Training] Epoch 042 | Batch 090/102 | Current Loss: 0.100973\n[Training] Epoch 042 | Batch 100/102 | Current Loss: 0.100856\n\n[Training] Epoch 042 Summary:\n  Avg Loss: 0.100944\n  Current LR: 8.60e-05\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 043 | Batch 000/102 | Current Loss: 0.101117\n[Training] Epoch 043 | Batch 010/102 | Current Loss: 0.100914\n[Training] Epoch 043 | Batch 020/102 | Current Loss: 0.100827\n[Training] Epoch 043 | Batch 030/102 | Current Loss: 0.100853\n[Training] Epoch 043 | Batch 040/102 | Current Loss: 0.100904\n[Training] Epoch 043 | Batch 050/102 | Current Loss: 0.100775\n[Training] Epoch 043 | Batch 060/102 | Current Loss: 0.100845\n[Training] Epoch 043 | Batch 070/102 | Current Loss: 0.100722\n[Training] Epoch 043 | Batch 080/102 | Current Loss: 0.100902\n[Training] Epoch 043 | Batch 090/102 | Current Loss: 0.100730\n[Training] Epoch 043 | Batch 100/102 | Current Loss: 0.100777\n\n[Training] Epoch 043 Summary:\n  Avg Loss: 0.100871\n  Current LR: 7.56e-05\n[EarlyStopping] Loss improved (0.100928 → 0.100871). Saving model...\n[Training] Epoch 044 | Batch 000/102 | Current Loss: 0.100722\n[Training] Epoch 044 | Batch 010/102 | Current Loss: 0.100735\n[Training] Epoch 044 | Batch 020/102 | Current Loss: 0.100804\n[Training] Epoch 044 | Batch 030/102 | Current Loss: 0.100856\n[Training] Epoch 044 | Batch 040/102 | Current Loss: 0.100959\n[Training] Epoch 044 | Batch 050/102 | Current Loss: 0.100842\n[Training] Epoch 044 | Batch 060/102 | Current Loss: 0.100896\n[Training] Epoch 044 | Batch 070/102 | Current Loss: 0.100800\n[Training] Epoch 044 | Batch 080/102 | Current Loss: 0.100746\n[Training] Epoch 044 | Batch 090/102 | Current Loss: 0.100859\n[Training] Epoch 044 | Batch 100/102 | Current Loss: 0.100824\n\n[Training] Epoch 044 Summary:\n  Avg Loss: 0.100805\n  Current LR: 6.25e-05\n[EarlyStopping] Loss improved (0.100871 → 0.100805). Saving model...\n[Training] Epoch 045 | Batch 000/102 | Current Loss: 0.100791\n[Training] Epoch 045 | Batch 010/102 | Current Loss: 0.100631\n[Training] Epoch 045 | Batch 020/102 | Current Loss: 0.100993\n[Training] Epoch 045 | Batch 030/102 | Current Loss: 0.100742\n[Training] Epoch 045 | Batch 040/102 | Current Loss: 0.100786\n[Training] Epoch 045 | Batch 050/102 | Current Loss: 0.100675\n[Training] Epoch 045 | Batch 060/102 | Current Loss: 0.100871\n[Training] Epoch 045 | Batch 070/102 | Current Loss: 0.100736\n[Training] Epoch 045 | Batch 080/102 | Current Loss: 0.100742\n[Training] Epoch 045 | Batch 090/102 | Current Loss: 0.100879\n[Training] Epoch 045 | Batch 100/102 | Current Loss: 0.100654\n\n[Training] Epoch 045 Summary:\n  Avg Loss: 0.100768\n  Current LR: 4.80e-05\n[EarlyStopping] Loss improved (0.100805 → 0.100768). Saving model...\n[Training] Epoch 046 | Batch 000/102 | Current Loss: 0.100673\n[Training] Epoch 046 | Batch 010/102 | Current Loss: 0.100712\n[Training] Epoch 046 | Batch 020/102 | Current Loss: 0.100768\n[Training] Epoch 046 | Batch 030/102 | Current Loss: 0.100744\n[Training] Epoch 046 | Batch 040/102 | Current Loss: 0.100664\n[Training] Epoch 046 | Batch 050/102 | Current Loss: 0.100737\n[Training] Epoch 046 | Batch 060/102 | Current Loss: 0.100786\n[Training] Epoch 046 | Batch 070/102 | Current Loss: 0.100965\n[Training] Epoch 046 | Batch 080/102 | Current Loss: 0.100665\n[Training] Epoch 046 | Batch 090/102 | Current Loss: 0.100666\n[Training] Epoch 046 | Batch 100/102 | Current Loss: 0.100720\n\n[Training] Epoch 046 Summary:\n  Avg Loss: 0.100752\n  Current LR: 3.35e-05\n[EarlyStopping] Loss improved (0.100768 → 0.100752). Saving model...\n[Training] Epoch 047 | Batch 000/102 | Current Loss: 0.100725\n[Training] Epoch 047 | Batch 010/102 | Current Loss: 0.100606\n[Training] Epoch 047 | Batch 020/102 | Current Loss: 0.100663\n[Training] Epoch 047 | Batch 030/102 | Current Loss: 0.100673\n[Training] Epoch 047 | Batch 040/102 | Current Loss: 0.100644\n[Training] Epoch 047 | Batch 050/102 | Current Loss: 0.100852\n[Training] Epoch 047 | Batch 060/102 | Current Loss: 0.100765\n[Training] Epoch 047 | Batch 070/102 | Current Loss: 0.100686\n[Training] Epoch 047 | Batch 080/102 | Current Loss: 0.100681\n[Training] Epoch 047 | Batch 090/102 | Current Loss: 0.100631\n[Training] Epoch 047 | Batch 100/102 | Current Loss: 0.100640\n\n[Training] Epoch 047 Summary:\n  Avg Loss: 0.100697\n  Current LR: 2.04e-05\n[EarlyStopping] Loss improved (0.100752 → 0.100697). Saving model...\n[Training] Epoch 048 | Batch 000/102 | Current Loss: 0.100679\n[Training] Epoch 048 | Batch 010/102 | Current Loss: 0.100668\n[Training] Epoch 048 | Batch 020/102 | Current Loss: 0.100654\n[Training] Epoch 048 | Batch 030/102 | Current Loss: 0.100716\n[Training] Epoch 048 | Batch 040/102 | Current Loss: 0.100665\n[Training] Epoch 048 | Batch 050/102 | Current Loss: 0.100709\n[Training] Epoch 048 | Batch 060/102 | Current Loss: 0.100622\n[Training] Epoch 048 | Batch 070/102 | Current Loss: 0.100609\n[Training] Epoch 048 | Batch 080/102 | Current Loss: 0.100719\n[Training] Epoch 048 | Batch 090/102 | Current Loss: 0.100586\n[Training] Epoch 048 | Batch 100/102 | Current Loss: 0.100634\n\n[Training] Epoch 048 Summary:\n  Avg Loss: 0.100683\n  Current LR: 9.97e-06\n[EarlyStopping] Loss improved (0.100697 → 0.100683). Saving model...\n[Training] Epoch 049 | Batch 000/102 | Current Loss: 0.100618\n[Training] Epoch 049 | Batch 010/102 | Current Loss: 0.100771\n[Training] Epoch 049 | Batch 020/102 | Current Loss: 0.100597\n[Training] Epoch 049 | Batch 030/102 | Current Loss: 0.100700\n[Training] Epoch 049 | Batch 040/102 | Current Loss: 0.100745\n[Training] Epoch 049 | Batch 050/102 | Current Loss: 0.100648\n[Training] Epoch 049 | Batch 060/102 | Current Loss: 0.100620\n[Training] Epoch 049 | Batch 070/102 | Current Loss: 0.100647\n[Training] Epoch 049 | Batch 080/102 | Current Loss: 0.100685\n[Training] Epoch 049 | Batch 090/102 | Current Loss: 0.100713\n[Training] Epoch 049 | Batch 100/102 | Current Loss: 0.100718\n\n[Training] Epoch 049 Summary:\n  Avg Loss: 0.100680\n  Current LR: 3.30e-06\n[EarlyStopping] Loss improved (0.100683 → 0.100680). Saving model...\n[Training] Epoch 050 | Batch 000/102 | Current Loss: 0.100603\n[Training] Epoch 050 | Batch 010/102 | Current Loss: 0.100638\n[Training] Epoch 050 | Batch 020/102 | Current Loss: 0.100698\n[Training] Epoch 050 | Batch 030/102 | Current Loss: 0.100691\n[Training] Epoch 050 | Batch 040/102 | Current Loss: 0.100778\n[Training] Epoch 050 | Batch 050/102 | Current Loss: 0.100705\n[Training] Epoch 050 | Batch 060/102 | Current Loss: 0.100733\n[Training] Epoch 050 | Batch 070/102 | Current Loss: 0.100710\n[Training] Epoch 050 | Batch 080/102 | Current Loss: 0.100685\n[Training] Epoch 050 | Batch 090/102 | Current Loss: 0.100701\n[Training] Epoch 050 | Batch 100/102 | Current Loss: 0.100611\n\n[Training] Epoch 050 Summary:\n  Avg Loss: 0.100674\n  Current LR: 9.50e-05\n[EarlyStopping] Loss improved (0.100680 → 0.100674). Saving model...\n[Training] Epoch 051 | Batch 000/102 | Current Loss: 0.100540\n[Training] Epoch 051 | Batch 010/102 | Current Loss: 0.100689\n[Training] Epoch 051 | Batch 020/102 | Current Loss: 0.100635\n[Training] Epoch 051 | Batch 030/102 | Current Loss: 0.100672\n[Training] Epoch 051 | Batch 040/102 | Current Loss: 0.100714\n[Training] Epoch 051 | Batch 050/102 | Current Loss: 0.100661\n[Training] Epoch 051 | Batch 060/102 | Current Loss: 0.100805\n[Training] Epoch 051 | Batch 070/102 | Current Loss: 0.100619\n[Training] Epoch 051 | Batch 080/102 | Current Loss: 0.100642\n[Training] Epoch 051 | Batch 090/102 | Current Loss: 0.100770\n[Training] Epoch 051 | Batch 100/102 | Current Loss: 0.100640\n\n[Training] Epoch 051 Summary:\n  Avg Loss: 0.100690\n  Current LR: 9.27e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 052 | Batch 000/102 | Current Loss: 0.100637\n[Training] Epoch 052 | Batch 010/102 | Current Loss: 0.100612\n[Training] Epoch 052 | Batch 020/102 | Current Loss: 0.100665\n[Training] Epoch 052 | Batch 030/102 | Current Loss: 0.100645\n[Training] Epoch 052 | Batch 040/102 | Current Loss: 0.100663\n[Training] Epoch 052 | Batch 050/102 | Current Loss: 0.100620\n[Training] Epoch 052 | Batch 060/102 | Current Loss: 0.100635\n[Training] Epoch 052 | Batch 070/102 | Current Loss: 0.100756\n[Training] Epoch 052 | Batch 080/102 | Current Loss: 0.100585\n[Training] Epoch 052 | Batch 090/102 | Current Loss: 0.100595\n[Training] Epoch 052 | Batch 100/102 | Current Loss: 0.100561\n\n[Training] Epoch 052 Summary:\n  Avg Loss: 0.100651\n  Current LR: 8.60e-05\n[EarlyStopping] Loss improved (0.100674 → 0.100651). Saving model...\n[Training] Epoch 053 | Batch 000/102 | Current Loss: 0.100692\n[Training] Epoch 053 | Batch 010/102 | Current Loss: 0.100609\n[Training] Epoch 053 | Batch 020/102 | Current Loss: 0.100656\n[Training] Epoch 053 | Batch 030/102 | Current Loss: 0.100620\n[Training] Epoch 053 | Batch 040/102 | Current Loss: 0.100597\n[Training] Epoch 053 | Batch 050/102 | Current Loss: 0.100639\n[Training] Epoch 053 | Batch 060/102 | Current Loss: 0.100582\n[Training] Epoch 053 | Batch 070/102 | Current Loss: 0.100581\n[Training] Epoch 053 | Batch 080/102 | Current Loss: 0.100545\n[Training] Epoch 053 | Batch 090/102 | Current Loss: 0.100774\n[Training] Epoch 053 | Batch 100/102 | Current Loss: 0.100555\n\n[Training] Epoch 053 Summary:\n  Avg Loss: 0.100637\n  Current LR: 7.56e-05\n[EarlyStopping] Loss improved (0.100651 → 0.100637). Saving model...\n[Training] Epoch 054 | Batch 000/102 | Current Loss: 0.100547\n[Training] Epoch 054 | Batch 010/102 | Current Loss: 0.100558\n[Training] Epoch 054 | Batch 020/102 | Current Loss: 0.100645\n[Training] Epoch 054 | Batch 030/102 | Current Loss: 0.100547\n[Training] Epoch 054 | Batch 040/102 | Current Loss: 0.100528\n[Training] Epoch 054 | Batch 050/102 | Current Loss: 0.100507\n[Training] Epoch 054 | Batch 060/102 | Current Loss: 0.100624\n[Training] Epoch 054 | Batch 070/102 | Current Loss: 0.100560\n[Training] Epoch 054 | Batch 080/102 | Current Loss: 0.100479\n[Training] Epoch 054 | Batch 090/102 | Current Loss: 0.100565\n[Training] Epoch 054 | Batch 100/102 | Current Loss: 0.100620\n\n[Training] Epoch 054 Summary:\n  Avg Loss: 0.100590\n  Current LR: 6.25e-05\n[EarlyStopping] Loss improved (0.100637 → 0.100590). Saving model...\n[Training] Epoch 055 | Batch 000/102 | Current Loss: 0.100647\n[Training] Epoch 055 | Batch 010/102 | Current Loss: 0.100636\n[Training] Epoch 055 | Batch 020/102 | Current Loss: 0.100541\n[Training] Epoch 055 | Batch 030/102 | Current Loss: 0.100613\n[Training] Epoch 055 | Batch 040/102 | Current Loss: 0.100564\n[Training] Epoch 055 | Batch 050/102 | Current Loss: 0.100575\n[Training] Epoch 055 | Batch 060/102 | Current Loss: 0.100582\n[Training] Epoch 055 | Batch 070/102 | Current Loss: 0.100649\n[Training] Epoch 055 | Batch 080/102 | Current Loss: 0.100622\n[Training] Epoch 055 | Batch 090/102 | Current Loss: 0.100420\n[Training] Epoch 055 | Batch 100/102 | Current Loss: 0.100567\n\n[Training] Epoch 055 Summary:\n  Avg Loss: 0.100564\n  Current LR: 4.80e-05\n[EarlyStopping] Loss improved (0.100590 → 0.100564). Saving model...\n[Training] Epoch 056 | Batch 000/102 | Current Loss: 0.100666\n[Training] Epoch 056 | Batch 010/102 | Current Loss: 0.100575\n[Training] Epoch 056 | Batch 020/102 | Current Loss: 0.100553\n[Training] Epoch 056 | Batch 030/102 | Current Loss: 0.100567\n[Training] Epoch 056 | Batch 040/102 | Current Loss: 0.100495\n[Training] Epoch 056 | Batch 050/102 | Current Loss: 0.100537\n[Training] Epoch 056 | Batch 060/102 | Current Loss: 0.100797\n[Training] Epoch 056 | Batch 070/102 | Current Loss: 0.100536\n[Training] Epoch 056 | Batch 080/102 | Current Loss: 0.100473\n[Training] Epoch 056 | Batch 090/102 | Current Loss: 0.100505\n[Training] Epoch 056 | Batch 100/102 | Current Loss: 0.100429\n\n[Training] Epoch 056 Summary:\n  Avg Loss: 0.100540\n  Current LR: 3.35e-05\n[EarlyStopping] Loss improved (0.100564 → 0.100540). Saving model...\n[Training] Epoch 057 | Batch 000/102 | Current Loss: 0.100493\n[Training] Epoch 057 | Batch 010/102 | Current Loss: 0.100532\n[Training] Epoch 057 | Batch 020/102 | Current Loss: 0.100444\n[Training] Epoch 057 | Batch 030/102 | Current Loss: 0.100526\n[Training] Epoch 057 | Batch 040/102 | Current Loss: 0.100601\n[Training] Epoch 057 | Batch 050/102 | Current Loss: 0.100614\n[Training] Epoch 057 | Batch 060/102 | Current Loss: 0.100473\n[Training] Epoch 057 | Batch 070/102 | Current Loss: 0.100482\n[Training] Epoch 057 | Batch 080/102 | Current Loss: 0.100576\n[Training] Epoch 057 | Batch 090/102 | Current Loss: 0.100486\n[Training] Epoch 057 | Batch 100/102 | Current Loss: 0.100483\n\n[Training] Epoch 057 Summary:\n  Avg Loss: 0.100508\n  Current LR: 2.04e-05\n[EarlyStopping] Loss improved (0.100540 → 0.100508). Saving model...\n[Training] Epoch 058 | Batch 000/102 | Current Loss: 0.100518\n[Training] Epoch 058 | Batch 010/102 | Current Loss: 0.100458\n[Training] Epoch 058 | Batch 020/102 | Current Loss: 0.100526\n[Training] Epoch 058 | Batch 030/102 | Current Loss: 0.100544\n[Training] Epoch 058 | Batch 040/102 | Current Loss: 0.100473\n[Training] Epoch 058 | Batch 050/102 | Current Loss: 0.100499\n[Training] Epoch 058 | Batch 060/102 | Current Loss: 0.100462\n[Training] Epoch 058 | Batch 070/102 | Current Loss: 0.100469\n[Training] Epoch 058 | Batch 080/102 | Current Loss: 0.100517\n[Training] Epoch 058 | Batch 090/102 | Current Loss: 0.100527\n[Training] Epoch 058 | Batch 100/102 | Current Loss: 0.100543\n\n[Training] Epoch 058 Summary:\n  Avg Loss: 0.100519\n  Current LR: 9.97e-06\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 059 | Batch 000/102 | Current Loss: 0.100475\n[Training] Epoch 059 | Batch 010/102 | Current Loss: 0.100527\n[Training] Epoch 059 | Batch 020/102 | Current Loss: 0.100473\n[Training] Epoch 059 | Batch 030/102 | Current Loss: 0.100559\n[Training] Epoch 059 | Batch 040/102 | Current Loss: 0.100418\n[Training] Epoch 059 | Batch 050/102 | Current Loss: 0.100507\n[Training] Epoch 059 | Batch 060/102 | Current Loss: 0.100591\n[Training] Epoch 059 | Batch 070/102 | Current Loss: 0.100445\n[Training] Epoch 059 | Batch 080/102 | Current Loss: 0.100538\n[Training] Epoch 059 | Batch 090/102 | Current Loss: 0.100504\n[Training] Epoch 059 | Batch 100/102 | Current Loss: 0.100514\n\n[Training] Epoch 059 Summary:\n  Avg Loss: 0.100505\n  Current LR: 3.30e-06\n[EarlyStopping] Loss improved (0.100508 → 0.100505). Saving model...\n[Training] Epoch 060 | Batch 000/102 | Current Loss: 0.100416\n[Training] Epoch 060 | Batch 010/102 | Current Loss: 0.100524\n[Training] Epoch 060 | Batch 020/102 | Current Loss: 0.100452\n[Training] Epoch 060 | Batch 030/102 | Current Loss: 0.100574\n[Training] Epoch 060 | Batch 040/102 | Current Loss: 0.100550\n[Training] Epoch 060 | Batch 050/102 | Current Loss: 0.100488\n[Training] Epoch 060 | Batch 060/102 | Current Loss: 0.100515\n[Training] Epoch 060 | Batch 070/102 | Current Loss: 0.100481\n[Training] Epoch 060 | Batch 080/102 | Current Loss: 0.100509\n[Training] Epoch 060 | Batch 090/102 | Current Loss: 0.100438\n[Training] Epoch 060 | Batch 100/102 | Current Loss: 0.100513\n\n[Training] Epoch 060 Summary:\n  Avg Loss: 0.100494\n  Current LR: 9.50e-05\n[EarlyStopping] Loss improved (0.100505 → 0.100494). Saving model...\n[Training] Epoch 061 | Batch 000/102 | Current Loss: 0.100597\n[Training] Epoch 061 | Batch 010/102 | Current Loss: 0.100498\n[Training] Epoch 061 | Batch 020/102 | Current Loss: 0.100467\n[Training] Epoch 061 | Batch 030/102 | Current Loss: 0.100556\n[Training] Epoch 061 | Batch 040/102 | Current Loss: 0.100487\n[Training] Epoch 061 | Batch 050/102 | Current Loss: 0.100473\n[Training] Epoch 061 | Batch 060/102 | Current Loss: 0.100739\n[Training] Epoch 061 | Batch 070/102 | Current Loss: 0.100591\n[Training] Epoch 061 | Batch 080/102 | Current Loss: 0.100513\n[Training] Epoch 061 | Batch 090/102 | Current Loss: 0.100596\n[Training] Epoch 061 | Batch 100/102 | Current Loss: 0.100502\n\n[Training] Epoch 061 Summary:\n  Avg Loss: 0.100538\n  Current LR: 9.27e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 062 | Batch 000/102 | Current Loss: 0.100530\n[Training] Epoch 062 | Batch 010/102 | Current Loss: 0.100491\n[Training] Epoch 062 | Batch 020/102 | Current Loss: 0.100505\n[Training] Epoch 062 | Batch 030/102 | Current Loss: 0.100515\n[Training] Epoch 062 | Batch 040/102 | Current Loss: 0.100507\n[Training] Epoch 062 | Batch 050/102 | Current Loss: 0.100511\n[Training] Epoch 062 | Batch 060/102 | Current Loss: 0.100439\n[Training] Epoch 062 | Batch 070/102 | Current Loss: 0.100473\n[Training] Epoch 062 | Batch 080/102 | Current Loss: 0.100381\n[Training] Epoch 062 | Batch 090/102 | Current Loss: 0.100459\n[Training] Epoch 062 | Batch 100/102 | Current Loss: 0.100474\n\n[Training] Epoch 062 Summary:\n  Avg Loss: 0.100493\n  Current LR: 8.60e-05\n[EarlyStopping] Loss improved (0.100494 → 0.100493). Saving model...\n[Training] Epoch 063 | Batch 000/102 | Current Loss: 0.100593\n[Training] Epoch 063 | Batch 010/102 | Current Loss: 0.100560\n[Training] Epoch 063 | Batch 020/102 | Current Loss: 0.100570\n[Training] Epoch 063 | Batch 030/102 | Current Loss: 0.100563\n[Training] Epoch 063 | Batch 040/102 | Current Loss: 0.100451\n[Training] Epoch 063 | Batch 050/102 | Current Loss: 0.100424\n[Training] Epoch 063 | Batch 060/102 | Current Loss: 0.100470\n[Training] Epoch 063 | Batch 070/102 | Current Loss: 0.100441\n[Training] Epoch 063 | Batch 080/102 | Current Loss: 0.100400\n[Training] Epoch 063 | Batch 090/102 | Current Loss: 0.100422\n[Training] Epoch 063 | Batch 100/102 | Current Loss: 0.100538\n\n[Training] Epoch 063 Summary:\n  Avg Loss: 0.100465\n  Current LR: 7.56e-05\n[EarlyStopping] Loss improved (0.100493 → 0.100465). Saving model...\n[Training] Epoch 064 | Batch 000/102 | Current Loss: 0.100456\n[Training] Epoch 064 | Batch 010/102 | Current Loss: 0.100444\n[Training] Epoch 064 | Batch 020/102 | Current Loss: 0.100400\n[Training] Epoch 064 | Batch 030/102 | Current Loss: 0.100469\n[Training] Epoch 064 | Batch 040/102 | Current Loss: 0.100709\n[Training] Epoch 064 | Batch 050/102 | Current Loss: 0.100423\n[Training] Epoch 064 | Batch 060/102 | Current Loss: 0.100493\n[Training] Epoch 064 | Batch 070/102 | Current Loss: 0.100351\n[Training] Epoch 064 | Batch 080/102 | Current Loss: 0.100428\n[Training] Epoch 064 | Batch 090/102 | Current Loss: 0.100379\n[Training] Epoch 064 | Batch 100/102 | Current Loss: 0.100441\n\n[Training] Epoch 064 Summary:\n  Avg Loss: 0.100448\n  Current LR: 6.25e-05\n[EarlyStopping] Loss improved (0.100465 → 0.100448). Saving model...\n[Training] Epoch 065 | Batch 000/102 | Current Loss: 0.100389\n[Training] Epoch 065 | Batch 010/102 | Current Loss: 0.100373\n[Training] Epoch 065 | Batch 020/102 | Current Loss: 0.100342\n[Training] Epoch 065 | Batch 030/102 | Current Loss: 0.100323\n[Training] Epoch 065 | Batch 040/102 | Current Loss: 0.100419\n[Training] Epoch 065 | Batch 050/102 | Current Loss: 0.100387\n[Training] Epoch 065 | Batch 060/102 | Current Loss: 0.100400\n[Training] Epoch 065 | Batch 070/102 | Current Loss: 0.100412\n[Training] Epoch 065 | Batch 080/102 | Current Loss: 0.100458\n[Training] Epoch 065 | Batch 090/102 | Current Loss: 0.100363\n[Training] Epoch 065 | Batch 100/102 | Current Loss: 0.100456\n\n[Training] Epoch 065 Summary:\n  Avg Loss: 0.100413\n  Current LR: 4.80e-05\n[EarlyStopping] Loss improved (0.100448 → 0.100413). Saving model...\n[Training] Epoch 066 | Batch 000/102 | Current Loss: 0.100365\n[Training] Epoch 066 | Batch 010/102 | Current Loss: 0.100426\n[Training] Epoch 066 | Batch 020/102 | Current Loss: 0.100432\n[Training] Epoch 066 | Batch 030/102 | Current Loss: 0.100402\n[Training] Epoch 066 | Batch 040/102 | Current Loss: 0.100374\n[Training] Epoch 066 | Batch 050/102 | Current Loss: 0.100373\n[Training] Epoch 066 | Batch 060/102 | Current Loss: 0.100471\n[Training] Epoch 066 | Batch 070/102 | Current Loss: 0.100354\n[Training] Epoch 066 | Batch 080/102 | Current Loss: 0.100372\n[Training] Epoch 066 | Batch 090/102 | Current Loss: 0.100377\n[Training] Epoch 066 | Batch 100/102 | Current Loss: 0.100353\n\n[Training] Epoch 066 Summary:\n  Avg Loss: 0.100403\n  Current LR: 3.35e-05\n[EarlyStopping] Loss improved (0.100413 → 0.100403). Saving model...\n[Training] Epoch 067 | Batch 000/102 | Current Loss: 0.100408\n[Training] Epoch 067 | Batch 010/102 | Current Loss: 0.100360\n[Training] Epoch 067 | Batch 020/102 | Current Loss: 0.100369\n[Training] Epoch 067 | Batch 030/102 | Current Loss: 0.100458\n[Training] Epoch 067 | Batch 040/102 | Current Loss: 0.100361\n[Training] Epoch 067 | Batch 050/102 | Current Loss: 0.100518\n[Training] Epoch 067 | Batch 060/102 | Current Loss: 0.100362\n[Training] Epoch 067 | Batch 070/102 | Current Loss: 0.100344\n[Training] Epoch 067 | Batch 080/102 | Current Loss: 0.100394\n[Training] Epoch 067 | Batch 090/102 | Current Loss: 0.100338\n[Training] Epoch 067 | Batch 100/102 | Current Loss: 0.100418\n\n[Training] Epoch 067 Summary:\n  Avg Loss: 0.100396\n  Current LR: 2.04e-05\n[EarlyStopping] Loss improved (0.100403 → 0.100396). Saving model...\n[Training] Epoch 068 | Batch 000/102 | Current Loss: 0.100455\n[Training] Epoch 068 | Batch 010/102 | Current Loss: 0.100391\n[Training] Epoch 068 | Batch 020/102 | Current Loss: 0.100327\n[Training] Epoch 068 | Batch 030/102 | Current Loss: 0.100264\n[Training] Epoch 068 | Batch 040/102 | Current Loss: 0.100411\n[Training] Epoch 068 | Batch 050/102 | Current Loss: 0.100376\n[Training] Epoch 068 | Batch 060/102 | Current Loss: 0.100298\n[Training] Epoch 068 | Batch 070/102 | Current Loss: 0.100355\n[Training] Epoch 068 | Batch 080/102 | Current Loss: 0.100275\n[Training] Epoch 068 | Batch 090/102 | Current Loss: 0.100340\n[Training] Epoch 068 | Batch 100/102 | Current Loss: 0.100319\n\n[Training] Epoch 068 Summary:\n  Avg Loss: 0.100387\n  Current LR: 9.97e-06\n[EarlyStopping] Loss improved (0.100396 → 0.100387). Saving model...\n[Training] Epoch 069 | Batch 000/102 | Current Loss: 0.100291\n[Training] Epoch 069 | Batch 010/102 | Current Loss: 0.100332\n[Training] Epoch 069 | Batch 020/102 | Current Loss: 0.100345\n[Training] Epoch 069 | Batch 030/102 | Current Loss: 0.100319\n[Training] Epoch 069 | Batch 040/102 | Current Loss: 0.100423\n[Training] Epoch 069 | Batch 050/102 | Current Loss: 0.100370\n[Training] Epoch 069 | Batch 060/102 | Current Loss: 0.100410\n[Training] Epoch 069 | Batch 070/102 | Current Loss: 0.100367\n[Training] Epoch 069 | Batch 080/102 | Current Loss: 0.100374\n[Training] Epoch 069 | Batch 090/102 | Current Loss: 0.100326\n[Training] Epoch 069 | Batch 100/102 | Current Loss: 0.100334\n\n[Training] Epoch 069 Summary:\n  Avg Loss: 0.100377\n  Current LR: 3.30e-06\n[EarlyStopping] Loss improved (0.100387 → 0.100377). Saving model...\n[Training] Epoch 070 | Batch 000/102 | Current Loss: 0.100394\n[Training] Epoch 070 | Batch 010/102 | Current Loss: 0.100332\n[Training] Epoch 070 | Batch 020/102 | Current Loss: 0.100321\n[Training] Epoch 070 | Batch 030/102 | Current Loss: 0.100323\n[Training] Epoch 070 | Batch 040/102 | Current Loss: 0.100427\n[Training] Epoch 070 | Batch 050/102 | Current Loss: 0.100423\n[Training] Epoch 070 | Batch 060/102 | Current Loss: 0.100462\n[Training] Epoch 070 | Batch 070/102 | Current Loss: 0.100324\n[Training] Epoch 070 | Batch 080/102 | Current Loss: 0.100385\n[Training] Epoch 070 | Batch 090/102 | Current Loss: 0.100377\n[Training] Epoch 070 | Batch 100/102 | Current Loss: 0.100450\n\n[Training] Epoch 070 Summary:\n  Avg Loss: 0.100379\n  Current LR: 9.50e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 071 | Batch 000/102 | Current Loss: 0.100333\n[Training] Epoch 071 | Batch 010/102 | Current Loss: 0.100382\n[Training] Epoch 071 | Batch 020/102 | Current Loss: 0.100355\n[Training] Epoch 071 | Batch 030/102 | Current Loss: 0.100380\n[Training] Epoch 071 | Batch 040/102 | Current Loss: 0.100414\n[Training] Epoch 071 | Batch 050/102 | Current Loss: 0.100365\n[Training] Epoch 071 | Batch 060/102 | Current Loss: 0.100308\n[Training] Epoch 071 | Batch 070/102 | Current Loss: 0.100334\n[Training] Epoch 071 | Batch 080/102 | Current Loss: 0.100426\n[Training] Epoch 071 | Batch 090/102 | Current Loss: 0.100356\n[Training] Epoch 071 | Batch 100/102 | Current Loss: 0.100449\n\n[Training] Epoch 071 Summary:\n  Avg Loss: 0.100400\n  Current LR: 9.27e-05\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 072 | Batch 000/102 | Current Loss: 0.100416\n[Training] Epoch 072 | Batch 010/102 | Current Loss: 0.100373\n[Training] Epoch 072 | Batch 020/102 | Current Loss: 0.100350\n[Training] Epoch 072 | Batch 030/102 | Current Loss: 0.100397\n[Training] Epoch 072 | Batch 040/102 | Current Loss: 0.100357\n[Training] Epoch 072 | Batch 050/102 | Current Loss: 0.100434\n[Training] Epoch 072 | Batch 060/102 | Current Loss: 0.100418\n[Training] Epoch 072 | Batch 070/102 | Current Loss: 0.100423\n[Training] Epoch 072 | Batch 080/102 | Current Loss: 0.100347\n[Training] Epoch 072 | Batch 090/102 | Current Loss: 0.100238\n[Training] Epoch 072 | Batch 100/102 | Current Loss: 0.100318\n\n[Training] Epoch 072 Summary:\n  Avg Loss: 0.100377\n  Current LR: 8.60e-05\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 073 | Batch 000/102 | Current Loss: 0.100440\n[Training] Epoch 073 | Batch 010/102 | Current Loss: 0.100345\n[Training] Epoch 073 | Batch 020/102 | Current Loss: 0.100334\n[Training] Epoch 073 | Batch 030/102 | Current Loss: 0.100363\n[Training] Epoch 073 | Batch 040/102 | Current Loss: 0.100381\n[Training] Epoch 073 | Batch 050/102 | Current Loss: 0.100430\n[Training] Epoch 073 | Batch 060/102 | Current Loss: 0.100303\n[Training] Epoch 073 | Batch 070/102 | Current Loss: 0.100378\n[Training] Epoch 073 | Batch 080/102 | Current Loss: 0.100450\n[Training] Epoch 073 | Batch 090/102 | Current Loss: 0.100336\n[Training] Epoch 073 | Batch 100/102 | Current Loss: 0.100291\n\n[Training] Epoch 073 Summary:\n  Avg Loss: 0.100363\n  Current LR: 7.56e-05\n[EarlyStopping] Loss improved (0.100377 → 0.100363). Saving model...\n[Training] Epoch 074 | Batch 000/102 | Current Loss: 0.100374\n[Training] Epoch 074 | Batch 010/102 | Current Loss: 0.100326\n[Training] Epoch 074 | Batch 020/102 | Current Loss: 0.100371\n[Training] Epoch 074 | Batch 030/102 | Current Loss: 0.100274\n[Training] Epoch 074 | Batch 040/102 | Current Loss: 0.100396\n[Training] Epoch 074 | Batch 050/102 | Current Loss: 0.100326\n[Training] Epoch 074 | Batch 060/102 | Current Loss: 0.100350\n[Training] Epoch 074 | Batch 070/102 | Current Loss: 0.100305\n[Training] Epoch 074 | Batch 080/102 | Current Loss: 0.100312\n[Training] Epoch 074 | Batch 090/102 | Current Loss: 0.100286\n[Training] Epoch 074 | Batch 100/102 | Current Loss: 0.100349\n\n[Training] Epoch 074 Summary:\n  Avg Loss: 0.100347\n  Current LR: 6.25e-05\n[EarlyStopping] Loss improved (0.100363 → 0.100347). Saving model...\n[Training] Epoch 075 | Batch 000/102 | Current Loss: 0.100283\n[Training] Epoch 075 | Batch 010/102 | Current Loss: 0.100365\n[Training] Epoch 075 | Batch 020/102 | Current Loss: 0.100281\n[Training] Epoch 075 | Batch 030/102 | Current Loss: 0.100306\n[Training] Epoch 075 | Batch 040/102 | Current Loss: 0.100368\n[Training] Epoch 075 | Batch 050/102 | Current Loss: 0.100350\n[Training] Epoch 075 | Batch 060/102 | Current Loss: 0.100364\n[Training] Epoch 075 | Batch 070/102 | Current Loss: 0.100329\n[Training] Epoch 075 | Batch 080/102 | Current Loss: 0.100327\n[Training] Epoch 075 | Batch 090/102 | Current Loss: 0.100415\n[Training] Epoch 075 | Batch 100/102 | Current Loss: 0.100281\n\n[Training] Epoch 075 Summary:\n  Avg Loss: 0.100329\n  Current LR: 4.80e-05\n[EarlyStopping] Loss improved (0.100347 → 0.100329). Saving model...\n[Training] Epoch 076 | Batch 000/102 | Current Loss: 0.100335\n[Training] Epoch 076 | Batch 010/102 | Current Loss: 0.100297\n[Training] Epoch 076 | Batch 020/102 | Current Loss: 0.100372\n[Training] Epoch 076 | Batch 030/102 | Current Loss: 0.100410\n[Training] Epoch 076 | Batch 040/102 | Current Loss: 0.100306\n[Training] Epoch 076 | Batch 050/102 | Current Loss: 0.100276\n[Training] Epoch 076 | Batch 060/102 | Current Loss: 0.100254\n[Training] Epoch 076 | Batch 070/102 | Current Loss: 0.100324\n[Training] Epoch 076 | Batch 080/102 | Current Loss: 0.100430\n[Training] Epoch 076 | Batch 090/102 | Current Loss: 0.100334\n[Training] Epoch 076 | Batch 100/102 | Current Loss: 0.100344\n\n[Training] Epoch 076 Summary:\n  Avg Loss: 0.100318\n  Current LR: 3.35e-05\n[EarlyStopping] Loss improved (0.100329 → 0.100318). Saving model...\n[Training] Epoch 077 | Batch 000/102 | Current Loss: 0.100309\n[Training] Epoch 077 | Batch 010/102 | Current Loss: 0.100253\n[Training] Epoch 077 | Batch 020/102 | Current Loss: 0.100350\n[Training] Epoch 077 | Batch 030/102 | Current Loss: 0.100338\n[Training] Epoch 077 | Batch 040/102 | Current Loss: 0.100336\n[Training] Epoch 077 | Batch 050/102 | Current Loss: 0.100300\n[Training] Epoch 077 | Batch 060/102 | Current Loss: 0.100265\n[Training] Epoch 077 | Batch 070/102 | Current Loss: 0.100222\n[Training] Epoch 077 | Batch 080/102 | Current Loss: 0.100323\n[Training] Epoch 077 | Batch 090/102 | Current Loss: 0.100252\n[Training] Epoch 077 | Batch 100/102 | Current Loss: 0.100299\n\n[Training] Epoch 077 Summary:\n  Avg Loss: 0.100309\n  Current LR: 2.04e-05\n[EarlyStopping] Loss improved (0.100318 → 0.100309). Saving model...\n[Training] Epoch 078 | Batch 000/102 | Current Loss: 0.100306\n[Training] Epoch 078 | Batch 010/102 | Current Loss: 0.100337\n[Training] Epoch 078 | Batch 020/102 | Current Loss: 0.100308\n[Training] Epoch 078 | Batch 030/102 | Current Loss: 0.100369\n[Training] Epoch 078 | Batch 040/102 | Current Loss: 0.100296\n[Training] Epoch 078 | Batch 050/102 | Current Loss: 0.100292\n[Training] Epoch 078 | Batch 060/102 | Current Loss: 0.100366\n[Training] Epoch 078 | Batch 070/102 | Current Loss: 0.100291\n[Training] Epoch 078 | Batch 080/102 | Current Loss: 0.100394\n[Training] Epoch 078 | Batch 090/102 | Current Loss: 0.100221\n[Training] Epoch 078 | Batch 100/102 | Current Loss: 0.100309\n\n[Training] Epoch 078 Summary:\n  Avg Loss: 0.100303\n  Current LR: 9.97e-06\n[EarlyStopping] Loss improved (0.100309 → 0.100303). Saving model...\n[Training] Epoch 079 | Batch 000/102 | Current Loss: 0.100308\n[Training] Epoch 079 | Batch 010/102 | Current Loss: 0.100331\n[Training] Epoch 079 | Batch 020/102 | Current Loss: 0.100273\n[Training] Epoch 079 | Batch 030/102 | Current Loss: 0.100216\n[Training] Epoch 079 | Batch 040/102 | Current Loss: 0.100292\n[Training] Epoch 079 | Batch 050/102 | Current Loss: 0.100247\n[Training] Epoch 079 | Batch 060/102 | Current Loss: 0.100302\n[Training] Epoch 079 | Batch 070/102 | Current Loss: 0.100331\n[Training] Epoch 079 | Batch 080/102 | Current Loss: 0.100278\n[Training] Epoch 079 | Batch 090/102 | Current Loss: 0.100286\n[Training] Epoch 079 | Batch 100/102 | Current Loss: 0.100282\n\n[Training] Epoch 079 Summary:\n  Avg Loss: 0.100297\n  Current LR: 3.30e-06\n[EarlyStopping] Loss improved (0.100303 → 0.100297). Saving model...\n[Training] Epoch 080 | Batch 000/102 | Current Loss: 0.100308\n[Training] Epoch 080 | Batch 010/102 | Current Loss: 0.100301\n[Training] Epoch 080 | Batch 020/102 | Current Loss: 0.100364\n[Training] Epoch 080 | Batch 030/102 | Current Loss: 0.100293\n[Training] Epoch 080 | Batch 040/102 | Current Loss: 0.100308\n[Training] Epoch 080 | Batch 050/102 | Current Loss: 0.100332\n[Training] Epoch 080 | Batch 060/102 | Current Loss: 0.100279\n[Training] Epoch 080 | Batch 070/102 | Current Loss: 0.100195\n[Training] Epoch 080 | Batch 080/102 | Current Loss: 0.100215\n[Training] Epoch 080 | Batch 090/102 | Current Loss: 0.100282\n[Training] Epoch 080 | Batch 100/102 | Current Loss: 0.100416\n\n[Training] Epoch 080 Summary:\n  Avg Loss: 0.100288\n  Current LR: 9.50e-05\n[EarlyStopping] Loss improved (0.100297 → 0.100288). Saving model...\n[Training] Epoch 081 | Batch 000/102 | Current Loss: 0.100253\n[Training] Epoch 081 | Batch 010/102 | Current Loss: 0.100383\n[Training] Epoch 081 | Batch 020/102 | Current Loss: 0.100253\n[Training] Epoch 081 | Batch 030/102 | Current Loss: 0.100312\n[Training] Epoch 081 | Batch 040/102 | Current Loss: 0.100363\n[Training] Epoch 081 | Batch 050/102 | Current Loss: 0.100321\n[Training] Epoch 081 | Batch 060/102 | Current Loss: 0.100324\n[Training] Epoch 081 | Batch 070/102 | Current Loss: 0.100255\n[Training] Epoch 081 | Batch 080/102 | Current Loss: 0.100357\n[Training] Epoch 081 | Batch 090/102 | Current Loss: 0.100298\n[Training] Epoch 081 | Batch 100/102 | Current Loss: 0.100338\n\n[Training] Epoch 081 Summary:\n  Avg Loss: 0.100314\n  Current LR: 9.27e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 082 | Batch 000/102 | Current Loss: 0.100368\n[Training] Epoch 082 | Batch 010/102 | Current Loss: 0.100365\n[Training] Epoch 082 | Batch 020/102 | Current Loss: 0.100300\n[Training] Epoch 082 | Batch 030/102 | Current Loss: 0.100345\n[Training] Epoch 082 | Batch 040/102 | Current Loss: 0.100297\n[Training] Epoch 082 | Batch 050/102 | Current Loss: 0.100258\n[Training] Epoch 082 | Batch 060/102 | Current Loss: 0.100278\n[Training] Epoch 082 | Batch 070/102 | Current Loss: 0.100315\n[Training] Epoch 082 | Batch 080/102 | Current Loss: 0.100316\n[Training] Epoch 082 | Batch 090/102 | Current Loss: 0.100330\n[Training] Epoch 082 | Batch 100/102 | Current Loss: 0.100278\n\n[Training] Epoch 082 Summary:\n  Avg Loss: 0.100299\n  Current LR: 8.60e-05\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 083 | Batch 000/102 | Current Loss: 0.100295\n[Training] Epoch 083 | Batch 010/102 | Current Loss: 0.100280\n[Training] Epoch 083 | Batch 020/102 | Current Loss: 0.100267\n[Training] Epoch 083 | Batch 030/102 | Current Loss: 0.100261\n[Training] Epoch 083 | Batch 040/102 | Current Loss: 0.100316\n[Training] Epoch 083 | Batch 050/102 | Current Loss: 0.100264\n[Training] Epoch 083 | Batch 060/102 | Current Loss: 0.100277\n[Training] Epoch 083 | Batch 070/102 | Current Loss: 0.100232\n[Training] Epoch 083 | Batch 080/102 | Current Loss: 0.100224\n[Training] Epoch 083 | Batch 090/102 | Current Loss: 0.100278\n[Training] Epoch 083 | Batch 100/102 | Current Loss: 0.100283\n\n[Training] Epoch 083 Summary:\n  Avg Loss: 0.100280\n  Current LR: 7.56e-05\n[EarlyStopping] Loss improved (0.100288 → 0.100280). Saving model...\n[Training] Epoch 084 | Batch 000/102 | Current Loss: 0.100365\n[Training] Epoch 084 | Batch 010/102 | Current Loss: 0.100312\n[Training] Epoch 084 | Batch 020/102 | Current Loss: 0.100321\n[Training] Epoch 084 | Batch 030/102 | Current Loss: 0.100356\n[Training] Epoch 084 | Batch 040/102 | Current Loss: 0.100291\n[Training] Epoch 084 | Batch 050/102 | Current Loss: 0.100325\n[Training] Epoch 084 | Batch 060/102 | Current Loss: 0.100324\n[Training] Epoch 084 | Batch 070/102 | Current Loss: 0.100215\n[Training] Epoch 084 | Batch 080/102 | Current Loss: 0.100223\n[Training] Epoch 084 | Batch 090/102 | Current Loss: 0.100296\n[Training] Epoch 084 | Batch 100/102 | Current Loss: 0.100228\n\n[Training] Epoch 084 Summary:\n  Avg Loss: 0.100277\n  Current LR: 6.25e-05\n[EarlyStopping] Loss improved (0.100280 → 0.100277). Saving model...\n[Training] Epoch 085 | Batch 000/102 | Current Loss: 0.100310\n[Training] Epoch 085 | Batch 010/102 | Current Loss: 0.100290\n[Training] Epoch 085 | Batch 020/102 | Current Loss: 0.100311\n[Training] Epoch 085 | Batch 030/102 | Current Loss: 0.100336\n[Training] Epoch 085 | Batch 040/102 | Current Loss: 0.100170\n[Training] Epoch 085 | Batch 050/102 | Current Loss: 0.100187\n[Training] Epoch 085 | Batch 060/102 | Current Loss: 0.100259\n[Training] Epoch 085 | Batch 070/102 | Current Loss: 0.100185\n[Training] Epoch 085 | Batch 080/102 | Current Loss: 0.100297\n[Training] Epoch 085 | Batch 090/102 | Current Loss: 0.100301\n[Training] Epoch 085 | Batch 100/102 | Current Loss: 0.100275\n\n[Training] Epoch 085 Summary:\n  Avg Loss: 0.100264\n  Current LR: 4.80e-05\n[EarlyStopping] Loss improved (0.100277 → 0.100264). Saving model...\n[Training] Epoch 086 | Batch 000/102 | Current Loss: 0.100378\n[Training] Epoch 086 | Batch 010/102 | Current Loss: 0.100273\n[Training] Epoch 086 | Batch 020/102 | Current Loss: 0.100309\n[Training] Epoch 086 | Batch 030/102 | Current Loss: 0.100184\n[Training] Epoch 086 | Batch 040/102 | Current Loss: 0.100259\n[Training] Epoch 086 | Batch 050/102 | Current Loss: 0.100278\n[Training] Epoch 086 | Batch 060/102 | Current Loss: 0.100278\n[Training] Epoch 086 | Batch 070/102 | Current Loss: 0.100255\n[Training] Epoch 086 | Batch 080/102 | Current Loss: 0.100274\n[Training] Epoch 086 | Batch 090/102 | Current Loss: 0.100322\n[Training] Epoch 086 | Batch 100/102 | Current Loss: 0.100319\n\n[Training] Epoch 086 Summary:\n  Avg Loss: 0.100260\n  Current LR: 3.35e-05\n[EarlyStopping] Loss improved (0.100264 → 0.100260). Saving model...\n[Training] Epoch 087 | Batch 000/102 | Current Loss: 0.100240\n[Training] Epoch 087 | Batch 010/102 | Current Loss: 0.100310\n[Training] Epoch 087 | Batch 020/102 | Current Loss: 0.100195\n[Training] Epoch 087 | Batch 030/102 | Current Loss: 0.100276\n[Training] Epoch 087 | Batch 040/102 | Current Loss: 0.100191\n[Training] Epoch 087 | Batch 050/102 | Current Loss: 0.100280\n[Training] Epoch 087 | Batch 060/102 | Current Loss: 0.100210\n[Training] Epoch 087 | Batch 070/102 | Current Loss: 0.100235\n[Training] Epoch 087 | Batch 080/102 | Current Loss: 0.100352\n[Training] Epoch 087 | Batch 090/102 | Current Loss: 0.100276\n[Training] Epoch 087 | Batch 100/102 | Current Loss: 0.100298\n\n[Training] Epoch 087 Summary:\n  Avg Loss: 0.100250\n  Current LR: 2.04e-05\n[EarlyStopping] Loss improved (0.100260 → 0.100250). Saving model...\n[Training] Epoch 088 | Batch 000/102 | Current Loss: 0.100183\n[Training] Epoch 088 | Batch 010/102 | Current Loss: 0.100179\n[Training] Epoch 088 | Batch 020/102 | Current Loss: 0.100276\n[Training] Epoch 088 | Batch 030/102 | Current Loss: 0.100198\n[Training] Epoch 088 | Batch 040/102 | Current Loss: 0.100332\n[Training] Epoch 088 | Batch 050/102 | Current Loss: 0.100153\n[Training] Epoch 088 | Batch 060/102 | Current Loss: 0.100235\n[Training] Epoch 088 | Batch 070/102 | Current Loss: 0.100263\n[Training] Epoch 088 | Batch 080/102 | Current Loss: 0.100242\n[Training] Epoch 088 | Batch 090/102 | Current Loss: 0.100146\n[Training] Epoch 088 | Batch 100/102 | Current Loss: 0.100233\n\n[Training] Epoch 088 Summary:\n  Avg Loss: 0.100246\n  Current LR: 9.97e-06\n[EarlyStopping] Loss improved (0.100250 → 0.100246). Saving model...\n[Training] Epoch 089 | Batch 000/102 | Current Loss: 0.100271\n[Training] Epoch 089 | Batch 010/102 | Current Loss: 0.100292\n[Training] Epoch 089 | Batch 020/102 | Current Loss: 0.100241\n[Training] Epoch 089 | Batch 030/102 | Current Loss: 0.100221\n[Training] Epoch 089 | Batch 040/102 | Current Loss: 0.100216\n[Training] Epoch 089 | Batch 050/102 | Current Loss: 0.100278\n[Training] Epoch 089 | Batch 060/102 | Current Loss: 0.100275\n[Training] Epoch 089 | Batch 070/102 | Current Loss: 0.100201\n[Training] Epoch 089 | Batch 080/102 | Current Loss: 0.100256\n[Training] Epoch 089 | Batch 090/102 | Current Loss: 0.100187\n[Training] Epoch 089 | Batch 100/102 | Current Loss: 0.100208\n\n[Training] Epoch 089 Summary:\n  Avg Loss: 0.100236\n  Current LR: 3.30e-06\n[EarlyStopping] Loss improved (0.100246 → 0.100236). Saving model...\n[Training] Epoch 090 | Batch 000/102 | Current Loss: 0.100310\n[Training] Epoch 090 | Batch 010/102 | Current Loss: 0.100217\n[Training] Epoch 090 | Batch 020/102 | Current Loss: 0.100223\n[Training] Epoch 090 | Batch 030/102 | Current Loss: 0.100170\n[Training] Epoch 090 | Batch 040/102 | Current Loss: 0.100110\n[Training] Epoch 090 | Batch 050/102 | Current Loss: 0.100266\n[Training] Epoch 090 | Batch 060/102 | Current Loss: 0.100212\n[Training] Epoch 090 | Batch 070/102 | Current Loss: 0.100325\n[Training] Epoch 090 | Batch 080/102 | Current Loss: 0.100209\n[Training] Epoch 090 | Batch 090/102 | Current Loss: 0.100252\n[Training] Epoch 090 | Batch 100/102 | Current Loss: 0.100181\n\n[Training] Epoch 090 Summary:\n  Avg Loss: 0.100237\n  Current LR: 9.50e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 091 | Batch 000/102 | Current Loss: 0.100231\n[Training] Epoch 091 | Batch 010/102 | Current Loss: 0.100160\n[Training] Epoch 091 | Batch 020/102 | Current Loss: 0.100301\n[Training] Epoch 091 | Batch 030/102 | Current Loss: 0.100290\n[Training] Epoch 091 | Batch 040/102 | Current Loss: 0.100351\n[Training] Epoch 091 | Batch 050/102 | Current Loss: 0.100300\n[Training] Epoch 091 | Batch 060/102 | Current Loss: 0.100256\n[Training] Epoch 091 | Batch 070/102 | Current Loss: 0.100237\n[Training] Epoch 091 | Batch 080/102 | Current Loss: 0.100343\n[Training] Epoch 091 | Batch 090/102 | Current Loss: 0.100237\n[Training] Epoch 091 | Batch 100/102 | Current Loss: 0.100199\n\n[Training] Epoch 091 Summary:\n  Avg Loss: 0.100250\n  Current LR: 9.27e-05\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 092 | Batch 000/102 | Current Loss: 0.100265\n[Training] Epoch 092 | Batch 010/102 | Current Loss: 0.100316\n[Training] Epoch 092 | Batch 020/102 | Current Loss: 0.100259\n[Training] Epoch 092 | Batch 030/102 | Current Loss: 0.100253\n[Training] Epoch 092 | Batch 040/102 | Current Loss: 0.100253\n[Training] Epoch 092 | Batch 050/102 | Current Loss: 0.100301\n[Training] Epoch 092 | Batch 060/102 | Current Loss: 0.100158\n[Training] Epoch 092 | Batch 070/102 | Current Loss: 0.100181\n[Training] Epoch 092 | Batch 080/102 | Current Loss: 0.100263\n[Training] Epoch 092 | Batch 090/102 | Current Loss: 0.100247\n[Training] Epoch 092 | Batch 100/102 | Current Loss: 0.100202\n\n[Training] Epoch 092 Summary:\n  Avg Loss: 0.100244\n  Current LR: 8.60e-05\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 093 | Batch 000/102 | Current Loss: 0.100298\n[Training] Epoch 093 | Batch 010/102 | Current Loss: 0.100277\n[Training] Epoch 093 | Batch 020/102 | Current Loss: 0.100220\n[Training] Epoch 093 | Batch 030/102 | Current Loss: 0.100248\n[Training] Epoch 093 | Batch 040/102 | Current Loss: 0.100213\n[Training] Epoch 093 | Batch 050/102 | Current Loss: 0.100248\n[Training] Epoch 093 | Batch 060/102 | Current Loss: 0.100221\n[Training] Epoch 093 | Batch 070/102 | Current Loss: 0.100267\n[Training] Epoch 093 | Batch 080/102 | Current Loss: 0.100294\n[Training] Epoch 093 | Batch 090/102 | Current Loss: 0.100167\n[Training] Epoch 093 | Batch 100/102 | Current Loss: 0.100239\n\n[Training] Epoch 093 Summary:\n  Avg Loss: 0.100242\n  Current LR: 7.56e-05\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 094 | Batch 000/102 | Current Loss: 0.100282\n[Training] Epoch 094 | Batch 010/102 | Current Loss: 0.100166\n[Training] Epoch 094 | Batch 020/102 | Current Loss: 0.100267\n[Training] Epoch 094 | Batch 030/102 | Current Loss: 0.100338\n[Training] Epoch 094 | Batch 040/102 | Current Loss: 0.100205\n[Training] Epoch 094 | Batch 050/102 | Current Loss: 0.100202\n[Training] Epoch 094 | Batch 060/102 | Current Loss: 0.100200\n[Training] Epoch 094 | Batch 070/102 | Current Loss: 0.100300\n[Training] Epoch 094 | Batch 080/102 | Current Loss: 0.100191\n[Training] Epoch 094 | Batch 090/102 | Current Loss: 0.100179\n[Training] Epoch 094 | Batch 100/102 | Current Loss: 0.100255\n\n[Training] Epoch 094 Summary:\n  Avg Loss: 0.100228\n  Current LR: 6.25e-05\n[EarlyStopping] Loss improved (0.100236 → 0.100228). Saving model...\n[Training] Epoch 095 | Batch 000/102 | Current Loss: 0.100289\n[Training] Epoch 095 | Batch 010/102 | Current Loss: 0.100228\n[Training] Epoch 095 | Batch 020/102 | Current Loss: 0.100206\n[Training] Epoch 095 | Batch 030/102 | Current Loss: 0.100216\n[Training] Epoch 095 | Batch 040/102 | Current Loss: 0.100232\n[Training] Epoch 095 | Batch 050/102 | Current Loss: 0.100151\n[Training] Epoch 095 | Batch 060/102 | Current Loss: 0.100269\n[Training] Epoch 095 | Batch 070/102 | Current Loss: 0.100114\n[Training] Epoch 095 | Batch 080/102 | Current Loss: 0.100216\n[Training] Epoch 095 | Batch 090/102 | Current Loss: 0.100262\n[Training] Epoch 095 | Batch 100/102 | Current Loss: 0.100159\n\n[Training] Epoch 095 Summary:\n  Avg Loss: 0.100215\n  Current LR: 4.80e-05\n[EarlyStopping] Loss improved (0.100228 → 0.100215). Saving model...\n[Training] Epoch 096 | Batch 000/102 | Current Loss: 0.100222\n[Training] Epoch 096 | Batch 010/102 | Current Loss: 0.100266\n[Training] Epoch 096 | Batch 020/102 | Current Loss: 0.100245\n[Training] Epoch 096 | Batch 030/102 | Current Loss: 0.100197\n[Training] Epoch 096 | Batch 040/102 | Current Loss: 0.100184\n[Training] Epoch 096 | Batch 050/102 | Current Loss: 0.100217\n[Training] Epoch 096 | Batch 060/102 | Current Loss: 0.100218\n[Training] Epoch 096 | Batch 070/102 | Current Loss: 0.100186\n[Training] Epoch 096 | Batch 080/102 | Current Loss: 0.100243\n[Training] Epoch 096 | Batch 090/102 | Current Loss: 0.100147\n[Training] Epoch 096 | Batch 100/102 | Current Loss: 0.100168\n\n[Training] Epoch 096 Summary:\n  Avg Loss: 0.100213\n  Current LR: 3.35e-05\n[EarlyStopping] Loss improved (0.100215 → 0.100213). Saving model...\n[Training] Epoch 097 | Batch 000/102 | Current Loss: 0.100165\n[Training] Epoch 097 | Batch 010/102 | Current Loss: 0.100212\n[Training] Epoch 097 | Batch 020/102 | Current Loss: 0.100234\n[Training] Epoch 097 | Batch 030/102 | Current Loss: 0.100321\n[Training] Epoch 097 | Batch 040/102 | Current Loss: 0.100111\n[Training] Epoch 097 | Batch 050/102 | Current Loss: 0.100278\n[Training] Epoch 097 | Batch 060/102 | Current Loss: 0.100181\n[Training] Epoch 097 | Batch 070/102 | Current Loss: 0.100211\n[Training] Epoch 097 | Batch 080/102 | Current Loss: 0.100242\n[Training] Epoch 097 | Batch 090/102 | Current Loss: 0.100228\n[Training] Epoch 097 | Batch 100/102 | Current Loss: 0.100149\n\n[Training] Epoch 097 Summary:\n  Avg Loss: 0.100207\n  Current LR: 2.04e-05\n[EarlyStopping] Loss improved (0.100213 → 0.100207). Saving model...\n[Training] Epoch 098 | Batch 000/102 | Current Loss: 0.100218\n[Training] Epoch 098 | Batch 010/102 | Current Loss: 0.100208\n[Training] Epoch 098 | Batch 020/102 | Current Loss: 0.100202\n[Training] Epoch 098 | Batch 030/102 | Current Loss: 0.100180\n[Training] Epoch 098 | Batch 040/102 | Current Loss: 0.100183\n[Training] Epoch 098 | Batch 050/102 | Current Loss: 0.100147\n[Training] Epoch 098 | Batch 060/102 | Current Loss: 0.100211\n[Training] Epoch 098 | Batch 070/102 | Current Loss: 0.100187\n[Training] Epoch 098 | Batch 080/102 | Current Loss: 0.100172\n[Training] Epoch 098 | Batch 090/102 | Current Loss: 0.100176\n[Training] Epoch 098 | Batch 100/102 | Current Loss: 0.100142\n\n[Training] Epoch 098 Summary:\n  Avg Loss: 0.100205\n  Current LR: 9.97e-06\n[EarlyStopping] Loss improved (0.100207 → 0.100205). Saving model...\n[Training] Epoch 099 | Batch 000/102 | Current Loss: 0.100184\n[Training] Epoch 099 | Batch 010/102 | Current Loss: 0.100236\n[Training] Epoch 099 | Batch 020/102 | Current Loss: 0.100262\n[Training] Epoch 099 | Batch 030/102 | Current Loss: 0.100230\n[Training] Epoch 099 | Batch 040/102 | Current Loss: 0.100206\n[Training] Epoch 099 | Batch 050/102 | Current Loss: 0.100146\n[Training] Epoch 099 | Batch 060/102 | Current Loss: 0.100130\n[Training] Epoch 099 | Batch 070/102 | Current Loss: 0.100225\n[Training] Epoch 099 | Batch 080/102 | Current Loss: 0.100239\n[Training] Epoch 099 | Batch 090/102 | Current Loss: 0.100243\n[Training] Epoch 099 | Batch 100/102 | Current Loss: 0.100193\n\n[Training] Epoch 099 Summary:\n  Avg Loss: 0.100197\n  Current LR: 3.30e-06\n[EarlyStopping] Loss improved (0.100205 → 0.100197). Saving model...\n[Training] Epoch 100 | Batch 000/102 | Current Loss: 0.100194\n[Training] Epoch 100 | Batch 010/102 | Current Loss: 0.100202\n[Training] Epoch 100 | Batch 020/102 | Current Loss: 0.100212\n[Training] Epoch 100 | Batch 030/102 | Current Loss: 0.100164\n[Training] Epoch 100 | Batch 040/102 | Current Loss: 0.100302\n[Training] Epoch 100 | Batch 050/102 | Current Loss: 0.100159\n[Training] Epoch 100 | Batch 060/102 | Current Loss: 0.100209\n[Training] Epoch 100 | Batch 070/102 | Current Loss: 0.100174\n[Training] Epoch 100 | Batch 080/102 | Current Loss: 0.100232\n[Training] Epoch 100 | Batch 090/102 | Current Loss: 0.100172\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 16:07:23,777] Trial 4 finished with value: 0.10019232887847751 and parameters: {'lr': 9.496433119337225e-05, 'batch_size': 128, 'bottleneck_width': 512, 'dropout_rate': 0.20473365624673523, 'alpha': 0.6536122610703325, 'weight_decay': 4.61096933635227e-06}. Best is trial 1 with value: 0.10011662966480442.\n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 100 | Batch 100/102 | Current Loss: 0.100171\n\n[Training] Epoch 100 Summary:\n  Avg Loss: 0.100192\n  Current LR: 9.50e-05\n[EarlyStopping] Loss improved (0.100197 → 0.100192). Saving model...\n\n[Optuna] Trial 4 completed with best loss: 0.100192\n\n==================================================\nStarting Optuna Trial 5\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.00012527124122234504\n  batch_size: 256\n  bottleneck_width: 1024\n  dropout_rate: 0.44533386674718345\n  alpha: 0.6521192029626868\n  weight_decay: 1.1548197110095881e-05\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 256, Total batches: 51\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 1024\n  - Dropout rate: 0.44533386674718345\n[Model] Architecture initialized successfully\n[Model] Total parameters: 2,396,992\n[Optimizer] Initialized with lr=1.25e-04, weight_decay=1.15e-05\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.6521192029626868\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/051 | Current Loss: 1.677240\n[Training] Epoch 001 | Batch 010/051 | Current Loss: 0.409708\n[Training] Epoch 001 | Batch 020/051 | Current Loss: 0.225654\n[Training] Epoch 001 | Batch 030/051 | Current Loss: 0.177120\n[Training] Epoch 001 | Batch 040/051 | Current Loss: 0.150098\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 16:07:39,317] Trial 5 pruned. \n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 001 | Batch 050/051 | Current Loss: 0.139761\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.327803\n  Current LR: 1.22e-04\n[EarlyStopping] Loss improved (inf → 0.327803). Saving model...\n\n[Optuna] Trial pruned\n\n==================================================\nStarting Optuna Trial 6\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.0006171845799545735\n  batch_size: 128\n  bottleneck_width: 2048\n  dropout_rate: 0.1725223414798908\n  alpha: 0.787342942048626\n  weight_decay: 6.947186900534145e-06\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 128, Total batches: 102\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 2048\n  - Dropout rate: 0.1725223414798908\n[Model] Architecture initialized successfully\n[Model] Total parameters: 4,495,168\n[Optimizer] Initialized with lr=6.17e-04, weight_decay=6.95e-06\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.787342942048626\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/102 | Current Loss: 1.588806\n[Training] Epoch 001 | Batch 010/102 | Current Loss: 0.130706\n[Training] Epoch 001 | Batch 020/102 | Current Loss: 0.111750\n[Training] Epoch 001 | Batch 030/102 | Current Loss: 0.116524\n[Training] Epoch 001 | Batch 040/102 | Current Loss: 0.113607\n[Training] Epoch 001 | Batch 050/102 | Current Loss: 0.104247\n[Training] Epoch 001 | Batch 060/102 | Current Loss: 0.109124\n[Training] Epoch 001 | Batch 070/102 | Current Loss: 0.103852\n[Training] Epoch 001 | Batch 080/102 | Current Loss: 0.103870\n[Training] Epoch 001 | Batch 090/102 | Current Loss: 0.103017\n[Training] Epoch 001 | Batch 100/102 | Current Loss: 0.103508\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.137660\n  Current LR: 6.02e-04\n[EarlyStopping] Loss improved (inf → 0.137660). Saving model...\n[Training] Epoch 002 | Batch 000/102 | Current Loss: 0.102862\n[Training] Epoch 002 | Batch 010/102 | Current Loss: 0.102644\n[Training] Epoch 002 | Batch 020/102 | Current Loss: 0.102558\n[Training] Epoch 002 | Batch 030/102 | Current Loss: 0.102609\n[Training] Epoch 002 | Batch 040/102 | Current Loss: 0.102798\n[Training] Epoch 002 | Batch 050/102 | Current Loss: 0.102495\n[Training] Epoch 002 | Batch 060/102 | Current Loss: 0.103053\n[Training] Epoch 002 | Batch 070/102 | Current Loss: 0.105706\n[Training] Epoch 002 | Batch 080/102 | Current Loss: 0.102277\n[Training] Epoch 002 | Batch 090/102 | Current Loss: 0.101851\n[Training] Epoch 002 | Batch 100/102 | Current Loss: 0.101650\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.102664\n  Current LR: 5.58e-04\n[EarlyStopping] Loss improved (0.137660 → 0.102664). Saving model...\n[Training] Epoch 003 | Batch 000/102 | Current Loss: 0.102549\n[Training] Epoch 003 | Batch 010/102 | Current Loss: 0.101656\n[Training] Epoch 003 | Batch 020/102 | Current Loss: 0.103254\n[Training] Epoch 003 | Batch 030/102 | Current Loss: 0.101714\n[Training] Epoch 003 | Batch 040/102 | Current Loss: 0.102337\n[Training] Epoch 003 | Batch 050/102 | Current Loss: 0.101571\n[Training] Epoch 003 | Batch 060/102 | Current Loss: 0.101624\n[Training] Epoch 003 | Batch 070/102 | Current Loss: 0.102105\n[Training] Epoch 003 | Batch 080/102 | Current Loss: 0.101650\n[Training] Epoch 003 | Batch 090/102 | Current Loss: 0.101539\n[Training] Epoch 003 | Batch 100/102 | Current Loss: 0.101608\n\n[Training] Epoch 003 Summary:\n  Avg Loss: 0.102073\n  Current LR: 4.90e-04\n[EarlyStopping] Loss improved (0.102664 → 0.102073). Saving model...\n[Training] Epoch 004 | Batch 000/102 | Current Loss: 0.101484\n[Training] Epoch 004 | Batch 010/102 | Current Loss: 0.101199\n[Training] Epoch 004 | Batch 020/102 | Current Loss: 0.101380\n[Training] Epoch 004 | Batch 030/102 | Current Loss: 0.101310\n[Training] Epoch 004 | Batch 040/102 | Current Loss: 0.101404\n[Training] Epoch 004 | Batch 050/102 | Current Loss: 0.101519\n[Training] Epoch 004 | Batch 060/102 | Current Loss: 0.101254\n[Training] Epoch 004 | Batch 070/102 | Current Loss: 0.100996\n[Training] Epoch 004 | Batch 080/102 | Current Loss: 0.101125\n[Training] Epoch 004 | Batch 090/102 | Current Loss: 0.101297\n[Training] Epoch 004 | Batch 100/102 | Current Loss: 0.101826\n\n[Training] Epoch 004 Summary:\n  Avg Loss: 0.101465\n  Current LR: 4.04e-04\n[EarlyStopping] Loss improved (0.102073 → 0.101465). Saving model...\n[Training] Epoch 005 | Batch 000/102 | Current Loss: 0.101211\n[Training] Epoch 005 | Batch 010/102 | Current Loss: 0.102028\n[Training] Epoch 005 | Batch 020/102 | Current Loss: 0.101742\n[Training] Epoch 005 | Batch 030/102 | Current Loss: 0.100934\n[Training] Epoch 005 | Batch 040/102 | Current Loss: 0.101932\n[Training] Epoch 005 | Batch 050/102 | Current Loss: 0.101107\n[Training] Epoch 005 | Batch 060/102 | Current Loss: 0.101003\n[Training] Epoch 005 | Batch 070/102 | Current Loss: 0.100885\n[Training] Epoch 005 | Batch 080/102 | Current Loss: 0.100963\n[Training] Epoch 005 | Batch 090/102 | Current Loss: 0.100932\n[Training] Epoch 005 | Batch 100/102 | Current Loss: 0.100859\n\n[Training] Epoch 005 Summary:\n  Avg Loss: 0.101348\n  Current LR: 3.09e-04\n[EarlyStopping] Loss improved (0.101465 → 0.101348). Saving model...\n[Training] Epoch 006 | Batch 000/102 | Current Loss: 0.101011\n[Training] Epoch 006 | Batch 010/102 | Current Loss: 0.100990\n[Training] Epoch 006 | Batch 020/102 | Current Loss: 0.101081\n[Training] Epoch 006 | Batch 030/102 | Current Loss: 0.100998\n[Training] Epoch 006 | Batch 040/102 | Current Loss: 0.100905\n[Training] Epoch 006 | Batch 050/102 | Current Loss: 0.100837\n[Training] Epoch 006 | Batch 060/102 | Current Loss: 0.100922\n[Training] Epoch 006 | Batch 070/102 | Current Loss: 0.100849\n[Training] Epoch 006 | Batch 080/102 | Current Loss: 0.100846\n[Training] Epoch 006 | Batch 090/102 | Current Loss: 0.100890\n[Training] Epoch 006 | Batch 100/102 | Current Loss: 0.100842\n\n[Training] Epoch 006 Summary:\n  Avg Loss: 0.100909\n  Current LR: 2.14e-04\n[EarlyStopping] Loss improved (0.101348 → 0.100909). Saving model...\n[Training] Epoch 007 | Batch 000/102 | Current Loss: 0.100737\n[Training] Epoch 007 | Batch 010/102 | Current Loss: 0.100994\n[Training] Epoch 007 | Batch 020/102 | Current Loss: 0.100841\n[Training] Epoch 007 | Batch 030/102 | Current Loss: 0.100955\n[Training] Epoch 007 | Batch 040/102 | Current Loss: 0.100770\n[Training] Epoch 007 | Batch 050/102 | Current Loss: 0.100843\n[Training] Epoch 007 | Batch 060/102 | Current Loss: 0.100673\n[Training] Epoch 007 | Batch 070/102 | Current Loss: 0.100782\n[Training] Epoch 007 | Batch 080/102 | Current Loss: 0.100846\n[Training] Epoch 007 | Batch 090/102 | Current Loss: 0.100692\n[Training] Epoch 007 | Batch 100/102 | Current Loss: 0.100619\n\n[Training] Epoch 007 Summary:\n  Avg Loss: 0.100788\n  Current LR: 1.28e-04\n[EarlyStopping] Loss improved (0.100909 → 0.100788). Saving model...\n[Training] Epoch 008 | Batch 000/102 | Current Loss: 0.100842\n[Training] Epoch 008 | Batch 010/102 | Current Loss: 0.100727\n[Training] Epoch 008 | Batch 020/102 | Current Loss: 0.100678\n[Training] Epoch 008 | Batch 030/102 | Current Loss: 0.100724\n[Training] Epoch 008 | Batch 040/102 | Current Loss: 0.100720\n[Training] Epoch 008 | Batch 050/102 | Current Loss: 0.100747\n[Training] Epoch 008 | Batch 060/102 | Current Loss: 0.100732\n[Training] Epoch 008 | Batch 070/102 | Current Loss: 0.100748\n[Training] Epoch 008 | Batch 080/102 | Current Loss: 0.100751\n[Training] Epoch 008 | Batch 090/102 | Current Loss: 0.100796\n[Training] Epoch 008 | Batch 100/102 | Current Loss: 0.100596\n\n[Training] Epoch 008 Summary:\n  Avg Loss: 0.100713\n  Current LR: 5.98e-05\n[EarlyStopping] Loss improved (0.100788 → 0.100713). Saving model...\n[Training] Epoch 009 | Batch 000/102 | Current Loss: 0.100768\n[Training] Epoch 009 | Batch 010/102 | Current Loss: 0.100834\n[Training] Epoch 009 | Batch 020/102 | Current Loss: 0.100695\n[Training] Epoch 009 | Batch 030/102 | Current Loss: 0.100708\n[Training] Epoch 009 | Batch 040/102 | Current Loss: 0.100816\n[Training] Epoch 009 | Batch 050/102 | Current Loss: 0.100557\n[Training] Epoch 009 | Batch 060/102 | Current Loss: 0.100655\n[Training] Epoch 009 | Batch 070/102 | Current Loss: 0.100538\n[Training] Epoch 009 | Batch 080/102 | Current Loss: 0.100627\n[Training] Epoch 009 | Batch 090/102 | Current Loss: 0.100723\n[Training] Epoch 009 | Batch 100/102 | Current Loss: 0.100745\n\n[Training] Epoch 009 Summary:\n  Avg Loss: 0.100672\n  Current LR: 1.61e-05\n[EarlyStopping] Loss improved (0.100713 → 0.100672). Saving model...\n[Training] Epoch 010 | Batch 000/102 | Current Loss: 0.100751\n[Training] Epoch 010 | Batch 010/102 | Current Loss: 0.100654\n[Training] Epoch 010 | Batch 020/102 | Current Loss: 0.100667\n[Training] Epoch 010 | Batch 030/102 | Current Loss: 0.100684\n[Training] Epoch 010 | Batch 040/102 | Current Loss: 0.100622\n[Training] Epoch 010 | Batch 050/102 | Current Loss: 0.100745\n[Training] Epoch 010 | Batch 060/102 | Current Loss: 0.100653\n[Training] Epoch 010 | Batch 070/102 | Current Loss: 0.100657\n[Training] Epoch 010 | Batch 080/102 | Current Loss: 0.100639\n[Training] Epoch 010 | Batch 090/102 | Current Loss: 0.100630\n[Training] Epoch 010 | Batch 100/102 | Current Loss: 0.100740\n\n[Training] Epoch 010 Summary:\n  Avg Loss: 0.100653\n  Current LR: 6.17e-04\n[EarlyStopping] Loss improved (0.100672 → 0.100653). Saving model...\n[Training] Epoch 011 | Batch 000/102 | Current Loss: 0.100648\n[Training] Epoch 011 | Batch 010/102 | Current Loss: 0.100912\n[Training] Epoch 011 | Batch 020/102 | Current Loss: 0.100695\n[Training] Epoch 011 | Batch 030/102 | Current Loss: 0.100670\n[Training] Epoch 011 | Batch 040/102 | Current Loss: 0.101267\n[Training] Epoch 011 | Batch 050/102 | Current Loss: 0.100863\n[Training] Epoch 011 | Batch 060/102 | Current Loss: 0.100991\n[Training] Epoch 011 | Batch 070/102 | Current Loss: 0.101354\n[Training] Epoch 011 | Batch 080/102 | Current Loss: 0.100695\n[Training] Epoch 011 | Batch 090/102 | Current Loss: 0.100704\n[Training] Epoch 011 | Batch 100/102 | Current Loss: 0.100606\n\n[Training] Epoch 011 Summary:\n  Avg Loss: 0.100833\n  Current LR: 6.02e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 012 | Batch 000/102 | Current Loss: 0.100669\n[Training] Epoch 012 | Batch 010/102 | Current Loss: 0.100676\n[Training] Epoch 012 | Batch 020/102 | Current Loss: 0.100577\n[Training] Epoch 012 | Batch 030/102 | Current Loss: 0.100608\n[Training] Epoch 012 | Batch 040/102 | Current Loss: 0.100572\n[Training] Epoch 012 | Batch 050/102 | Current Loss: 0.100745\n[Training] Epoch 012 | Batch 060/102 | Current Loss: 0.100764\n[Training] Epoch 012 | Batch 070/102 | Current Loss: 0.100699\n[Training] Epoch 012 | Batch 080/102 | Current Loss: 0.100878\n[Training] Epoch 012 | Batch 090/102 | Current Loss: 0.100531\n[Training] Epoch 012 | Batch 100/102 | Current Loss: 0.100512\n\n[Training] Epoch 012 Summary:\n  Avg Loss: 0.100650\n  Current LR: 5.58e-04\n[EarlyStopping] Loss improved (0.100653 → 0.100650). Saving model...\n[Training] Epoch 013 | Batch 000/102 | Current Loss: 0.100526\n[Training] Epoch 013 | Batch 010/102 | Current Loss: 0.100717\n[Training] Epoch 013 | Batch 020/102 | Current Loss: 0.100632\n[Training] Epoch 013 | Batch 030/102 | Current Loss: 0.100466\n[Training] Epoch 013 | Batch 040/102 | Current Loss: 0.100534\n[Training] Epoch 013 | Batch 050/102 | Current Loss: 0.100427\n[Training] Epoch 013 | Batch 060/102 | Current Loss: 0.100533\n[Training] Epoch 013 | Batch 070/102 | Current Loss: 0.100437\n[Training] Epoch 013 | Batch 080/102 | Current Loss: 0.100464\n[Training] Epoch 013 | Batch 090/102 | Current Loss: 0.100506\n[Training] Epoch 013 | Batch 100/102 | Current Loss: 0.100585\n\n[Training] Epoch 013 Summary:\n  Avg Loss: 0.100513\n  Current LR: 4.90e-04\n[EarlyStopping] Loss improved (0.100650 → 0.100513). Saving model...\n[Training] Epoch 014 | Batch 000/102 | Current Loss: 0.100414\n[Training] Epoch 014 | Batch 010/102 | Current Loss: 0.100448\n[Training] Epoch 014 | Batch 020/102 | Current Loss: 0.100385\n[Training] Epoch 014 | Batch 030/102 | Current Loss: 0.100391\n[Training] Epoch 014 | Batch 040/102 | Current Loss: 0.100503\n[Training] Epoch 014 | Batch 050/102 | Current Loss: 0.100402\n[Training] Epoch 014 | Batch 060/102 | Current Loss: 0.100334\n[Training] Epoch 014 | Batch 070/102 | Current Loss: 0.100438\n[Training] Epoch 014 | Batch 080/102 | Current Loss: 0.100322\n[Training] Epoch 014 | Batch 090/102 | Current Loss: 0.100335\n[Training] Epoch 014 | Batch 100/102 | Current Loss: 0.100364\n\n[Training] Epoch 014 Summary:\n  Avg Loss: 0.100398\n  Current LR: 4.04e-04\n[EarlyStopping] Loss improved (0.100513 → 0.100398). Saving model...\n[Training] Epoch 015 | Batch 000/102 | Current Loss: 0.100292\n[Training] Epoch 015 | Batch 010/102 | Current Loss: 0.100437\n[Training] Epoch 015 | Batch 020/102 | Current Loss: 0.100364\n[Training] Epoch 015 | Batch 030/102 | Current Loss: 0.100406\n[Training] Epoch 015 | Batch 040/102 | Current Loss: 0.100440\n[Training] Epoch 015 | Batch 050/102 | Current Loss: 0.100400\n[Training] Epoch 015 | Batch 060/102 | Current Loss: 0.100417\n[Training] Epoch 015 | Batch 070/102 | Current Loss: 0.100340\n[Training] Epoch 015 | Batch 080/102 | Current Loss: 0.100389\n[Training] Epoch 015 | Batch 090/102 | Current Loss: 0.100365\n[Training] Epoch 015 | Batch 100/102 | Current Loss: 0.100351\n\n[Training] Epoch 015 Summary:\n  Avg Loss: 0.100369\n  Current LR: 3.09e-04\n[EarlyStopping] Loss improved (0.100398 → 0.100369). Saving model...\n[Training] Epoch 016 | Batch 000/102 | Current Loss: 0.100361\n[Training] Epoch 016 | Batch 010/102 | Current Loss: 0.100422\n[Training] Epoch 016 | Batch 020/102 | Current Loss: 0.100373\n[Training] Epoch 016 | Batch 030/102 | Current Loss: 0.100406\n[Training] Epoch 016 | Batch 040/102 | Current Loss: 0.100280\n[Training] Epoch 016 | Batch 050/102 | Current Loss: 0.100245\n[Training] Epoch 016 | Batch 060/102 | Current Loss: 0.100323\n[Training] Epoch 016 | Batch 070/102 | Current Loss: 0.100538\n[Training] Epoch 016 | Batch 080/102 | Current Loss: 0.100286\n[Training] Epoch 016 | Batch 090/102 | Current Loss: 0.100375\n[Training] Epoch 016 | Batch 100/102 | Current Loss: 0.100369\n\n[Training] Epoch 016 Summary:\n  Avg Loss: 0.100348\n  Current LR: 2.14e-04\n[EarlyStopping] Loss improved (0.100369 → 0.100348). Saving model...\n[Training] Epoch 017 | Batch 000/102 | Current Loss: 0.100286\n[Training] Epoch 017 | Batch 010/102 | Current Loss: 0.100308\n[Training] Epoch 017 | Batch 020/102 | Current Loss: 0.100345\n[Training] Epoch 017 | Batch 030/102 | Current Loss: 0.100380\n[Training] Epoch 017 | Batch 040/102 | Current Loss: 0.100316\n[Training] Epoch 017 | Batch 050/102 | Current Loss: 0.100311\n[Training] Epoch 017 | Batch 060/102 | Current Loss: 0.100320\n[Training] Epoch 017 | Batch 070/102 | Current Loss: 0.100246\n[Training] Epoch 017 | Batch 080/102 | Current Loss: 0.100323\n[Training] Epoch 017 | Batch 090/102 | Current Loss: 0.100381\n[Training] Epoch 017 | Batch 100/102 | Current Loss: 0.100313\n\n[Training] Epoch 017 Summary:\n  Avg Loss: 0.100324\n  Current LR: 1.28e-04\n[EarlyStopping] Loss improved (0.100348 → 0.100324). Saving model...\n[Training] Epoch 018 | Batch 000/102 | Current Loss: 0.100289\n[Training] Epoch 018 | Batch 010/102 | Current Loss: 0.100306\n[Training] Epoch 018 | Batch 020/102 | Current Loss: 0.100279\n[Training] Epoch 018 | Batch 030/102 | Current Loss: 0.100315\n[Training] Epoch 018 | Batch 040/102 | Current Loss: 0.100377\n[Training] Epoch 018 | Batch 050/102 | Current Loss: 0.100315\n[Training] Epoch 018 | Batch 060/102 | Current Loss: 0.100281\n[Training] Epoch 018 | Batch 070/102 | Current Loss: 0.100276\n[Training] Epoch 018 | Batch 080/102 | Current Loss: 0.100357\n[Training] Epoch 018 | Batch 090/102 | Current Loss: 0.100351\n[Training] Epoch 018 | Batch 100/102 | Current Loss: 0.100299\n\n[Training] Epoch 018 Summary:\n  Avg Loss: 0.100306\n  Current LR: 5.98e-05\n[EarlyStopping] Loss improved (0.100324 → 0.100306). Saving model...\n[Training] Epoch 019 | Batch 000/102 | Current Loss: 0.100328\n[Training] Epoch 019 | Batch 010/102 | Current Loss: 0.100354\n[Training] Epoch 019 | Batch 020/102 | Current Loss: 0.100220\n[Training] Epoch 019 | Batch 030/102 | Current Loss: 0.100322\n[Training] Epoch 019 | Batch 040/102 | Current Loss: 0.100312\n[Training] Epoch 019 | Batch 050/102 | Current Loss: 0.100365\n[Training] Epoch 019 | Batch 060/102 | Current Loss: 0.100297\n[Training] Epoch 019 | Batch 070/102 | Current Loss: 0.100283\n[Training] Epoch 019 | Batch 080/102 | Current Loss: 0.100240\n[Training] Epoch 019 | Batch 090/102 | Current Loss: 0.100315\n[Training] Epoch 019 | Batch 100/102 | Current Loss: 0.100286\n\n[Training] Epoch 019 Summary:\n  Avg Loss: 0.100295\n  Current LR: 1.61e-05\n[EarlyStopping] Loss improved (0.100306 → 0.100295). Saving model...\n[Training] Epoch 020 | Batch 000/102 | Current Loss: 0.100402\n[Training] Epoch 020 | Batch 010/102 | Current Loss: 0.100264\n[Training] Epoch 020 | Batch 020/102 | Current Loss: 0.100334\n[Training] Epoch 020 | Batch 030/102 | Current Loss: 0.100349\n[Training] Epoch 020 | Batch 040/102 | Current Loss: 0.100428\n[Training] Epoch 020 | Batch 050/102 | Current Loss: 0.100289\n[Training] Epoch 020 | Batch 060/102 | Current Loss: 0.100231\n[Training] Epoch 020 | Batch 070/102 | Current Loss: 0.100324\n[Training] Epoch 020 | Batch 080/102 | Current Loss: 0.100250\n[Training] Epoch 020 | Batch 090/102 | Current Loss: 0.100263\n[Training] Epoch 020 | Batch 100/102 | Current Loss: 0.100269\n\n[Training] Epoch 020 Summary:\n  Avg Loss: 0.100296\n  Current LR: 6.17e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 021 | Batch 000/102 | Current Loss: 0.100295\n[Training] Epoch 021 | Batch 010/102 | Current Loss: 0.100314\n[Training] Epoch 021 | Batch 020/102 | Current Loss: 0.100335\n[Training] Epoch 021 | Batch 030/102 | Current Loss: 0.100401\n[Training] Epoch 021 | Batch 040/102 | Current Loss: 0.100418\n[Training] Epoch 021 | Batch 050/102 | Current Loss: 0.100315\n[Training] Epoch 021 | Batch 060/102 | Current Loss: 0.100434\n[Training] Epoch 021 | Batch 070/102 | Current Loss: 0.100338\n[Training] Epoch 021 | Batch 080/102 | Current Loss: 0.100334\n[Training] Epoch 021 | Batch 090/102 | Current Loss: 0.100246\n[Training] Epoch 021 | Batch 100/102 | Current Loss: 0.100262\n\n[Training] Epoch 021 Summary:\n  Avg Loss: 0.100348\n  Current LR: 6.02e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 022 | Batch 000/102 | Current Loss: 0.100293\n[Training] Epoch 022 | Batch 010/102 | Current Loss: 0.100220\n[Training] Epoch 022 | Batch 020/102 | Current Loss: 0.100384\n[Training] Epoch 022 | Batch 030/102 | Current Loss: 0.100372\n[Training] Epoch 022 | Batch 040/102 | Current Loss: 0.100240\n[Training] Epoch 022 | Batch 050/102 | Current Loss: 0.100263\n[Training] Epoch 022 | Batch 060/102 | Current Loss: 0.100304\n[Training] Epoch 022 | Batch 070/102 | Current Loss: 0.100267\n[Training] Epoch 022 | Batch 080/102 | Current Loss: 0.100316\n[Training] Epoch 022 | Batch 090/102 | Current Loss: 0.100253\n[Training] Epoch 022 | Batch 100/102 | Current Loss: 0.100304\n\n[Training] Epoch 022 Summary:\n  Avg Loss: 0.100297\n  Current LR: 5.58e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 023 | Batch 000/102 | Current Loss: 0.100283\n[Training] Epoch 023 | Batch 010/102 | Current Loss: 0.100235\n[Training] Epoch 023 | Batch 020/102 | Current Loss: 0.100221\n[Training] Epoch 023 | Batch 030/102 | Current Loss: 0.100283\n[Training] Epoch 023 | Batch 040/102 | Current Loss: 0.100282\n[Training] Epoch 023 | Batch 050/102 | Current Loss: 0.100325\n[Training] Epoch 023 | Batch 060/102 | Current Loss: 0.100284\n[Training] Epoch 023 | Batch 070/102 | Current Loss: 0.100328\n[Training] Epoch 023 | Batch 080/102 | Current Loss: 0.100327\n[Training] Epoch 023 | Batch 090/102 | Current Loss: 0.100279\n[Training] Epoch 023 | Batch 100/102 | Current Loss: 0.100257\n\n[Training] Epoch 023 Summary:\n  Avg Loss: 0.100289\n  Current LR: 4.90e-04\n[EarlyStopping] Loss improved (0.100295 → 0.100289). Saving model...\n[Training] Epoch 024 | Batch 000/102 | Current Loss: 0.100420\n[Training] Epoch 024 | Batch 010/102 | Current Loss: 0.100350\n[Training] Epoch 024 | Batch 020/102 | Current Loss: 0.100250\n[Training] Epoch 024 | Batch 030/102 | Current Loss: 0.100325\n[Training] Epoch 024 | Batch 040/102 | Current Loss: 0.100310\n[Training] Epoch 024 | Batch 050/102 | Current Loss: 0.100214\n[Training] Epoch 024 | Batch 060/102 | Current Loss: 0.100266\n[Training] Epoch 024 | Batch 070/102 | Current Loss: 0.100253\n[Training] Epoch 024 | Batch 080/102 | Current Loss: 0.100232\n[Training] Epoch 024 | Batch 090/102 | Current Loss: 0.100275\n[Training] Epoch 024 | Batch 100/102 | Current Loss: 0.100261\n\n[Training] Epoch 024 Summary:\n  Avg Loss: 0.100261\n  Current LR: 4.04e-04\n[EarlyStopping] Loss improved (0.100289 → 0.100261). Saving model...\n[Training] Epoch 025 | Batch 000/102 | Current Loss: 0.100205\n[Training] Epoch 025 | Batch 010/102 | Current Loss: 0.100174\n[Training] Epoch 025 | Batch 020/102 | Current Loss: 0.100229\n[Training] Epoch 025 | Batch 030/102 | Current Loss: 0.100276\n[Training] Epoch 025 | Batch 040/102 | Current Loss: 0.100229\n[Training] Epoch 025 | Batch 050/102 | Current Loss: 0.100240\n[Training] Epoch 025 | Batch 060/102 | Current Loss: 0.100281\n[Training] Epoch 025 | Batch 070/102 | Current Loss: 0.100257\n[Training] Epoch 025 | Batch 080/102 | Current Loss: 0.100226\n[Training] Epoch 025 | Batch 090/102 | Current Loss: 0.100283\n[Training] Epoch 025 | Batch 100/102 | Current Loss: 0.100275\n\n[Training] Epoch 025 Summary:\n  Avg Loss: 0.100237\n  Current LR: 3.09e-04\n[EarlyStopping] Loss improved (0.100261 → 0.100237). Saving model...\n[Training] Epoch 026 | Batch 000/102 | Current Loss: 0.100191\n[Training] Epoch 026 | Batch 010/102 | Current Loss: 0.100223\n[Training] Epoch 026 | Batch 020/102 | Current Loss: 0.100253\n[Training] Epoch 026 | Batch 030/102 | Current Loss: 0.100187\n[Training] Epoch 026 | Batch 040/102 | Current Loss: 0.100234\n[Training] Epoch 026 | Batch 050/102 | Current Loss: 0.100172\n[Training] Epoch 026 | Batch 060/102 | Current Loss: 0.100235\n[Training] Epoch 026 | Batch 070/102 | Current Loss: 0.100337\n[Training] Epoch 026 | Batch 080/102 | Current Loss: 0.100261\n[Training] Epoch 026 | Batch 090/102 | Current Loss: 0.100219\n[Training] Epoch 026 | Batch 100/102 | Current Loss: 0.100266\n\n[Training] Epoch 026 Summary:\n  Avg Loss: 0.100224\n  Current LR: 2.14e-04\n[EarlyStopping] Loss improved (0.100237 → 0.100224). Saving model...\n[Training] Epoch 027 | Batch 000/102 | Current Loss: 0.100248\n[Training] Epoch 027 | Batch 010/102 | Current Loss: 0.100209\n[Training] Epoch 027 | Batch 020/102 | Current Loss: 0.100137\n[Training] Epoch 027 | Batch 030/102 | Current Loss: 0.100279\n[Training] Epoch 027 | Batch 040/102 | Current Loss: 0.100237\n[Training] Epoch 027 | Batch 050/102 | Current Loss: 0.100192\n[Training] Epoch 027 | Batch 060/102 | Current Loss: 0.100191\n[Training] Epoch 027 | Batch 070/102 | Current Loss: 0.100252\n[Training] Epoch 027 | Batch 080/102 | Current Loss: 0.100139\n[Training] Epoch 027 | Batch 090/102 | Current Loss: 0.100275\n[Training] Epoch 027 | Batch 100/102 | Current Loss: 0.100248\n\n[Training] Epoch 027 Summary:\n  Avg Loss: 0.100221\n  Current LR: 1.28e-04\n[EarlyStopping] Loss improved (0.100224 → 0.100221). Saving model...\n[Training] Epoch 028 | Batch 000/102 | Current Loss: 0.100225\n[Training] Epoch 028 | Batch 010/102 | Current Loss: 0.100134\n[Training] Epoch 028 | Batch 020/102 | Current Loss: 0.100226\n[Training] Epoch 028 | Batch 030/102 | Current Loss: 0.100212\n[Training] Epoch 028 | Batch 040/102 | Current Loss: 0.100198\n[Training] Epoch 028 | Batch 050/102 | Current Loss: 0.100232\n[Training] Epoch 028 | Batch 060/102 | Current Loss: 0.100219\n[Training] Epoch 028 | Batch 070/102 | Current Loss: 0.100256\n[Training] Epoch 028 | Batch 080/102 | Current Loss: 0.100188\n[Training] Epoch 028 | Batch 090/102 | Current Loss: 0.100158\n[Training] Epoch 028 | Batch 100/102 | Current Loss: 0.100193\n\n[Training] Epoch 028 Summary:\n  Avg Loss: 0.100207\n  Current LR: 5.98e-05\n[EarlyStopping] Loss improved (0.100221 → 0.100207). Saving model...\n[Training] Epoch 029 | Batch 000/102 | Current Loss: 0.100183\n[Training] Epoch 029 | Batch 010/102 | Current Loss: 0.100184\n[Training] Epoch 029 | Batch 020/102 | Current Loss: 0.100154\n[Training] Epoch 029 | Batch 030/102 | Current Loss: 0.100162\n[Training] Epoch 029 | Batch 040/102 | Current Loss: 0.100167\n[Training] Epoch 029 | Batch 050/102 | Current Loss: 0.100217\n[Training] Epoch 029 | Batch 060/102 | Current Loss: 0.100184\n[Training] Epoch 029 | Batch 070/102 | Current Loss: 0.100237\n[Training] Epoch 029 | Batch 080/102 | Current Loss: 0.100167\n[Training] Epoch 029 | Batch 090/102 | Current Loss: 0.100185\n[Training] Epoch 029 | Batch 100/102 | Current Loss: 0.100180\n\n[Training] Epoch 029 Summary:\n  Avg Loss: 0.100197\n  Current LR: 1.61e-05\n[EarlyStopping] Loss improved (0.100207 → 0.100197). Saving model...\n[Training] Epoch 030 | Batch 000/102 | Current Loss: 0.100198\n[Training] Epoch 030 | Batch 010/102 | Current Loss: 0.100196\n[Training] Epoch 030 | Batch 020/102 | Current Loss: 0.100192\n[Training] Epoch 030 | Batch 030/102 | Current Loss: 0.100214\n[Training] Epoch 030 | Batch 040/102 | Current Loss: 0.100178\n[Training] Epoch 030 | Batch 050/102 | Current Loss: 0.100180\n[Training] Epoch 030 | Batch 060/102 | Current Loss: 0.100190\n[Training] Epoch 030 | Batch 070/102 | Current Loss: 0.100195\n[Training] Epoch 030 | Batch 080/102 | Current Loss: 0.100295\n[Training] Epoch 030 | Batch 090/102 | Current Loss: 0.100167\n[Training] Epoch 030 | Batch 100/102 | Current Loss: 0.100114\n\n[Training] Epoch 030 Summary:\n  Avg Loss: 0.100192\n  Current LR: 6.17e-04\n[EarlyStopping] Loss improved (0.100197 → 0.100192). Saving model...\n[Training] Epoch 031 | Batch 000/102 | Current Loss: 0.100144\n[Training] Epoch 031 | Batch 010/102 | Current Loss: 0.100875\n[Training] Epoch 031 | Batch 020/102 | Current Loss: 0.104507\n[Training] Epoch 031 | Batch 030/102 | Current Loss: 0.100987\n[Training] Epoch 031 | Batch 040/102 | Current Loss: 0.100415\n[Training] Epoch 031 | Batch 050/102 | Current Loss: 0.100451\n[Training] Epoch 031 | Batch 060/102 | Current Loss: 0.100342\n[Training] Epoch 031 | Batch 070/102 | Current Loss: 0.100272\n[Training] Epoch 031 | Batch 080/102 | Current Loss: 0.100213\n[Training] Epoch 031 | Batch 090/102 | Current Loss: 0.100327\n[Training] Epoch 031 | Batch 100/102 | Current Loss: 0.100245\n\n[Training] Epoch 031 Summary:\n  Avg Loss: 0.100664\n  Current LR: 6.02e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 032 | Batch 000/102 | Current Loss: 0.100257\n[Training] Epoch 032 | Batch 010/102 | Current Loss: 0.100167\n[Training] Epoch 032 | Batch 020/102 | Current Loss: 0.100258\n[Training] Epoch 032 | Batch 030/102 | Current Loss: 0.100225\n[Training] Epoch 032 | Batch 040/102 | Current Loss: 0.100225\n[Training] Epoch 032 | Batch 050/102 | Current Loss: 0.100283\n[Training] Epoch 032 | Batch 060/102 | Current Loss: 0.100129\n[Training] Epoch 032 | Batch 070/102 | Current Loss: 0.100166\n[Training] Epoch 032 | Batch 080/102 | Current Loss: 0.100191\n[Training] Epoch 032 | Batch 090/102 | Current Loss: 0.100299\n[Training] Epoch 032 | Batch 100/102 | Current Loss: 0.100194\n\n[Training] Epoch 032 Summary:\n  Avg Loss: 0.100226\n  Current LR: 5.58e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 033 | Batch 000/102 | Current Loss: 0.100212\n[Training] Epoch 033 | Batch 010/102 | Current Loss: 0.100190\n[Training] Epoch 033 | Batch 020/102 | Current Loss: 0.100142\n[Training] Epoch 033 | Batch 030/102 | Current Loss: 0.100151\n[Training] Epoch 033 | Batch 040/102 | Current Loss: 0.100214\n[Training] Epoch 033 | Batch 050/102 | Current Loss: 0.100236\n[Training] Epoch 033 | Batch 060/102 | Current Loss: 0.100183\n[Training] Epoch 033 | Batch 070/102 | Current Loss: 0.100510\n[Training] Epoch 033 | Batch 080/102 | Current Loss: 0.100454\n[Training] Epoch 033 | Batch 090/102 | Current Loss: 0.100625\n[Training] Epoch 033 | Batch 100/102 | Current Loss: 0.100540\n\n[Training] Epoch 033 Summary:\n  Avg Loss: 0.100310\n  Current LR: 4.90e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 034 | Batch 000/102 | Current Loss: 0.100384\n[Training] Epoch 034 | Batch 010/102 | Current Loss: 0.100339\n[Training] Epoch 034 | Batch 020/102 | Current Loss: 0.100181\n[Training] Epoch 034 | Batch 030/102 | Current Loss: 0.100233\n[Training] Epoch 034 | Batch 040/102 | Current Loss: 0.100187\n[Training] Epoch 034 | Batch 050/102 | Current Loss: 0.100244\n[Training] Epoch 034 | Batch 060/102 | Current Loss: 0.100173\n[Training] Epoch 034 | Batch 070/102 | Current Loss: 0.100201\n[Training] Epoch 034 | Batch 080/102 | Current Loss: 0.100283\n[Training] Epoch 034 | Batch 090/102 | Current Loss: 0.100151\n[Training] Epoch 034 | Batch 100/102 | Current Loss: 0.100115\n\n[Training] Epoch 034 Summary:\n  Avg Loss: 0.100225\n  Current LR: 4.04e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 035 | Batch 000/102 | Current Loss: 0.100150\n[Training] Epoch 035 | Batch 010/102 | Current Loss: 0.100225\n[Training] Epoch 035 | Batch 020/102 | Current Loss: 0.100303\n[Training] Epoch 035 | Batch 030/102 | Current Loss: 0.100192\n[Training] Epoch 035 | Batch 040/102 | Current Loss: 0.100209\n[Training] Epoch 035 | Batch 050/102 | Current Loss: 0.100135\n[Training] Epoch 035 | Batch 060/102 | Current Loss: 0.100231\n[Training] Epoch 035 | Batch 070/102 | Current Loss: 0.100187\n[Training] Epoch 035 | Batch 080/102 | Current Loss: 0.100169\n[Training] Epoch 035 | Batch 090/102 | Current Loss: 0.100114\n[Training] Epoch 035 | Batch 100/102 | Current Loss: 0.100193\n\n[Training] Epoch 035 Summary:\n  Avg Loss: 0.100181\n  Current LR: 3.09e-04\n[EarlyStopping] Loss improved (0.100192 → 0.100181). Saving model...\n[Training] Epoch 036 | Batch 000/102 | Current Loss: 0.100121\n[Training] Epoch 036 | Batch 010/102 | Current Loss: 0.100206\n[Training] Epoch 036 | Batch 020/102 | Current Loss: 0.100150\n[Training] Epoch 036 | Batch 030/102 | Current Loss: 0.100198\n[Training] Epoch 036 | Batch 040/102 | Current Loss: 0.100106\n[Training] Epoch 036 | Batch 050/102 | Current Loss: 0.100172\n[Training] Epoch 036 | Batch 060/102 | Current Loss: 0.100118\n[Training] Epoch 036 | Batch 070/102 | Current Loss: 0.100140\n[Training] Epoch 036 | Batch 080/102 | Current Loss: 0.100205\n[Training] Epoch 036 | Batch 090/102 | Current Loss: 0.100167\n[Training] Epoch 036 | Batch 100/102 | Current Loss: 0.100125\n\n[Training] Epoch 036 Summary:\n  Avg Loss: 0.100171\n  Current LR: 2.14e-04\n[EarlyStopping] Loss improved (0.100181 → 0.100171). Saving model...\n[Training] Epoch 037 | Batch 000/102 | Current Loss: 0.100193\n[Training] Epoch 037 | Batch 010/102 | Current Loss: 0.100172\n[Training] Epoch 037 | Batch 020/102 | Current Loss: 0.100164\n[Training] Epoch 037 | Batch 030/102 | Current Loss: 0.100199\n[Training] Epoch 037 | Batch 040/102 | Current Loss: 0.100144\n[Training] Epoch 037 | Batch 050/102 | Current Loss: 0.100157\n[Training] Epoch 037 | Batch 060/102 | Current Loss: 0.100152\n[Training] Epoch 037 | Batch 070/102 | Current Loss: 0.100094\n[Training] Epoch 037 | Batch 080/102 | Current Loss: 0.100063\n[Training] Epoch 037 | Batch 090/102 | Current Loss: 0.100254\n[Training] Epoch 037 | Batch 100/102 | Current Loss: 0.100202\n\n[Training] Epoch 037 Summary:\n  Avg Loss: 0.100162\n  Current LR: 1.28e-04\n[EarlyStopping] Loss improved (0.100171 → 0.100162). Saving model...\n[Training] Epoch 038 | Batch 000/102 | Current Loss: 0.100221\n[Training] Epoch 038 | Batch 010/102 | Current Loss: 0.100164\n[Training] Epoch 038 | Batch 020/102 | Current Loss: 0.100121\n[Training] Epoch 038 | Batch 030/102 | Current Loss: 0.100187\n[Training] Epoch 038 | Batch 040/102 | Current Loss: 0.100140\n[Training] Epoch 038 | Batch 050/102 | Current Loss: 0.100160\n[Training] Epoch 038 | Batch 060/102 | Current Loss: 0.100180\n[Training] Epoch 038 | Batch 070/102 | Current Loss: 0.100134\n[Training] Epoch 038 | Batch 080/102 | Current Loss: 0.100167\n[Training] Epoch 038 | Batch 090/102 | Current Loss: 0.100107\n[Training] Epoch 038 | Batch 100/102 | Current Loss: 0.100183\n\n[Training] Epoch 038 Summary:\n  Avg Loss: 0.100155\n  Current LR: 5.98e-05\n[EarlyStopping] Loss improved (0.100162 → 0.100155). Saving model...\n[Training] Epoch 039 | Batch 000/102 | Current Loss: 0.100242\n[Training] Epoch 039 | Batch 010/102 | Current Loss: 0.100140\n[Training] Epoch 039 | Batch 020/102 | Current Loss: 0.100194\n[Training] Epoch 039 | Batch 030/102 | Current Loss: 0.100232\n[Training] Epoch 039 | Batch 040/102 | Current Loss: 0.100183\n[Training] Epoch 039 | Batch 050/102 | Current Loss: 0.100209\n[Training] Epoch 039 | Batch 060/102 | Current Loss: 0.100132\n[Training] Epoch 039 | Batch 070/102 | Current Loss: 0.100075\n[Training] Epoch 039 | Batch 080/102 | Current Loss: 0.100135\n[Training] Epoch 039 | Batch 090/102 | Current Loss: 0.100090\n[Training] Epoch 039 | Batch 100/102 | Current Loss: 0.100158\n\n[Training] Epoch 039 Summary:\n  Avg Loss: 0.100151\n  Current LR: 1.61e-05\n[EarlyStopping] Loss improved (0.100155 → 0.100151). Saving model...\n[Training] Epoch 040 | Batch 000/102 | Current Loss: 0.100134\n[Training] Epoch 040 | Batch 010/102 | Current Loss: 0.100173\n[Training] Epoch 040 | Batch 020/102 | Current Loss: 0.100164\n[Training] Epoch 040 | Batch 030/102 | Current Loss: 0.100172\n[Training] Epoch 040 | Batch 040/102 | Current Loss: 0.100321\n[Training] Epoch 040 | Batch 050/102 | Current Loss: 0.100176\n[Training] Epoch 040 | Batch 060/102 | Current Loss: 0.100123\n[Training] Epoch 040 | Batch 070/102 | Current Loss: 0.100174\n[Training] Epoch 040 | Batch 080/102 | Current Loss: 0.100161\n[Training] Epoch 040 | Batch 090/102 | Current Loss: 0.100152\n[Training] Epoch 040 | Batch 100/102 | Current Loss: 0.100150\n\n[Training] Epoch 040 Summary:\n  Avg Loss: 0.100150\n  Current LR: 6.17e-04\n[EarlyStopping] Loss improved (0.100151 → 0.100150). Saving model...\n[Training] Epoch 041 | Batch 000/102 | Current Loss: 0.100200\n[Training] Epoch 041 | Batch 010/102 | Current Loss: 0.100188\n[Training] Epoch 041 | Batch 020/102 | Current Loss: 0.100243\n[Training] Epoch 041 | Batch 030/102 | Current Loss: 0.100300\n[Training] Epoch 041 | Batch 040/102 | Current Loss: 0.100261\n[Training] Epoch 041 | Batch 050/102 | Current Loss: 0.100376\n[Training] Epoch 041 | Batch 060/102 | Current Loss: 0.100702\n[Training] Epoch 041 | Batch 070/102 | Current Loss: 0.100504\n[Training] Epoch 041 | Batch 080/102 | Current Loss: 0.100215\n[Training] Epoch 041 | Batch 090/102 | Current Loss: 0.101241\n[Training] Epoch 041 | Batch 100/102 | Current Loss: 0.100256\n\n[Training] Epoch 041 Summary:\n  Avg Loss: 0.100401\n  Current LR: 6.02e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 042 | Batch 000/102 | Current Loss: 0.100516\n[Training] Epoch 042 | Batch 010/102 | Current Loss: 0.100637\n[Training] Epoch 042 | Batch 020/102 | Current Loss: 0.100329\n[Training] Epoch 042 | Batch 030/102 | Current Loss: 0.100168\n[Training] Epoch 042 | Batch 040/102 | Current Loss: 0.100162\n[Training] Epoch 042 | Batch 050/102 | Current Loss: 0.100157\n[Training] Epoch 042 | Batch 060/102 | Current Loss: 0.100203\n[Training] Epoch 042 | Batch 070/102 | Current Loss: 0.100163\n[Training] Epoch 042 | Batch 080/102 | Current Loss: 0.100205\n[Training] Epoch 042 | Batch 090/102 | Current Loss: 0.100242\n[Training] Epoch 042 | Batch 100/102 | Current Loss: 0.100228\n\n[Training] Epoch 042 Summary:\n  Avg Loss: 0.100245\n  Current LR: 5.58e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 043 | Batch 000/102 | Current Loss: 0.100210\n[Training] Epoch 043 | Batch 010/102 | Current Loss: 0.100167\n[Training] Epoch 043 | Batch 020/102 | Current Loss: 0.100138\n[Training] Epoch 043 | Batch 030/102 | Current Loss: 0.100209\n[Training] Epoch 043 | Batch 040/102 | Current Loss: 0.100164\n[Training] Epoch 043 | Batch 050/102 | Current Loss: 0.100185\n[Training] Epoch 043 | Batch 060/102 | Current Loss: 0.100232\n[Training] Epoch 043 | Batch 070/102 | Current Loss: 0.100185\n[Training] Epoch 043 | Batch 080/102 | Current Loss: 0.100132\n[Training] Epoch 043 | Batch 090/102 | Current Loss: 0.100145\n[Training] Epoch 043 | Batch 100/102 | Current Loss: 0.100183\n\n[Training] Epoch 043 Summary:\n  Avg Loss: 0.100173\n  Current LR: 4.90e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 044 | Batch 000/102 | Current Loss: 0.100186\n[Training] Epoch 044 | Batch 010/102 | Current Loss: 0.100139\n[Training] Epoch 044 | Batch 020/102 | Current Loss: 0.100149\n[Training] Epoch 044 | Batch 030/102 | Current Loss: 0.100133\n[Training] Epoch 044 | Batch 040/102 | Current Loss: 0.100143\n[Training] Epoch 044 | Batch 050/102 | Current Loss: 0.100079\n[Training] Epoch 044 | Batch 060/102 | Current Loss: 0.100075\n[Training] Epoch 044 | Batch 070/102 | Current Loss: 0.100160\n[Training] Epoch 044 | Batch 080/102 | Current Loss: 0.100164\n[Training] Epoch 044 | Batch 090/102 | Current Loss: 0.100164\n[Training] Epoch 044 | Batch 100/102 | Current Loss: 0.100160\n\n[Training] Epoch 044 Summary:\n  Avg Loss: 0.100163\n  Current LR: 4.04e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 045 | Batch 000/102 | Current Loss: 0.100207\n[Training] Epoch 045 | Batch 010/102 | Current Loss: 0.100184\n[Training] Epoch 045 | Batch 020/102 | Current Loss: 0.100174\n[Training] Epoch 045 | Batch 030/102 | Current Loss: 0.100115\n[Training] Epoch 045 | Batch 040/102 | Current Loss: 0.100170\n[Training] Epoch 045 | Batch 050/102 | Current Loss: 0.100133\n[Training] Epoch 045 | Batch 060/102 | Current Loss: 0.100188\n[Training] Epoch 045 | Batch 070/102 | Current Loss: 0.100155\n[Training] Epoch 045 | Batch 080/102 | Current Loss: 0.100081\n[Training] Epoch 045 | Batch 090/102 | Current Loss: 0.100097\n[Training] Epoch 045 | Batch 100/102 | Current Loss: 0.100074\n\n[Training] Epoch 045 Summary:\n  Avg Loss: 0.100146\n  Current LR: 3.09e-04\n[EarlyStopping] Loss improved (0.100150 → 0.100146). Saving model...\n[Training] Epoch 046 | Batch 000/102 | Current Loss: 0.100229\n[Training] Epoch 046 | Batch 010/102 | Current Loss: 0.100103\n[Training] Epoch 046 | Batch 020/102 | Current Loss: 0.100205\n[Training] Epoch 046 | Batch 030/102 | Current Loss: 0.100173\n[Training] Epoch 046 | Batch 040/102 | Current Loss: 0.100169\n[Training] Epoch 046 | Batch 050/102 | Current Loss: 0.100113\n[Training] Epoch 046 | Batch 060/102 | Current Loss: 0.100078\n[Training] Epoch 046 | Batch 070/102 | Current Loss: 0.100109\n[Training] Epoch 046 | Batch 080/102 | Current Loss: 0.100134\n[Training] Epoch 046 | Batch 090/102 | Current Loss: 0.100153\n[Training] Epoch 046 | Batch 100/102 | Current Loss: 0.100161\n\n[Training] Epoch 046 Summary:\n  Avg Loss: 0.100145\n  Current LR: 2.14e-04\n[EarlyStopping] Loss improved (0.100146 → 0.100145). Saving model...\n[Training] Epoch 047 | Batch 000/102 | Current Loss: 0.100149\n[Training] Epoch 047 | Batch 010/102 | Current Loss: 0.100172\n[Training] Epoch 047 | Batch 020/102 | Current Loss: 0.100146\n[Training] Epoch 047 | Batch 030/102 | Current Loss: 0.100127\n[Training] Epoch 047 | Batch 040/102 | Current Loss: 0.100182\n[Training] Epoch 047 | Batch 050/102 | Current Loss: 0.100124\n[Training] Epoch 047 | Batch 060/102 | Current Loss: 0.100120\n[Training] Epoch 047 | Batch 070/102 | Current Loss: 0.100157\n[Training] Epoch 047 | Batch 080/102 | Current Loss: 0.100123\n[Training] Epoch 047 | Batch 090/102 | Current Loss: 0.100123\n[Training] Epoch 047 | Batch 100/102 | Current Loss: 0.100088\n\n[Training] Epoch 047 Summary:\n  Avg Loss: 0.100137\n  Current LR: 1.28e-04\n[EarlyStopping] Loss improved (0.100145 → 0.100137). Saving model...\n[Training] Epoch 048 | Batch 000/102 | Current Loss: 0.100079\n[Training] Epoch 048 | Batch 010/102 | Current Loss: 0.100059\n[Training] Epoch 048 | Batch 020/102 | Current Loss: 0.100192\n[Training] Epoch 048 | Batch 030/102 | Current Loss: 0.100109\n[Training] Epoch 048 | Batch 040/102 | Current Loss: 0.100096\n[Training] Epoch 048 | Batch 050/102 | Current Loss: 0.100124\n[Training] Epoch 048 | Batch 060/102 | Current Loss: 0.100035\n[Training] Epoch 048 | Batch 070/102 | Current Loss: 0.100138\n[Training] Epoch 048 | Batch 080/102 | Current Loss: 0.100120\n[Training] Epoch 048 | Batch 090/102 | Current Loss: 0.100103\n[Training] Epoch 048 | Batch 100/102 | Current Loss: 0.100171\n\n[Training] Epoch 048 Summary:\n  Avg Loss: 0.100125\n  Current LR: 5.98e-05\n[EarlyStopping] Loss improved (0.100137 → 0.100125). Saving model...\n[Training] Epoch 049 | Batch 000/102 | Current Loss: 0.100166\n[Training] Epoch 049 | Batch 010/102 | Current Loss: 0.100149\n[Training] Epoch 049 | Batch 020/102 | Current Loss: 0.100102\n[Training] Epoch 049 | Batch 030/102 | Current Loss: 0.100074\n[Training] Epoch 049 | Batch 040/102 | Current Loss: 0.100111\n[Training] Epoch 049 | Batch 050/102 | Current Loss: 0.100092\n[Training] Epoch 049 | Batch 060/102 | Current Loss: 0.100198\n[Training] Epoch 049 | Batch 070/102 | Current Loss: 0.100173\n[Training] Epoch 049 | Batch 080/102 | Current Loss: 0.100013\n[Training] Epoch 049 | Batch 090/102 | Current Loss: 0.100111\n[Training] Epoch 049 | Batch 100/102 | Current Loss: 0.100131\n\n[Training] Epoch 049 Summary:\n  Avg Loss: 0.100118\n  Current LR: 1.61e-05\n[EarlyStopping] Loss improved (0.100125 → 0.100118). Saving model...\n[Training] Epoch 050 | Batch 000/102 | Current Loss: 0.100129\n[Training] Epoch 050 | Batch 010/102 | Current Loss: 0.100086\n[Training] Epoch 050 | Batch 020/102 | Current Loss: 0.100088\n[Training] Epoch 050 | Batch 030/102 | Current Loss: 0.100102\n[Training] Epoch 050 | Batch 040/102 | Current Loss: 0.100158\n[Training] Epoch 050 | Batch 050/102 | Current Loss: 0.100080\n[Training] Epoch 050 | Batch 060/102 | Current Loss: 0.100150\n[Training] Epoch 050 | Batch 070/102 | Current Loss: 0.100101\n[Training] Epoch 050 | Batch 080/102 | Current Loss: 0.100092\n[Training] Epoch 050 | Batch 090/102 | Current Loss: 0.100067\n[Training] Epoch 050 | Batch 100/102 | Current Loss: 0.100160\n\n[Training] Epoch 050 Summary:\n  Avg Loss: 0.100118\n  Current LR: 6.17e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 051 | Batch 000/102 | Current Loss: 0.100118\n[Training] Epoch 051 | Batch 010/102 | Current Loss: 0.100458\n[Training] Epoch 051 | Batch 020/102 | Current Loss: 0.100268\n[Training] Epoch 051 | Batch 030/102 | Current Loss: 0.100234\n[Training] Epoch 051 | Batch 040/102 | Current Loss: 0.100283\n[Training] Epoch 051 | Batch 050/102 | Current Loss: 0.100219\n[Training] Epoch 051 | Batch 060/102 | Current Loss: 0.100136\n[Training] Epoch 051 | Batch 070/102 | Current Loss: 0.100196\n[Training] Epoch 051 | Batch 080/102 | Current Loss: 0.100212\n[Training] Epoch 051 | Batch 090/102 | Current Loss: 0.100134\n[Training] Epoch 051 | Batch 100/102 | Current Loss: 0.100200\n\n[Training] Epoch 051 Summary:\n  Avg Loss: 0.100230\n  Current LR: 6.02e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 052 | Batch 000/102 | Current Loss: 0.100219\n[Training] Epoch 052 | Batch 010/102 | Current Loss: 0.100246\n[Training] Epoch 052 | Batch 020/102 | Current Loss: 0.100186\n[Training] Epoch 052 | Batch 030/102 | Current Loss: 0.100162\n[Training] Epoch 052 | Batch 040/102 | Current Loss: 0.100224\n[Training] Epoch 052 | Batch 050/102 | Current Loss: 0.100058\n[Training] Epoch 052 | Batch 060/102 | Current Loss: 0.100198\n[Training] Epoch 052 | Batch 070/102 | Current Loss: 0.100137\n[Training] Epoch 052 | Batch 080/102 | Current Loss: 0.100185\n[Training] Epoch 052 | Batch 090/102 | Current Loss: 0.100328\n[Training] Epoch 052 | Batch 100/102 | Current Loss: 0.100733\n\n[Training] Epoch 052 Summary:\n  Avg Loss: 0.100234\n  Current LR: 5.58e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 053 | Batch 000/102 | Current Loss: 0.100533\n[Training] Epoch 053 | Batch 010/102 | Current Loss: 0.100217\n[Training] Epoch 053 | Batch 020/102 | Current Loss: 0.100156\n[Training] Epoch 053 | Batch 030/102 | Current Loss: 0.100210\n[Training] Epoch 053 | Batch 040/102 | Current Loss: 0.100185\n[Training] Epoch 053 | Batch 050/102 | Current Loss: 0.100164\n[Training] Epoch 053 | Batch 060/102 | Current Loss: 0.100178\n[Training] Epoch 053 | Batch 070/102 | Current Loss: 0.100134\n[Training] Epoch 053 | Batch 080/102 | Current Loss: 0.100147\n[Training] Epoch 053 | Batch 090/102 | Current Loss: 0.100194\n[Training] Epoch 053 | Batch 100/102 | Current Loss: 0.100092\n\n[Training] Epoch 053 Summary:\n  Avg Loss: 0.100180\n  Current LR: 4.90e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 054 | Batch 000/102 | Current Loss: 0.100147\n[Training] Epoch 054 | Batch 010/102 | Current Loss: 0.100124\n[Training] Epoch 054 | Batch 020/102 | Current Loss: 0.100129\n[Training] Epoch 054 | Batch 030/102 | Current Loss: 0.100051\n[Training] Epoch 054 | Batch 040/102 | Current Loss: 0.100190\n[Training] Epoch 054 | Batch 050/102 | Current Loss: 0.100216\n[Training] Epoch 054 | Batch 060/102 | Current Loss: 0.100154\n[Training] Epoch 054 | Batch 070/102 | Current Loss: 0.100184\n[Training] Epoch 054 | Batch 080/102 | Current Loss: 0.100121\n[Training] Epoch 054 | Batch 090/102 | Current Loss: 0.100105\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 16:19:50,326] Trial 6 finished with value: 0.10011763432446648 and parameters: {'lr': 0.0006171845799545735, 'batch_size': 128, 'bottleneck_width': 2048, 'dropout_rate': 0.1725223414798908, 'alpha': 0.787342942048626, 'weight_decay': 6.947186900534145e-06}. Best is trial 1 with value: 0.10011662966480442.\n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 054 | Batch 100/102 | Current Loss: 0.100098\n\n[Training] Epoch 054 Summary:\n  Avg Loss: 0.100139\n  Current LR: 4.04e-04\n[EarlyStopping] Counter: 5/5\n\n[Training] Early stopping triggered\n\n[Optuna] Trial 6 completed with best loss: 0.100118\n\n==================================================\nStarting Optuna Trial 7\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 2.7020501193981095e-05\n  batch_size: 32\n  bottleneck_width: 512\n  dropout_rate: 0.37013712874403026\n  alpha: 0.7584180759951304\n  weight_decay: 6.253112533848764e-05\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 32, Total batches: 407\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 512\n  - Dropout rate: 0.37013712874403026\n[Model] Architecture initialized successfully\n[Model] Total parameters: 1,347,904\n[Optimizer] Initialized with lr=2.70e-05, weight_decay=6.25e-05\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.7584180759951304\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/407 | Current Loss: 1.734741\n[Training] Epoch 001 | Batch 010/407 | Current Loss: 1.165798\n[Training] Epoch 001 | Batch 020/407 | Current Loss: 0.865707\n[Training] Epoch 001 | Batch 030/407 | Current Loss: 0.741901\n[Training] Epoch 001 | Batch 040/407 | Current Loss: 0.629147\n[Training] Epoch 001 | Batch 050/407 | Current Loss: 0.541416\n[Training] Epoch 001 | Batch 060/407 | Current Loss: 0.465636\n[Training] Epoch 001 | Batch 070/407 | Current Loss: 0.411908\n[Training] Epoch 001 | Batch 080/407 | Current Loss: 0.349633\n[Training] Epoch 001 | Batch 090/407 | Current Loss: 0.306748\n[Training] Epoch 001 | Batch 100/407 | Current Loss: 0.275087\n[Training] Epoch 001 | Batch 110/407 | Current Loss: 0.260659\n[Training] Epoch 001 | Batch 120/407 | Current Loss: 0.263859\n[Training] Epoch 001 | Batch 130/407 | Current Loss: 0.222827\n[Training] Epoch 001 | Batch 140/407 | Current Loss: 0.211400\n[Training] Epoch 001 | Batch 150/407 | Current Loss: 0.205036\n[Training] Epoch 001 | Batch 160/407 | Current Loss: 0.198298\n[Training] Epoch 001 | Batch 170/407 | Current Loss: 0.182420\n[Training] Epoch 001 | Batch 180/407 | Current Loss: 0.182968\n[Training] Epoch 001 | Batch 190/407 | Current Loss: 0.176287\n[Training] Epoch 001 | Batch 200/407 | Current Loss: 0.174299\n[Training] Epoch 001 | Batch 210/407 | Current Loss: 0.173245\n[Training] Epoch 001 | Batch 220/407 | Current Loss: 0.156887\n[Training] Epoch 001 | Batch 230/407 | Current Loss: 0.157994\n[Training] Epoch 001 | Batch 240/407 | Current Loss: 0.160488\n[Training] Epoch 001 | Batch 250/407 | Current Loss: 0.158370\n[Training] Epoch 001 | Batch 260/407 | Current Loss: 0.150566\n[Training] Epoch 001 | Batch 270/407 | Current Loss: 0.161297\n[Training] Epoch 001 | Batch 280/407 | Current Loss: 0.153840\n[Training] Epoch 001 | Batch 290/407 | Current Loss: 0.147527\n[Training] Epoch 001 | Batch 300/407 | Current Loss: 0.143385\n[Training] Epoch 001 | Batch 310/407 | Current Loss: 0.156194\n[Training] Epoch 001 | Batch 320/407 | Current Loss: 0.139434\n[Training] Epoch 001 | Batch 330/407 | Current Loss: 0.139540\n[Training] Epoch 001 | Batch 340/407 | Current Loss: 0.138836\n[Training] Epoch 001 | Batch 350/407 | Current Loss: 0.136735\n[Training] Epoch 001 | Batch 360/407 | Current Loss: 0.146080\n[Training] Epoch 001 | Batch 370/407 | Current Loss: 0.147442\n[Training] Epoch 001 | Batch 380/407 | Current Loss: 0.131923\n[Training] Epoch 001 | Batch 390/407 | Current Loss: 0.131188\n[Training] Epoch 001 | Batch 400/407 | Current Loss: 0.132820\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.286497\n  Current LR: 2.64e-05\n[EarlyStopping] Loss improved (inf → 0.286497). Saving model...\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 16:20:07,743] Trial 7 pruned. \n","output_type":"stream"},{"name":"stdout","text":"\n[Optuna] Trial pruned\n\n==================================================\nStarting Optuna Trial 8\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.000254511520589056\n  batch_size: 128\n  bottleneck_width: 1024\n  dropout_rate: 0.41596538820277085\n  alpha: 0.40669237845820655\n  weight_decay: 5.3058970800461645e-05\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 128, Total batches: 102\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 1024\n  - Dropout rate: 0.41596538820277085\n[Model] Architecture initialized successfully\n[Model] Total parameters: 2,396,992\n[Optimizer] Initialized with lr=2.55e-04, weight_decay=5.31e-05\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.40669237845820655\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/102 | Current Loss: 1.418115\n[Training] Epoch 001 | Batch 010/102 | Current Loss: 0.216172\n[Training] Epoch 001 | Batch 020/102 | Current Loss: 0.149827\n[Training] Epoch 001 | Batch 030/102 | Current Loss: 0.128388\n[Training] Epoch 001 | Batch 040/102 | Current Loss: 0.118378\n[Training] Epoch 001 | Batch 050/102 | Current Loss: 0.114973\n[Training] Epoch 001 | Batch 060/102 | Current Loss: 0.112261\n[Training] Epoch 001 | Batch 070/102 | Current Loss: 0.114722\n[Training] Epoch 001 | Batch 080/102 | Current Loss: 0.109927\n[Training] Epoch 001 | Batch 090/102 | Current Loss: 0.108404\n[Training] Epoch 001 | Batch 100/102 | Current Loss: 0.107396\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.164199\n  Current LR: 2.48e-04\n[EarlyStopping] Loss improved (inf → 0.164199). Saving model...\n[Training] Epoch 002 | Batch 000/102 | Current Loss: 0.107562\n[Training] Epoch 002 | Batch 010/102 | Current Loss: 0.106201\n[Training] Epoch 002 | Batch 020/102 | Current Loss: 0.106526\n[Training] Epoch 002 | Batch 030/102 | Current Loss: 0.106484\n[Training] Epoch 002 | Batch 040/102 | Current Loss: 0.105957\n[Training] Epoch 002 | Batch 050/102 | Current Loss: 0.105124\n[Training] Epoch 002 | Batch 060/102 | Current Loss: 0.104890\n[Training] Epoch 002 | Batch 070/102 | Current Loss: 0.105112\n[Training] Epoch 002 | Batch 080/102 | Current Loss: 0.104836\n[Training] Epoch 002 | Batch 090/102 | Current Loss: 0.104872\n[Training] Epoch 002 | Batch 100/102 | Current Loss: 0.104360\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.106142\n  Current LR: 2.30e-04\n[EarlyStopping] Loss improved (0.164199 → 0.106142). Saving model...\n[Training] Epoch 003 | Batch 000/102 | Current Loss: 0.104295\n[Training] Epoch 003 | Batch 010/102 | Current Loss: 0.103748\n[Training] Epoch 003 | Batch 020/102 | Current Loss: 0.104110\n[Training] Epoch 003 | Batch 030/102 | Current Loss: 0.105780\n[Training] Epoch 003 | Batch 040/102 | Current Loss: 0.103596\n[Training] Epoch 003 | Batch 050/102 | Current Loss: 0.103465\n[Training] Epoch 003 | Batch 060/102 | Current Loss: 0.103499\n[Training] Epoch 003 | Batch 070/102 | Current Loss: 0.104392\n[Training] Epoch 003 | Batch 080/102 | Current Loss: 0.103630\n[Training] Epoch 003 | Batch 090/102 | Current Loss: 0.103380\n[Training] Epoch 003 | Batch 100/102 | Current Loss: 0.102894\n\n[Training] Epoch 003 Summary:\n  Avg Loss: 0.103984\n  Current LR: 2.02e-04\n[EarlyStopping] Loss improved (0.106142 → 0.103984). Saving model...\n[Training] Epoch 004 | Batch 000/102 | Current Loss: 0.105901\n[Training] Epoch 004 | Batch 010/102 | Current Loss: 0.103415\n[Training] Epoch 004 | Batch 020/102 | Current Loss: 0.102954\n[Training] Epoch 004 | Batch 030/102 | Current Loss: 0.102567\n[Training] Epoch 004 | Batch 040/102 | Current Loss: 0.104229\n[Training] Epoch 004 | Batch 050/102 | Current Loss: 0.102672\n[Training] Epoch 004 | Batch 060/102 | Current Loss: 0.103515\n[Training] Epoch 004 | Batch 070/102 | Current Loss: 0.102732\n[Training] Epoch 004 | Batch 080/102 | Current Loss: 0.103296\n[Training] Epoch 004 | Batch 090/102 | Current Loss: 0.102721\n[Training] Epoch 004 | Batch 100/102 | Current Loss: 0.102377\n\n[Training] Epoch 004 Summary:\n  Avg Loss: 0.103119\n  Current LR: 1.67e-04\n[EarlyStopping] Loss improved (0.103984 → 0.103119). Saving model...\n[Training] Epoch 005 | Batch 000/102 | Current Loss: 0.102797\n[Training] Epoch 005 | Batch 010/102 | Current Loss: 0.102384\n[Training] Epoch 005 | Batch 020/102 | Current Loss: 0.102457\n[Training] Epoch 005 | Batch 030/102 | Current Loss: 0.102466\n[Training] Epoch 005 | Batch 040/102 | Current Loss: 0.102356\n[Training] Epoch 005 | Batch 050/102 | Current Loss: 0.102548\n[Training] Epoch 005 | Batch 060/102 | Current Loss: 0.102369\n[Training] Epoch 005 | Batch 070/102 | Current Loss: 0.102180\n[Training] Epoch 005 | Batch 080/102 | Current Loss: 0.102174\n[Training] Epoch 005 | Batch 090/102 | Current Loss: 0.102044\n[Training] Epoch 005 | Batch 100/102 | Current Loss: 0.102329\n\n[Training] Epoch 005 Summary:\n  Avg Loss: 0.102507\n  Current LR: 1.28e-04\n[EarlyStopping] Loss improved (0.103119 → 0.102507). Saving model...\n[Training] Epoch 006 | Batch 000/102 | Current Loss: 0.102139\n[Training] Epoch 006 | Batch 010/102 | Current Loss: 0.102032\n[Training] Epoch 006 | Batch 020/102 | Current Loss: 0.102107\n[Training] Epoch 006 | Batch 030/102 | Current Loss: 0.101901\n[Training] Epoch 006 | Batch 040/102 | Current Loss: 0.101975\n[Training] Epoch 006 | Batch 050/102 | Current Loss: 0.102042\n[Training] Epoch 006 | Batch 060/102 | Current Loss: 0.102111\n[Training] Epoch 006 | Batch 070/102 | Current Loss: 0.102546\n[Training] Epoch 006 | Batch 080/102 | Current Loss: 0.102606\n[Training] Epoch 006 | Batch 090/102 | Current Loss: 0.102165\n[Training] Epoch 006 | Batch 100/102 | Current Loss: 0.101758\n\n[Training] Epoch 006 Summary:\n  Avg Loss: 0.102233\n  Current LR: 8.86e-05\n[EarlyStopping] Loss improved (0.102507 → 0.102233). Saving model...\n[Training] Epoch 007 | Batch 000/102 | Current Loss: 0.101861\n[Training] Epoch 007 | Batch 010/102 | Current Loss: 0.102680\n[Training] Epoch 007 | Batch 020/102 | Current Loss: 0.101947\n[Training] Epoch 007 | Batch 030/102 | Current Loss: 0.101986\n[Training] Epoch 007 | Batch 040/102 | Current Loss: 0.101996\n[Training] Epoch 007 | Batch 050/102 | Current Loss: 0.102260\n[Training] Epoch 007 | Batch 060/102 | Current Loss: 0.101845\n[Training] Epoch 007 | Batch 070/102 | Current Loss: 0.101909\n[Training] Epoch 007 | Batch 080/102 | Current Loss: 0.102009\n[Training] Epoch 007 | Batch 090/102 | Current Loss: 0.102953\n[Training] Epoch 007 | Batch 100/102 | Current Loss: 0.102133\n\n[Training] Epoch 007 Summary:\n  Avg Loss: 0.101995\n  Current LR: 5.33e-05\n[EarlyStopping] Loss improved (0.102233 → 0.101995). Saving model...\n[Training] Epoch 008 | Batch 000/102 | Current Loss: 0.101729\n[Training] Epoch 008 | Batch 010/102 | Current Loss: 0.102514\n[Training] Epoch 008 | Batch 020/102 | Current Loss: 0.102077\n[Training] Epoch 008 | Batch 030/102 | Current Loss: 0.101935\n[Training] Epoch 008 | Batch 040/102 | Current Loss: 0.102138\n[Training] Epoch 008 | Batch 050/102 | Current Loss: 0.101683\n[Training] Epoch 008 | Batch 060/102 | Current Loss: 0.101628\n[Training] Epoch 008 | Batch 070/102 | Current Loss: 0.101848\n[Training] Epoch 008 | Batch 080/102 | Current Loss: 0.101881\n[Training] Epoch 008 | Batch 090/102 | Current Loss: 0.101800\n[Training] Epoch 008 | Batch 100/102 | Current Loss: 0.101704\n\n[Training] Epoch 008 Summary:\n  Avg Loss: 0.101827\n  Current LR: 2.52e-05\n[EarlyStopping] Loss improved (0.101995 → 0.101827). Saving model...\n[Training] Epoch 009 | Batch 000/102 | Current Loss: 0.101620\n[Training] Epoch 009 | Batch 010/102 | Current Loss: 0.101673\n[Training] Epoch 009 | Batch 020/102 | Current Loss: 0.101636\n[Training] Epoch 009 | Batch 030/102 | Current Loss: 0.101684\n[Training] Epoch 009 | Batch 040/102 | Current Loss: 0.101678\n[Training] Epoch 009 | Batch 050/102 | Current Loss: 0.101750\n[Training] Epoch 009 | Batch 060/102 | Current Loss: 0.102191\n[Training] Epoch 009 | Batch 070/102 | Current Loss: 0.101837\n[Training] Epoch 009 | Batch 080/102 | Current Loss: 0.101725\n[Training] Epoch 009 | Batch 090/102 | Current Loss: 0.101566\n[Training] Epoch 009 | Batch 100/102 | Current Loss: 0.101588\n\n[Training] Epoch 009 Summary:\n  Avg Loss: 0.101758\n  Current LR: 7.20e-06\n[EarlyStopping] Loss improved (0.101827 → 0.101758). Saving model...\n[Training] Epoch 010 | Batch 000/102 | Current Loss: 0.101749\n[Training] Epoch 010 | Batch 010/102 | Current Loss: 0.101664\n[Training] Epoch 010 | Batch 020/102 | Current Loss: 0.101718\n[Training] Epoch 010 | Batch 030/102 | Current Loss: 0.101576\n[Training] Epoch 010 | Batch 040/102 | Current Loss: 0.102213\n[Training] Epoch 010 | Batch 050/102 | Current Loss: 0.101592\n[Training] Epoch 010 | Batch 060/102 | Current Loss: 0.101540\n[Training] Epoch 010 | Batch 070/102 | Current Loss: 0.101701\n[Training] Epoch 010 | Batch 080/102 | Current Loss: 0.101611\n[Training] Epoch 010 | Batch 090/102 | Current Loss: 0.101604\n[Training] Epoch 010 | Batch 100/102 | Current Loss: 0.101595\n\n[Training] Epoch 010 Summary:\n  Avg Loss: 0.101711\n  Current LR: 2.55e-04\n[EarlyStopping] Loss improved (0.101758 → 0.101711). Saving model...\n[Training] Epoch 011 | Batch 000/102 | Current Loss: 0.101578\n[Training] Epoch 011 | Batch 010/102 | Current Loss: 0.101927\n[Training] Epoch 011 | Batch 020/102 | Current Loss: 0.101770\n[Training] Epoch 011 | Batch 030/102 | Current Loss: 0.102056\n[Training] Epoch 011 | Batch 040/102 | Current Loss: 0.101681\n[Training] Epoch 011 | Batch 050/102 | Current Loss: 0.101880\n[Training] Epoch 011 | Batch 060/102 | Current Loss: 0.101635\n[Training] Epoch 011 | Batch 070/102 | Current Loss: 0.101523\n[Training] Epoch 011 | Batch 080/102 | Current Loss: 0.101471\n[Training] Epoch 011 | Batch 090/102 | Current Loss: 0.101649\n[Training] Epoch 011 | Batch 100/102 | Current Loss: 0.101250\n\n[Training] Epoch 011 Summary:\n  Avg Loss: 0.101800\n  Current LR: 2.48e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 012 | Batch 000/102 | Current Loss: 0.101471\n[Training] Epoch 012 | Batch 010/102 | Current Loss: 0.101371\n[Training] Epoch 012 | Batch 020/102 | Current Loss: 0.101371\n[Training] Epoch 012 | Batch 030/102 | Current Loss: 0.101367\n[Training] Epoch 012 | Batch 040/102 | Current Loss: 0.101240\n[Training] Epoch 012 | Batch 050/102 | Current Loss: 0.101360\n[Training] Epoch 012 | Batch 060/102 | Current Loss: 0.101236\n[Training] Epoch 012 | Batch 070/102 | Current Loss: 0.101252\n[Training] Epoch 012 | Batch 080/102 | Current Loss: 0.101280\n[Training] Epoch 012 | Batch 090/102 | Current Loss: 0.101205\n[Training] Epoch 012 | Batch 100/102 | Current Loss: 0.101102\n\n[Training] Epoch 012 Summary:\n  Avg Loss: 0.101321\n  Current LR: 2.30e-04\n[EarlyStopping] Loss improved (0.101711 → 0.101321). Saving model...\n[Training] Epoch 013 | Batch 000/102 | Current Loss: 0.101173\n[Training] Epoch 013 | Batch 010/102 | Current Loss: 0.101126\n[Training] Epoch 013 | Batch 020/102 | Current Loss: 0.101044\n[Training] Epoch 013 | Batch 030/102 | Current Loss: 0.101036\n[Training] Epoch 013 | Batch 040/102 | Current Loss: 0.101115\n[Training] Epoch 013 | Batch 050/102 | Current Loss: 0.101085\n[Training] Epoch 013 | Batch 060/102 | Current Loss: 0.101011\n[Training] Epoch 013 | Batch 070/102 | Current Loss: 0.101047\n[Training] Epoch 013 | Batch 080/102 | Current Loss: 0.101004\n[Training] Epoch 013 | Batch 090/102 | Current Loss: 0.100973\n[Training] Epoch 013 | Batch 100/102 | Current Loss: 0.100936\n\n[Training] Epoch 013 Summary:\n  Avg Loss: 0.101078\n  Current LR: 2.02e-04\n[EarlyStopping] Loss improved (0.101321 → 0.101078). Saving model...\n[Training] Epoch 014 | Batch 000/102 | Current Loss: 0.100950\n[Training] Epoch 014 | Batch 010/102 | Current Loss: 0.100904\n[Training] Epoch 014 | Batch 020/102 | Current Loss: 0.100889\n[Training] Epoch 014 | Batch 030/102 | Current Loss: 0.100852\n[Training] Epoch 014 | Batch 040/102 | Current Loss: 0.101061\n[Training] Epoch 014 | Batch 050/102 | Current Loss: 0.100888\n[Training] Epoch 014 | Batch 060/102 | Current Loss: 0.100835\n[Training] Epoch 014 | Batch 070/102 | Current Loss: 0.100850\n[Training] Epoch 014 | Batch 080/102 | Current Loss: 0.100863\n[Training] Epoch 014 | Batch 090/102 | Current Loss: 0.100889\n[Training] Epoch 014 | Batch 100/102 | Current Loss: 0.100765\n\n[Training] Epoch 014 Summary:\n  Avg Loss: 0.100965\n  Current LR: 1.67e-04\n[EarlyStopping] Loss improved (0.101078 → 0.100965). Saving model...\n[Training] Epoch 015 | Batch 000/102 | Current Loss: 0.100904\n[Training] Epoch 015 | Batch 010/102 | Current Loss: 0.100949\n[Training] Epoch 015 | Batch 020/102 | Current Loss: 0.100830\n[Training] Epoch 015 | Batch 030/102 | Current Loss: 0.100806\n[Training] Epoch 015 | Batch 040/102 | Current Loss: 0.100900\n[Training] Epoch 015 | Batch 050/102 | Current Loss: 0.100855\n[Training] Epoch 015 | Batch 060/102 | Current Loss: 0.100708\n[Training] Epoch 015 | Batch 070/102 | Current Loss: 0.101274\n[Training] Epoch 015 | Batch 080/102 | Current Loss: 0.100828\n[Training] Epoch 015 | Batch 090/102 | Current Loss: 0.100910\n[Training] Epoch 015 | Batch 100/102 | Current Loss: 0.100819\n\n[Training] Epoch 015 Summary:\n  Avg Loss: 0.100860\n  Current LR: 1.28e-04\n[EarlyStopping] Loss improved (0.100965 → 0.100860). Saving model...\n[Training] Epoch 016 | Batch 000/102 | Current Loss: 0.100833\n[Training] Epoch 016 | Batch 010/102 | Current Loss: 0.100907\n[Training] Epoch 016 | Batch 020/102 | Current Loss: 0.100849\n[Training] Epoch 016 | Batch 030/102 | Current Loss: 0.100776\n[Training] Epoch 016 | Batch 040/102 | Current Loss: 0.100755\n[Training] Epoch 016 | Batch 050/102 | Current Loss: 0.100724\n[Training] Epoch 016 | Batch 060/102 | Current Loss: 0.100846\n[Training] Epoch 016 | Batch 070/102 | Current Loss: 0.100747\n[Training] Epoch 016 | Batch 080/102 | Current Loss: 0.100716\n[Training] Epoch 016 | Batch 090/102 | Current Loss: 0.100839\n[Training] Epoch 016 | Batch 100/102 | Current Loss: 0.100665\n\n[Training] Epoch 016 Summary:\n  Avg Loss: 0.100775\n  Current LR: 8.86e-05\n[EarlyStopping] Loss improved (0.100860 → 0.100775). Saving model...\n[Training] Epoch 017 | Batch 000/102 | Current Loss: 0.100728\n[Training] Epoch 017 | Batch 010/102 | Current Loss: 0.100748\n[Training] Epoch 017 | Batch 020/102 | Current Loss: 0.100741\n[Training] Epoch 017 | Batch 030/102 | Current Loss: 0.100642\n[Training] Epoch 017 | Batch 040/102 | Current Loss: 0.100778\n[Training] Epoch 017 | Batch 050/102 | Current Loss: 0.100743\n[Training] Epoch 017 | Batch 060/102 | Current Loss: 0.100648\n[Training] Epoch 017 | Batch 070/102 | Current Loss: 0.100671\n[Training] Epoch 017 | Batch 080/102 | Current Loss: 0.100721\n[Training] Epoch 017 | Batch 090/102 | Current Loss: 0.100725\n[Training] Epoch 017 | Batch 100/102 | Current Loss: 0.100662\n\n[Training] Epoch 017 Summary:\n  Avg Loss: 0.100730\n  Current LR: 5.33e-05\n[EarlyStopping] Loss improved (0.100775 → 0.100730). Saving model...\n[Training] Epoch 018 | Batch 000/102 | Current Loss: 0.101001\n[Training] Epoch 018 | Batch 010/102 | Current Loss: 0.100716\n[Training] Epoch 018 | Batch 020/102 | Current Loss: 0.100659\n[Training] Epoch 018 | Batch 030/102 | Current Loss: 0.100751\n[Training] Epoch 018 | Batch 040/102 | Current Loss: 0.100748\n[Training] Epoch 018 | Batch 050/102 | Current Loss: 0.100726\n[Training] Epoch 018 | Batch 060/102 | Current Loss: 0.100723\n[Training] Epoch 018 | Batch 070/102 | Current Loss: 0.100616\n[Training] Epoch 018 | Batch 080/102 | Current Loss: 0.100665\n[Training] Epoch 018 | Batch 090/102 | Current Loss: 0.100600\n[Training] Epoch 018 | Batch 100/102 | Current Loss: 0.100672\n\n[Training] Epoch 018 Summary:\n  Avg Loss: 0.100705\n  Current LR: 2.52e-05\n[EarlyStopping] Loss improved (0.100730 → 0.100705). Saving model...\n[Training] Epoch 019 | Batch 000/102 | Current Loss: 0.100624\n[Training] Epoch 019 | Batch 010/102 | Current Loss: 0.100781\n[Training] Epoch 019 | Batch 020/102 | Current Loss: 0.100703\n[Training] Epoch 019 | Batch 030/102 | Current Loss: 0.100605\n[Training] Epoch 019 | Batch 040/102 | Current Loss: 0.100634\n[Training] Epoch 019 | Batch 050/102 | Current Loss: 0.100645\n[Training] Epoch 019 | Batch 060/102 | Current Loss: 0.100717\n[Training] Epoch 019 | Batch 070/102 | Current Loss: 0.100729\n[Training] Epoch 019 | Batch 080/102 | Current Loss: 0.100614\n[Training] Epoch 019 | Batch 090/102 | Current Loss: 0.100754\n[Training] Epoch 019 | Batch 100/102 | Current Loss: 0.100729\n\n[Training] Epoch 019 Summary:\n  Avg Loss: 0.100685\n  Current LR: 7.20e-06\n[EarlyStopping] Loss improved (0.100705 → 0.100685). Saving model...\n[Training] Epoch 020 | Batch 000/102 | Current Loss: 0.100605\n[Training] Epoch 020 | Batch 010/102 | Current Loss: 0.100703\n[Training] Epoch 020 | Batch 020/102 | Current Loss: 0.100648\n[Training] Epoch 020 | Batch 030/102 | Current Loss: 0.100676\n[Training] Epoch 020 | Batch 040/102 | Current Loss: 0.100682\n[Training] Epoch 020 | Batch 050/102 | Current Loss: 0.100673\n[Training] Epoch 020 | Batch 060/102 | Current Loss: 0.100709\n[Training] Epoch 020 | Batch 070/102 | Current Loss: 0.100701\n[Training] Epoch 020 | Batch 080/102 | Current Loss: 0.100750\n[Training] Epoch 020 | Batch 090/102 | Current Loss: 0.100678\n[Training] Epoch 020 | Batch 100/102 | Current Loss: 0.100666\n\n[Training] Epoch 020 Summary:\n  Avg Loss: 0.100678\n  Current LR: 2.55e-04\n[EarlyStopping] Loss improved (0.100685 → 0.100678). Saving model...\n[Training] Epoch 021 | Batch 000/102 | Current Loss: 0.100672\n[Training] Epoch 021 | Batch 010/102 | Current Loss: 0.100743\n[Training] Epoch 021 | Batch 020/102 | Current Loss: 0.100699\n[Training] Epoch 021 | Batch 030/102 | Current Loss: 0.100788\n[Training] Epoch 021 | Batch 040/102 | Current Loss: 0.100747\n[Training] Epoch 021 | Batch 050/102 | Current Loss: 0.100669\n[Training] Epoch 021 | Batch 060/102 | Current Loss: 0.100710\n[Training] Epoch 021 | Batch 070/102 | Current Loss: 0.100698\n[Training] Epoch 021 | Batch 080/102 | Current Loss: 0.100603\n[Training] Epoch 021 | Batch 090/102 | Current Loss: 0.100639\n[Training] Epoch 021 | Batch 100/102 | Current Loss: 0.100694\n\n[Training] Epoch 021 Summary:\n  Avg Loss: 0.100688\n  Current LR: 2.48e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 022 | Batch 000/102 | Current Loss: 0.100465\n[Training] Epoch 022 | Batch 010/102 | Current Loss: 0.100594\n[Training] Epoch 022 | Batch 020/102 | Current Loss: 0.100590\n[Training] Epoch 022 | Batch 030/102 | Current Loss: 0.100554\n[Training] Epoch 022 | Batch 040/102 | Current Loss: 0.100592\n[Training] Epoch 022 | Batch 050/102 | Current Loss: 0.100571\n[Training] Epoch 022 | Batch 060/102 | Current Loss: 0.100616\n[Training] Epoch 022 | Batch 070/102 | Current Loss: 0.100598\n[Training] Epoch 022 | Batch 080/102 | Current Loss: 0.100570\n[Training] Epoch 022 | Batch 090/102 | Current Loss: 0.100516\n[Training] Epoch 022 | Batch 100/102 | Current Loss: 0.100572\n\n[Training] Epoch 022 Summary:\n  Avg Loss: 0.100595\n  Current LR: 2.30e-04\n[EarlyStopping] Loss improved (0.100678 → 0.100595). Saving model...\n[Training] Epoch 023 | Batch 000/102 | Current Loss: 0.100538\n[Training] Epoch 023 | Batch 010/102 | Current Loss: 0.100506\n[Training] Epoch 023 | Batch 020/102 | Current Loss: 0.100615\n[Training] Epoch 023 | Batch 030/102 | Current Loss: 0.100527\n[Training] Epoch 023 | Batch 040/102 | Current Loss: 0.100568\n[Training] Epoch 023 | Batch 050/102 | Current Loss: 0.100505\n[Training] Epoch 023 | Batch 060/102 | Current Loss: 0.100801\n[Training] Epoch 023 | Batch 070/102 | Current Loss: 0.100610\n[Training] Epoch 023 | Batch 080/102 | Current Loss: 0.100515\n[Training] Epoch 023 | Batch 090/102 | Current Loss: 0.100535\n[Training] Epoch 023 | Batch 100/102 | Current Loss: 0.100448\n\n[Training] Epoch 023 Summary:\n  Avg Loss: 0.100545\n  Current LR: 2.02e-04\n[EarlyStopping] Loss improved (0.100595 → 0.100545). Saving model...\n[Training] Epoch 024 | Batch 000/102 | Current Loss: 0.100549\n[Training] Epoch 024 | Batch 010/102 | Current Loss: 0.100462\n[Training] Epoch 024 | Batch 020/102 | Current Loss: 0.100532\n[Training] Epoch 024 | Batch 030/102 | Current Loss: 0.100507\n[Training] Epoch 024 | Batch 040/102 | Current Loss: 0.100480\n[Training] Epoch 024 | Batch 050/102 | Current Loss: 0.100512\n[Training] Epoch 024 | Batch 060/102 | Current Loss: 0.100478\n[Training] Epoch 024 | Batch 070/102 | Current Loss: 0.100489\n[Training] Epoch 024 | Batch 080/102 | Current Loss: 0.100544\n[Training] Epoch 024 | Batch 090/102 | Current Loss: 0.100553\n[Training] Epoch 024 | Batch 100/102 | Current Loss: 0.100560\n\n[Training] Epoch 024 Summary:\n  Avg Loss: 0.100496\n  Current LR: 1.67e-04\n[EarlyStopping] Loss improved (0.100545 → 0.100496). Saving model...\n[Training] Epoch 025 | Batch 000/102 | Current Loss: 0.100425\n[Training] Epoch 025 | Batch 010/102 | Current Loss: 0.100491\n[Training] Epoch 025 | Batch 020/102 | Current Loss: 0.100480\n[Training] Epoch 025 | Batch 030/102 | Current Loss: 0.100550\n[Training] Epoch 025 | Batch 040/102 | Current Loss: 0.100493\n[Training] Epoch 025 | Batch 050/102 | Current Loss: 0.100565\n[Training] Epoch 025 | Batch 060/102 | Current Loss: 0.100365\n[Training] Epoch 025 | Batch 070/102 | Current Loss: 0.100555\n[Training] Epoch 025 | Batch 080/102 | Current Loss: 0.100382\n[Training] Epoch 025 | Batch 090/102 | Current Loss: 0.100458\n[Training] Epoch 025 | Batch 100/102 | Current Loss: 0.100492\n\n[Training] Epoch 025 Summary:\n  Avg Loss: 0.100461\n  Current LR: 1.28e-04\n[EarlyStopping] Loss improved (0.100496 → 0.100461). Saving model...\n[Training] Epoch 026 | Batch 000/102 | Current Loss: 0.100434\n[Training] Epoch 026 | Batch 010/102 | Current Loss: 0.100405\n[Training] Epoch 026 | Batch 020/102 | Current Loss: 0.100472\n[Training] Epoch 026 | Batch 030/102 | Current Loss: 0.100453\n[Training] Epoch 026 | Batch 040/102 | Current Loss: 0.100435\n[Training] Epoch 026 | Batch 050/102 | Current Loss: 0.100400\n[Training] Epoch 026 | Batch 060/102 | Current Loss: 0.100439\n[Training] Epoch 026 | Batch 070/102 | Current Loss: 0.100403\n[Training] Epoch 026 | Batch 080/102 | Current Loss: 0.100386\n[Training] Epoch 026 | Batch 090/102 | Current Loss: 0.100453\n[Training] Epoch 026 | Batch 100/102 | Current Loss: 0.100311\n\n[Training] Epoch 026 Summary:\n  Avg Loss: 0.100433\n  Current LR: 8.86e-05\n[EarlyStopping] Loss improved (0.100461 → 0.100433). Saving model...\n[Training] Epoch 027 | Batch 000/102 | Current Loss: 0.100497\n[Training] Epoch 027 | Batch 010/102 | Current Loss: 0.100492\n[Training] Epoch 027 | Batch 020/102 | Current Loss: 0.100386\n[Training] Epoch 027 | Batch 030/102 | Current Loss: 0.100423\n[Training] Epoch 027 | Batch 040/102 | Current Loss: 0.100499\n[Training] Epoch 027 | Batch 050/102 | Current Loss: 0.100434\n[Training] Epoch 027 | Batch 060/102 | Current Loss: 0.100416\n[Training] Epoch 027 | Batch 070/102 | Current Loss: 0.100431\n[Training] Epoch 027 | Batch 080/102 | Current Loss: 0.100467\n[Training] Epoch 027 | Batch 090/102 | Current Loss: 0.100413\n[Training] Epoch 027 | Batch 100/102 | Current Loss: 0.100454\n\n[Training] Epoch 027 Summary:\n  Avg Loss: 0.100413\n  Current LR: 5.33e-05\n[EarlyStopping] Loss improved (0.100433 → 0.100413). Saving model...\n[Training] Epoch 028 | Batch 000/102 | Current Loss: 0.100357\n[Training] Epoch 028 | Batch 010/102 | Current Loss: 0.100400\n[Training] Epoch 028 | Batch 020/102 | Current Loss: 0.100491\n[Training] Epoch 028 | Batch 030/102 | Current Loss: 0.100429\n[Training] Epoch 028 | Batch 040/102 | Current Loss: 0.100491\n[Training] Epoch 028 | Batch 050/102 | Current Loss: 0.100345\n[Training] Epoch 028 | Batch 060/102 | Current Loss: 0.100398\n[Training] Epoch 028 | Batch 070/102 | Current Loss: 0.100481\n[Training] Epoch 028 | Batch 080/102 | Current Loss: 0.100337\n[Training] Epoch 028 | Batch 090/102 | Current Loss: 0.100376\n[Training] Epoch 028 | Batch 100/102 | Current Loss: 0.100370\n\n[Training] Epoch 028 Summary:\n  Avg Loss: 0.100400\n  Current LR: 2.52e-05\n[EarlyStopping] Loss improved (0.100413 → 0.100400). Saving model...\n[Training] Epoch 029 | Batch 000/102 | Current Loss: 0.100296\n[Training] Epoch 029 | Batch 010/102 | Current Loss: 0.100418\n[Training] Epoch 029 | Batch 020/102 | Current Loss: 0.100408\n[Training] Epoch 029 | Batch 030/102 | Current Loss: 0.100409\n[Training] Epoch 029 | Batch 040/102 | Current Loss: 0.100389\n[Training] Epoch 029 | Batch 050/102 | Current Loss: 0.100431\n[Training] Epoch 029 | Batch 060/102 | Current Loss: 0.100442\n[Training] Epoch 029 | Batch 070/102 | Current Loss: 0.100402\n[Training] Epoch 029 | Batch 080/102 | Current Loss: 0.100339\n[Training] Epoch 029 | Batch 090/102 | Current Loss: 0.100395\n[Training] Epoch 029 | Batch 100/102 | Current Loss: 0.100395\n\n[Training] Epoch 029 Summary:\n  Avg Loss: 0.100394\n  Current LR: 7.20e-06\n[EarlyStopping] Loss improved (0.100400 → 0.100394). Saving model...\n[Training] Epoch 030 | Batch 000/102 | Current Loss: 0.100414\n[Training] Epoch 030 | Batch 010/102 | Current Loss: 0.100456\n[Training] Epoch 030 | Batch 020/102 | Current Loss: 0.100397\n[Training] Epoch 030 | Batch 030/102 | Current Loss: 0.100306\n[Training] Epoch 030 | Batch 040/102 | Current Loss: 0.100372\n[Training] Epoch 030 | Batch 050/102 | Current Loss: 0.100341\n[Training] Epoch 030 | Batch 060/102 | Current Loss: 0.100342\n[Training] Epoch 030 | Batch 070/102 | Current Loss: 0.100392\n[Training] Epoch 030 | Batch 080/102 | Current Loss: 0.100486\n[Training] Epoch 030 | Batch 090/102 | Current Loss: 0.100394\n[Training] Epoch 030 | Batch 100/102 | Current Loss: 0.100397\n\n[Training] Epoch 030 Summary:\n  Avg Loss: 0.100385\n  Current LR: 2.55e-04\n[EarlyStopping] Loss improved (0.100394 → 0.100385). Saving model...\n[Training] Epoch 031 | Batch 000/102 | Current Loss: 0.100365\n[Training] Epoch 031 | Batch 010/102 | Current Loss: 0.100354\n[Training] Epoch 031 | Batch 020/102 | Current Loss: 0.100489\n[Training] Epoch 031 | Batch 030/102 | Current Loss: 0.100469\n[Training] Epoch 031 | Batch 040/102 | Current Loss: 0.100366\n[Training] Epoch 031 | Batch 050/102 | Current Loss: 0.100467\n[Training] Epoch 031 | Batch 060/102 | Current Loss: 0.100437\n[Training] Epoch 031 | Batch 070/102 | Current Loss: 0.100453\n[Training] Epoch 031 | Batch 080/102 | Current Loss: 0.100345\n[Training] Epoch 031 | Batch 090/102 | Current Loss: 0.100313\n[Training] Epoch 031 | Batch 100/102 | Current Loss: 0.100467\n\n[Training] Epoch 031 Summary:\n  Avg Loss: 0.100408\n  Current LR: 2.48e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 032 | Batch 000/102 | Current Loss: 0.100361\n[Training] Epoch 032 | Batch 010/102 | Current Loss: 0.100392\n[Training] Epoch 032 | Batch 020/102 | Current Loss: 0.100358\n[Training] Epoch 032 | Batch 030/102 | Current Loss: 0.100362\n[Training] Epoch 032 | Batch 040/102 | Current Loss: 0.100332\n[Training] Epoch 032 | Batch 050/102 | Current Loss: 0.100352\n[Training] Epoch 032 | Batch 060/102 | Current Loss: 0.100322\n[Training] Epoch 032 | Batch 070/102 | Current Loss: 0.100427\n[Training] Epoch 032 | Batch 080/102 | Current Loss: 0.100411\n[Training] Epoch 032 | Batch 090/102 | Current Loss: 0.100370\n[Training] Epoch 032 | Batch 100/102 | Current Loss: 0.100385\n\n[Training] Epoch 032 Summary:\n  Avg Loss: 0.100380\n  Current LR: 2.30e-04\n[EarlyStopping] Loss improved (0.100385 → 0.100380). Saving model...\n[Training] Epoch 033 | Batch 000/102 | Current Loss: 0.100395\n[Training] Epoch 033 | Batch 010/102 | Current Loss: 0.100318\n[Training] Epoch 033 | Batch 020/102 | Current Loss: 0.100360\n[Training] Epoch 033 | Batch 030/102 | Current Loss: 0.100363\n[Training] Epoch 033 | Batch 040/102 | Current Loss: 0.100421\n[Training] Epoch 033 | Batch 050/102 | Current Loss: 0.100402\n[Training] Epoch 033 | Batch 060/102 | Current Loss: 0.100356\n[Training] Epoch 033 | Batch 070/102 | Current Loss: 0.100307\n[Training] Epoch 033 | Batch 080/102 | Current Loss: 0.100282\n[Training] Epoch 033 | Batch 090/102 | Current Loss: 0.100415\n[Training] Epoch 033 | Batch 100/102 | Current Loss: 0.100353\n\n[Training] Epoch 033 Summary:\n  Avg Loss: 0.100344\n  Current LR: 2.02e-04\n[EarlyStopping] Loss improved (0.100380 → 0.100344). Saving model...\n[Training] Epoch 034 | Batch 000/102 | Current Loss: 0.100388\n[Training] Epoch 034 | Batch 010/102 | Current Loss: 0.100351\n[Training] Epoch 034 | Batch 020/102 | Current Loss: 0.100386\n[Training] Epoch 034 | Batch 030/102 | Current Loss: 0.100354\n[Training] Epoch 034 | Batch 040/102 | Current Loss: 0.100271\n[Training] Epoch 034 | Batch 050/102 | Current Loss: 0.100297\n[Training] Epoch 034 | Batch 060/102 | Current Loss: 0.100314\n[Training] Epoch 034 | Batch 070/102 | Current Loss: 0.100277\n[Training] Epoch 034 | Batch 080/102 | Current Loss: 0.100277\n[Training] Epoch 034 | Batch 090/102 | Current Loss: 0.100272\n[Training] Epoch 034 | Batch 100/102 | Current Loss: 0.100305\n\n[Training] Epoch 034 Summary:\n  Avg Loss: 0.100328\n  Current LR: 1.67e-04\n[EarlyStopping] Loss improved (0.100344 → 0.100328). Saving model...\n[Training] Epoch 035 | Batch 000/102 | Current Loss: 0.100323\n[Training] Epoch 035 | Batch 010/102 | Current Loss: 0.100350\n[Training] Epoch 035 | Batch 020/102 | Current Loss: 0.100276\n[Training] Epoch 035 | Batch 030/102 | Current Loss: 0.100197\n[Training] Epoch 035 | Batch 040/102 | Current Loss: 0.100354\n[Training] Epoch 035 | Batch 050/102 | Current Loss: 0.100386\n[Training] Epoch 035 | Batch 060/102 | Current Loss: 0.100346\n[Training] Epoch 035 | Batch 070/102 | Current Loss: 0.100291\n[Training] Epoch 035 | Batch 080/102 | Current Loss: 0.100279\n[Training] Epoch 035 | Batch 090/102 | Current Loss: 0.100255\n[Training] Epoch 035 | Batch 100/102 | Current Loss: 0.100373\n\n[Training] Epoch 035 Summary:\n  Avg Loss: 0.100309\n  Current LR: 1.28e-04\n[EarlyStopping] Loss improved (0.100328 → 0.100309). Saving model...\n[Training] Epoch 036 | Batch 000/102 | Current Loss: 0.100279\n[Training] Epoch 036 | Batch 010/102 | Current Loss: 0.100279\n[Training] Epoch 036 | Batch 020/102 | Current Loss: 0.100289\n[Training] Epoch 036 | Batch 030/102 | Current Loss: 0.100257\n[Training] Epoch 036 | Batch 040/102 | Current Loss: 0.100318\n[Training] Epoch 036 | Batch 050/102 | Current Loss: 0.100232\n[Training] Epoch 036 | Batch 060/102 | Current Loss: 0.100238\n[Training] Epoch 036 | Batch 070/102 | Current Loss: 0.100334\n[Training] Epoch 036 | Batch 080/102 | Current Loss: 0.100327\n[Training] Epoch 036 | Batch 090/102 | Current Loss: 0.100336\n[Training] Epoch 036 | Batch 100/102 | Current Loss: 0.100242\n\n[Training] Epoch 036 Summary:\n  Avg Loss: 0.100294\n  Current LR: 8.86e-05\n[EarlyStopping] Loss improved (0.100309 → 0.100294). Saving model...\n[Training] Epoch 037 | Batch 000/102 | Current Loss: 0.100222\n[Training] Epoch 037 | Batch 010/102 | Current Loss: 0.100293\n[Training] Epoch 037 | Batch 020/102 | Current Loss: 0.100253\n[Training] Epoch 037 | Batch 030/102 | Current Loss: 0.100330\n[Training] Epoch 037 | Batch 040/102 | Current Loss: 0.100244\n[Training] Epoch 037 | Batch 050/102 | Current Loss: 0.100245\n[Training] Epoch 037 | Batch 060/102 | Current Loss: 0.100293\n[Training] Epoch 037 | Batch 070/102 | Current Loss: 0.100234\n[Training] Epoch 037 | Batch 080/102 | Current Loss: 0.100303\n[Training] Epoch 037 | Batch 090/102 | Current Loss: 0.100262\n[Training] Epoch 037 | Batch 100/102 | Current Loss: 0.100203\n\n[Training] Epoch 037 Summary:\n  Avg Loss: 0.100278\n  Current LR: 5.33e-05\n[EarlyStopping] Loss improved (0.100294 → 0.100278). Saving model...\n[Training] Epoch 038 | Batch 000/102 | Current Loss: 0.100285\n[Training] Epoch 038 | Batch 010/102 | Current Loss: 0.100230\n[Training] Epoch 038 | Batch 020/102 | Current Loss: 0.100274\n[Training] Epoch 038 | Batch 030/102 | Current Loss: 0.100221\n[Training] Epoch 038 | Batch 040/102 | Current Loss: 0.100325\n[Training] Epoch 038 | Batch 050/102 | Current Loss: 0.100256\n[Training] Epoch 038 | Batch 060/102 | Current Loss: 0.100249\n[Training] Epoch 038 | Batch 070/102 | Current Loss: 0.100382\n[Training] Epoch 038 | Batch 080/102 | Current Loss: 0.100259\n[Training] Epoch 038 | Batch 090/102 | Current Loss: 0.100305\n[Training] Epoch 038 | Batch 100/102 | Current Loss: 0.100231\n\n[Training] Epoch 038 Summary:\n  Avg Loss: 0.100271\n  Current LR: 2.52e-05\n[EarlyStopping] Loss improved (0.100278 → 0.100271). Saving model...\n[Training] Epoch 039 | Batch 000/102 | Current Loss: 0.100210\n[Training] Epoch 039 | Batch 010/102 | Current Loss: 0.100316\n[Training] Epoch 039 | Batch 020/102 | Current Loss: 0.100302\n[Training] Epoch 039 | Batch 030/102 | Current Loss: 0.100246\n[Training] Epoch 039 | Batch 040/102 | Current Loss: 0.100195\n[Training] Epoch 039 | Batch 050/102 | Current Loss: 0.100243\n[Training] Epoch 039 | Batch 060/102 | Current Loss: 0.100248\n[Training] Epoch 039 | Batch 070/102 | Current Loss: 0.100291\n[Training] Epoch 039 | Batch 080/102 | Current Loss: 0.100265\n[Training] Epoch 039 | Batch 090/102 | Current Loss: 0.100295\n[Training] Epoch 039 | Batch 100/102 | Current Loss: 0.100224\n\n[Training] Epoch 039 Summary:\n  Avg Loss: 0.100266\n  Current LR: 7.20e-06\n[EarlyStopping] Loss improved (0.100271 → 0.100266). Saving model...\n[Training] Epoch 040 | Batch 000/102 | Current Loss: 0.100331\n[Training] Epoch 040 | Batch 010/102 | Current Loss: 0.100310\n[Training] Epoch 040 | Batch 020/102 | Current Loss: 0.100284\n[Training] Epoch 040 | Batch 030/102 | Current Loss: 0.100220\n[Training] Epoch 040 | Batch 040/102 | Current Loss: 0.100287\n[Training] Epoch 040 | Batch 050/102 | Current Loss: 0.100262\n[Training] Epoch 040 | Batch 060/102 | Current Loss: 0.100316\n[Training] Epoch 040 | Batch 070/102 | Current Loss: 0.100227\n[Training] Epoch 040 | Batch 080/102 | Current Loss: 0.100184\n[Training] Epoch 040 | Batch 090/102 | Current Loss: 0.100227\n[Training] Epoch 040 | Batch 100/102 | Current Loss: 0.100237\n\n[Training] Epoch 040 Summary:\n  Avg Loss: 0.100263\n  Current LR: 2.55e-04\n[EarlyStopping] Loss improved (0.100266 → 0.100263). Saving model...\n[Training] Epoch 041 | Batch 000/102 | Current Loss: 0.100284\n[Training] Epoch 041 | Batch 010/102 | Current Loss: 0.100213\n[Training] Epoch 041 | Batch 020/102 | Current Loss: 0.100320\n[Training] Epoch 041 | Batch 030/102 | Current Loss: 0.100237\n[Training] Epoch 041 | Batch 040/102 | Current Loss: 0.100285\n[Training] Epoch 041 | Batch 050/102 | Current Loss: 0.100222\n[Training] Epoch 041 | Batch 060/102 | Current Loss: 0.100368\n[Training] Epoch 041 | Batch 070/102 | Current Loss: 0.100246\n[Training] Epoch 041 | Batch 080/102 | Current Loss: 0.100272\n[Training] Epoch 041 | Batch 090/102 | Current Loss: 0.100245\n[Training] Epoch 041 | Batch 100/102 | Current Loss: 0.100303\n\n[Training] Epoch 041 Summary:\n  Avg Loss: 0.100284\n  Current LR: 2.48e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 042 | Batch 000/102 | Current Loss: 0.100241\n[Training] Epoch 042 | Batch 010/102 | Current Loss: 0.100242\n[Training] Epoch 042 | Batch 020/102 | Current Loss: 0.100314\n[Training] Epoch 042 | Batch 030/102 | Current Loss: 0.100376\n[Training] Epoch 042 | Batch 040/102 | Current Loss: 0.100217\n[Training] Epoch 042 | Batch 050/102 | Current Loss: 0.100225\n[Training] Epoch 042 | Batch 060/102 | Current Loss: 0.100295\n[Training] Epoch 042 | Batch 070/102 | Current Loss: 0.100221\n[Training] Epoch 042 | Batch 080/102 | Current Loss: 0.100287\n[Training] Epoch 042 | Batch 090/102 | Current Loss: 0.100373\n[Training] Epoch 042 | Batch 100/102 | Current Loss: 0.100269\n\n[Training] Epoch 042 Summary:\n  Avg Loss: 0.100269\n  Current LR: 2.30e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 043 | Batch 000/102 | Current Loss: 0.100336\n[Training] Epoch 043 | Batch 010/102 | Current Loss: 0.100318\n[Training] Epoch 043 | Batch 020/102 | Current Loss: 0.100336\n[Training] Epoch 043 | Batch 030/102 | Current Loss: 0.100295\n[Training] Epoch 043 | Batch 040/102 | Current Loss: 0.100218\n[Training] Epoch 043 | Batch 050/102 | Current Loss: 0.100190\n[Training] Epoch 043 | Batch 060/102 | Current Loss: 0.100211\n[Training] Epoch 043 | Batch 070/102 | Current Loss: 0.100248\n[Training] Epoch 043 | Batch 080/102 | Current Loss: 0.100352\n[Training] Epoch 043 | Batch 090/102 | Current Loss: 0.100148\n[Training] Epoch 043 | Batch 100/102 | Current Loss: 0.100243\n\n[Training] Epoch 043 Summary:\n  Avg Loss: 0.100257\n  Current LR: 2.02e-04\n[EarlyStopping] Loss improved (0.100263 → 0.100257). Saving model...\n[Training] Epoch 044 | Batch 000/102 | Current Loss: 0.100234\n[Training] Epoch 044 | Batch 010/102 | Current Loss: 0.100311\n[Training] Epoch 044 | Batch 020/102 | Current Loss: 0.100214\n[Training] Epoch 044 | Batch 030/102 | Current Loss: 0.100132\n[Training] Epoch 044 | Batch 040/102 | Current Loss: 0.100252\n[Training] Epoch 044 | Batch 050/102 | Current Loss: 0.100152\n[Training] Epoch 044 | Batch 060/102 | Current Loss: 0.100268\n[Training] Epoch 044 | Batch 070/102 | Current Loss: 0.100236\n[Training] Epoch 044 | Batch 080/102 | Current Loss: 0.100243\n[Training] Epoch 044 | Batch 090/102 | Current Loss: 0.100262\n[Training] Epoch 044 | Batch 100/102 | Current Loss: 0.100238\n\n[Training] Epoch 044 Summary:\n  Avg Loss: 0.100237\n  Current LR: 1.67e-04\n[EarlyStopping] Loss improved (0.100257 → 0.100237). Saving model...\n[Training] Epoch 045 | Batch 000/102 | Current Loss: 0.100186\n[Training] Epoch 045 | Batch 010/102 | Current Loss: 0.100187\n[Training] Epoch 045 | Batch 020/102 | Current Loss: 0.100204\n[Training] Epoch 045 | Batch 030/102 | Current Loss: 0.100194\n[Training] Epoch 045 | Batch 040/102 | Current Loss: 0.100231\n[Training] Epoch 045 | Batch 050/102 | Current Loss: 0.100255\n[Training] Epoch 045 | Batch 060/102 | Current Loss: 0.100188\n[Training] Epoch 045 | Batch 070/102 | Current Loss: 0.100265\n[Training] Epoch 045 | Batch 080/102 | Current Loss: 0.100176\n[Training] Epoch 045 | Batch 090/102 | Current Loss: 0.100235\n[Training] Epoch 045 | Batch 100/102 | Current Loss: 0.100188\n\n[Training] Epoch 045 Summary:\n  Avg Loss: 0.100226\n  Current LR: 1.28e-04\n[EarlyStopping] Loss improved (0.100237 → 0.100226). Saving model...\n[Training] Epoch 046 | Batch 000/102 | Current Loss: 0.100240\n[Training] Epoch 046 | Batch 010/102 | Current Loss: 0.100264\n[Training] Epoch 046 | Batch 020/102 | Current Loss: 0.100303\n[Training] Epoch 046 | Batch 030/102 | Current Loss: 0.100207\n[Training] Epoch 046 | Batch 040/102 | Current Loss: 0.100273\n[Training] Epoch 046 | Batch 050/102 | Current Loss: 0.100240\n[Training] Epoch 046 | Batch 060/102 | Current Loss: 0.100192\n[Training] Epoch 046 | Batch 070/102 | Current Loss: 0.100364\n[Training] Epoch 046 | Batch 080/102 | Current Loss: 0.100190\n[Training] Epoch 046 | Batch 090/102 | Current Loss: 0.100285\n[Training] Epoch 046 | Batch 100/102 | Current Loss: 0.100298\n\n[Training] Epoch 046 Summary:\n  Avg Loss: 0.100218\n  Current LR: 8.86e-05\n[EarlyStopping] Loss improved (0.100226 → 0.100218). Saving model...\n[Training] Epoch 047 | Batch 000/102 | Current Loss: 0.100245\n[Training] Epoch 047 | Batch 010/102 | Current Loss: 0.100152\n[Training] Epoch 047 | Batch 020/102 | Current Loss: 0.100169\n[Training] Epoch 047 | Batch 030/102 | Current Loss: 0.100174\n[Training] Epoch 047 | Batch 040/102 | Current Loss: 0.100203\n[Training] Epoch 047 | Batch 050/102 | Current Loss: 0.100174\n[Training] Epoch 047 | Batch 060/102 | Current Loss: 0.100155\n[Training] Epoch 047 | Batch 070/102 | Current Loss: 0.100232\n[Training] Epoch 047 | Batch 080/102 | Current Loss: 0.100184\n[Training] Epoch 047 | Batch 090/102 | Current Loss: 0.100258\n[Training] Epoch 047 | Batch 100/102 | Current Loss: 0.100265\n\n[Training] Epoch 047 Summary:\n  Avg Loss: 0.100205\n  Current LR: 5.33e-05\n[EarlyStopping] Loss improved (0.100218 → 0.100205). Saving model...\n[Training] Epoch 048 | Batch 000/102 | Current Loss: 0.100234\n[Training] Epoch 048 | Batch 010/102 | Current Loss: 0.100177\n[Training] Epoch 048 | Batch 020/102 | Current Loss: 0.100176\n[Training] Epoch 048 | Batch 030/102 | Current Loss: 0.100294\n[Training] Epoch 048 | Batch 040/102 | Current Loss: 0.100236\n[Training] Epoch 048 | Batch 050/102 | Current Loss: 0.100167\n[Training] Epoch 048 | Batch 060/102 | Current Loss: 0.100232\n[Training] Epoch 048 | Batch 070/102 | Current Loss: 0.100224\n[Training] Epoch 048 | Batch 080/102 | Current Loss: 0.100176\n[Training] Epoch 048 | Batch 090/102 | Current Loss: 0.100170\n[Training] Epoch 048 | Batch 100/102 | Current Loss: 0.100219\n\n[Training] Epoch 048 Summary:\n  Avg Loss: 0.100205\n  Current LR: 2.52e-05\n[EarlyStopping] Loss improved (0.100205 → 0.100205). Saving model...\n[Training] Epoch 049 | Batch 000/102 | Current Loss: 0.100220\n[Training] Epoch 049 | Batch 010/102 | Current Loss: 0.100194\n[Training] Epoch 049 | Batch 020/102 | Current Loss: 0.100222\n[Training] Epoch 049 | Batch 030/102 | Current Loss: 0.100281\n[Training] Epoch 049 | Batch 040/102 | Current Loss: 0.100176\n[Training] Epoch 049 | Batch 050/102 | Current Loss: 0.100123\n[Training] Epoch 049 | Batch 060/102 | Current Loss: 0.100277\n[Training] Epoch 049 | Batch 070/102 | Current Loss: 0.100181\n[Training] Epoch 049 | Batch 080/102 | Current Loss: 0.100249\n[Training] Epoch 049 | Batch 090/102 | Current Loss: 0.100176\n[Training] Epoch 049 | Batch 100/102 | Current Loss: 0.100200\n\n[Training] Epoch 049 Summary:\n  Avg Loss: 0.100197\n  Current LR: 7.20e-06\n[EarlyStopping] Loss improved (0.100205 → 0.100197). Saving model...\n[Training] Epoch 050 | Batch 000/102 | Current Loss: 0.100263\n[Training] Epoch 050 | Batch 010/102 | Current Loss: 0.100173\n[Training] Epoch 050 | Batch 020/102 | Current Loss: 0.100222\n[Training] Epoch 050 | Batch 030/102 | Current Loss: 0.100214\n[Training] Epoch 050 | Batch 040/102 | Current Loss: 0.100206\n[Training] Epoch 050 | Batch 050/102 | Current Loss: 0.100177\n[Training] Epoch 050 | Batch 060/102 | Current Loss: 0.100160\n[Training] Epoch 050 | Batch 070/102 | Current Loss: 0.100171\n[Training] Epoch 050 | Batch 080/102 | Current Loss: 0.100267\n[Training] Epoch 050 | Batch 090/102 | Current Loss: 0.100210\n[Training] Epoch 050 | Batch 100/102 | Current Loss: 0.100138\n\n[Training] Epoch 050 Summary:\n  Avg Loss: 0.100197\n  Current LR: 2.55e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 051 | Batch 000/102 | Current Loss: 0.100240\n[Training] Epoch 051 | Batch 010/102 | Current Loss: 0.100238\n[Training] Epoch 051 | Batch 020/102 | Current Loss: 0.100207\n[Training] Epoch 051 | Batch 030/102 | Current Loss: 0.100190\n[Training] Epoch 051 | Batch 040/102 | Current Loss: 0.100269\n[Training] Epoch 051 | Batch 050/102 | Current Loss: 0.100297\n[Training] Epoch 051 | Batch 060/102 | Current Loss: 0.100201\n[Training] Epoch 051 | Batch 070/102 | Current Loss: 0.100265\n[Training] Epoch 051 | Batch 080/102 | Current Loss: 0.100306\n[Training] Epoch 051 | Batch 090/102 | Current Loss: 0.100198\n[Training] Epoch 051 | Batch 100/102 | Current Loss: 0.100227\n\n[Training] Epoch 051 Summary:\n  Avg Loss: 0.100220\n  Current LR: 2.48e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 052 | Batch 000/102 | Current Loss: 0.100217\n[Training] Epoch 052 | Batch 010/102 | Current Loss: 0.100253\n[Training] Epoch 052 | Batch 020/102 | Current Loss: 0.100196\n[Training] Epoch 052 | Batch 030/102 | Current Loss: 0.100239\n[Training] Epoch 052 | Batch 040/102 | Current Loss: 0.100246\n[Training] Epoch 052 | Batch 050/102 | Current Loss: 0.100141\n[Training] Epoch 052 | Batch 060/102 | Current Loss: 0.100201\n[Training] Epoch 052 | Batch 070/102 | Current Loss: 0.100234\n[Training] Epoch 052 | Batch 080/102 | Current Loss: 0.100228\n[Training] Epoch 052 | Batch 090/102 | Current Loss: 0.100226\n[Training] Epoch 052 | Batch 100/102 | Current Loss: 0.100200\n\n[Training] Epoch 052 Summary:\n  Avg Loss: 0.100216\n  Current LR: 2.30e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 053 | Batch 000/102 | Current Loss: 0.100180\n[Training] Epoch 053 | Batch 010/102 | Current Loss: 0.100119\n[Training] Epoch 053 | Batch 020/102 | Current Loss: 0.100146\n[Training] Epoch 053 | Batch 030/102 | Current Loss: 0.100189\n[Training] Epoch 053 | Batch 040/102 | Current Loss: 0.100163\n[Training] Epoch 053 | Batch 050/102 | Current Loss: 0.100238\n[Training] Epoch 053 | Batch 060/102 | Current Loss: 0.100144\n[Training] Epoch 053 | Batch 070/102 | Current Loss: 0.100162\n[Training] Epoch 053 | Batch 080/102 | Current Loss: 0.100164\n[Training] Epoch 053 | Batch 090/102 | Current Loss: 0.100275\n[Training] Epoch 053 | Batch 100/102 | Current Loss: 0.100263\n\n[Training] Epoch 053 Summary:\n  Avg Loss: 0.100200\n  Current LR: 2.02e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 054 | Batch 000/102 | Current Loss: 0.100236\n[Training] Epoch 054 | Batch 010/102 | Current Loss: 0.100135\n[Training] Epoch 054 | Batch 020/102 | Current Loss: 0.100198\n[Training] Epoch 054 | Batch 030/102 | Current Loss: 0.100182\n[Training] Epoch 054 | Batch 040/102 | Current Loss: 0.100198\n[Training] Epoch 054 | Batch 050/102 | Current Loss: 0.100135\n[Training] Epoch 054 | Batch 060/102 | Current Loss: 0.100167\n[Training] Epoch 054 | Batch 070/102 | Current Loss: 0.100145\n[Training] Epoch 054 | Batch 080/102 | Current Loss: 0.100092\n[Training] Epoch 054 | Batch 090/102 | Current Loss: 0.100158\n[Training] Epoch 054 | Batch 100/102 | Current Loss: 0.100149\n\n[Training] Epoch 054 Summary:\n  Avg Loss: 0.100185\n  Current LR: 1.67e-04\n[EarlyStopping] Loss improved (0.100197 → 0.100185). Saving model...\n[Training] Epoch 055 | Batch 000/102 | Current Loss: 0.100167\n[Training] Epoch 055 | Batch 010/102 | Current Loss: 0.100141\n[Training] Epoch 055 | Batch 020/102 | Current Loss: 0.100251\n[Training] Epoch 055 | Batch 030/102 | Current Loss: 0.100216\n[Training] Epoch 055 | Batch 040/102 | Current Loss: 0.100202\n[Training] Epoch 055 | Batch 050/102 | Current Loss: 0.100097\n[Training] Epoch 055 | Batch 060/102 | Current Loss: 0.100157\n[Training] Epoch 055 | Batch 070/102 | Current Loss: 0.100216\n[Training] Epoch 055 | Batch 080/102 | Current Loss: 0.100247\n[Training] Epoch 055 | Batch 090/102 | Current Loss: 0.100188\n[Training] Epoch 055 | Batch 100/102 | Current Loss: 0.100121\n\n[Training] Epoch 055 Summary:\n  Avg Loss: 0.100182\n  Current LR: 1.28e-04\n[EarlyStopping] Loss improved (0.100185 → 0.100182). Saving model...\n[Training] Epoch 056 | Batch 000/102 | Current Loss: 0.100175\n[Training] Epoch 056 | Batch 010/102 | Current Loss: 0.100131\n[Training] Epoch 056 | Batch 020/102 | Current Loss: 0.100177\n[Training] Epoch 056 | Batch 030/102 | Current Loss: 0.100158\n[Training] Epoch 056 | Batch 040/102 | Current Loss: 0.100210\n[Training] Epoch 056 | Batch 050/102 | Current Loss: 0.100161\n[Training] Epoch 056 | Batch 060/102 | Current Loss: 0.100148\n[Training] Epoch 056 | Batch 070/102 | Current Loss: 0.100147\n[Training] Epoch 056 | Batch 080/102 | Current Loss: 0.100085\n[Training] Epoch 056 | Batch 090/102 | Current Loss: 0.100253\n[Training] Epoch 056 | Batch 100/102 | Current Loss: 0.100170\n\n[Training] Epoch 056 Summary:\n  Avg Loss: 0.100170\n  Current LR: 8.86e-05\n[EarlyStopping] Loss improved (0.100182 → 0.100170). Saving model...\n[Training] Epoch 057 | Batch 000/102 | Current Loss: 0.100144\n[Training] Epoch 057 | Batch 010/102 | Current Loss: 0.100172\n[Training] Epoch 057 | Batch 020/102 | Current Loss: 0.100187\n[Training] Epoch 057 | Batch 030/102 | Current Loss: 0.100144\n[Training] Epoch 057 | Batch 040/102 | Current Loss: 0.100116\n[Training] Epoch 057 | Batch 050/102 | Current Loss: 0.100145\n[Training] Epoch 057 | Batch 060/102 | Current Loss: 0.100192\n[Training] Epoch 057 | Batch 070/102 | Current Loss: 0.100148\n[Training] Epoch 057 | Batch 080/102 | Current Loss: 0.100182\n[Training] Epoch 057 | Batch 090/102 | Current Loss: 0.100202\n[Training] Epoch 057 | Batch 100/102 | Current Loss: 0.100151\n\n[Training] Epoch 057 Summary:\n  Avg Loss: 0.100162\n  Current LR: 5.33e-05\n[EarlyStopping] Loss improved (0.100170 → 0.100162). Saving model...\n[Training] Epoch 058 | Batch 000/102 | Current Loss: 0.100100\n[Training] Epoch 058 | Batch 010/102 | Current Loss: 0.100151\n[Training] Epoch 058 | Batch 020/102 | Current Loss: 0.100209\n[Training] Epoch 058 | Batch 030/102 | Current Loss: 0.100244\n[Training] Epoch 058 | Batch 040/102 | Current Loss: 0.100136\n[Training] Epoch 058 | Batch 050/102 | Current Loss: 0.100202\n[Training] Epoch 058 | Batch 060/102 | Current Loss: 0.100159\n[Training] Epoch 058 | Batch 070/102 | Current Loss: 0.100146\n[Training] Epoch 058 | Batch 080/102 | Current Loss: 0.100142\n[Training] Epoch 058 | Batch 090/102 | Current Loss: 0.100131\n[Training] Epoch 058 | Batch 100/102 | Current Loss: 0.100191\n\n[Training] Epoch 058 Summary:\n  Avg Loss: 0.100159\n  Current LR: 2.52e-05\n[EarlyStopping] Loss improved (0.100162 → 0.100159). Saving model...\n[Training] Epoch 059 | Batch 000/102 | Current Loss: 0.100180\n[Training] Epoch 059 | Batch 010/102 | Current Loss: 0.100144\n[Training] Epoch 059 | Batch 020/102 | Current Loss: 0.100177\n[Training] Epoch 059 | Batch 030/102 | Current Loss: 0.100120\n[Training] Epoch 059 | Batch 040/102 | Current Loss: 0.100096\n[Training] Epoch 059 | Batch 050/102 | Current Loss: 0.100097\n[Training] Epoch 059 | Batch 060/102 | Current Loss: 0.100180\n[Training] Epoch 059 | Batch 070/102 | Current Loss: 0.100162\n[Training] Epoch 059 | Batch 080/102 | Current Loss: 0.100149\n[Training] Epoch 059 | Batch 090/102 | Current Loss: 0.100201\n[Training] Epoch 059 | Batch 100/102 | Current Loss: 0.100126\n\n[Training] Epoch 059 Summary:\n  Avg Loss: 0.100154\n  Current LR: 7.20e-06\n[EarlyStopping] Loss improved (0.100159 → 0.100154). Saving model...\n[Training] Epoch 060 | Batch 000/102 | Current Loss: 0.100185\n[Training] Epoch 060 | Batch 010/102 | Current Loss: 0.100139\n[Training] Epoch 060 | Batch 020/102 | Current Loss: 0.100133\n[Training] Epoch 060 | Batch 030/102 | Current Loss: 0.100148\n[Training] Epoch 060 | Batch 040/102 | Current Loss: 0.100105\n[Training] Epoch 060 | Batch 050/102 | Current Loss: 0.100166\n[Training] Epoch 060 | Batch 060/102 | Current Loss: 0.100120\n[Training] Epoch 060 | Batch 070/102 | Current Loss: 0.100122\n[Training] Epoch 060 | Batch 080/102 | Current Loss: 0.100077\n[Training] Epoch 060 | Batch 090/102 | Current Loss: 0.100180\n[Training] Epoch 060 | Batch 100/102 | Current Loss: 0.100099\n\n[Training] Epoch 060 Summary:\n  Avg Loss: 0.100152\n  Current LR: 2.55e-04\n[EarlyStopping] Loss improved (0.100154 → 0.100152). Saving model...\n[Training] Epoch 061 | Batch 000/102 | Current Loss: 0.100160\n[Training] Epoch 061 | Batch 010/102 | Current Loss: 0.100162\n[Training] Epoch 061 | Batch 020/102 | Current Loss: 0.100152\n[Training] Epoch 061 | Batch 030/102 | Current Loss: 0.100176\n[Training] Epoch 061 | Batch 040/102 | Current Loss: 0.100203\n[Training] Epoch 061 | Batch 050/102 | Current Loss: 0.100172\n[Training] Epoch 061 | Batch 060/102 | Current Loss: 0.100178\n[Training] Epoch 061 | Batch 070/102 | Current Loss: 0.100179\n[Training] Epoch 061 | Batch 080/102 | Current Loss: 0.100142\n[Training] Epoch 061 | Batch 090/102 | Current Loss: 0.100167\n[Training] Epoch 061 | Batch 100/102 | Current Loss: 0.100207\n\n[Training] Epoch 061 Summary:\n  Avg Loss: 0.100176\n  Current LR: 2.48e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 062 | Batch 000/102 | Current Loss: 0.100239\n[Training] Epoch 062 | Batch 010/102 | Current Loss: 0.100102\n[Training] Epoch 062 | Batch 020/102 | Current Loss: 0.100141\n[Training] Epoch 062 | Batch 030/102 | Current Loss: 0.100193\n[Training] Epoch 062 | Batch 040/102 | Current Loss: 0.100210\n[Training] Epoch 062 | Batch 050/102 | Current Loss: 0.100185\n[Training] Epoch 062 | Batch 060/102 | Current Loss: 0.100177\n[Training] Epoch 062 | Batch 070/102 | Current Loss: 0.100173\n[Training] Epoch 062 | Batch 080/102 | Current Loss: 0.100154\n[Training] Epoch 062 | Batch 090/102 | Current Loss: 0.100213\n[Training] Epoch 062 | Batch 100/102 | Current Loss: 0.100183\n\n[Training] Epoch 062 Summary:\n  Avg Loss: 0.100178\n  Current LR: 2.30e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 063 | Batch 000/102 | Current Loss: 0.100131\n[Training] Epoch 063 | Batch 010/102 | Current Loss: 0.100205\n[Training] Epoch 063 | Batch 020/102 | Current Loss: 0.100231\n[Training] Epoch 063 | Batch 030/102 | Current Loss: 0.100168\n[Training] Epoch 063 | Batch 040/102 | Current Loss: 0.100236\n[Training] Epoch 063 | Batch 050/102 | Current Loss: 0.100224\n[Training] Epoch 063 | Batch 060/102 | Current Loss: 0.100172\n[Training] Epoch 063 | Batch 070/102 | Current Loss: 0.100153\n[Training] Epoch 063 | Batch 080/102 | Current Loss: 0.100236\n[Training] Epoch 063 | Batch 090/102 | Current Loss: 0.100182\n[Training] Epoch 063 | Batch 100/102 | Current Loss: 0.100283\n\n[Training] Epoch 063 Summary:\n  Avg Loss: 0.100168\n  Current LR: 2.02e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 064 | Batch 000/102 | Current Loss: 0.100147\n[Training] Epoch 064 | Batch 010/102 | Current Loss: 0.100144\n[Training] Epoch 064 | Batch 020/102 | Current Loss: 0.100113\n[Training] Epoch 064 | Batch 030/102 | Current Loss: 0.100165\n[Training] Epoch 064 | Batch 040/102 | Current Loss: 0.100180\n[Training] Epoch 064 | Batch 050/102 | Current Loss: 0.100171\n[Training] Epoch 064 | Batch 060/102 | Current Loss: 0.100114\n[Training] Epoch 064 | Batch 070/102 | Current Loss: 0.100155\n[Training] Epoch 064 | Batch 080/102 | Current Loss: 0.100173\n[Training] Epoch 064 | Batch 090/102 | Current Loss: 0.100262\n[Training] Epoch 064 | Batch 100/102 | Current Loss: 0.100124\n\n[Training] Epoch 064 Summary:\n  Avg Loss: 0.100159\n  Current LR: 1.67e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 065 | Batch 000/102 | Current Loss: 0.100246\n[Training] Epoch 065 | Batch 010/102 | Current Loss: 0.100112\n[Training] Epoch 065 | Batch 020/102 | Current Loss: 0.100163\n[Training] Epoch 065 | Batch 030/102 | Current Loss: 0.100130\n[Training] Epoch 065 | Batch 040/102 | Current Loss: 0.100100\n[Training] Epoch 065 | Batch 050/102 | Current Loss: 0.100186\n[Training] Epoch 065 | Batch 060/102 | Current Loss: 0.100232\n[Training] Epoch 065 | Batch 070/102 | Current Loss: 0.100179\n[Training] Epoch 065 | Batch 080/102 | Current Loss: 0.100097\n[Training] Epoch 065 | Batch 090/102 | Current Loss: 0.100125\n[Training] Epoch 065 | Batch 100/102 | Current Loss: 0.100202\n\n[Training] Epoch 065 Summary:\n  Avg Loss: 0.100151\n  Current LR: 1.28e-04\n[EarlyStopping] Loss improved (0.100152 → 0.100151). Saving model...\n[Training] Epoch 066 | Batch 000/102 | Current Loss: 0.100195\n[Training] Epoch 066 | Batch 010/102 | Current Loss: 0.100094\n[Training] Epoch 066 | Batch 020/102 | Current Loss: 0.100174\n[Training] Epoch 066 | Batch 030/102 | Current Loss: 0.100041\n[Training] Epoch 066 | Batch 040/102 | Current Loss: 0.100129\n[Training] Epoch 066 | Batch 050/102 | Current Loss: 0.100095\n[Training] Epoch 066 | Batch 060/102 | Current Loss: 0.100167\n[Training] Epoch 066 | Batch 070/102 | Current Loss: 0.100121\n[Training] Epoch 066 | Batch 080/102 | Current Loss: 0.100144\n[Training] Epoch 066 | Batch 090/102 | Current Loss: 0.100164\n[Training] Epoch 066 | Batch 100/102 | Current Loss: 0.100024\n\n[Training] Epoch 066 Summary:\n  Avg Loss: 0.100137\n  Current LR: 8.86e-05\n[EarlyStopping] Loss improved (0.100151 → 0.100137). Saving model...\n[Training] Epoch 067 | Batch 000/102 | Current Loss: 0.100112\n[Training] Epoch 067 | Batch 010/102 | Current Loss: 0.100102\n[Training] Epoch 067 | Batch 020/102 | Current Loss: 0.100118\n[Training] Epoch 067 | Batch 030/102 | Current Loss: 0.100091\n[Training] Epoch 067 | Batch 040/102 | Current Loss: 0.100153\n[Training] Epoch 067 | Batch 050/102 | Current Loss: 0.100128\n[Training] Epoch 067 | Batch 060/102 | Current Loss: 0.100168\n[Training] Epoch 067 | Batch 070/102 | Current Loss: 0.100163\n[Training] Epoch 067 | Batch 080/102 | Current Loss: 0.100088\n[Training] Epoch 067 | Batch 090/102 | Current Loss: 0.100130\n[Training] Epoch 067 | Batch 100/102 | Current Loss: 0.100151\n\n[Training] Epoch 067 Summary:\n  Avg Loss: 0.100131\n  Current LR: 5.33e-05\n[EarlyStopping] Loss improved (0.100137 → 0.100131). Saving model...\n[Training] Epoch 068 | Batch 000/102 | Current Loss: 0.100124\n[Training] Epoch 068 | Batch 010/102 | Current Loss: 0.100120\n[Training] Epoch 068 | Batch 020/102 | Current Loss: 0.100166\n[Training] Epoch 068 | Batch 030/102 | Current Loss: 0.100076\n[Training] Epoch 068 | Batch 040/102 | Current Loss: 0.100119\n[Training] Epoch 068 | Batch 050/102 | Current Loss: 0.100049\n[Training] Epoch 068 | Batch 060/102 | Current Loss: 0.100135\n[Training] Epoch 068 | Batch 070/102 | Current Loss: 0.100096\n[Training] Epoch 068 | Batch 080/102 | Current Loss: 0.100152\n[Training] Epoch 068 | Batch 090/102 | Current Loss: 0.100130\n[Training] Epoch 068 | Batch 100/102 | Current Loss: 0.100177\n\n[Training] Epoch 068 Summary:\n  Avg Loss: 0.100125\n  Current LR: 2.52e-05\n[EarlyStopping] Loss improved (0.100131 → 0.100125). Saving model...\n[Training] Epoch 069 | Batch 000/102 | Current Loss: 0.100174\n[Training] Epoch 069 | Batch 010/102 | Current Loss: 0.100134\n[Training] Epoch 069 | Batch 020/102 | Current Loss: 0.100037\n[Training] Epoch 069 | Batch 030/102 | Current Loss: 0.100114\n[Training] Epoch 069 | Batch 040/102 | Current Loss: 0.100163\n[Training] Epoch 069 | Batch 050/102 | Current Loss: 0.100118\n[Training] Epoch 069 | Batch 060/102 | Current Loss: 0.100146\n[Training] Epoch 069 | Batch 070/102 | Current Loss: 0.100084\n[Training] Epoch 069 | Batch 080/102 | Current Loss: 0.100096\n[Training] Epoch 069 | Batch 090/102 | Current Loss: 0.100093\n[Training] Epoch 069 | Batch 100/102 | Current Loss: 0.100138\n\n[Training] Epoch 069 Summary:\n  Avg Loss: 0.100127\n  Current LR: 7.20e-06\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 070 | Batch 000/102 | Current Loss: 0.100116\n[Training] Epoch 070 | Batch 010/102 | Current Loss: 0.100146\n[Training] Epoch 070 | Batch 020/102 | Current Loss: 0.100137\n[Training] Epoch 070 | Batch 030/102 | Current Loss: 0.100099\n[Training] Epoch 070 | Batch 040/102 | Current Loss: 0.100114\n[Training] Epoch 070 | Batch 050/102 | Current Loss: 0.100117\n[Training] Epoch 070 | Batch 060/102 | Current Loss: 0.100127\n[Training] Epoch 070 | Batch 070/102 | Current Loss: 0.100039\n[Training] Epoch 070 | Batch 080/102 | Current Loss: 0.100124\n[Training] Epoch 070 | Batch 090/102 | Current Loss: 0.100121\n[Training] Epoch 070 | Batch 100/102 | Current Loss: 0.100099\n\n[Training] Epoch 070 Summary:\n  Avg Loss: 0.100124\n  Current LR: 2.55e-04\n[EarlyStopping] Loss improved (0.100125 → 0.100124). Saving model...\n[Training] Epoch 071 | Batch 000/102 | Current Loss: 0.100122\n[Training] Epoch 071 | Batch 010/102 | Current Loss: 0.100157\n[Training] Epoch 071 | Batch 020/102 | Current Loss: 0.100220\n[Training] Epoch 071 | Batch 030/102 | Current Loss: 0.100106\n[Training] Epoch 071 | Batch 040/102 | Current Loss: 0.100094\n[Training] Epoch 071 | Batch 050/102 | Current Loss: 0.100151\n[Training] Epoch 071 | Batch 060/102 | Current Loss: 0.100172\n[Training] Epoch 071 | Batch 070/102 | Current Loss: 0.100157\n[Training] Epoch 071 | Batch 080/102 | Current Loss: 0.100167\n[Training] Epoch 071 | Batch 090/102 | Current Loss: 0.100125\n[Training] Epoch 071 | Batch 100/102 | Current Loss: 0.100157\n\n[Training] Epoch 071 Summary:\n  Avg Loss: 0.100151\n  Current LR: 2.48e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 072 | Batch 000/102 | Current Loss: 0.100174\n[Training] Epoch 072 | Batch 010/102 | Current Loss: 0.100148\n[Training] Epoch 072 | Batch 020/102 | Current Loss: 0.100145\n[Training] Epoch 072 | Batch 030/102 | Current Loss: 0.100136\n[Training] Epoch 072 | Batch 040/102 | Current Loss: 0.100167\n[Training] Epoch 072 | Batch 050/102 | Current Loss: 0.100194\n[Training] Epoch 072 | Batch 060/102 | Current Loss: 0.100155\n[Training] Epoch 072 | Batch 070/102 | Current Loss: 0.100110\n[Training] Epoch 072 | Batch 080/102 | Current Loss: 0.100091\n[Training] Epoch 072 | Batch 090/102 | Current Loss: 0.100239\n[Training] Epoch 072 | Batch 100/102 | Current Loss: 0.100048\n\n[Training] Epoch 072 Summary:\n  Avg Loss: 0.100141\n  Current LR: 2.30e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 073 | Batch 000/102 | Current Loss: 0.100182\n[Training] Epoch 073 | Batch 010/102 | Current Loss: 0.100118\n[Training] Epoch 073 | Batch 020/102 | Current Loss: 0.100051\n[Training] Epoch 073 | Batch 030/102 | Current Loss: 0.100119\n[Training] Epoch 073 | Batch 040/102 | Current Loss: 0.100180\n[Training] Epoch 073 | Batch 050/102 | Current Loss: 0.100176\n[Training] Epoch 073 | Batch 060/102 | Current Loss: 0.100145\n[Training] Epoch 073 | Batch 070/102 | Current Loss: 0.100156\n[Training] Epoch 073 | Batch 080/102 | Current Loss: 0.100144\n[Training] Epoch 073 | Batch 090/102 | Current Loss: 0.100106\n[Training] Epoch 073 | Batch 100/102 | Current Loss: 0.100138\n\n[Training] Epoch 073 Summary:\n  Avg Loss: 0.100133\n  Current LR: 2.02e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 074 | Batch 000/102 | Current Loss: 0.100155\n[Training] Epoch 074 | Batch 010/102 | Current Loss: 0.100092\n[Training] Epoch 074 | Batch 020/102 | Current Loss: 0.100148\n[Training] Epoch 074 | Batch 030/102 | Current Loss: 0.100099\n[Training] Epoch 074 | Batch 040/102 | Current Loss: 0.100124\n[Training] Epoch 074 | Batch 050/102 | Current Loss: 0.100106\n[Training] Epoch 074 | Batch 060/102 | Current Loss: 0.100121\n[Training] Epoch 074 | Batch 070/102 | Current Loss: 0.100131\n[Training] Epoch 074 | Batch 080/102 | Current Loss: 0.100131\n[Training] Epoch 074 | Batch 090/102 | Current Loss: 0.100098\n[Training] Epoch 074 | Batch 100/102 | Current Loss: 0.100244\n\n[Training] Epoch 074 Summary:\n  Avg Loss: 0.100127\n  Current LR: 1.67e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 075 | Batch 000/102 | Current Loss: 0.100083\n[Training] Epoch 075 | Batch 010/102 | Current Loss: 0.100140\n[Training] Epoch 075 | Batch 020/102 | Current Loss: 0.100055\n[Training] Epoch 075 | Batch 030/102 | Current Loss: 0.100104\n[Training] Epoch 075 | Batch 040/102 | Current Loss: 0.100077\n[Training] Epoch 075 | Batch 050/102 | Current Loss: 0.100123\n[Training] Epoch 075 | Batch 060/102 | Current Loss: 0.100112\n[Training] Epoch 075 | Batch 070/102 | Current Loss: 0.100075\n[Training] Epoch 075 | Batch 080/102 | Current Loss: 0.100115\n[Training] Epoch 075 | Batch 090/102 | Current Loss: 0.100182\n[Training] Epoch 075 | Batch 100/102 | Current Loss: 0.100141\n\n[Training] Epoch 075 Summary:\n  Avg Loss: 0.100122\n  Current LR: 1.28e-04\n[EarlyStopping] Loss improved (0.100124 → 0.100122). Saving model...\n[Training] Epoch 076 | Batch 000/102 | Current Loss: 0.100096\n[Training] Epoch 076 | Batch 010/102 | Current Loss: 0.100164\n[Training] Epoch 076 | Batch 020/102 | Current Loss: 0.100067\n[Training] Epoch 076 | Batch 030/102 | Current Loss: 0.100161\n[Training] Epoch 076 | Batch 040/102 | Current Loss: 0.100112\n[Training] Epoch 076 | Batch 050/102 | Current Loss: 0.100134\n[Training] Epoch 076 | Batch 060/102 | Current Loss: 0.100141\n[Training] Epoch 076 | Batch 070/102 | Current Loss: 0.100017\n[Training] Epoch 076 | Batch 080/102 | Current Loss: 0.100098\n[Training] Epoch 076 | Batch 090/102 | Current Loss: 0.100118\n[Training] Epoch 076 | Batch 100/102 | Current Loss: 0.100094\n\n[Training] Epoch 076 Summary:\n  Avg Loss: 0.100120\n  Current LR: 8.86e-05\n[EarlyStopping] Loss improved (0.100122 → 0.100120). Saving model...\n[Training] Epoch 077 | Batch 000/102 | Current Loss: 0.100190\n[Training] Epoch 077 | Batch 010/102 | Current Loss: 0.100094\n[Training] Epoch 077 | Batch 020/102 | Current Loss: 0.100113\n[Training] Epoch 077 | Batch 030/102 | Current Loss: 0.100074\n[Training] Epoch 077 | Batch 040/102 | Current Loss: 0.100194\n[Training] Epoch 077 | Batch 050/102 | Current Loss: 0.100157\n[Training] Epoch 077 | Batch 060/102 | Current Loss: 0.100040\n[Training] Epoch 077 | Batch 070/102 | Current Loss: 0.100104\n[Training] Epoch 077 | Batch 080/102 | Current Loss: 0.100182\n[Training] Epoch 077 | Batch 090/102 | Current Loss: 0.100049\n[Training] Epoch 077 | Batch 100/102 | Current Loss: 0.100105\n\n[Training] Epoch 077 Summary:\n  Avg Loss: 0.100116\n  Current LR: 5.33e-05\n[EarlyStopping] Loss improved (0.100120 → 0.100116). Saving model...\n[Training] Epoch 078 | Batch 000/102 | Current Loss: 0.100118\n[Training] Epoch 078 | Batch 010/102 | Current Loss: 0.100120\n[Training] Epoch 078 | Batch 020/102 | Current Loss: 0.100144\n[Training] Epoch 078 | Batch 030/102 | Current Loss: 0.100063\n[Training] Epoch 078 | Batch 040/102 | Current Loss: 0.100082\n[Training] Epoch 078 | Batch 050/102 | Current Loss: 0.100094\n[Training] Epoch 078 | Batch 060/102 | Current Loss: 0.100018\n[Training] Epoch 078 | Batch 070/102 | Current Loss: 0.100133\n[Training] Epoch 078 | Batch 080/102 | Current Loss: 0.100051\n[Training] Epoch 078 | Batch 090/102 | Current Loss: 0.100146\n[Training] Epoch 078 | Batch 100/102 | Current Loss: 0.100088\n\n[Training] Epoch 078 Summary:\n  Avg Loss: 0.100105\n  Current LR: 2.52e-05\n[EarlyStopping] Loss improved (0.100116 → 0.100105). Saving model...\n[Training] Epoch 079 | Batch 000/102 | Current Loss: 0.100055\n[Training] Epoch 079 | Batch 010/102 | Current Loss: 0.100114\n[Training] Epoch 079 | Batch 020/102 | Current Loss: 0.100090\n[Training] Epoch 079 | Batch 030/102 | Current Loss: 0.100114\n[Training] Epoch 079 | Batch 040/102 | Current Loss: 0.100090\n[Training] Epoch 079 | Batch 050/102 | Current Loss: 0.100146\n[Training] Epoch 079 | Batch 060/102 | Current Loss: 0.100082\n[Training] Epoch 079 | Batch 070/102 | Current Loss: 0.100150\n[Training] Epoch 079 | Batch 080/102 | Current Loss: 0.100077\n[Training] Epoch 079 | Batch 090/102 | Current Loss: 0.100141\n[Training] Epoch 079 | Batch 100/102 | Current Loss: 0.100103\n\n[Training] Epoch 079 Summary:\n  Avg Loss: 0.100098\n  Current LR: 7.20e-06\n[EarlyStopping] Loss improved (0.100105 → 0.100098). Saving model...\n[Training] Epoch 080 | Batch 000/102 | Current Loss: 0.100193\n[Training] Epoch 080 | Batch 010/102 | Current Loss: 0.100058\n[Training] Epoch 080 | Batch 020/102 | Current Loss: 0.100092\n[Training] Epoch 080 | Batch 030/102 | Current Loss: 0.100084\n[Training] Epoch 080 | Batch 040/102 | Current Loss: 0.100068\n[Training] Epoch 080 | Batch 050/102 | Current Loss: 0.100098\n[Training] Epoch 080 | Batch 060/102 | Current Loss: 0.100098\n[Training] Epoch 080 | Batch 070/102 | Current Loss: 0.100091\n[Training] Epoch 080 | Batch 080/102 | Current Loss: 0.100043\n[Training] Epoch 080 | Batch 090/102 | Current Loss: 0.100099\n[Training] Epoch 080 | Batch 100/102 | Current Loss: 0.100121\n\n[Training] Epoch 080 Summary:\n  Avg Loss: 0.100097\n  Current LR: 2.55e-04\n[EarlyStopping] Loss improved (0.100098 → 0.100097). Saving model...\n[Training] Epoch 081 | Batch 000/102 | Current Loss: 0.100099\n[Training] Epoch 081 | Batch 010/102 | Current Loss: 0.100072\n[Training] Epoch 081 | Batch 020/102 | Current Loss: 0.100156\n[Training] Epoch 081 | Batch 030/102 | Current Loss: 0.100202\n[Training] Epoch 081 | Batch 040/102 | Current Loss: 0.100133\n[Training] Epoch 081 | Batch 050/102 | Current Loss: 0.100165\n[Training] Epoch 081 | Batch 060/102 | Current Loss: 0.100143\n[Training] Epoch 081 | Batch 070/102 | Current Loss: 0.100168\n[Training] Epoch 081 | Batch 080/102 | Current Loss: 0.100113\n[Training] Epoch 081 | Batch 090/102 | Current Loss: 0.100061\n[Training] Epoch 081 | Batch 100/102 | Current Loss: 0.100095\n\n[Training] Epoch 081 Summary:\n  Avg Loss: 0.100130\n  Current LR: 2.48e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 082 | Batch 000/102 | Current Loss: 0.100051\n[Training] Epoch 082 | Batch 010/102 | Current Loss: 0.100102\n[Training] Epoch 082 | Batch 020/102 | Current Loss: 0.100154\n[Training] Epoch 082 | Batch 030/102 | Current Loss: 0.100095\n[Training] Epoch 082 | Batch 040/102 | Current Loss: 0.100114\n[Training] Epoch 082 | Batch 050/102 | Current Loss: 0.100144\n[Training] Epoch 082 | Batch 060/102 | Current Loss: 0.100148\n[Training] Epoch 082 | Batch 070/102 | Current Loss: 0.100140\n[Training] Epoch 082 | Batch 080/102 | Current Loss: 0.100185\n[Training] Epoch 082 | Batch 090/102 | Current Loss: 0.100091\n[Training] Epoch 082 | Batch 100/102 | Current Loss: 0.100105\n\n[Training] Epoch 082 Summary:\n  Avg Loss: 0.100129\n  Current LR: 2.30e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 083 | Batch 000/102 | Current Loss: 0.100183\n[Training] Epoch 083 | Batch 010/102 | Current Loss: 0.100122\n[Training] Epoch 083 | Batch 020/102 | Current Loss: 0.100207\n[Training] Epoch 083 | Batch 030/102 | Current Loss: 0.100105\n[Training] Epoch 083 | Batch 040/102 | Current Loss: 0.100102\n[Training] Epoch 083 | Batch 050/102 | Current Loss: 0.100057\n[Training] Epoch 083 | Batch 060/102 | Current Loss: 0.100092\n[Training] Epoch 083 | Batch 070/102 | Current Loss: 0.100166\n[Training] Epoch 083 | Batch 080/102 | Current Loss: 0.100101\n[Training] Epoch 083 | Batch 090/102 | Current Loss: 0.100166\n[Training] Epoch 083 | Batch 100/102 | Current Loss: 0.100207\n\n[Training] Epoch 083 Summary:\n  Avg Loss: 0.100121\n  Current LR: 2.02e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 084 | Batch 000/102 | Current Loss: 0.100060\n[Training] Epoch 084 | Batch 010/102 | Current Loss: 0.100093\n[Training] Epoch 084 | Batch 020/102 | Current Loss: 0.100128\n[Training] Epoch 084 | Batch 030/102 | Current Loss: 0.100076\n[Training] Epoch 084 | Batch 040/102 | Current Loss: 0.100064\n[Training] Epoch 084 | Batch 050/102 | Current Loss: 0.100116\n[Training] Epoch 084 | Batch 060/102 | Current Loss: 0.100088\n[Training] Epoch 084 | Batch 070/102 | Current Loss: 0.100133\n[Training] Epoch 084 | Batch 080/102 | Current Loss: 0.100106\n[Training] Epoch 084 | Batch 090/102 | Current Loss: 0.100171\n[Training] Epoch 084 | Batch 100/102 | Current Loss: 0.100054\n\n[Training] Epoch 084 Summary:\n  Avg Loss: 0.100108\n  Current LR: 1.67e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 085 | Batch 000/102 | Current Loss: 0.100025\n[Training] Epoch 085 | Batch 010/102 | Current Loss: 0.100103\n[Training] Epoch 085 | Batch 020/102 | Current Loss: 0.100129\n[Training] Epoch 085 | Batch 030/102 | Current Loss: 0.100064\n[Training] Epoch 085 | Batch 040/102 | Current Loss: 0.100122\n[Training] Epoch 085 | Batch 050/102 | Current Loss: 0.100076\n[Training] Epoch 085 | Batch 060/102 | Current Loss: 0.100052\n[Training] Epoch 085 | Batch 070/102 | Current Loss: 0.100123\n[Training] Epoch 085 | Batch 080/102 | Current Loss: 0.100092\n[Training] Epoch 085 | Batch 090/102 | Current Loss: 0.100017\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 16:38:36,349] Trial 8 finished with value: 0.10009658994043574 and parameters: {'lr': 0.000254511520589056, 'batch_size': 128, 'bottleneck_width': 1024, 'dropout_rate': 0.41596538820277085, 'alpha': 0.40669237845820655, 'weight_decay': 5.3058970800461645e-05}. Best is trial 8 with value: 0.10009658994043574.\n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 085 | Batch 100/102 | Current Loss: 0.100093\n\n[Training] Epoch 085 Summary:\n  Avg Loss: 0.100105\n  Current LR: 1.28e-04\n[EarlyStopping] Counter: 5/5\n\n[Training] Early stopping triggered\n\n[Optuna] Trial 8 completed with best loss: 0.100097\n\n==================================================\nStarting Optuna Trial 9\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 5.508761126701689e-05\n  batch_size: 256\n  bottleneck_width: 1024\n  dropout_rate: 0.48859735137649907\n  alpha: 0.7150027095890954\n  weight_decay: 4.4033611599766264e-05\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 256, Total batches: 51\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 1024\n  - Dropout rate: 0.48859735137649907\n[Model] Architecture initialized successfully\n[Model] Total parameters: 2,396,992\n[Optimizer] Initialized with lr=5.51e-05, weight_decay=4.40e-05\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.7150027095890954\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/051 | Current Loss: 1.855055\n[Training] Epoch 001 | Batch 010/051 | Current Loss: 0.738192\n[Training] Epoch 001 | Batch 020/051 | Current Loss: 0.441535\n[Training] Epoch 001 | Batch 030/051 | Current Loss: 0.314252\n[Training] Epoch 001 | Batch 040/051 | Current Loss: 0.241536\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 16:38:51,424] Trial 9 pruned. \n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 001 | Batch 050/051 | Current Loss: 0.204330\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.529125\n  Current LR: 5.38e-05\n[EarlyStopping] Loss improved (inf → 0.529125). Saving model...\n\n[Optuna] Trial pruned\n\n==================================================\nStarting Optuna Trial 10\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.00032621654698148115\n  batch_size: 128\n  bottleneck_width: 1024\n  dropout_rate: 0.25530728358474364\n  alpha: 0.4057686474355913\n  weight_decay: 2.834417625395888e-05\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 128, Total batches: 102\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 1024\n  - Dropout rate: 0.25530728358474364\n[Model] Architecture initialized successfully\n[Model] Total parameters: 2,396,992\n[Optimizer] Initialized with lr=3.26e-04, weight_decay=2.83e-05\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.4057686474355913\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/102 | Current Loss: 1.282983\n[Training] Epoch 001 | Batch 010/102 | Current Loss: 0.168081\n[Training] Epoch 001 | Batch 020/102 | Current Loss: 0.125575\n[Training] Epoch 001 | Batch 030/102 | Current Loss: 0.115324\n[Training] Epoch 001 | Batch 040/102 | Current Loss: 0.112464\n[Training] Epoch 001 | Batch 050/102 | Current Loss: 0.108878\n[Training] Epoch 001 | Batch 060/102 | Current Loss: 0.109223\n[Training] Epoch 001 | Batch 070/102 | Current Loss: 0.107711\n[Training] Epoch 001 | Batch 080/102 | Current Loss: 0.106154\n[Training] Epoch 001 | Batch 090/102 | Current Loss: 0.104897\n[Training] Epoch 001 | Batch 100/102 | Current Loss: 0.105731\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.145263\n  Current LR: 3.18e-04\n[EarlyStopping] Loss improved (inf → 0.145263). Saving model...\n[Training] Epoch 002 | Batch 000/102 | Current Loss: 0.107557\n[Training] Epoch 002 | Batch 010/102 | Current Loss: 0.105608\n[Training] Epoch 002 | Batch 020/102 | Current Loss: 0.104219\n[Training] Epoch 002 | Batch 030/102 | Current Loss: 0.103673\n[Training] Epoch 002 | Batch 040/102 | Current Loss: 0.104696\n[Training] Epoch 002 | Batch 050/102 | Current Loss: 0.103644\n[Training] Epoch 002 | Batch 060/102 | Current Loss: 0.104070\n[Training] Epoch 002 | Batch 070/102 | Current Loss: 0.103273\n[Training] Epoch 002 | Batch 080/102 | Current Loss: 0.103058\n[Training] Epoch 002 | Batch 090/102 | Current Loss: 0.103180\n[Training] Epoch 002 | Batch 100/102 | Current Loss: 0.102716\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.104064\n  Current LR: 2.95e-04\n[EarlyStopping] Loss improved (0.145263 → 0.104064). Saving model...\n[Training] Epoch 003 | Batch 000/102 | Current Loss: 0.102778\n[Training] Epoch 003 | Batch 010/102 | Current Loss: 0.102732\n[Training] Epoch 003 | Batch 020/102 | Current Loss: 0.103288\n[Training] Epoch 003 | Batch 030/102 | Current Loss: 0.102974\n[Training] Epoch 003 | Batch 040/102 | Current Loss: 0.102452\n[Training] Epoch 003 | Batch 050/102 | Current Loss: 0.102816\n[Training] Epoch 003 | Batch 060/102 | Current Loss: 0.102573\n[Training] Epoch 003 | Batch 070/102 | Current Loss: 0.102522\n[Training] Epoch 003 | Batch 080/102 | Current Loss: 0.102399\n[Training] Epoch 003 | Batch 090/102 | Current Loss: 0.104706\n[Training] Epoch 003 | Batch 100/102 | Current Loss: 0.102200\n\n[Training] Epoch 003 Summary:\n  Avg Loss: 0.102911\n  Current LR: 2.59e-04\n[EarlyStopping] Loss improved (0.104064 → 0.102911). Saving model...\n[Training] Epoch 004 | Batch 000/102 | Current Loss: 0.102288\n[Training] Epoch 004 | Batch 010/102 | Current Loss: 0.102114\n[Training] Epoch 004 | Batch 020/102 | Current Loss: 0.105846\n[Training] Epoch 004 | Batch 030/102 | Current Loss: 0.102281\n[Training] Epoch 004 | Batch 040/102 | Current Loss: 0.102183\n[Training] Epoch 004 | Batch 050/102 | Current Loss: 0.102331\n[Training] Epoch 004 | Batch 060/102 | Current Loss: 0.102170\n[Training] Epoch 004 | Batch 070/102 | Current Loss: 0.102104\n[Training] Epoch 004 | Batch 080/102 | Current Loss: 0.101921\n[Training] Epoch 004 | Batch 090/102 | Current Loss: 0.102059\n[Training] Epoch 004 | Batch 100/102 | Current Loss: 0.102049\n\n[Training] Epoch 004 Summary:\n  Avg Loss: 0.102255\n  Current LR: 2.14e-04\n[EarlyStopping] Loss improved (0.102911 → 0.102255). Saving model...\n[Training] Epoch 005 | Batch 000/102 | Current Loss: 0.102439\n[Training] Epoch 005 | Batch 010/102 | Current Loss: 0.102178\n[Training] Epoch 005 | Batch 020/102 | Current Loss: 0.101775\n[Training] Epoch 005 | Batch 030/102 | Current Loss: 0.101576\n[Training] Epoch 005 | Batch 040/102 | Current Loss: 0.101437\n[Training] Epoch 005 | Batch 050/102 | Current Loss: 0.101761\n[Training] Epoch 005 | Batch 060/102 | Current Loss: 0.101786\n[Training] Epoch 005 | Batch 070/102 | Current Loss: 0.101733\n[Training] Epoch 005 | Batch 080/102 | Current Loss: 0.101588\n[Training] Epoch 005 | Batch 090/102 | Current Loss: 0.102161\n[Training] Epoch 005 | Batch 100/102 | Current Loss: 0.102107\n\n[Training] Epoch 005 Summary:\n  Avg Loss: 0.101915\n  Current LR: 1.64e-04\n[EarlyStopping] Loss improved (0.102255 → 0.101915). Saving model...\n[Training] Epoch 006 | Batch 000/102 | Current Loss: 0.101974\n[Training] Epoch 006 | Batch 010/102 | Current Loss: 0.101522\n[Training] Epoch 006 | Batch 020/102 | Current Loss: 0.101376\n[Training] Epoch 006 | Batch 030/102 | Current Loss: 0.101554\n[Training] Epoch 006 | Batch 040/102 | Current Loss: 0.101391\n[Training] Epoch 006 | Batch 050/102 | Current Loss: 0.101590\n[Training] Epoch 006 | Batch 060/102 | Current Loss: 0.101306\n[Training] Epoch 006 | Batch 070/102 | Current Loss: 0.101333\n[Training] Epoch 006 | Batch 080/102 | Current Loss: 0.101780\n[Training] Epoch 006 | Batch 090/102 | Current Loss: 0.101961\n[Training] Epoch 006 | Batch 100/102 | Current Loss: 0.101321\n\n[Training] Epoch 006 Summary:\n  Avg Loss: 0.101533\n  Current LR: 1.13e-04\n[EarlyStopping] Loss improved (0.101915 → 0.101533). Saving model...\n[Training] Epoch 007 | Batch 000/102 | Current Loss: 0.101356\n[Training] Epoch 007 | Batch 010/102 | Current Loss: 0.101629\n[Training] Epoch 007 | Batch 020/102 | Current Loss: 0.101268\n[Training] Epoch 007 | Batch 030/102 | Current Loss: 0.101272\n[Training] Epoch 007 | Batch 040/102 | Current Loss: 0.101172\n[Training] Epoch 007 | Batch 050/102 | Current Loss: 0.101520\n[Training] Epoch 007 | Batch 060/102 | Current Loss: 0.101111\n[Training] Epoch 007 | Batch 070/102 | Current Loss: 0.101324\n[Training] Epoch 007 | Batch 080/102 | Current Loss: 0.101183\n[Training] Epoch 007 | Batch 090/102 | Current Loss: 0.101517\n[Training] Epoch 007 | Batch 100/102 | Current Loss: 0.101239\n\n[Training] Epoch 007 Summary:\n  Avg Loss: 0.101325\n  Current LR: 6.80e-05\n[EarlyStopping] Loss improved (0.101533 → 0.101325). Saving model...\n[Training] Epoch 008 | Batch 000/102 | Current Loss: 0.101169\n[Training] Epoch 008 | Batch 010/102 | Current Loss: 0.101216\n[Training] Epoch 008 | Batch 020/102 | Current Loss: 0.101046\n[Training] Epoch 008 | Batch 030/102 | Current Loss: 0.101139\n[Training] Epoch 008 | Batch 040/102 | Current Loss: 0.101585\n[Training] Epoch 008 | Batch 050/102 | Current Loss: 0.101232\n[Training] Epoch 008 | Batch 060/102 | Current Loss: 0.101106\n[Training] Epoch 008 | Batch 070/102 | Current Loss: 0.101246\n[Training] Epoch 008 | Batch 080/102 | Current Loss: 0.101218\n[Training] Epoch 008 | Batch 090/102 | Current Loss: 0.101349\n[Training] Epoch 008 | Batch 100/102 | Current Loss: 0.101134\n\n[Training] Epoch 008 Summary:\n  Avg Loss: 0.101298\n  Current LR: 3.21e-05\n[EarlyStopping] Loss improved (0.101325 → 0.101298). Saving model...\n[Training] Epoch 009 | Batch 000/102 | Current Loss: 0.101146\n[Training] Epoch 009 | Batch 010/102 | Current Loss: 0.101261\n[Training] Epoch 009 | Batch 020/102 | Current Loss: 0.101125\n[Training] Epoch 009 | Batch 030/102 | Current Loss: 0.101540\n[Training] Epoch 009 | Batch 040/102 | Current Loss: 0.101148\n[Training] Epoch 009 | Batch 050/102 | Current Loss: 0.101421\n[Training] Epoch 009 | Batch 060/102 | Current Loss: 0.101533\n[Training] Epoch 009 | Batch 070/102 | Current Loss: 0.101045\n[Training] Epoch 009 | Batch 080/102 | Current Loss: 0.101745\n[Training] Epoch 009 | Batch 090/102 | Current Loss: 0.101190\n[Training] Epoch 009 | Batch 100/102 | Current Loss: 0.101300\n\n[Training] Epoch 009 Summary:\n  Avg Loss: 0.101232\n  Current LR: 8.96e-06\n[EarlyStopping] Loss improved (0.101298 → 0.101232). Saving model...\n[Training] Epoch 010 | Batch 000/102 | Current Loss: 0.101104\n[Training] Epoch 010 | Batch 010/102 | Current Loss: 0.101095\n[Training] Epoch 010 | Batch 020/102 | Current Loss: 0.101039\n[Training] Epoch 010 | Batch 030/102 | Current Loss: 0.101114\n[Training] Epoch 010 | Batch 040/102 | Current Loss: 0.101096\n[Training] Epoch 010 | Batch 050/102 | Current Loss: 0.101139\n[Training] Epoch 010 | Batch 060/102 | Current Loss: 0.101063\n[Training] Epoch 010 | Batch 070/102 | Current Loss: 0.101176\n[Training] Epoch 010 | Batch 080/102 | Current Loss: 0.101209\n[Training] Epoch 010 | Batch 090/102 | Current Loss: 0.101649\n[Training] Epoch 010 | Batch 100/102 | Current Loss: 0.101177\n\n[Training] Epoch 010 Summary:\n  Avg Loss: 0.101174\n  Current LR: 3.26e-04\n[EarlyStopping] Loss improved (0.101232 → 0.101174). Saving model...\n[Training] Epoch 011 | Batch 000/102 | Current Loss: 0.101138\n[Training] Epoch 011 | Batch 010/102 | Current Loss: 0.101173\n[Training] Epoch 011 | Batch 020/102 | Current Loss: 0.101320\n[Training] Epoch 011 | Batch 030/102 | Current Loss: 0.101503\n[Training] Epoch 011 | Batch 040/102 | Current Loss: 0.101117\n[Training] Epoch 011 | Batch 050/102 | Current Loss: 0.101123\n[Training] Epoch 011 | Batch 060/102 | Current Loss: 0.100986\n[Training] Epoch 011 | Batch 070/102 | Current Loss: 0.101869\n[Training] Epoch 011 | Batch 080/102 | Current Loss: 0.101087\n[Training] Epoch 011 | Batch 090/102 | Current Loss: 0.101197\n[Training] Epoch 011 | Batch 100/102 | Current Loss: 0.100948\n\n[Training] Epoch 011 Summary:\n  Avg Loss: 0.101189\n  Current LR: 3.18e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 012 | Batch 000/102 | Current Loss: 0.101190\n[Training] Epoch 012 | Batch 010/102 | Current Loss: 0.100999\n[Training] Epoch 012 | Batch 020/102 | Current Loss: 0.100952\n[Training] Epoch 012 | Batch 030/102 | Current Loss: 0.101264\n[Training] Epoch 012 | Batch 040/102 | Current Loss: 0.101005\n[Training] Epoch 012 | Batch 050/102 | Current Loss: 0.101160\n[Training] Epoch 012 | Batch 060/102 | Current Loss: 0.101118\n[Training] Epoch 012 | Batch 070/102 | Current Loss: 0.101083\n[Training] Epoch 012 | Batch 080/102 | Current Loss: 0.101184\n[Training] Epoch 012 | Batch 090/102 | Current Loss: 0.101147\n[Training] Epoch 012 | Batch 100/102 | Current Loss: 0.100722\n\n[Training] Epoch 012 Summary:\n  Avg Loss: 0.101066\n  Current LR: 2.95e-04\n[EarlyStopping] Loss improved (0.101174 → 0.101066). Saving model...\n[Training] Epoch 013 | Batch 000/102 | Current Loss: 0.100828\n[Training] Epoch 013 | Batch 010/102 | Current Loss: 0.100709\n[Training] Epoch 013 | Batch 020/102 | Current Loss: 0.100779\n[Training] Epoch 013 | Batch 030/102 | Current Loss: 0.100744\n[Training] Epoch 013 | Batch 040/102 | Current Loss: 0.100835\n[Training] Epoch 013 | Batch 050/102 | Current Loss: 0.100741\n[Training] Epoch 013 | Batch 060/102 | Current Loss: 0.100781\n[Training] Epoch 013 | Batch 070/102 | Current Loss: 0.100872\n[Training] Epoch 013 | Batch 080/102 | Current Loss: 0.100855\n[Training] Epoch 013 | Batch 090/102 | Current Loss: 0.100863\n[Training] Epoch 013 | Batch 100/102 | Current Loss: 0.100843\n\n[Training] Epoch 013 Summary:\n  Avg Loss: 0.100823\n  Current LR: 2.59e-04\n[EarlyStopping] Loss improved (0.101066 → 0.100823). Saving model...\n[Training] Epoch 014 | Batch 000/102 | Current Loss: 0.100809\n[Training] Epoch 014 | Batch 010/102 | Current Loss: 0.100705\n[Training] Epoch 014 | Batch 020/102 | Current Loss: 0.100858\n[Training] Epoch 014 | Batch 030/102 | Current Loss: 0.100654\n[Training] Epoch 014 | Batch 040/102 | Current Loss: 0.100626\n[Training] Epoch 014 | Batch 050/102 | Current Loss: 0.100710\n[Training] Epoch 014 | Batch 060/102 | Current Loss: 0.100652\n[Training] Epoch 014 | Batch 070/102 | Current Loss: 0.100646\n[Training] Epoch 014 | Batch 080/102 | Current Loss: 0.100855\n[Training] Epoch 014 | Batch 090/102 | Current Loss: 0.100770\n[Training] Epoch 014 | Batch 100/102 | Current Loss: 0.100694\n\n[Training] Epoch 014 Summary:\n  Avg Loss: 0.100715\n  Current LR: 2.14e-04\n[EarlyStopping] Loss improved (0.100823 → 0.100715). Saving model...\n[Training] Epoch 015 | Batch 000/102 | Current Loss: 0.100643\n[Training] Epoch 015 | Batch 010/102 | Current Loss: 0.100769\n[Training] Epoch 015 | Batch 020/102 | Current Loss: 0.100519\n[Training] Epoch 015 | Batch 030/102 | Current Loss: 0.100712\n[Training] Epoch 015 | Batch 040/102 | Current Loss: 0.100671\n[Training] Epoch 015 | Batch 050/102 | Current Loss: 0.100737\n[Training] Epoch 015 | Batch 060/102 | Current Loss: 0.100703\n[Training] Epoch 015 | Batch 070/102 | Current Loss: 0.100552\n[Training] Epoch 015 | Batch 080/102 | Current Loss: 0.100666\n[Training] Epoch 015 | Batch 090/102 | Current Loss: 0.100640\n[Training] Epoch 015 | Batch 100/102 | Current Loss: 0.100575\n\n[Training] Epoch 015 Summary:\n  Avg Loss: 0.100621\n  Current LR: 1.64e-04\n[EarlyStopping] Loss improved (0.100715 → 0.100621). Saving model...\n[Training] Epoch 016 | Batch 000/102 | Current Loss: 0.100580\n[Training] Epoch 016 | Batch 010/102 | Current Loss: 0.100615\n[Training] Epoch 016 | Batch 020/102 | Current Loss: 0.100625\n[Training] Epoch 016 | Batch 030/102 | Current Loss: 0.100494\n[Training] Epoch 016 | Batch 040/102 | Current Loss: 0.100536\n[Training] Epoch 016 | Batch 050/102 | Current Loss: 0.100492\n[Training] Epoch 016 | Batch 060/102 | Current Loss: 0.100661\n[Training] Epoch 016 | Batch 070/102 | Current Loss: 0.100555\n[Training] Epoch 016 | Batch 080/102 | Current Loss: 0.100476\n[Training] Epoch 016 | Batch 090/102 | Current Loss: 0.100575\n[Training] Epoch 016 | Batch 100/102 | Current Loss: 0.100579\n\n[Training] Epoch 016 Summary:\n  Avg Loss: 0.100565\n  Current LR: 1.13e-04\n[EarlyStopping] Loss improved (0.100621 → 0.100565). Saving model...\n[Training] Epoch 017 | Batch 000/102 | Current Loss: 0.100581\n[Training] Epoch 017 | Batch 010/102 | Current Loss: 0.100514\n[Training] Epoch 017 | Batch 020/102 | Current Loss: 0.100567\n[Training] Epoch 017 | Batch 030/102 | Current Loss: 0.100533\n[Training] Epoch 017 | Batch 040/102 | Current Loss: 0.100483\n[Training] Epoch 017 | Batch 050/102 | Current Loss: 0.100495\n[Training] Epoch 017 | Batch 060/102 | Current Loss: 0.100504\n[Training] Epoch 017 | Batch 070/102 | Current Loss: 0.100629\n[Training] Epoch 017 | Batch 080/102 | Current Loss: 0.100499\n[Training] Epoch 017 | Batch 090/102 | Current Loss: 0.100493\n[Training] Epoch 017 | Batch 100/102 | Current Loss: 0.100536\n\n[Training] Epoch 017 Summary:\n  Avg Loss: 0.100528\n  Current LR: 6.80e-05\n[EarlyStopping] Loss improved (0.100565 → 0.100528). Saving model...\n[Training] Epoch 018 | Batch 000/102 | Current Loss: 0.100555\n[Training] Epoch 018 | Batch 010/102 | Current Loss: 0.100478\n[Training] Epoch 018 | Batch 020/102 | Current Loss: 0.100553\n[Training] Epoch 018 | Batch 030/102 | Current Loss: 0.100574\n[Training] Epoch 018 | Batch 040/102 | Current Loss: 0.100476\n[Training] Epoch 018 | Batch 050/102 | Current Loss: 0.100507\n[Training] Epoch 018 | Batch 060/102 | Current Loss: 0.100489\n[Training] Epoch 018 | Batch 070/102 | Current Loss: 0.100493\n[Training] Epoch 018 | Batch 080/102 | Current Loss: 0.100530\n[Training] Epoch 018 | Batch 090/102 | Current Loss: 0.100606\n[Training] Epoch 018 | Batch 100/102 | Current Loss: 0.100480\n\n[Training] Epoch 018 Summary:\n  Avg Loss: 0.100510\n  Current LR: 3.21e-05\n[EarlyStopping] Loss improved (0.100528 → 0.100510). Saving model...\n[Training] Epoch 019 | Batch 000/102 | Current Loss: 0.100514\n[Training] Epoch 019 | Batch 010/102 | Current Loss: 0.100469\n[Training] Epoch 019 | Batch 020/102 | Current Loss: 0.100499\n[Training] Epoch 019 | Batch 030/102 | Current Loss: 0.100517\n[Training] Epoch 019 | Batch 040/102 | Current Loss: 0.100470\n[Training] Epoch 019 | Batch 050/102 | Current Loss: 0.100440\n[Training] Epoch 019 | Batch 060/102 | Current Loss: 0.100471\n[Training] Epoch 019 | Batch 070/102 | Current Loss: 0.100430\n[Training] Epoch 019 | Batch 080/102 | Current Loss: 0.100463\n[Training] Epoch 019 | Batch 090/102 | Current Loss: 0.100455\n[Training] Epoch 019 | Batch 100/102 | Current Loss: 0.100402\n\n[Training] Epoch 019 Summary:\n  Avg Loss: 0.100494\n  Current LR: 8.96e-06\n[EarlyStopping] Loss improved (0.100510 → 0.100494). Saving model...\n[Training] Epoch 020 | Batch 000/102 | Current Loss: 0.100499\n[Training] Epoch 020 | Batch 010/102 | Current Loss: 0.100423\n[Training] Epoch 020 | Batch 020/102 | Current Loss: 0.100442\n[Training] Epoch 020 | Batch 030/102 | Current Loss: 0.100545\n[Training] Epoch 020 | Batch 040/102 | Current Loss: 0.100472\n[Training] Epoch 020 | Batch 050/102 | Current Loss: 0.100492\n[Training] Epoch 020 | Batch 060/102 | Current Loss: 0.100555\n[Training] Epoch 020 | Batch 070/102 | Current Loss: 0.100560\n[Training] Epoch 020 | Batch 080/102 | Current Loss: 0.100524\n[Training] Epoch 020 | Batch 090/102 | Current Loss: 0.100522\n[Training] Epoch 020 | Batch 100/102 | Current Loss: 0.100486\n\n[Training] Epoch 020 Summary:\n  Avg Loss: 0.100491\n  Current LR: 3.26e-04\n[EarlyStopping] Loss improved (0.100494 → 0.100491). Saving model...\n[Training] Epoch 021 | Batch 000/102 | Current Loss: 0.100425\n[Training] Epoch 021 | Batch 010/102 | Current Loss: 0.100499\n[Training] Epoch 021 | Batch 020/102 | Current Loss: 0.100476\n[Training] Epoch 021 | Batch 030/102 | Current Loss: 0.100476\n[Training] Epoch 021 | Batch 040/102 | Current Loss: 0.100533\n[Training] Epoch 021 | Batch 050/102 | Current Loss: 0.100519\n[Training] Epoch 021 | Batch 060/102 | Current Loss: 0.100442\n[Training] Epoch 021 | Batch 070/102 | Current Loss: 0.100439\n[Training] Epoch 021 | Batch 080/102 | Current Loss: 0.100559\n[Training] Epoch 021 | Batch 090/102 | Current Loss: 0.100451\n[Training] Epoch 021 | Batch 100/102 | Current Loss: 0.100440\n\n[Training] Epoch 021 Summary:\n  Avg Loss: 0.100495\n  Current LR: 3.18e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 022 | Batch 000/102 | Current Loss: 0.100521\n[Training] Epoch 022 | Batch 010/102 | Current Loss: 0.100496\n[Training] Epoch 022 | Batch 020/102 | Current Loss: 0.100414\n[Training] Epoch 022 | Batch 030/102 | Current Loss: 0.100489\n[Training] Epoch 022 | Batch 040/102 | Current Loss: 0.100378\n[Training] Epoch 022 | Batch 050/102 | Current Loss: 0.100429\n[Training] Epoch 022 | Batch 060/102 | Current Loss: 0.100414\n[Training] Epoch 022 | Batch 070/102 | Current Loss: 0.100379\n[Training] Epoch 022 | Batch 080/102 | Current Loss: 0.100458\n[Training] Epoch 022 | Batch 090/102 | Current Loss: 0.100348\n[Training] Epoch 022 | Batch 100/102 | Current Loss: 0.100464\n\n[Training] Epoch 022 Summary:\n  Avg Loss: 0.100447\n  Current LR: 2.95e-04\n[EarlyStopping] Loss improved (0.100491 → 0.100447). Saving model...\n[Training] Epoch 023 | Batch 000/102 | Current Loss: 0.100420\n[Training] Epoch 023 | Batch 010/102 | Current Loss: 0.100444\n[Training] Epoch 023 | Batch 020/102 | Current Loss: 0.100328\n[Training] Epoch 023 | Batch 030/102 | Current Loss: 0.100358\n[Training] Epoch 023 | Batch 040/102 | Current Loss: 0.100406\n[Training] Epoch 023 | Batch 050/102 | Current Loss: 0.100391\n[Training] Epoch 023 | Batch 060/102 | Current Loss: 0.100371\n[Training] Epoch 023 | Batch 070/102 | Current Loss: 0.100474\n[Training] Epoch 023 | Batch 080/102 | Current Loss: 0.100360\n[Training] Epoch 023 | Batch 090/102 | Current Loss: 0.100483\n[Training] Epoch 023 | Batch 100/102 | Current Loss: 0.100453\n\n[Training] Epoch 023 Summary:\n  Avg Loss: 0.100399\n  Current LR: 2.59e-04\n[EarlyStopping] Loss improved (0.100447 → 0.100399). Saving model...\n[Training] Epoch 024 | Batch 000/102 | Current Loss: 0.100478\n[Training] Epoch 024 | Batch 010/102 | Current Loss: 0.100384\n[Training] Epoch 024 | Batch 020/102 | Current Loss: 0.100370\n[Training] Epoch 024 | Batch 030/102 | Current Loss: 0.100380\n[Training] Epoch 024 | Batch 040/102 | Current Loss: 0.100367\n[Training] Epoch 024 | Batch 050/102 | Current Loss: 0.100408\n[Training] Epoch 024 | Batch 060/102 | Current Loss: 0.100364\n[Training] Epoch 024 | Batch 070/102 | Current Loss: 0.100345\n[Training] Epoch 024 | Batch 080/102 | Current Loss: 0.100374\n[Training] Epoch 024 | Batch 090/102 | Current Loss: 0.100469\n[Training] Epoch 024 | Batch 100/102 | Current Loss: 0.100406\n\n[Training] Epoch 024 Summary:\n  Avg Loss: 0.100377\n  Current LR: 2.14e-04\n[EarlyStopping] Loss improved (0.100399 → 0.100377). Saving model...\n[Training] Epoch 025 | Batch 000/102 | Current Loss: 0.100403\n[Training] Epoch 025 | Batch 010/102 | Current Loss: 0.100366\n[Training] Epoch 025 | Batch 020/102 | Current Loss: 0.100305\n[Training] Epoch 025 | Batch 030/102 | Current Loss: 0.100374\n[Training] Epoch 025 | Batch 040/102 | Current Loss: 0.100311\n[Training] Epoch 025 | Batch 050/102 | Current Loss: 0.100347\n[Training] Epoch 025 | Batch 060/102 | Current Loss: 0.100380\n[Training] Epoch 025 | Batch 070/102 | Current Loss: 0.100265\n[Training] Epoch 025 | Batch 080/102 | Current Loss: 0.100363\n[Training] Epoch 025 | Batch 090/102 | Current Loss: 0.100367\n[Training] Epoch 025 | Batch 100/102 | Current Loss: 0.100356\n\n[Training] Epoch 025 Summary:\n  Avg Loss: 0.100346\n  Current LR: 1.64e-04\n[EarlyStopping] Loss improved (0.100377 → 0.100346). Saving model...\n[Training] Epoch 026 | Batch 000/102 | Current Loss: 0.100269\n[Training] Epoch 026 | Batch 010/102 | Current Loss: 0.100366\n[Training] Epoch 026 | Batch 020/102 | Current Loss: 0.100342\n[Training] Epoch 026 | Batch 030/102 | Current Loss: 0.100320\n[Training] Epoch 026 | Batch 040/102 | Current Loss: 0.100349\n[Training] Epoch 026 | Batch 050/102 | Current Loss: 0.100405\n[Training] Epoch 026 | Batch 060/102 | Current Loss: 0.100439\n[Training] Epoch 026 | Batch 070/102 | Current Loss: 0.100330\n[Training] Epoch 026 | Batch 080/102 | Current Loss: 0.100262\n[Training] Epoch 026 | Batch 090/102 | Current Loss: 0.100295\n[Training] Epoch 026 | Batch 100/102 | Current Loss: 0.100247\n\n[Training] Epoch 026 Summary:\n  Avg Loss: 0.100331\n  Current LR: 1.13e-04\n[EarlyStopping] Loss improved (0.100346 → 0.100331). Saving model...\n[Training] Epoch 027 | Batch 000/102 | Current Loss: 0.100320\n[Training] Epoch 027 | Batch 010/102 | Current Loss: 0.100276\n[Training] Epoch 027 | Batch 020/102 | Current Loss: 0.100297\n[Training] Epoch 027 | Batch 030/102 | Current Loss: 0.100400\n[Training] Epoch 027 | Batch 040/102 | Current Loss: 0.100286\n[Training] Epoch 027 | Batch 050/102 | Current Loss: 0.100386\n[Training] Epoch 027 | Batch 060/102 | Current Loss: 0.100333\n[Training] Epoch 027 | Batch 070/102 | Current Loss: 0.100324\n[Training] Epoch 027 | Batch 080/102 | Current Loss: 0.100291\n[Training] Epoch 027 | Batch 090/102 | Current Loss: 0.100358\n[Training] Epoch 027 | Batch 100/102 | Current Loss: 0.100303\n\n[Training] Epoch 027 Summary:\n  Avg Loss: 0.100312\n  Current LR: 6.80e-05\n[EarlyStopping] Loss improved (0.100331 → 0.100312). Saving model...\n[Training] Epoch 028 | Batch 000/102 | Current Loss: 0.100313\n[Training] Epoch 028 | Batch 010/102 | Current Loss: 0.100262\n[Training] Epoch 028 | Batch 020/102 | Current Loss: 0.100289\n[Training] Epoch 028 | Batch 030/102 | Current Loss: 0.100250\n[Training] Epoch 028 | Batch 040/102 | Current Loss: 0.100321\n[Training] Epoch 028 | Batch 050/102 | Current Loss: 0.100283\n[Training] Epoch 028 | Batch 060/102 | Current Loss: 0.100273\n[Training] Epoch 028 | Batch 070/102 | Current Loss: 0.100332\n[Training] Epoch 028 | Batch 080/102 | Current Loss: 0.100318\n[Training] Epoch 028 | Batch 090/102 | Current Loss: 0.100312\n[Training] Epoch 028 | Batch 100/102 | Current Loss: 0.100313\n\n[Training] Epoch 028 Summary:\n  Avg Loss: 0.100301\n  Current LR: 3.21e-05\n[EarlyStopping] Loss improved (0.100312 → 0.100301). Saving model...\n[Training] Epoch 029 | Batch 000/102 | Current Loss: 0.100368\n[Training] Epoch 029 | Batch 010/102 | Current Loss: 0.100241\n[Training] Epoch 029 | Batch 020/102 | Current Loss: 0.100291\n[Training] Epoch 029 | Batch 030/102 | Current Loss: 0.100309\n[Training] Epoch 029 | Batch 040/102 | Current Loss: 0.100269\n[Training] Epoch 029 | Batch 050/102 | Current Loss: 0.100271\n[Training] Epoch 029 | Batch 060/102 | Current Loss: 0.100317\n[Training] Epoch 029 | Batch 070/102 | Current Loss: 0.100349\n[Training] Epoch 029 | Batch 080/102 | Current Loss: 0.100304\n[Training] Epoch 029 | Batch 090/102 | Current Loss: 0.100321\n[Training] Epoch 029 | Batch 100/102 | Current Loss: 0.100312\n\n[Training] Epoch 029 Summary:\n  Avg Loss: 0.100292\n  Current LR: 8.96e-06\n[EarlyStopping] Loss improved (0.100301 → 0.100292). Saving model...\n[Training] Epoch 030 | Batch 000/102 | Current Loss: 0.100283\n[Training] Epoch 030 | Batch 010/102 | Current Loss: 0.100332\n[Training] Epoch 030 | Batch 020/102 | Current Loss: 0.100275\n[Training] Epoch 030 | Batch 030/102 | Current Loss: 0.100248\n[Training] Epoch 030 | Batch 040/102 | Current Loss: 0.100356\n[Training] Epoch 030 | Batch 050/102 | Current Loss: 0.100319\n[Training] Epoch 030 | Batch 060/102 | Current Loss: 0.100333\n[Training] Epoch 030 | Batch 070/102 | Current Loss: 0.100300\n[Training] Epoch 030 | Batch 080/102 | Current Loss: 0.100314\n[Training] Epoch 030 | Batch 090/102 | Current Loss: 0.100238\n[Training] Epoch 030 | Batch 100/102 | Current Loss: 0.100261\n\n[Training] Epoch 030 Summary:\n  Avg Loss: 0.100290\n  Current LR: 3.26e-04\n[EarlyStopping] Loss improved (0.100292 → 0.100290). Saving model...\n[Training] Epoch 031 | Batch 000/102 | Current Loss: 0.100266\n[Training] Epoch 031 | Batch 010/102 | Current Loss: 0.100304\n[Training] Epoch 031 | Batch 020/102 | Current Loss: 0.100305\n[Training] Epoch 031 | Batch 030/102 | Current Loss: 0.100268\n[Training] Epoch 031 | Batch 040/102 | Current Loss: 0.100286\n[Training] Epoch 031 | Batch 050/102 | Current Loss: 0.100282\n[Training] Epoch 031 | Batch 060/102 | Current Loss: 0.100284\n[Training] Epoch 031 | Batch 070/102 | Current Loss: 0.100251\n[Training] Epoch 031 | Batch 080/102 | Current Loss: 0.100469\n[Training] Epoch 031 | Batch 090/102 | Current Loss: 0.100308\n[Training] Epoch 031 | Batch 100/102 | Current Loss: 0.100341\n\n[Training] Epoch 031 Summary:\n  Avg Loss: 0.100309\n  Current LR: 3.18e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 032 | Batch 000/102 | Current Loss: 0.100388\n[Training] Epoch 032 | Batch 010/102 | Current Loss: 0.100316\n[Training] Epoch 032 | Batch 020/102 | Current Loss: 0.100318\n[Training] Epoch 032 | Batch 030/102 | Current Loss: 0.100225\n[Training] Epoch 032 | Batch 040/102 | Current Loss: 0.100336\n[Training] Epoch 032 | Batch 050/102 | Current Loss: 0.100289\n[Training] Epoch 032 | Batch 060/102 | Current Loss: 0.100267\n[Training] Epoch 032 | Batch 070/102 | Current Loss: 0.100327\n[Training] Epoch 032 | Batch 080/102 | Current Loss: 0.100259\n[Training] Epoch 032 | Batch 090/102 | Current Loss: 0.100354\n[Training] Epoch 032 | Batch 100/102 | Current Loss: 0.100219\n\n[Training] Epoch 032 Summary:\n  Avg Loss: 0.100292\n  Current LR: 2.95e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 033 | Batch 000/102 | Current Loss: 0.100198\n[Training] Epoch 033 | Batch 010/102 | Current Loss: 0.100303\n[Training] Epoch 033 | Batch 020/102 | Current Loss: 0.100260\n[Training] Epoch 033 | Batch 030/102 | Current Loss: 0.100334\n[Training] Epoch 033 | Batch 040/102 | Current Loss: 0.100228\n[Training] Epoch 033 | Batch 050/102 | Current Loss: 0.100247\n[Training] Epoch 033 | Batch 060/102 | Current Loss: 0.100247\n[Training] Epoch 033 | Batch 070/102 | Current Loss: 0.100265\n[Training] Epoch 033 | Batch 080/102 | Current Loss: 0.100255\n[Training] Epoch 033 | Batch 090/102 | Current Loss: 0.100227\n[Training] Epoch 033 | Batch 100/102 | Current Loss: 0.100229\n\n[Training] Epoch 033 Summary:\n  Avg Loss: 0.100265\n  Current LR: 2.59e-04\n[EarlyStopping] Loss improved (0.100290 → 0.100265). Saving model...\n[Training] Epoch 034 | Batch 000/102 | Current Loss: 0.100314\n[Training] Epoch 034 | Batch 010/102 | Current Loss: 0.100178\n[Training] Epoch 034 | Batch 020/102 | Current Loss: 0.100201\n[Training] Epoch 034 | Batch 030/102 | Current Loss: 0.100234\n[Training] Epoch 034 | Batch 040/102 | Current Loss: 0.100361\n[Training] Epoch 034 | Batch 050/102 | Current Loss: 0.100293\n[Training] Epoch 034 | Batch 060/102 | Current Loss: 0.100190\n[Training] Epoch 034 | Batch 070/102 | Current Loss: 0.100229\n[Training] Epoch 034 | Batch 080/102 | Current Loss: 0.100213\n[Training] Epoch 034 | Batch 090/102 | Current Loss: 0.100253\n[Training] Epoch 034 | Batch 100/102 | Current Loss: 0.100194\n\n[Training] Epoch 034 Summary:\n  Avg Loss: 0.100251\n  Current LR: 2.14e-04\n[EarlyStopping] Loss improved (0.100265 → 0.100251). Saving model...\n[Training] Epoch 035 | Batch 000/102 | Current Loss: 0.100203\n[Training] Epoch 035 | Batch 010/102 | Current Loss: 0.100293\n[Training] Epoch 035 | Batch 020/102 | Current Loss: 0.100220\n[Training] Epoch 035 | Batch 030/102 | Current Loss: 0.100228\n[Training] Epoch 035 | Batch 040/102 | Current Loss: 0.100218\n[Training] Epoch 035 | Batch 050/102 | Current Loss: 0.100222\n[Training] Epoch 035 | Batch 060/102 | Current Loss: 0.100203\n[Training] Epoch 035 | Batch 070/102 | Current Loss: 0.100130\n[Training] Epoch 035 | Batch 080/102 | Current Loss: 0.100281\n[Training] Epoch 035 | Batch 090/102 | Current Loss: 0.100207\n[Training] Epoch 035 | Batch 100/102 | Current Loss: 0.100322\n\n[Training] Epoch 035 Summary:\n  Avg Loss: 0.100238\n  Current LR: 1.64e-04\n[EarlyStopping] Loss improved (0.100251 → 0.100238). Saving model...\n[Training] Epoch 036 | Batch 000/102 | Current Loss: 0.100231\n[Training] Epoch 036 | Batch 010/102 | Current Loss: 0.100348\n[Training] Epoch 036 | Batch 020/102 | Current Loss: 0.100203\n[Training] Epoch 036 | Batch 030/102 | Current Loss: 0.100283\n[Training] Epoch 036 | Batch 040/102 | Current Loss: 0.100227\n[Training] Epoch 036 | Batch 050/102 | Current Loss: 0.100167\n[Training] Epoch 036 | Batch 060/102 | Current Loss: 0.100225\n[Training] Epoch 036 | Batch 070/102 | Current Loss: 0.100200\n[Training] Epoch 036 | Batch 080/102 | Current Loss: 0.100205\n[Training] Epoch 036 | Batch 090/102 | Current Loss: 0.100250\n[Training] Epoch 036 | Batch 100/102 | Current Loss: 0.100274\n\n[Training] Epoch 036 Summary:\n  Avg Loss: 0.100224\n  Current LR: 1.13e-04\n[EarlyStopping] Loss improved (0.100238 → 0.100224). Saving model...\n[Training] Epoch 037 | Batch 000/102 | Current Loss: 0.100284\n[Training] Epoch 037 | Batch 010/102 | Current Loss: 0.100170\n[Training] Epoch 037 | Batch 020/102 | Current Loss: 0.100181\n[Training] Epoch 037 | Batch 030/102 | Current Loss: 0.100259\n[Training] Epoch 037 | Batch 040/102 | Current Loss: 0.100214\n[Training] Epoch 037 | Batch 050/102 | Current Loss: 0.100256\n[Training] Epoch 037 | Batch 060/102 | Current Loss: 0.100251\n[Training] Epoch 037 | Batch 070/102 | Current Loss: 0.100237\n[Training] Epoch 037 | Batch 080/102 | Current Loss: 0.100274\n[Training] Epoch 037 | Batch 090/102 | Current Loss: 0.100207\n[Training] Epoch 037 | Batch 100/102 | Current Loss: 0.100202\n\n[Training] Epoch 037 Summary:\n  Avg Loss: 0.100221\n  Current LR: 6.80e-05\n[EarlyStopping] Loss improved (0.100224 → 0.100221). Saving model...\n[Training] Epoch 038 | Batch 000/102 | Current Loss: 0.100182\n[Training] Epoch 038 | Batch 010/102 | Current Loss: 0.100247\n[Training] Epoch 038 | Batch 020/102 | Current Loss: 0.100276\n[Training] Epoch 038 | Batch 030/102 | Current Loss: 0.100155\n[Training] Epoch 038 | Batch 040/102 | Current Loss: 0.100217\n[Training] Epoch 038 | Batch 050/102 | Current Loss: 0.100202\n[Training] Epoch 038 | Batch 060/102 | Current Loss: 0.100217\n[Training] Epoch 038 | Batch 070/102 | Current Loss: 0.100236\n[Training] Epoch 038 | Batch 080/102 | Current Loss: 0.100195\n[Training] Epoch 038 | Batch 090/102 | Current Loss: 0.100198\n[Training] Epoch 038 | Batch 100/102 | Current Loss: 0.100256\n\n[Training] Epoch 038 Summary:\n  Avg Loss: 0.100210\n  Current LR: 3.21e-05\n[EarlyStopping] Loss improved (0.100221 → 0.100210). Saving model...\n[Training] Epoch 039 | Batch 000/102 | Current Loss: 0.100207\n[Training] Epoch 039 | Batch 010/102 | Current Loss: 0.100146\n[Training] Epoch 039 | Batch 020/102 | Current Loss: 0.100189\n[Training] Epoch 039 | Batch 030/102 | Current Loss: 0.100155\n[Training] Epoch 039 | Batch 040/102 | Current Loss: 0.100221\n[Training] Epoch 039 | Batch 050/102 | Current Loss: 0.100232\n[Training] Epoch 039 | Batch 060/102 | Current Loss: 0.100159\n[Training] Epoch 039 | Batch 070/102 | Current Loss: 0.100164\n[Training] Epoch 039 | Batch 080/102 | Current Loss: 0.100184\n[Training] Epoch 039 | Batch 090/102 | Current Loss: 0.100173\n[Training] Epoch 039 | Batch 100/102 | Current Loss: 0.100205\n\n[Training] Epoch 039 Summary:\n  Avg Loss: 0.100200\n  Current LR: 8.96e-06\n[EarlyStopping] Loss improved (0.100210 → 0.100200). Saving model...\n[Training] Epoch 040 | Batch 000/102 | Current Loss: 0.100194\n[Training] Epoch 040 | Batch 010/102 | Current Loss: 0.100192\n[Training] Epoch 040 | Batch 020/102 | Current Loss: 0.100243\n[Training] Epoch 040 | Batch 030/102 | Current Loss: 0.100171\n[Training] Epoch 040 | Batch 040/102 | Current Loss: 0.100227\n[Training] Epoch 040 | Batch 050/102 | Current Loss: 0.100244\n[Training] Epoch 040 | Batch 060/102 | Current Loss: 0.100171\n[Training] Epoch 040 | Batch 070/102 | Current Loss: 0.100187\n[Training] Epoch 040 | Batch 080/102 | Current Loss: 0.100175\n[Training] Epoch 040 | Batch 090/102 | Current Loss: 0.100149\n[Training] Epoch 040 | Batch 100/102 | Current Loss: 0.100189\n\n[Training] Epoch 040 Summary:\n  Avg Loss: 0.100197\n  Current LR: 3.26e-04\n[EarlyStopping] Loss improved (0.100200 → 0.100197). Saving model...\n[Training] Epoch 041 | Batch 000/102 | Current Loss: 0.100264\n[Training] Epoch 041 | Batch 010/102 | Current Loss: 0.100252\n[Training] Epoch 041 | Batch 020/102 | Current Loss: 0.100265\n[Training] Epoch 041 | Batch 030/102 | Current Loss: 0.100283\n[Training] Epoch 041 | Batch 040/102 | Current Loss: 0.100147\n[Training] Epoch 041 | Batch 050/102 | Current Loss: 0.100248\n[Training] Epoch 041 | Batch 060/102 | Current Loss: 0.100256\n[Training] Epoch 041 | Batch 070/102 | Current Loss: 0.100190\n[Training] Epoch 041 | Batch 080/102 | Current Loss: 0.100272\n[Training] Epoch 041 | Batch 090/102 | Current Loss: 0.100268\n[Training] Epoch 041 | Batch 100/102 | Current Loss: 0.100239\n\n[Training] Epoch 041 Summary:\n  Avg Loss: 0.100235\n  Current LR: 3.18e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 042 | Batch 000/102 | Current Loss: 0.100243\n[Training] Epoch 042 | Batch 010/102 | Current Loss: 0.100212\n[Training] Epoch 042 | Batch 020/102 | Current Loss: 0.100230\n[Training] Epoch 042 | Batch 030/102 | Current Loss: 0.100199\n[Training] Epoch 042 | Batch 040/102 | Current Loss: 0.100344\n[Training] Epoch 042 | Batch 050/102 | Current Loss: 0.100204\n[Training] Epoch 042 | Batch 060/102 | Current Loss: 0.100246\n[Training] Epoch 042 | Batch 070/102 | Current Loss: 0.100205\n[Training] Epoch 042 | Batch 080/102 | Current Loss: 0.100259\n[Training] Epoch 042 | Batch 090/102 | Current Loss: 0.100212\n[Training] Epoch 042 | Batch 100/102 | Current Loss: 0.100184\n\n[Training] Epoch 042 Summary:\n  Avg Loss: 0.100211\n  Current LR: 2.95e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 043 | Batch 000/102 | Current Loss: 0.100226\n[Training] Epoch 043 | Batch 010/102 | Current Loss: 0.100202\n[Training] Epoch 043 | Batch 020/102 | Current Loss: 0.100177\n[Training] Epoch 043 | Batch 030/102 | Current Loss: 0.100146\n[Training] Epoch 043 | Batch 040/102 | Current Loss: 0.100140\n[Training] Epoch 043 | Batch 050/102 | Current Loss: 0.100203\n[Training] Epoch 043 | Batch 060/102 | Current Loss: 0.100171\n[Training] Epoch 043 | Batch 070/102 | Current Loss: 0.100175\n[Training] Epoch 043 | Batch 080/102 | Current Loss: 0.100241\n[Training] Epoch 043 | Batch 090/102 | Current Loss: 0.100206\n[Training] Epoch 043 | Batch 100/102 | Current Loss: 0.100213\n\n[Training] Epoch 043 Summary:\n  Avg Loss: 0.100201\n  Current LR: 2.59e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 044 | Batch 000/102 | Current Loss: 0.100196\n[Training] Epoch 044 | Batch 010/102 | Current Loss: 0.100172\n[Training] Epoch 044 | Batch 020/102 | Current Loss: 0.100295\n[Training] Epoch 044 | Batch 030/102 | Current Loss: 0.100163\n[Training] Epoch 044 | Batch 040/102 | Current Loss: 0.100189\n[Training] Epoch 044 | Batch 050/102 | Current Loss: 0.100208\n[Training] Epoch 044 | Batch 060/102 | Current Loss: 0.100154\n[Training] Epoch 044 | Batch 070/102 | Current Loss: 0.100131\n[Training] Epoch 044 | Batch 080/102 | Current Loss: 0.100177\n[Training] Epoch 044 | Batch 090/102 | Current Loss: 0.100148\n[Training] Epoch 044 | Batch 100/102 | Current Loss: 0.100204\n\n[Training] Epoch 044 Summary:\n  Avg Loss: 0.100196\n  Current LR: 2.14e-04\n[EarlyStopping] Loss improved (0.100197 → 0.100196). Saving model...\n[Training] Epoch 045 | Batch 000/102 | Current Loss: 0.100194\n[Training] Epoch 045 | Batch 010/102 | Current Loss: 0.100173\n[Training] Epoch 045 | Batch 020/102 | Current Loss: 0.100149\n[Training] Epoch 045 | Batch 030/102 | Current Loss: 0.100268\n[Training] Epoch 045 | Batch 040/102 | Current Loss: 0.100199\n[Training] Epoch 045 | Batch 050/102 | Current Loss: 0.100142\n[Training] Epoch 045 | Batch 060/102 | Current Loss: 0.100167\n[Training] Epoch 045 | Batch 070/102 | Current Loss: 0.100110\n[Training] Epoch 045 | Batch 080/102 | Current Loss: 0.100174\n[Training] Epoch 045 | Batch 090/102 | Current Loss: 0.100143\n[Training] Epoch 045 | Batch 100/102 | Current Loss: 0.100182\n\n[Training] Epoch 045 Summary:\n  Avg Loss: 0.100185\n  Current LR: 1.64e-04\n[EarlyStopping] Loss improved (0.100196 → 0.100185). Saving model...\n[Training] Epoch 046 | Batch 000/102 | Current Loss: 0.100119\n[Training] Epoch 046 | Batch 010/102 | Current Loss: 0.100136\n[Training] Epoch 046 | Batch 020/102 | Current Loss: 0.100244\n[Training] Epoch 046 | Batch 030/102 | Current Loss: 0.100173\n[Training] Epoch 046 | Batch 040/102 | Current Loss: 0.100123\n[Training] Epoch 046 | Batch 050/102 | Current Loss: 0.100276\n[Training] Epoch 046 | Batch 060/102 | Current Loss: 0.100212\n[Training] Epoch 046 | Batch 070/102 | Current Loss: 0.100173\n[Training] Epoch 046 | Batch 080/102 | Current Loss: 0.100225\n[Training] Epoch 046 | Batch 090/102 | Current Loss: 0.100161\n[Training] Epoch 046 | Batch 100/102 | Current Loss: 0.100201\n\n[Training] Epoch 046 Summary:\n  Avg Loss: 0.100167\n  Current LR: 1.13e-04\n[EarlyStopping] Loss improved (0.100185 → 0.100167). Saving model...\n[Training] Epoch 047 | Batch 000/102 | Current Loss: 0.100177\n[Training] Epoch 047 | Batch 010/102 | Current Loss: 0.100229\n[Training] Epoch 047 | Batch 020/102 | Current Loss: 0.100213\n[Training] Epoch 047 | Batch 030/102 | Current Loss: 0.100184\n[Training] Epoch 047 | Batch 040/102 | Current Loss: 0.100162\n[Training] Epoch 047 | Batch 050/102 | Current Loss: 0.100188\n[Training] Epoch 047 | Batch 060/102 | Current Loss: 0.100221\n[Training] Epoch 047 | Batch 070/102 | Current Loss: 0.100210\n[Training] Epoch 047 | Batch 080/102 | Current Loss: 0.100164\n[Training] Epoch 047 | Batch 090/102 | Current Loss: 0.100218\n[Training] Epoch 047 | Batch 100/102 | Current Loss: 0.100155\n\n[Training] Epoch 047 Summary:\n  Avg Loss: 0.100163\n  Current LR: 6.80e-05\n[EarlyStopping] Loss improved (0.100167 → 0.100163). Saving model...\n[Training] Epoch 048 | Batch 000/102 | Current Loss: 0.100168\n[Training] Epoch 048 | Batch 010/102 | Current Loss: 0.100178\n[Training] Epoch 048 | Batch 020/102 | Current Loss: 0.100189\n[Training] Epoch 048 | Batch 030/102 | Current Loss: 0.100180\n[Training] Epoch 048 | Batch 040/102 | Current Loss: 0.100164\n[Training] Epoch 048 | Batch 050/102 | Current Loss: 0.100163\n[Training] Epoch 048 | Batch 060/102 | Current Loss: 0.100112\n[Training] Epoch 048 | Batch 070/102 | Current Loss: 0.100137\n[Training] Epoch 048 | Batch 080/102 | Current Loss: 0.100156\n[Training] Epoch 048 | Batch 090/102 | Current Loss: 0.100118\n[Training] Epoch 048 | Batch 100/102 | Current Loss: 0.100201\n\n[Training] Epoch 048 Summary:\n  Avg Loss: 0.100153\n  Current LR: 3.21e-05\n[EarlyStopping] Loss improved (0.100163 → 0.100153). Saving model...\n[Training] Epoch 049 | Batch 000/102 | Current Loss: 0.100119\n[Training] Epoch 049 | Batch 010/102 | Current Loss: 0.100149\n[Training] Epoch 049 | Batch 020/102 | Current Loss: 0.100189\n[Training] Epoch 049 | Batch 030/102 | Current Loss: 0.100141\n[Training] Epoch 049 | Batch 040/102 | Current Loss: 0.100122\n[Training] Epoch 049 | Batch 050/102 | Current Loss: 0.100192\n[Training] Epoch 049 | Batch 060/102 | Current Loss: 0.100159\n[Training] Epoch 049 | Batch 070/102 | Current Loss: 0.100195\n[Training] Epoch 049 | Batch 080/102 | Current Loss: 0.100149\n[Training] Epoch 049 | Batch 090/102 | Current Loss: 0.100115\n[Training] Epoch 049 | Batch 100/102 | Current Loss: 0.100141\n\n[Training] Epoch 049 Summary:\n  Avg Loss: 0.100150\n  Current LR: 8.96e-06\n[EarlyStopping] Loss improved (0.100153 → 0.100150). Saving model...\n[Training] Epoch 050 | Batch 000/102 | Current Loss: 0.100153\n[Training] Epoch 050 | Batch 010/102 | Current Loss: 0.100113\n[Training] Epoch 050 | Batch 020/102 | Current Loss: 0.100123\n[Training] Epoch 050 | Batch 030/102 | Current Loss: 0.100196\n[Training] Epoch 050 | Batch 040/102 | Current Loss: 0.100145\n[Training] Epoch 050 | Batch 050/102 | Current Loss: 0.100152\n[Training] Epoch 050 | Batch 060/102 | Current Loss: 0.100120\n[Training] Epoch 050 | Batch 070/102 | Current Loss: 0.100127\n[Training] Epoch 050 | Batch 080/102 | Current Loss: 0.100140\n[Training] Epoch 050 | Batch 090/102 | Current Loss: 0.100101\n[Training] Epoch 050 | Batch 100/102 | Current Loss: 0.100179\n\n[Training] Epoch 050 Summary:\n  Avg Loss: 0.100148\n  Current LR: 3.26e-04\n[EarlyStopping] Loss improved (0.100150 → 0.100148). Saving model...\n[Training] Epoch 051 | Batch 000/102 | Current Loss: 0.100152\n[Training] Epoch 051 | Batch 010/102 | Current Loss: 0.100163\n[Training] Epoch 051 | Batch 020/102 | Current Loss: 0.100213\n[Training] Epoch 051 | Batch 030/102 | Current Loss: 0.100168\n[Training] Epoch 051 | Batch 040/102 | Current Loss: 0.100177\n[Training] Epoch 051 | Batch 050/102 | Current Loss: 0.100235\n[Training] Epoch 051 | Batch 060/102 | Current Loss: 0.100100\n[Training] Epoch 051 | Batch 070/102 | Current Loss: 0.100165\n[Training] Epoch 051 | Batch 080/102 | Current Loss: 0.100201\n[Training] Epoch 051 | Batch 090/102 | Current Loss: 0.100166\n[Training] Epoch 051 | Batch 100/102 | Current Loss: 0.100164\n\n[Training] Epoch 051 Summary:\n  Avg Loss: 0.100177\n  Current LR: 3.18e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 052 | Batch 000/102 | Current Loss: 0.100259\n[Training] Epoch 052 | Batch 010/102 | Current Loss: 0.100170\n[Training] Epoch 052 | Batch 020/102 | Current Loss: 0.100175\n[Training] Epoch 052 | Batch 030/102 | Current Loss: 0.100250\n[Training] Epoch 052 | Batch 040/102 | Current Loss: 0.100213\n[Training] Epoch 052 | Batch 050/102 | Current Loss: 0.100139\n[Training] Epoch 052 | Batch 060/102 | Current Loss: 0.100186\n[Training] Epoch 052 | Batch 070/102 | Current Loss: 0.100135\n[Training] Epoch 052 | Batch 080/102 | Current Loss: 0.100197\n[Training] Epoch 052 | Batch 090/102 | Current Loss: 0.100166\n[Training] Epoch 052 | Batch 100/102 | Current Loss: 0.100172\n\n[Training] Epoch 052 Summary:\n  Avg Loss: 0.100182\n  Current LR: 2.95e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 053 | Batch 000/102 | Current Loss: 0.100145\n[Training] Epoch 053 | Batch 010/102 | Current Loss: 0.100194\n[Training] Epoch 053 | Batch 020/102 | Current Loss: 0.100176\n[Training] Epoch 053 | Batch 030/102 | Current Loss: 0.100137\n[Training] Epoch 053 | Batch 040/102 | Current Loss: 0.100140\n[Training] Epoch 053 | Batch 050/102 | Current Loss: 0.100203\n[Training] Epoch 053 | Batch 060/102 | Current Loss: 0.100140\n[Training] Epoch 053 | Batch 070/102 | Current Loss: 0.100166\n[Training] Epoch 053 | Batch 080/102 | Current Loss: 0.100218\n[Training] Epoch 053 | Batch 090/102 | Current Loss: 0.100174\n[Training] Epoch 053 | Batch 100/102 | Current Loss: 0.100172\n\n[Training] Epoch 053 Summary:\n  Avg Loss: 0.100159\n  Current LR: 2.59e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 054 | Batch 000/102 | Current Loss: 0.100082\n[Training] Epoch 054 | Batch 010/102 | Current Loss: 0.100132\n[Training] Epoch 054 | Batch 020/102 | Current Loss: 0.100216\n[Training] Epoch 054 | Batch 030/102 | Current Loss: 0.100232\n[Training] Epoch 054 | Batch 040/102 | Current Loss: 0.100139\n[Training] Epoch 054 | Batch 050/102 | Current Loss: 0.100145\n[Training] Epoch 054 | Batch 060/102 | Current Loss: 0.100149\n[Training] Epoch 054 | Batch 070/102 | Current Loss: 0.100116\n[Training] Epoch 054 | Batch 080/102 | Current Loss: 0.100183\n[Training] Epoch 054 | Batch 090/102 | Current Loss: 0.100105\n[Training] Epoch 054 | Batch 100/102 | Current Loss: 0.100120\n\n[Training] Epoch 054 Summary:\n  Avg Loss: 0.100148\n  Current LR: 2.14e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 055 | Batch 000/102 | Current Loss: 0.100138\n[Training] Epoch 055 | Batch 010/102 | Current Loss: 0.100159\n[Training] Epoch 055 | Batch 020/102 | Current Loss: 0.100176\n[Training] Epoch 055 | Batch 030/102 | Current Loss: 0.100100\n[Training] Epoch 055 | Batch 040/102 | Current Loss: 0.100120\n[Training] Epoch 055 | Batch 050/102 | Current Loss: 0.100147\n[Training] Epoch 055 | Batch 060/102 | Current Loss: 0.100173\n[Training] Epoch 055 | Batch 070/102 | Current Loss: 0.100151\n[Training] Epoch 055 | Batch 080/102 | Current Loss: 0.100118\n[Training] Epoch 055 | Batch 090/102 | Current Loss: 0.100105\n[Training] Epoch 055 | Batch 100/102 | Current Loss: 0.100226\n\n[Training] Epoch 055 Summary:\n  Avg Loss: 0.100136\n  Current LR: 1.64e-04\n[EarlyStopping] Loss improved (0.100148 → 0.100136). Saving model...\n[Training] Epoch 056 | Batch 000/102 | Current Loss: 0.100126\n[Training] Epoch 056 | Batch 010/102 | Current Loss: 0.100106\n[Training] Epoch 056 | Batch 020/102 | Current Loss: 0.100112\n[Training] Epoch 056 | Batch 030/102 | Current Loss: 0.100129\n[Training] Epoch 056 | Batch 040/102 | Current Loss: 0.100136\n[Training] Epoch 056 | Batch 050/102 | Current Loss: 0.100143\n[Training] Epoch 056 | Batch 060/102 | Current Loss: 0.100146\n[Training] Epoch 056 | Batch 070/102 | Current Loss: 0.100140\n[Training] Epoch 056 | Batch 080/102 | Current Loss: 0.100126\n[Training] Epoch 056 | Batch 090/102 | Current Loss: 0.100191\n[Training] Epoch 056 | Batch 100/102 | Current Loss: 0.100099\n\n[Training] Epoch 056 Summary:\n  Avg Loss: 0.100133\n  Current LR: 1.13e-04\n[EarlyStopping] Loss improved (0.100136 → 0.100133). Saving model...\n[Training] Epoch 057 | Batch 000/102 | Current Loss: 0.100201\n[Training] Epoch 057 | Batch 010/102 | Current Loss: 0.100067\n[Training] Epoch 057 | Batch 020/102 | Current Loss: 0.100109\n[Training] Epoch 057 | Batch 030/102 | Current Loss: 0.100114\n[Training] Epoch 057 | Batch 040/102 | Current Loss: 0.100149\n[Training] Epoch 057 | Batch 050/102 | Current Loss: 0.100157\n[Training] Epoch 057 | Batch 060/102 | Current Loss: 0.100119\n[Training] Epoch 057 | Batch 070/102 | Current Loss: 0.100156\n[Training] Epoch 057 | Batch 080/102 | Current Loss: 0.100138\n[Training] Epoch 057 | Batch 090/102 | Current Loss: 0.100166\n[Training] Epoch 057 | Batch 100/102 | Current Loss: 0.100181\n\n[Training] Epoch 057 Summary:\n  Avg Loss: 0.100131\n  Current LR: 6.80e-05\n[EarlyStopping] Loss improved (0.100133 → 0.100131). Saving model...\n[Training] Epoch 058 | Batch 000/102 | Current Loss: 0.100140\n[Training] Epoch 058 | Batch 010/102 | Current Loss: 0.100110\n[Training] Epoch 058 | Batch 020/102 | Current Loss: 0.100117\n[Training] Epoch 058 | Batch 030/102 | Current Loss: 0.100156\n[Training] Epoch 058 | Batch 040/102 | Current Loss: 0.100093\n[Training] Epoch 058 | Batch 050/102 | Current Loss: 0.100157\n[Training] Epoch 058 | Batch 060/102 | Current Loss: 0.100077\n[Training] Epoch 058 | Batch 070/102 | Current Loss: 0.100142\n[Training] Epoch 058 | Batch 080/102 | Current Loss: 0.100179\n[Training] Epoch 058 | Batch 090/102 | Current Loss: 0.100236\n[Training] Epoch 058 | Batch 100/102 | Current Loss: 0.100132\n\n[Training] Epoch 058 Summary:\n  Avg Loss: 0.100119\n  Current LR: 3.21e-05\n[EarlyStopping] Loss improved (0.100131 → 0.100119). Saving model...\n[Training] Epoch 059 | Batch 000/102 | Current Loss: 0.100133\n[Training] Epoch 059 | Batch 010/102 | Current Loss: 0.100140\n[Training] Epoch 059 | Batch 020/102 | Current Loss: 0.100079\n[Training] Epoch 059 | Batch 030/102 | Current Loss: 0.100055\n[Training] Epoch 059 | Batch 040/102 | Current Loss: 0.100135\n[Training] Epoch 059 | Batch 050/102 | Current Loss: 0.100135\n[Training] Epoch 059 | Batch 060/102 | Current Loss: 0.100170\n[Training] Epoch 059 | Batch 070/102 | Current Loss: 0.100166\n[Training] Epoch 059 | Batch 080/102 | Current Loss: 0.100060\n[Training] Epoch 059 | Batch 090/102 | Current Loss: 0.100112\n[Training] Epoch 059 | Batch 100/102 | Current Loss: 0.100078\n\n[Training] Epoch 059 Summary:\n  Avg Loss: 0.100117\n  Current LR: 8.96e-06\n[EarlyStopping] Loss improved (0.100119 → 0.100117). Saving model...\n[Training] Epoch 060 | Batch 000/102 | Current Loss: 0.100144\n[Training] Epoch 060 | Batch 010/102 | Current Loss: 0.100155\n[Training] Epoch 060 | Batch 020/102 | Current Loss: 0.100153\n[Training] Epoch 060 | Batch 030/102 | Current Loss: 0.100147\n[Training] Epoch 060 | Batch 040/102 | Current Loss: 0.100080\n[Training] Epoch 060 | Batch 050/102 | Current Loss: 0.100177\n[Training] Epoch 060 | Batch 060/102 | Current Loss: 0.100116\n[Training] Epoch 060 | Batch 070/102 | Current Loss: 0.100084\n[Training] Epoch 060 | Batch 080/102 | Current Loss: 0.100112\n[Training] Epoch 060 | Batch 090/102 | Current Loss: 0.100104\n[Training] Epoch 060 | Batch 100/102 | Current Loss: 0.100056\n\n[Training] Epoch 060 Summary:\n  Avg Loss: 0.100111\n  Current LR: 3.26e-04\n[EarlyStopping] Loss improved (0.100117 → 0.100111). Saving model...\n[Training] Epoch 061 | Batch 000/102 | Current Loss: 0.100106\n[Training] Epoch 061 | Batch 010/102 | Current Loss: 0.100095\n[Training] Epoch 061 | Batch 020/102 | Current Loss: 0.100140\n[Training] Epoch 061 | Batch 030/102 | Current Loss: 0.100094\n[Training] Epoch 061 | Batch 040/102 | Current Loss: 0.100132\n[Training] Epoch 061 | Batch 050/102 | Current Loss: 0.100121\n[Training] Epoch 061 | Batch 060/102 | Current Loss: 0.100162\n[Training] Epoch 061 | Batch 070/102 | Current Loss: 0.100167\n[Training] Epoch 061 | Batch 080/102 | Current Loss: 0.100217\n[Training] Epoch 061 | Batch 090/102 | Current Loss: 0.100199\n[Training] Epoch 061 | Batch 100/102 | Current Loss: 0.100148\n\n[Training] Epoch 061 Summary:\n  Avg Loss: 0.100148\n  Current LR: 3.18e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 062 | Batch 000/102 | Current Loss: 0.100140\n[Training] Epoch 062 | Batch 010/102 | Current Loss: 0.100117\n[Training] Epoch 062 | Batch 020/102 | Current Loss: 0.100175\n[Training] Epoch 062 | Batch 030/102 | Current Loss: 0.100167\n[Training] Epoch 062 | Batch 040/102 | Current Loss: 0.100090\n[Training] Epoch 062 | Batch 050/102 | Current Loss: 0.100189\n[Training] Epoch 062 | Batch 060/102 | Current Loss: 0.100153\n[Training] Epoch 062 | Batch 070/102 | Current Loss: 0.100143\n[Training] Epoch 062 | Batch 080/102 | Current Loss: 0.100117\n[Training] Epoch 062 | Batch 090/102 | Current Loss: 0.100177\n[Training] Epoch 062 | Batch 100/102 | Current Loss: 0.100311\n\n[Training] Epoch 062 Summary:\n  Avg Loss: 0.100148\n  Current LR: 2.95e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 063 | Batch 000/102 | Current Loss: 0.100089\n[Training] Epoch 063 | Batch 010/102 | Current Loss: 0.100183\n[Training] Epoch 063 | Batch 020/102 | Current Loss: 0.100138\n[Training] Epoch 063 | Batch 030/102 | Current Loss: 0.100120\n[Training] Epoch 063 | Batch 040/102 | Current Loss: 0.100104\n[Training] Epoch 063 | Batch 050/102 | Current Loss: 0.100115\n[Training] Epoch 063 | Batch 060/102 | Current Loss: 0.100204\n[Training] Epoch 063 | Batch 070/102 | Current Loss: 0.100191\n[Training] Epoch 063 | Batch 080/102 | Current Loss: 0.100053\n[Training] Epoch 063 | Batch 090/102 | Current Loss: 0.100135\n[Training] Epoch 063 | Batch 100/102 | Current Loss: 0.100102\n\n[Training] Epoch 063 Summary:\n  Avg Loss: 0.100129\n  Current LR: 2.59e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 064 | Batch 000/102 | Current Loss: 0.100143\n[Training] Epoch 064 | Batch 010/102 | Current Loss: 0.100099\n[Training] Epoch 064 | Batch 020/102 | Current Loss: 0.100184\n[Training] Epoch 064 | Batch 030/102 | Current Loss: 0.100139\n[Training] Epoch 064 | Batch 040/102 | Current Loss: 0.100197\n[Training] Epoch 064 | Batch 050/102 | Current Loss: 0.100165\n[Training] Epoch 064 | Batch 060/102 | Current Loss: 0.100089\n[Training] Epoch 064 | Batch 070/102 | Current Loss: 0.100183\n[Training] Epoch 064 | Batch 080/102 | Current Loss: 0.100141\n[Training] Epoch 064 | Batch 090/102 | Current Loss: 0.100126\n[Training] Epoch 064 | Batch 100/102 | Current Loss: 0.100048\n\n[Training] Epoch 064 Summary:\n  Avg Loss: 0.100123\n  Current LR: 2.14e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 065 | Batch 000/102 | Current Loss: 0.100064\n[Training] Epoch 065 | Batch 010/102 | Current Loss: 0.100139\n[Training] Epoch 065 | Batch 020/102 | Current Loss: 0.100060\n[Training] Epoch 065 | Batch 030/102 | Current Loss: 0.100165\n[Training] Epoch 065 | Batch 040/102 | Current Loss: 0.100114\n[Training] Epoch 065 | Batch 050/102 | Current Loss: 0.100100\n[Training] Epoch 065 | Batch 060/102 | Current Loss: 0.100072\n[Training] Epoch 065 | Batch 070/102 | Current Loss: 0.100088\n[Training] Epoch 065 | Batch 080/102 | Current Loss: 0.100077\n[Training] Epoch 065 | Batch 090/102 | Current Loss: 0.100121\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 16:52:52,927] Trial 10 finished with value: 0.10011125056474816 and parameters: {'lr': 0.00032621654698148115, 'batch_size': 128, 'bottleneck_width': 1024, 'dropout_rate': 0.25530728358474364, 'alpha': 0.4057686474355913, 'weight_decay': 2.834417625395888e-05}. Best is trial 8 with value: 0.10009658994043574.\n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 065 | Batch 100/102 | Current Loss: 0.100108\n\n[Training] Epoch 065 Summary:\n  Avg Loss: 0.100113\n  Current LR: 1.64e-04\n[EarlyStopping] Counter: 5/5\n\n[Training] Early stopping triggered\n\n[Optuna] Trial 10 completed with best loss: 0.100111\n\n==================================================\nStarting Optuna Trial 11\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.0002509917461542358\n  batch_size: 128\n  bottleneck_width: 1024\n  dropout_rate: 0.2711189373865902\n  alpha: 0.40628149224477644\n  weight_decay: 2.961558479046716e-05\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 128, Total batches: 102\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 1024\n  - Dropout rate: 0.2711189373865902\n[Model] Architecture initialized successfully\n[Model] Total parameters: 2,396,992\n[Optimizer] Initialized with lr=2.51e-04, weight_decay=2.96e-05\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.40628149224477644\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/102 | Current Loss: 1.251605\n[Training] Epoch 001 | Batch 010/102 | Current Loss: 0.186037\n[Training] Epoch 001 | Batch 020/102 | Current Loss: 0.139145\n[Training] Epoch 001 | Batch 030/102 | Current Loss: 0.119076\n[Training] Epoch 001 | Batch 040/102 | Current Loss: 0.115515\n[Training] Epoch 001 | Batch 050/102 | Current Loss: 0.110417\n[Training] Epoch 001 | Batch 060/102 | Current Loss: 0.109965\n[Training] Epoch 001 | Batch 070/102 | Current Loss: 0.109001\n[Training] Epoch 001 | Batch 080/102 | Current Loss: 0.107805\n[Training] Epoch 001 | Batch 090/102 | Current Loss: 0.106902\n[Training] Epoch 001 | Batch 100/102 | Current Loss: 0.106988\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.152522\n  Current LR: 2.45e-04\n[EarlyStopping] Loss improved (inf → 0.152522). Saving model...\n[Training] Epoch 002 | Batch 000/102 | Current Loss: 0.106368\n[Training] Epoch 002 | Batch 010/102 | Current Loss: 0.105738\n[Training] Epoch 002 | Batch 020/102 | Current Loss: 0.105391\n[Training] Epoch 002 | Batch 030/102 | Current Loss: 0.105084\n[Training] Epoch 002 | Batch 040/102 | Current Loss: 0.105655\n[Training] Epoch 002 | Batch 050/102 | Current Loss: 0.105360\n[Training] Epoch 002 | Batch 060/102 | Current Loss: 0.104990\n[Training] Epoch 002 | Batch 070/102 | Current Loss: 0.106782\n[Training] Epoch 002 | Batch 080/102 | Current Loss: 0.104927\n[Training] Epoch 002 | Batch 090/102 | Current Loss: 0.103763\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 16:53:21,275] Trial 11 pruned. \n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 002 | Batch 100/102 | Current Loss: 0.104919\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.105273\n  Current LR: 2.27e-04\n[EarlyStopping] Loss improved (0.152522 → 0.105273). Saving model...\n\n[Optuna] Trial pruned\n\n==================================================\nStarting Optuna Trial 12\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.0002877230690836362\n  batch_size: 128\n  bottleneck_width: 1024\n  dropout_rate: 0.2816276958807796\n  alpha: 0.40123484953380534\n  weight_decay: 2.2385618448297324e-05\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 128, Total batches: 102\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 1024\n  - Dropout rate: 0.2816276958807796\n[Model] Architecture initialized successfully\n[Model] Total parameters: 2,396,992\n[Optimizer] Initialized with lr=2.88e-04, weight_decay=2.24e-05\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.40123484953380534\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/102 | Current Loss: 1.314852\n[Training] Epoch 001 | Batch 010/102 | Current Loss: 0.186956\n[Training] Epoch 001 | Batch 020/102 | Current Loss: 0.136909\n[Training] Epoch 001 | Batch 030/102 | Current Loss: 0.119395\n[Training] Epoch 001 | Batch 040/102 | Current Loss: 0.115987\n[Training] Epoch 001 | Batch 050/102 | Current Loss: 0.111950\n[Training] Epoch 001 | Batch 060/102 | Current Loss: 0.109945\n[Training] Epoch 001 | Batch 070/102 | Current Loss: 0.110668\n[Training] Epoch 001 | Batch 080/102 | Current Loss: 0.108389\n[Training] Epoch 001 | Batch 090/102 | Current Loss: 0.107627\n[Training] Epoch 001 | Batch 100/102 | Current Loss: 0.106881\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.151981\n  Current LR: 2.81e-04\n[EarlyStopping] Loss improved (inf → 0.151981). Saving model...\n[Training] Epoch 002 | Batch 000/102 | Current Loss: 0.105564\n[Training] Epoch 002 | Batch 010/102 | Current Loss: 0.105580\n[Training] Epoch 002 | Batch 020/102 | Current Loss: 0.105132\n[Training] Epoch 002 | Batch 030/102 | Current Loss: 0.105291\n[Training] Epoch 002 | Batch 040/102 | Current Loss: 0.104772\n[Training] Epoch 002 | Batch 050/102 | Current Loss: 0.104653\n[Training] Epoch 002 | Batch 060/102 | Current Loss: 0.104200\n[Training] Epoch 002 | Batch 070/102 | Current Loss: 0.104212\n[Training] Epoch 002 | Batch 080/102 | Current Loss: 0.103859\n[Training] Epoch 002 | Batch 090/102 | Current Loss: 0.103621\n[Training] Epoch 002 | Batch 100/102 | Current Loss: 0.103549\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.104972\n  Current LR: 2.60e-04\n[EarlyStopping] Loss improved (0.151981 → 0.104972). Saving model...\n[Training] Epoch 003 | Batch 000/102 | Current Loss: 0.103539\n[Training] Epoch 003 | Batch 010/102 | Current Loss: 0.103484\n[Training] Epoch 003 | Batch 020/102 | Current Loss: 0.103489\n[Training] Epoch 003 | Batch 030/102 | Current Loss: 0.103058\n[Training] Epoch 003 | Batch 040/102 | Current Loss: 0.103173\n[Training] Epoch 003 | Batch 050/102 | Current Loss: 0.103106\n[Training] Epoch 003 | Batch 060/102 | Current Loss: 0.102983\n[Training] Epoch 003 | Batch 070/102 | Current Loss: 0.103124\n[Training] Epoch 003 | Batch 080/102 | Current Loss: 0.103028\n[Training] Epoch 003 | Batch 090/102 | Current Loss: 0.102644\n[Training] Epoch 003 | Batch 100/102 | Current Loss: 0.102564\n\n[Training] Epoch 003 Summary:\n  Avg Loss: 0.103153\n  Current LR: 2.29e-04\n[EarlyStopping] Loss improved (0.104972 → 0.103153). Saving model...\n[Training] Epoch 004 | Batch 000/102 | Current Loss: 0.102520\n[Training] Epoch 004 | Batch 010/102 | Current Loss: 0.102551\n[Training] Epoch 004 | Batch 020/102 | Current Loss: 0.102377\n[Training] Epoch 004 | Batch 030/102 | Current Loss: 0.102637\n[Training] Epoch 004 | Batch 040/102 | Current Loss: 0.102823\n[Training] Epoch 004 | Batch 050/102 | Current Loss: 0.102545\n[Training] Epoch 004 | Batch 060/102 | Current Loss: 0.102407\n[Training] Epoch 004 | Batch 070/102 | Current Loss: 0.102589\n[Training] Epoch 004 | Batch 080/102 | Current Loss: 0.102346\n[Training] Epoch 004 | Batch 090/102 | Current Loss: 0.102332\n[Training] Epoch 004 | Batch 100/102 | Current Loss: 0.102027\n\n[Training] Epoch 004 Summary:\n  Avg Loss: 0.102390\n  Current LR: 1.89e-04\n[EarlyStopping] Loss improved (0.103153 → 0.102390). Saving model...\n[Training] Epoch 005 | Batch 000/102 | Current Loss: 0.102028\n[Training] Epoch 005 | Batch 010/102 | Current Loss: 0.101970\n[Training] Epoch 005 | Batch 020/102 | Current Loss: 0.101929\n[Training] Epoch 005 | Batch 030/102 | Current Loss: 0.102086\n[Training] Epoch 005 | Batch 040/102 | Current Loss: 0.102243\n[Training] Epoch 005 | Batch 050/102 | Current Loss: 0.101986\n[Training] Epoch 005 | Batch 060/102 | Current Loss: 0.102047\n[Training] Epoch 005 | Batch 070/102 | Current Loss: 0.101734\n[Training] Epoch 005 | Batch 080/102 | Current Loss: 0.101975\n[Training] Epoch 005 | Batch 090/102 | Current Loss: 0.102987\n[Training] Epoch 005 | Batch 100/102 | Current Loss: 0.102932\n\n[Training] Epoch 005 Summary:\n  Avg Loss: 0.101989\n  Current LR: 1.44e-04\n[EarlyStopping] Loss improved (0.102390 → 0.101989). Saving model...\n[Training] Epoch 006 | Batch 000/102 | Current Loss: 0.101685\n[Training] Epoch 006 | Batch 010/102 | Current Loss: 0.101894\n[Training] Epoch 006 | Batch 020/102 | Current Loss: 0.101638\n[Training] Epoch 006 | Batch 030/102 | Current Loss: 0.101613\n[Training] Epoch 006 | Batch 040/102 | Current Loss: 0.101777\n[Training] Epoch 006 | Batch 050/102 | Current Loss: 0.101886\n[Training] Epoch 006 | Batch 060/102 | Current Loss: 0.101775\n[Training] Epoch 006 | Batch 070/102 | Current Loss: 0.101797\n[Training] Epoch 006 | Batch 080/102 | Current Loss: 0.101463\n[Training] Epoch 006 | Batch 090/102 | Current Loss: 0.101606\n[Training] Epoch 006 | Batch 100/102 | Current Loss: 0.101404\n\n[Training] Epoch 006 Summary:\n  Avg Loss: 0.101666\n  Current LR: 1.00e-04\n[EarlyStopping] Loss improved (0.101989 → 0.101666). Saving model...\n[Training] Epoch 007 | Batch 000/102 | Current Loss: 0.101489\n[Training] Epoch 007 | Batch 010/102 | Current Loss: 0.101437\n[Training] Epoch 007 | Batch 020/102 | Current Loss: 0.101732\n[Training] Epoch 007 | Batch 030/102 | Current Loss: 0.101411\n[Training] Epoch 007 | Batch 040/102 | Current Loss: 0.101574\n[Training] Epoch 007 | Batch 050/102 | Current Loss: 0.101357\n[Training] Epoch 007 | Batch 060/102 | Current Loss: 0.101392\n[Training] Epoch 007 | Batch 070/102 | Current Loss: 0.101487\n[Training] Epoch 007 | Batch 080/102 | Current Loss: 0.101655\n[Training] Epoch 007 | Batch 090/102 | Current Loss: 0.101411\n[Training] Epoch 007 | Batch 100/102 | Current Loss: 0.101302\n\n[Training] Epoch 007 Summary:\n  Avg Loss: 0.101547\n  Current LR: 6.01e-05\n[EarlyStopping] Loss improved (0.101666 → 0.101547). Saving model...\n[Training] Epoch 008 | Batch 000/102 | Current Loss: 0.101340\n[Training] Epoch 008 | Batch 010/102 | Current Loss: 0.101350\n[Training] Epoch 008 | Batch 020/102 | Current Loss: 0.101399\n[Training] Epoch 008 | Batch 030/102 | Current Loss: 0.101405\n[Training] Epoch 008 | Batch 040/102 | Current Loss: 0.101543\n[Training] Epoch 008 | Batch 050/102 | Current Loss: 0.101364\n[Training] Epoch 008 | Batch 060/102 | Current Loss: 0.101292\n[Training] Epoch 008 | Batch 070/102 | Current Loss: 0.101497\n[Training] Epoch 008 | Batch 080/102 | Current Loss: 0.101311\n[Training] Epoch 008 | Batch 090/102 | Current Loss: 0.101373\n[Training] Epoch 008 | Batch 100/102 | Current Loss: 0.101325\n\n[Training] Epoch 008 Summary:\n  Avg Loss: 0.101430\n  Current LR: 2.84e-05\n[EarlyStopping] Loss improved (0.101547 → 0.101430). Saving model...\n[Training] Epoch 009 | Batch 000/102 | Current Loss: 0.101230\n[Training] Epoch 009 | Batch 010/102 | Current Loss: 0.101362\n[Training] Epoch 009 | Batch 020/102 | Current Loss: 0.101284\n[Training] Epoch 009 | Batch 030/102 | Current Loss: 0.101318\n[Training] Epoch 009 | Batch 040/102 | Current Loss: 0.101456\n[Training] Epoch 009 | Batch 050/102 | Current Loss: 0.101354\n[Training] Epoch 009 | Batch 060/102 | Current Loss: 0.101408\n[Training] Epoch 009 | Batch 070/102 | Current Loss: 0.101467\n[Training] Epoch 009 | Batch 080/102 | Current Loss: 0.101316\n[Training] Epoch 009 | Batch 090/102 | Current Loss: 0.101444\n[Training] Epoch 009 | Batch 100/102 | Current Loss: 0.101356\n\n[Training] Epoch 009 Summary:\n  Avg Loss: 0.101360\n  Current LR: 8.02e-06\n[EarlyStopping] Loss improved (0.101430 → 0.101360). Saving model...\n[Training] Epoch 010 | Batch 000/102 | Current Loss: 0.101382\n[Training] Epoch 010 | Batch 010/102 | Current Loss: 0.101328\n[Training] Epoch 010 | Batch 020/102 | Current Loss: 0.101209\n[Training] Epoch 010 | Batch 030/102 | Current Loss: 0.101390\n[Training] Epoch 010 | Batch 040/102 | Current Loss: 0.101294\n[Training] Epoch 010 | Batch 050/102 | Current Loss: 0.101103\n[Training] Epoch 010 | Batch 060/102 | Current Loss: 0.101410\n[Training] Epoch 010 | Batch 070/102 | Current Loss: 0.101349\n[Training] Epoch 010 | Batch 080/102 | Current Loss: 0.101265\n[Training] Epoch 010 | Batch 090/102 | Current Loss: 0.101319\n[Training] Epoch 010 | Batch 100/102 | Current Loss: 0.101328\n\n[Training] Epoch 010 Summary:\n  Avg Loss: 0.101323\n  Current LR: 2.88e-04\n[EarlyStopping] Loss improved (0.101360 → 0.101323). Saving model...\n[Training] Epoch 011 | Batch 000/102 | Current Loss: 0.101343\n[Training] Epoch 011 | Batch 010/102 | Current Loss: 0.101452\n[Training] Epoch 011 | Batch 020/102 | Current Loss: 0.101444\n[Training] Epoch 011 | Batch 030/102 | Current Loss: 0.101297\n[Training] Epoch 011 | Batch 040/102 | Current Loss: 0.101295\n[Training] Epoch 011 | Batch 050/102 | Current Loss: 0.101209\n[Training] Epoch 011 | Batch 060/102 | Current Loss: 0.101187\n[Training] Epoch 011 | Batch 070/102 | Current Loss: 0.101362\n[Training] Epoch 011 | Batch 080/102 | Current Loss: 0.101197\n[Training] Epoch 011 | Batch 090/102 | Current Loss: 0.101300\n[Training] Epoch 011 | Batch 100/102 | Current Loss: 0.101125\n\n[Training] Epoch 011 Summary:\n  Avg Loss: 0.101403\n  Current LR: 2.81e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 012 | Batch 000/102 | Current Loss: 0.101523\n[Training] Epoch 012 | Batch 010/102 | Current Loss: 0.101536\n[Training] Epoch 012 | Batch 020/102 | Current Loss: 0.101310\n[Training] Epoch 012 | Batch 030/102 | Current Loss: 0.101224\n[Training] Epoch 012 | Batch 040/102 | Current Loss: 0.100942\n[Training] Epoch 012 | Batch 050/102 | Current Loss: 0.101046\n[Training] Epoch 012 | Batch 060/102 | Current Loss: 0.101222\n[Training] Epoch 012 | Batch 070/102 | Current Loss: 0.100950\n[Training] Epoch 012 | Batch 080/102 | Current Loss: 0.100987\n[Training] Epoch 012 | Batch 090/102 | Current Loss: 0.101145\n[Training] Epoch 012 | Batch 100/102 | Current Loss: 0.101059\n\n[Training] Epoch 012 Summary:\n  Avg Loss: 0.101070\n  Current LR: 2.60e-04\n[EarlyStopping] Loss improved (0.101323 → 0.101070). Saving model...\n[Training] Epoch 013 | Batch 000/102 | Current Loss: 0.100972\n[Training] Epoch 013 | Batch 010/102 | Current Loss: 0.101016\n[Training] Epoch 013 | Batch 020/102 | Current Loss: 0.100919\n[Training] Epoch 013 | Batch 030/102 | Current Loss: 0.100872\n[Training] Epoch 013 | Batch 040/102 | Current Loss: 0.100828\n[Training] Epoch 013 | Batch 050/102 | Current Loss: 0.101050\n[Training] Epoch 013 | Batch 060/102 | Current Loss: 0.101067\n[Training] Epoch 013 | Batch 070/102 | Current Loss: 0.100810\n[Training] Epoch 013 | Batch 080/102 | Current Loss: 0.100879\n[Training] Epoch 013 | Batch 090/102 | Current Loss: 0.100816\n[Training] Epoch 013 | Batch 100/102 | Current Loss: 0.100772\n\n[Training] Epoch 013 Summary:\n  Avg Loss: 0.100902\n  Current LR: 2.29e-04\n[EarlyStopping] Loss improved (0.101070 → 0.100902). Saving model...\n[Training] Epoch 014 | Batch 000/102 | Current Loss: 0.100741\n[Training] Epoch 014 | Batch 010/102 | Current Loss: 0.100847\n[Training] Epoch 014 | Batch 020/102 | Current Loss: 0.100786\n[Training] Epoch 014 | Batch 030/102 | Current Loss: 0.100721\n[Training] Epoch 014 | Batch 040/102 | Current Loss: 0.100685\n[Training] Epoch 014 | Batch 050/102 | Current Loss: 0.100971\n[Training] Epoch 014 | Batch 060/102 | Current Loss: 0.100733\n[Training] Epoch 014 | Batch 070/102 | Current Loss: 0.100833\n[Training] Epoch 014 | Batch 080/102 | Current Loss: 0.100971\n[Training] Epoch 014 | Batch 090/102 | Current Loss: 0.100733\n[Training] Epoch 014 | Batch 100/102 | Current Loss: 0.100718\n\n[Training] Epoch 014 Summary:\n  Avg Loss: 0.100752\n  Current LR: 1.89e-04\n[EarlyStopping] Loss improved (0.100902 → 0.100752). Saving model...\n[Training] Epoch 015 | Batch 000/102 | Current Loss: 0.100691\n[Training] Epoch 015 | Batch 010/102 | Current Loss: 0.100802\n[Training] Epoch 015 | Batch 020/102 | Current Loss: 0.100670\n[Training] Epoch 015 | Batch 030/102 | Current Loss: 0.100775\n[Training] Epoch 015 | Batch 040/102 | Current Loss: 0.100651\n[Training] Epoch 015 | Batch 050/102 | Current Loss: 0.100747\n[Training] Epoch 015 | Batch 060/102 | Current Loss: 0.100761\n[Training] Epoch 015 | Batch 070/102 | Current Loss: 0.100628\n[Training] Epoch 015 | Batch 080/102 | Current Loss: 0.100710\n[Training] Epoch 015 | Batch 090/102 | Current Loss: 0.100704\n[Training] Epoch 015 | Batch 100/102 | Current Loss: 0.100648\n\n[Training] Epoch 015 Summary:\n  Avg Loss: 0.100692\n  Current LR: 1.44e-04\n[EarlyStopping] Loss improved (0.100752 → 0.100692). Saving model...\n[Training] Epoch 016 | Batch 000/102 | Current Loss: 0.100669\n[Training] Epoch 016 | Batch 010/102 | Current Loss: 0.100623\n[Training] Epoch 016 | Batch 020/102 | Current Loss: 0.100707\n[Training] Epoch 016 | Batch 030/102 | Current Loss: 0.100652\n[Training] Epoch 016 | Batch 040/102 | Current Loss: 0.100586\n[Training] Epoch 016 | Batch 050/102 | Current Loss: 0.100710\n[Training] Epoch 016 | Batch 060/102 | Current Loss: 0.100757\n[Training] Epoch 016 | Batch 070/102 | Current Loss: 0.100596\n[Training] Epoch 016 | Batch 080/102 | Current Loss: 0.100812\n[Training] Epoch 016 | Batch 090/102 | Current Loss: 0.100614\n[Training] Epoch 016 | Batch 100/102 | Current Loss: 0.100600\n\n[Training] Epoch 016 Summary:\n  Avg Loss: 0.100642\n  Current LR: 1.00e-04\n[EarlyStopping] Loss improved (0.100692 → 0.100642). Saving model...\n[Training] Epoch 017 | Batch 000/102 | Current Loss: 0.100551\n[Training] Epoch 017 | Batch 010/102 | Current Loss: 0.100528\n[Training] Epoch 017 | Batch 020/102 | Current Loss: 0.100594\n[Training] Epoch 017 | Batch 030/102 | Current Loss: 0.100575\n[Training] Epoch 017 | Batch 040/102 | Current Loss: 0.100550\n[Training] Epoch 017 | Batch 050/102 | Current Loss: 0.100562\n[Training] Epoch 017 | Batch 060/102 | Current Loss: 0.100519\n[Training] Epoch 017 | Batch 070/102 | Current Loss: 0.100585\n[Training] Epoch 017 | Batch 080/102 | Current Loss: 0.100533\n[Training] Epoch 017 | Batch 090/102 | Current Loss: 0.100641\n[Training] Epoch 017 | Batch 100/102 | Current Loss: 0.100577\n\n[Training] Epoch 017 Summary:\n  Avg Loss: 0.100588\n  Current LR: 6.01e-05\n[EarlyStopping] Loss improved (0.100642 → 0.100588). Saving model...\n[Training] Epoch 018 | Batch 000/102 | Current Loss: 0.100592\n[Training] Epoch 018 | Batch 010/102 | Current Loss: 0.100559\n[Training] Epoch 018 | Batch 020/102 | Current Loss: 0.100639\n[Training] Epoch 018 | Batch 030/102 | Current Loss: 0.100467\n[Training] Epoch 018 | Batch 040/102 | Current Loss: 0.100720\n[Training] Epoch 018 | Batch 050/102 | Current Loss: 0.100483\n[Training] Epoch 018 | Batch 060/102 | Current Loss: 0.100494\n[Training] Epoch 018 | Batch 070/102 | Current Loss: 0.100639\n[Training] Epoch 018 | Batch 080/102 | Current Loss: 0.100551\n[Training] Epoch 018 | Batch 090/102 | Current Loss: 0.100536\n[Training] Epoch 018 | Batch 100/102 | Current Loss: 0.100541\n\n[Training] Epoch 018 Summary:\n  Avg Loss: 0.100564\n  Current LR: 2.84e-05\n[EarlyStopping] Loss improved (0.100588 → 0.100564). Saving model...\n[Training] Epoch 019 | Batch 000/102 | Current Loss: 0.100586\n[Training] Epoch 019 | Batch 010/102 | Current Loss: 0.100529\n[Training] Epoch 019 | Batch 020/102 | Current Loss: 0.100550\n[Training] Epoch 019 | Batch 030/102 | Current Loss: 0.100472\n[Training] Epoch 019 | Batch 040/102 | Current Loss: 0.100507\n[Training] Epoch 019 | Batch 050/102 | Current Loss: 0.100541\n[Training] Epoch 019 | Batch 060/102 | Current Loss: 0.100506\n[Training] Epoch 019 | Batch 070/102 | Current Loss: 0.100486\n[Training] Epoch 019 | Batch 080/102 | Current Loss: 0.100433\n[Training] Epoch 019 | Batch 090/102 | Current Loss: 0.100621\n[Training] Epoch 019 | Batch 100/102 | Current Loss: 0.100590\n\n[Training] Epoch 019 Summary:\n  Avg Loss: 0.100550\n  Current LR: 8.02e-06\n[EarlyStopping] Loss improved (0.100564 → 0.100550). Saving model...\n[Training] Epoch 020 | Batch 000/102 | Current Loss: 0.100530\n[Training] Epoch 020 | Batch 010/102 | Current Loss: 0.100495\n[Training] Epoch 020 | Batch 020/102 | Current Loss: 0.100506\n[Training] Epoch 020 | Batch 030/102 | Current Loss: 0.100486\n[Training] Epoch 020 | Batch 040/102 | Current Loss: 0.100528\n[Training] Epoch 020 | Batch 050/102 | Current Loss: 0.100483\n[Training] Epoch 020 | Batch 060/102 | Current Loss: 0.100504\n[Training] Epoch 020 | Batch 070/102 | Current Loss: 0.100605\n[Training] Epoch 020 | Batch 080/102 | Current Loss: 0.100557\n[Training] Epoch 020 | Batch 090/102 | Current Loss: 0.100434\n[Training] Epoch 020 | Batch 100/102 | Current Loss: 0.100543\n\n[Training] Epoch 020 Summary:\n  Avg Loss: 0.100533\n  Current LR: 2.88e-04\n[EarlyStopping] Loss improved (0.100550 → 0.100533). Saving model...\n[Training] Epoch 021 | Batch 000/102 | Current Loss: 0.100635\n[Training] Epoch 021 | Batch 010/102 | Current Loss: 0.100556\n[Training] Epoch 021 | Batch 020/102 | Current Loss: 0.100564\n[Training] Epoch 021 | Batch 030/102 | Current Loss: 0.100513\n[Training] Epoch 021 | Batch 040/102 | Current Loss: 0.100564\n[Training] Epoch 021 | Batch 050/102 | Current Loss: 0.100555\n[Training] Epoch 021 | Batch 060/102 | Current Loss: 0.100540\n[Training] Epoch 021 | Batch 070/102 | Current Loss: 0.100606\n[Training] Epoch 021 | Batch 080/102 | Current Loss: 0.100491\n[Training] Epoch 021 | Batch 090/102 | Current Loss: 0.100497\n[Training] Epoch 021 | Batch 100/102 | Current Loss: 0.100441\n\n[Training] Epoch 021 Summary:\n  Avg Loss: 0.100546\n  Current LR: 2.81e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 022 | Batch 000/102 | Current Loss: 0.100580\n[Training] Epoch 022 | Batch 010/102 | Current Loss: 0.100426\n[Training] Epoch 022 | Batch 020/102 | Current Loss: 0.100477\n[Training] Epoch 022 | Batch 030/102 | Current Loss: 0.100562\n[Training] Epoch 022 | Batch 040/102 | Current Loss: 0.100475\n[Training] Epoch 022 | Batch 050/102 | Current Loss: 0.100599\n[Training] Epoch 022 | Batch 060/102 | Current Loss: 0.100412\n[Training] Epoch 022 | Batch 070/102 | Current Loss: 0.100469\n[Training] Epoch 022 | Batch 080/102 | Current Loss: 0.100387\n[Training] Epoch 022 | Batch 090/102 | Current Loss: 0.100496\n[Training] Epoch 022 | Batch 100/102 | Current Loss: 0.100351\n\n[Training] Epoch 022 Summary:\n  Avg Loss: 0.100482\n  Current LR: 2.60e-04\n[EarlyStopping] Loss improved (0.100533 → 0.100482). Saving model...\n[Training] Epoch 023 | Batch 000/102 | Current Loss: 0.100461\n[Training] Epoch 023 | Batch 010/102 | Current Loss: 0.100469\n[Training] Epoch 023 | Batch 020/102 | Current Loss: 0.100416\n[Training] Epoch 023 | Batch 030/102 | Current Loss: 0.100582\n[Training] Epoch 023 | Batch 040/102 | Current Loss: 0.100371\n[Training] Epoch 023 | Batch 050/102 | Current Loss: 0.100466\n[Training] Epoch 023 | Batch 060/102 | Current Loss: 0.100386\n[Training] Epoch 023 | Batch 070/102 | Current Loss: 0.100372\n[Training] Epoch 023 | Batch 080/102 | Current Loss: 0.100329\n[Training] Epoch 023 | Batch 090/102 | Current Loss: 0.100421\n[Training] Epoch 023 | Batch 100/102 | Current Loss: 0.100420\n\n[Training] Epoch 023 Summary:\n  Avg Loss: 0.100432\n  Current LR: 2.29e-04\n[EarlyStopping] Loss improved (0.100482 → 0.100432). Saving model...\n[Training] Epoch 024 | Batch 000/102 | Current Loss: 0.100350\n[Training] Epoch 024 | Batch 010/102 | Current Loss: 0.100380\n[Training] Epoch 024 | Batch 020/102 | Current Loss: 0.100437\n[Training] Epoch 024 | Batch 030/102 | Current Loss: 0.100419\n[Training] Epoch 024 | Batch 040/102 | Current Loss: 0.100387\n[Training] Epoch 024 | Batch 050/102 | Current Loss: 0.100389\n[Training] Epoch 024 | Batch 060/102 | Current Loss: 0.100385\n[Training] Epoch 024 | Batch 070/102 | Current Loss: 0.100409\n[Training] Epoch 024 | Batch 080/102 | Current Loss: 0.100437\n[Training] Epoch 024 | Batch 090/102 | Current Loss: 0.100436\n[Training] Epoch 024 | Batch 100/102 | Current Loss: 0.100330\n\n[Training] Epoch 024 Summary:\n  Avg Loss: 0.100398\n  Current LR: 1.89e-04\n[EarlyStopping] Loss improved (0.100432 → 0.100398). Saving model...\n[Training] Epoch 025 | Batch 000/102 | Current Loss: 0.100333\n[Training] Epoch 025 | Batch 010/102 | Current Loss: 0.100317\n[Training] Epoch 025 | Batch 020/102 | Current Loss: 0.100338\n[Training] Epoch 025 | Batch 030/102 | Current Loss: 0.100443\n[Training] Epoch 025 | Batch 040/102 | Current Loss: 0.100456\n[Training] Epoch 025 | Batch 050/102 | Current Loss: 0.100447\n[Training] Epoch 025 | Batch 060/102 | Current Loss: 0.100398\n[Training] Epoch 025 | Batch 070/102 | Current Loss: 0.100488\n[Training] Epoch 025 | Batch 080/102 | Current Loss: 0.100332\n[Training] Epoch 025 | Batch 090/102 | Current Loss: 0.100352\n[Training] Epoch 025 | Batch 100/102 | Current Loss: 0.100434\n\n[Training] Epoch 025 Summary:\n  Avg Loss: 0.100361\n  Current LR: 1.44e-04\n[EarlyStopping] Loss improved (0.100398 → 0.100361). Saving model...\n[Training] Epoch 026 | Batch 000/102 | Current Loss: 0.100344\n[Training] Epoch 026 | Batch 010/102 | Current Loss: 0.100380\n[Training] Epoch 026 | Batch 020/102 | Current Loss: 0.100381\n[Training] Epoch 026 | Batch 030/102 | Current Loss: 0.100360\n[Training] Epoch 026 | Batch 040/102 | Current Loss: 0.100275\n[Training] Epoch 026 | Batch 050/102 | Current Loss: 0.100299\n[Training] Epoch 026 | Batch 060/102 | Current Loss: 0.100361\n[Training] Epoch 026 | Batch 070/102 | Current Loss: 0.100339\n[Training] Epoch 026 | Batch 080/102 | Current Loss: 0.100343\n[Training] Epoch 026 | Batch 090/102 | Current Loss: 0.100305\n[Training] Epoch 026 | Batch 100/102 | Current Loss: 0.100251\n\n[Training] Epoch 026 Summary:\n  Avg Loss: 0.100345\n  Current LR: 1.00e-04\n[EarlyStopping] Loss improved (0.100361 → 0.100345). Saving model...\n[Training] Epoch 027 | Batch 000/102 | Current Loss: 0.100367\n[Training] Epoch 027 | Batch 010/102 | Current Loss: 0.100341\n[Training] Epoch 027 | Batch 020/102 | Current Loss: 0.100299\n[Training] Epoch 027 | Batch 030/102 | Current Loss: 0.100313\n[Training] Epoch 027 | Batch 040/102 | Current Loss: 0.100305\n[Training] Epoch 027 | Batch 050/102 | Current Loss: 0.100281\n[Training] Epoch 027 | Batch 060/102 | Current Loss: 0.100340\n[Training] Epoch 027 | Batch 070/102 | Current Loss: 0.100362\n[Training] Epoch 027 | Batch 080/102 | Current Loss: 0.100311\n[Training] Epoch 027 | Batch 090/102 | Current Loss: 0.100384\n[Training] Epoch 027 | Batch 100/102 | Current Loss: 0.100352\n\n[Training] Epoch 027 Summary:\n  Avg Loss: 0.100322\n  Current LR: 6.01e-05\n[EarlyStopping] Loss improved (0.100345 → 0.100322). Saving model...\n[Training] Epoch 028 | Batch 000/102 | Current Loss: 0.100324\n[Training] Epoch 028 | Batch 010/102 | Current Loss: 0.100315\n[Training] Epoch 028 | Batch 020/102 | Current Loss: 0.100367\n[Training] Epoch 028 | Batch 030/102 | Current Loss: 0.100319\n[Training] Epoch 028 | Batch 040/102 | Current Loss: 0.100277\n[Training] Epoch 028 | Batch 050/102 | Current Loss: 0.100240\n[Training] Epoch 028 | Batch 060/102 | Current Loss: 0.100282\n[Training] Epoch 028 | Batch 070/102 | Current Loss: 0.100281\n[Training] Epoch 028 | Batch 080/102 | Current Loss: 0.100285\n[Training] Epoch 028 | Batch 090/102 | Current Loss: 0.100376\n[Training] Epoch 028 | Batch 100/102 | Current Loss: 0.100335\n\n[Training] Epoch 028 Summary:\n  Avg Loss: 0.100311\n  Current LR: 2.84e-05\n[EarlyStopping] Loss improved (0.100322 → 0.100311). Saving model...\n[Training] Epoch 029 | Batch 000/102 | Current Loss: 0.100348\n[Training] Epoch 029 | Batch 010/102 | Current Loss: 0.100295\n[Training] Epoch 029 | Batch 020/102 | Current Loss: 0.100293\n[Training] Epoch 029 | Batch 030/102 | Current Loss: 0.100256\n[Training] Epoch 029 | Batch 040/102 | Current Loss: 0.100381\n[Training] Epoch 029 | Batch 050/102 | Current Loss: 0.100336\n[Training] Epoch 029 | Batch 060/102 | Current Loss: 0.100242\n[Training] Epoch 029 | Batch 070/102 | Current Loss: 0.100352\n[Training] Epoch 029 | Batch 080/102 | Current Loss: 0.100351\n[Training] Epoch 029 | Batch 090/102 | Current Loss: 0.100226\n[Training] Epoch 029 | Batch 100/102 | Current Loss: 0.100307\n\n[Training] Epoch 029 Summary:\n  Avg Loss: 0.100310\n  Current LR: 8.02e-06\n[EarlyStopping] Loss improved (0.100311 → 0.100310). Saving model...\n[Training] Epoch 030 | Batch 000/102 | Current Loss: 0.100264\n[Training] Epoch 030 | Batch 010/102 | Current Loss: 0.100330\n[Training] Epoch 030 | Batch 020/102 | Current Loss: 0.100237\n[Training] Epoch 030 | Batch 030/102 | Current Loss: 0.100192\n[Training] Epoch 030 | Batch 040/102 | Current Loss: 0.100364\n[Training] Epoch 030 | Batch 050/102 | Current Loss: 0.100297\n[Training] Epoch 030 | Batch 060/102 | Current Loss: 0.100298\n[Training] Epoch 030 | Batch 070/102 | Current Loss: 0.100304\n[Training] Epoch 030 | Batch 080/102 | Current Loss: 0.100291\n[Training] Epoch 030 | Batch 090/102 | Current Loss: 0.100269\n[Training] Epoch 030 | Batch 100/102 | Current Loss: 0.100373\n\n[Training] Epoch 030 Summary:\n  Avg Loss: 0.100303\n  Current LR: 2.88e-04\n[EarlyStopping] Loss improved (0.100310 → 0.100303). Saving model...\n[Training] Epoch 031 | Batch 000/102 | Current Loss: 0.100247\n[Training] Epoch 031 | Batch 010/102 | Current Loss: 0.100312\n[Training] Epoch 031 | Batch 020/102 | Current Loss: 0.100334\n[Training] Epoch 031 | Batch 030/102 | Current Loss: 0.100290\n[Training] Epoch 031 | Batch 040/102 | Current Loss: 0.100223\n[Training] Epoch 031 | Batch 050/102 | Current Loss: 0.100274\n[Training] Epoch 031 | Batch 060/102 | Current Loss: 0.100331\n[Training] Epoch 031 | Batch 070/102 | Current Loss: 0.100352\n[Training] Epoch 031 | Batch 080/102 | Current Loss: 0.100331\n[Training] Epoch 031 | Batch 090/102 | Current Loss: 0.100321\n[Training] Epoch 031 | Batch 100/102 | Current Loss: 0.100294\n\n[Training] Epoch 031 Summary:\n  Avg Loss: 0.100318\n  Current LR: 2.81e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 032 | Batch 000/102 | Current Loss: 0.100257\n[Training] Epoch 032 | Batch 010/102 | Current Loss: 0.100243\n[Training] Epoch 032 | Batch 020/102 | Current Loss: 0.100349\n[Training] Epoch 032 | Batch 030/102 | Current Loss: 0.100332\n[Training] Epoch 032 | Batch 040/102 | Current Loss: 0.100271\n[Training] Epoch 032 | Batch 050/102 | Current Loss: 0.100325\n[Training] Epoch 032 | Batch 060/102 | Current Loss: 0.100267\n[Training] Epoch 032 | Batch 070/102 | Current Loss: 0.100373\n[Training] Epoch 032 | Batch 080/102 | Current Loss: 0.100343\n[Training] Epoch 032 | Batch 090/102 | Current Loss: 0.100305\n[Training] Epoch 032 | Batch 100/102 | Current Loss: 0.100289\n\n[Training] Epoch 032 Summary:\n  Avg Loss: 0.100292\n  Current LR: 2.60e-04\n[EarlyStopping] Loss improved (0.100303 → 0.100292). Saving model...\n[Training] Epoch 033 | Batch 000/102 | Current Loss: 0.100284\n[Training] Epoch 033 | Batch 010/102 | Current Loss: 0.100289\n[Training] Epoch 033 | Batch 020/102 | Current Loss: 0.100252\n[Training] Epoch 033 | Batch 030/102 | Current Loss: 0.100212\n[Training] Epoch 033 | Batch 040/102 | Current Loss: 0.100408\n[Training] Epoch 033 | Batch 050/102 | Current Loss: 0.100217\n[Training] Epoch 033 | Batch 060/102 | Current Loss: 0.100215\n[Training] Epoch 033 | Batch 070/102 | Current Loss: 0.100310\n[Training] Epoch 033 | Batch 080/102 | Current Loss: 0.100373\n[Training] Epoch 033 | Batch 090/102 | Current Loss: 0.100280\n[Training] Epoch 033 | Batch 100/102 | Current Loss: 0.100265\n\n[Training] Epoch 033 Summary:\n  Avg Loss: 0.100280\n  Current LR: 2.29e-04\n[EarlyStopping] Loss improved (0.100292 → 0.100280). Saving model...\n[Training] Epoch 034 | Batch 000/102 | Current Loss: 0.100191\n[Training] Epoch 034 | Batch 010/102 | Current Loss: 0.100298\n[Training] Epoch 034 | Batch 020/102 | Current Loss: 0.100219\n[Training] Epoch 034 | Batch 030/102 | Current Loss: 0.100185\n[Training] Epoch 034 | Batch 040/102 | Current Loss: 0.100238\n[Training] Epoch 034 | Batch 050/102 | Current Loss: 0.100220\n[Training] Epoch 034 | Batch 060/102 | Current Loss: 0.100240\n[Training] Epoch 034 | Batch 070/102 | Current Loss: 0.100254\n[Training] Epoch 034 | Batch 080/102 | Current Loss: 0.100254\n[Training] Epoch 034 | Batch 090/102 | Current Loss: 0.100216\n[Training] Epoch 034 | Batch 100/102 | Current Loss: 0.100237\n\n[Training] Epoch 034 Summary:\n  Avg Loss: 0.100255\n  Current LR: 1.89e-04\n[EarlyStopping] Loss improved (0.100280 → 0.100255). Saving model...\n[Training] Epoch 035 | Batch 000/102 | Current Loss: 0.100224\n[Training] Epoch 035 | Batch 010/102 | Current Loss: 0.100250\n[Training] Epoch 035 | Batch 020/102 | Current Loss: 0.100188\n[Training] Epoch 035 | Batch 030/102 | Current Loss: 0.100325\n[Training] Epoch 035 | Batch 040/102 | Current Loss: 0.100198\n[Training] Epoch 035 | Batch 050/102 | Current Loss: 0.100238\n[Training] Epoch 035 | Batch 060/102 | Current Loss: 0.100253\n[Training] Epoch 035 | Batch 070/102 | Current Loss: 0.100200\n[Training] Epoch 035 | Batch 080/102 | Current Loss: 0.100167\n[Training] Epoch 035 | Batch 090/102 | Current Loss: 0.100272\n[Training] Epoch 035 | Batch 100/102 | Current Loss: 0.100225\n\n[Training] Epoch 035 Summary:\n  Avg Loss: 0.100245\n  Current LR: 1.44e-04\n[EarlyStopping] Loss improved (0.100255 → 0.100245). Saving model...\n[Training] Epoch 036 | Batch 000/102 | Current Loss: 0.100255\n[Training] Epoch 036 | Batch 010/102 | Current Loss: 0.100157\n[Training] Epoch 036 | Batch 020/102 | Current Loss: 0.100234\n[Training] Epoch 036 | Batch 030/102 | Current Loss: 0.100248\n[Training] Epoch 036 | Batch 040/102 | Current Loss: 0.100248\n[Training] Epoch 036 | Batch 050/102 | Current Loss: 0.100203\n[Training] Epoch 036 | Batch 060/102 | Current Loss: 0.100145\n[Training] Epoch 036 | Batch 070/102 | Current Loss: 0.100104\n[Training] Epoch 036 | Batch 080/102 | Current Loss: 0.100326\n[Training] Epoch 036 | Batch 090/102 | Current Loss: 0.100214\n[Training] Epoch 036 | Batch 100/102 | Current Loss: 0.100181\n\n[Training] Epoch 036 Summary:\n  Avg Loss: 0.100234\n  Current LR: 1.00e-04\n[EarlyStopping] Loss improved (0.100245 → 0.100234). Saving model...\n[Training] Epoch 037 | Batch 000/102 | Current Loss: 0.100228\n[Training] Epoch 037 | Batch 010/102 | Current Loss: 0.100153\n[Training] Epoch 037 | Batch 020/102 | Current Loss: 0.100226\n[Training] Epoch 037 | Batch 030/102 | Current Loss: 0.100264\n[Training] Epoch 037 | Batch 040/102 | Current Loss: 0.100248\n[Training] Epoch 037 | Batch 050/102 | Current Loss: 0.100252\n[Training] Epoch 037 | Batch 060/102 | Current Loss: 0.100245\n[Training] Epoch 037 | Batch 070/102 | Current Loss: 0.100273\n[Training] Epoch 037 | Batch 080/102 | Current Loss: 0.100208\n[Training] Epoch 037 | Batch 090/102 | Current Loss: 0.100242\n[Training] Epoch 037 | Batch 100/102 | Current Loss: 0.100216\n\n[Training] Epoch 037 Summary:\n  Avg Loss: 0.100225\n  Current LR: 6.01e-05\n[EarlyStopping] Loss improved (0.100234 → 0.100225). Saving model...\n[Training] Epoch 038 | Batch 000/102 | Current Loss: 0.100228\n[Training] Epoch 038 | Batch 010/102 | Current Loss: 0.100281\n[Training] Epoch 038 | Batch 020/102 | Current Loss: 0.100215\n[Training] Epoch 038 | Batch 030/102 | Current Loss: 0.100148\n[Training] Epoch 038 | Batch 040/102 | Current Loss: 0.100221\n[Training] Epoch 038 | Batch 050/102 | Current Loss: 0.100253\n[Training] Epoch 038 | Batch 060/102 | Current Loss: 0.100206\n[Training] Epoch 038 | Batch 070/102 | Current Loss: 0.100208\n[Training] Epoch 038 | Batch 080/102 | Current Loss: 0.100225\n[Training] Epoch 038 | Batch 090/102 | Current Loss: 0.100225\n[Training] Epoch 038 | Batch 100/102 | Current Loss: 0.100239\n\n[Training] Epoch 038 Summary:\n  Avg Loss: 0.100220\n  Current LR: 2.84e-05\n[EarlyStopping] Loss improved (0.100225 → 0.100220). Saving model...\n[Training] Epoch 039 | Batch 000/102 | Current Loss: 0.100227\n[Training] Epoch 039 | Batch 010/102 | Current Loss: 0.100255\n[Training] Epoch 039 | Batch 020/102 | Current Loss: 0.100226\n[Training] Epoch 039 | Batch 030/102 | Current Loss: 0.100241\n[Training] Epoch 039 | Batch 040/102 | Current Loss: 0.100177\n[Training] Epoch 039 | Batch 050/102 | Current Loss: 0.100209\n[Training] Epoch 039 | Batch 060/102 | Current Loss: 0.100193\n[Training] Epoch 039 | Batch 070/102 | Current Loss: 0.100154\n[Training] Epoch 039 | Batch 080/102 | Current Loss: 0.100207\n[Training] Epoch 039 | Batch 090/102 | Current Loss: 0.100197\n[Training] Epoch 039 | Batch 100/102 | Current Loss: 0.100207\n\n[Training] Epoch 039 Summary:\n  Avg Loss: 0.100208\n  Current LR: 8.02e-06\n[EarlyStopping] Loss improved (0.100220 → 0.100208). Saving model...\n[Training] Epoch 040 | Batch 000/102 | Current Loss: 0.100205\n[Training] Epoch 040 | Batch 010/102 | Current Loss: 0.100164\n[Training] Epoch 040 | Batch 020/102 | Current Loss: 0.100309\n[Training] Epoch 040 | Batch 030/102 | Current Loss: 0.100147\n[Training] Epoch 040 | Batch 040/102 | Current Loss: 0.100302\n[Training] Epoch 040 | Batch 050/102 | Current Loss: 0.100238\n[Training] Epoch 040 | Batch 060/102 | Current Loss: 0.100300\n[Training] Epoch 040 | Batch 070/102 | Current Loss: 0.100197\n[Training] Epoch 040 | Batch 080/102 | Current Loss: 0.100222\n[Training] Epoch 040 | Batch 090/102 | Current Loss: 0.100225\n[Training] Epoch 040 | Batch 100/102 | Current Loss: 0.100207\n\n[Training] Epoch 040 Summary:\n  Avg Loss: 0.100208\n  Current LR: 2.88e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 041 | Batch 000/102 | Current Loss: 0.100186\n[Training] Epoch 041 | Batch 010/102 | Current Loss: 0.100231\n[Training] Epoch 041 | Batch 020/102 | Current Loss: 0.100258\n[Training] Epoch 041 | Batch 030/102 | Current Loss: 0.100293\n[Training] Epoch 041 | Batch 040/102 | Current Loss: 0.100207\n[Training] Epoch 041 | Batch 050/102 | Current Loss: 0.100245\n[Training] Epoch 041 | Batch 060/102 | Current Loss: 0.100203\n[Training] Epoch 041 | Batch 070/102 | Current Loss: 0.100199\n[Training] Epoch 041 | Batch 080/102 | Current Loss: 0.100217\n[Training] Epoch 041 | Batch 090/102 | Current Loss: 0.100129\n[Training] Epoch 041 | Batch 100/102 | Current Loss: 0.100261\n\n[Training] Epoch 041 Summary:\n  Avg Loss: 0.100235\n  Current LR: 2.81e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 042 | Batch 000/102 | Current Loss: 0.100274\n[Training] Epoch 042 | Batch 010/102 | Current Loss: 0.100199\n[Training] Epoch 042 | Batch 020/102 | Current Loss: 0.100145\n[Training] Epoch 042 | Batch 030/102 | Current Loss: 0.100246\n[Training] Epoch 042 | Batch 040/102 | Current Loss: 0.100240\n[Training] Epoch 042 | Batch 050/102 | Current Loss: 0.100185\n[Training] Epoch 042 | Batch 060/102 | Current Loss: 0.100153\n[Training] Epoch 042 | Batch 070/102 | Current Loss: 0.100219\n[Training] Epoch 042 | Batch 080/102 | Current Loss: 0.100259\n[Training] Epoch 042 | Batch 090/102 | Current Loss: 0.100225\n[Training] Epoch 042 | Batch 100/102 | Current Loss: 0.100224\n\n[Training] Epoch 042 Summary:\n  Avg Loss: 0.100222\n  Current LR: 2.60e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 043 | Batch 000/102 | Current Loss: 0.100222\n[Training] Epoch 043 | Batch 010/102 | Current Loss: 0.100253\n[Training] Epoch 043 | Batch 020/102 | Current Loss: 0.100283\n[Training] Epoch 043 | Batch 030/102 | Current Loss: 0.100177\n[Training] Epoch 043 | Batch 040/102 | Current Loss: 0.100106\n[Training] Epoch 043 | Batch 050/102 | Current Loss: 0.100201\n[Training] Epoch 043 | Batch 060/102 | Current Loss: 0.100222\n[Training] Epoch 043 | Batch 070/102 | Current Loss: 0.100128\n[Training] Epoch 043 | Batch 080/102 | Current Loss: 0.100242\n[Training] Epoch 043 | Batch 090/102 | Current Loss: 0.100172\n[Training] Epoch 043 | Batch 100/102 | Current Loss: 0.100210\n\n[Training] Epoch 043 Summary:\n  Avg Loss: 0.100217\n  Current LR: 2.29e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 044 | Batch 000/102 | Current Loss: 0.100203\n[Training] Epoch 044 | Batch 010/102 | Current Loss: 0.100200\n[Training] Epoch 044 | Batch 020/102 | Current Loss: 0.100155\n[Training] Epoch 044 | Batch 030/102 | Current Loss: 0.100168\n[Training] Epoch 044 | Batch 040/102 | Current Loss: 0.100165\n[Training] Epoch 044 | Batch 050/102 | Current Loss: 0.100157\n[Training] Epoch 044 | Batch 060/102 | Current Loss: 0.100167\n[Training] Epoch 044 | Batch 070/102 | Current Loss: 0.100137\n[Training] Epoch 044 | Batch 080/102 | Current Loss: 0.100239\n[Training] Epoch 044 | Batch 090/102 | Current Loss: 0.100195\n[Training] Epoch 044 | Batch 100/102 | Current Loss: 0.100197\n\n[Training] Epoch 044 Summary:\n  Avg Loss: 0.100196\n  Current LR: 1.89e-04\n[EarlyStopping] Loss improved (0.100208 → 0.100196). Saving model...\n[Training] Epoch 045 | Batch 000/102 | Current Loss: 0.100183\n[Training] Epoch 045 | Batch 010/102 | Current Loss: 0.100121\n[Training] Epoch 045 | Batch 020/102 | Current Loss: 0.100203\n[Training] Epoch 045 | Batch 030/102 | Current Loss: 0.100168\n[Training] Epoch 045 | Batch 040/102 | Current Loss: 0.100213\n[Training] Epoch 045 | Batch 050/102 | Current Loss: 0.100187\n[Training] Epoch 045 | Batch 060/102 | Current Loss: 0.100194\n[Training] Epoch 045 | Batch 070/102 | Current Loss: 0.100204\n[Training] Epoch 045 | Batch 080/102 | Current Loss: 0.100151\n[Training] Epoch 045 | Batch 090/102 | Current Loss: 0.100174\n[Training] Epoch 045 | Batch 100/102 | Current Loss: 0.100218\n\n[Training] Epoch 045 Summary:\n  Avg Loss: 0.100187\n  Current LR: 1.44e-04\n[EarlyStopping] Loss improved (0.100196 → 0.100187). Saving model...\n[Training] Epoch 046 | Batch 000/102 | Current Loss: 0.100130\n[Training] Epoch 046 | Batch 010/102 | Current Loss: 0.100168\n[Training] Epoch 046 | Batch 020/102 | Current Loss: 0.100209\n[Training] Epoch 046 | Batch 030/102 | Current Loss: 0.100173\n[Training] Epoch 046 | Batch 040/102 | Current Loss: 0.100178\n[Training] Epoch 046 | Batch 050/102 | Current Loss: 0.100122\n[Training] Epoch 046 | Batch 060/102 | Current Loss: 0.100221\n[Training] Epoch 046 | Batch 070/102 | Current Loss: 0.100164\n[Training] Epoch 046 | Batch 080/102 | Current Loss: 0.100184\n[Training] Epoch 046 | Batch 090/102 | Current Loss: 0.100182\n[Training] Epoch 046 | Batch 100/102 | Current Loss: 0.100246\n\n[Training] Epoch 046 Summary:\n  Avg Loss: 0.100173\n  Current LR: 1.00e-04\n[EarlyStopping] Loss improved (0.100187 → 0.100173). Saving model...\n[Training] Epoch 047 | Batch 000/102 | Current Loss: 0.100198\n[Training] Epoch 047 | Batch 010/102 | Current Loss: 0.100197\n[Training] Epoch 047 | Batch 020/102 | Current Loss: 0.100177\n[Training] Epoch 047 | Batch 030/102 | Current Loss: 0.100151\n[Training] Epoch 047 | Batch 040/102 | Current Loss: 0.100201\n[Training] Epoch 047 | Batch 050/102 | Current Loss: 0.100164\n[Training] Epoch 047 | Batch 060/102 | Current Loss: 0.100121\n[Training] Epoch 047 | Batch 070/102 | Current Loss: 0.100182\n[Training] Epoch 047 | Batch 080/102 | Current Loss: 0.100185\n[Training] Epoch 047 | Batch 090/102 | Current Loss: 0.100180\n[Training] Epoch 047 | Batch 100/102 | Current Loss: 0.100108\n\n[Training] Epoch 047 Summary:\n  Avg Loss: 0.100173\n  Current LR: 6.01e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 048 | Batch 000/102 | Current Loss: 0.100184\n[Training] Epoch 048 | Batch 010/102 | Current Loss: 0.100057\n[Training] Epoch 048 | Batch 020/102 | Current Loss: 0.100154\n[Training] Epoch 048 | Batch 030/102 | Current Loss: 0.100192\n[Training] Epoch 048 | Batch 040/102 | Current Loss: 0.100165\n[Training] Epoch 048 | Batch 050/102 | Current Loss: 0.100204\n[Training] Epoch 048 | Batch 060/102 | Current Loss: 0.100136\n[Training] Epoch 048 | Batch 070/102 | Current Loss: 0.100132\n[Training] Epoch 048 | Batch 080/102 | Current Loss: 0.100134\n[Training] Epoch 048 | Batch 090/102 | Current Loss: 0.100114\n[Training] Epoch 048 | Batch 100/102 | Current Loss: 0.100155\n\n[Training] Epoch 048 Summary:\n  Avg Loss: 0.100161\n  Current LR: 2.84e-05\n[EarlyStopping] Loss improved (0.100173 → 0.100161). Saving model...\n[Training] Epoch 049 | Batch 000/102 | Current Loss: 0.100196\n[Training] Epoch 049 | Batch 010/102 | Current Loss: 0.100094\n[Training] Epoch 049 | Batch 020/102 | Current Loss: 0.100193\n[Training] Epoch 049 | Batch 030/102 | Current Loss: 0.100139\n[Training] Epoch 049 | Batch 040/102 | Current Loss: 0.100180\n[Training] Epoch 049 | Batch 050/102 | Current Loss: 0.100209\n[Training] Epoch 049 | Batch 060/102 | Current Loss: 0.100214\n[Training] Epoch 049 | Batch 070/102 | Current Loss: 0.100081\n[Training] Epoch 049 | Batch 080/102 | Current Loss: 0.100145\n[Training] Epoch 049 | Batch 090/102 | Current Loss: 0.100144\n[Training] Epoch 049 | Batch 100/102 | Current Loss: 0.100167\n\n[Training] Epoch 049 Summary:\n  Avg Loss: 0.100153\n  Current LR: 8.02e-06\n[EarlyStopping] Loss improved (0.100161 → 0.100153). Saving model...\n[Training] Epoch 050 | Batch 000/102 | Current Loss: 0.100159\n[Training] Epoch 050 | Batch 010/102 | Current Loss: 0.100226\n[Training] Epoch 050 | Batch 020/102 | Current Loss: 0.100131\n[Training] Epoch 050 | Batch 030/102 | Current Loss: 0.100164\n[Training] Epoch 050 | Batch 040/102 | Current Loss: 0.100127\n[Training] Epoch 050 | Batch 050/102 | Current Loss: 0.100114\n[Training] Epoch 050 | Batch 060/102 | Current Loss: 0.100123\n[Training] Epoch 050 | Batch 070/102 | Current Loss: 0.100145\n[Training] Epoch 050 | Batch 080/102 | Current Loss: 0.100088\n[Training] Epoch 050 | Batch 090/102 | Current Loss: 0.100152\n[Training] Epoch 050 | Batch 100/102 | Current Loss: 0.100067\n\n[Training] Epoch 050 Summary:\n  Avg Loss: 0.100152\n  Current LR: 2.88e-04\n[EarlyStopping] Loss improved (0.100153 → 0.100152). Saving model...\n[Training] Epoch 051 | Batch 000/102 | Current Loss: 0.100208\n[Training] Epoch 051 | Batch 010/102 | Current Loss: 0.100176\n[Training] Epoch 051 | Batch 020/102 | Current Loss: 0.100211\n[Training] Epoch 051 | Batch 030/102 | Current Loss: 0.100229\n[Training] Epoch 051 | Batch 040/102 | Current Loss: 0.100236\n[Training] Epoch 051 | Batch 050/102 | Current Loss: 0.100209\n[Training] Epoch 051 | Batch 060/102 | Current Loss: 0.100210\n[Training] Epoch 051 | Batch 070/102 | Current Loss: 0.100171\n[Training] Epoch 051 | Batch 080/102 | Current Loss: 0.100072\n[Training] Epoch 051 | Batch 090/102 | Current Loss: 0.100223\n[Training] Epoch 051 | Batch 100/102 | Current Loss: 0.100223\n\n[Training] Epoch 051 Summary:\n  Avg Loss: 0.100180\n  Current LR: 2.81e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 052 | Batch 000/102 | Current Loss: 0.100301\n[Training] Epoch 052 | Batch 010/102 | Current Loss: 0.100162\n[Training] Epoch 052 | Batch 020/102 | Current Loss: 0.100155\n[Training] Epoch 052 | Batch 030/102 | Current Loss: 0.100172\n[Training] Epoch 052 | Batch 040/102 | Current Loss: 0.100151\n[Training] Epoch 052 | Batch 050/102 | Current Loss: 0.100274\n[Training] Epoch 052 | Batch 060/102 | Current Loss: 0.100239\n[Training] Epoch 052 | Batch 070/102 | Current Loss: 0.100197\n[Training] Epoch 052 | Batch 080/102 | Current Loss: 0.100251\n[Training] Epoch 052 | Batch 090/102 | Current Loss: 0.100134\n[Training] Epoch 052 | Batch 100/102 | Current Loss: 0.100227\n\n[Training] Epoch 052 Summary:\n  Avg Loss: 0.100177\n  Current LR: 2.60e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 053 | Batch 000/102 | Current Loss: 0.100212\n[Training] Epoch 053 | Batch 010/102 | Current Loss: 0.100121\n[Training] Epoch 053 | Batch 020/102 | Current Loss: 0.100176\n[Training] Epoch 053 | Batch 030/102 | Current Loss: 0.100076\n[Training] Epoch 053 | Batch 040/102 | Current Loss: 0.100140\n[Training] Epoch 053 | Batch 050/102 | Current Loss: 0.100135\n[Training] Epoch 053 | Batch 060/102 | Current Loss: 0.100137\n[Training] Epoch 053 | Batch 070/102 | Current Loss: 0.100186\n[Training] Epoch 053 | Batch 080/102 | Current Loss: 0.100169\n[Training] Epoch 053 | Batch 090/102 | Current Loss: 0.100210\n[Training] Epoch 053 | Batch 100/102 | Current Loss: 0.100161\n\n[Training] Epoch 053 Summary:\n  Avg Loss: 0.100160\n  Current LR: 2.29e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 054 | Batch 000/102 | Current Loss: 0.100213\n[Training] Epoch 054 | Batch 010/102 | Current Loss: 0.100166\n[Training] Epoch 054 | Batch 020/102 | Current Loss: 0.100093\n[Training] Epoch 054 | Batch 030/102 | Current Loss: 0.100130\n[Training] Epoch 054 | Batch 040/102 | Current Loss: 0.100236\n[Training] Epoch 054 | Batch 050/102 | Current Loss: 0.100150\n[Training] Epoch 054 | Batch 060/102 | Current Loss: 0.100167\n[Training] Epoch 054 | Batch 070/102 | Current Loss: 0.100188\n[Training] Epoch 054 | Batch 080/102 | Current Loss: 0.100222\n[Training] Epoch 054 | Batch 090/102 | Current Loss: 0.100152\n[Training] Epoch 054 | Batch 100/102 | Current Loss: 0.100152\n\n[Training] Epoch 054 Summary:\n  Avg Loss: 0.100154\n  Current LR: 1.89e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 055 | Batch 000/102 | Current Loss: 0.100163\n[Training] Epoch 055 | Batch 010/102 | Current Loss: 0.100109\n[Training] Epoch 055 | Batch 020/102 | Current Loss: 0.100113\n[Training] Epoch 055 | Batch 030/102 | Current Loss: 0.100137\n[Training] Epoch 055 | Batch 040/102 | Current Loss: 0.100166\n[Training] Epoch 055 | Batch 050/102 | Current Loss: 0.100183\n[Training] Epoch 055 | Batch 060/102 | Current Loss: 0.100099\n[Training] Epoch 055 | Batch 070/102 | Current Loss: 0.100167\n[Training] Epoch 055 | Batch 080/102 | Current Loss: 0.100114\n[Training] Epoch 055 | Batch 090/102 | Current Loss: 0.100121\n[Training] Epoch 055 | Batch 100/102 | Current Loss: 0.100163\n\n[Training] Epoch 055 Summary:\n  Avg Loss: 0.100147\n  Current LR: 1.44e-04\n[EarlyStopping] Loss improved (0.100152 → 0.100147). Saving model...\n[Training] Epoch 056 | Batch 000/102 | Current Loss: 0.100120\n[Training] Epoch 056 | Batch 010/102 | Current Loss: 0.100207\n[Training] Epoch 056 | Batch 020/102 | Current Loss: 0.100155\n[Training] Epoch 056 | Batch 030/102 | Current Loss: 0.100139\n[Training] Epoch 056 | Batch 040/102 | Current Loss: 0.100081\n[Training] Epoch 056 | Batch 050/102 | Current Loss: 0.100146\n[Training] Epoch 056 | Batch 060/102 | Current Loss: 0.100183\n[Training] Epoch 056 | Batch 070/102 | Current Loss: 0.100062\n[Training] Epoch 056 | Batch 080/102 | Current Loss: 0.100189\n[Training] Epoch 056 | Batch 090/102 | Current Loss: 0.100166\n[Training] Epoch 056 | Batch 100/102 | Current Loss: 0.100145\n\n[Training] Epoch 056 Summary:\n  Avg Loss: 0.100142\n  Current LR: 1.00e-04\n[EarlyStopping] Loss improved (0.100147 → 0.100142). Saving model...\n[Training] Epoch 057 | Batch 000/102 | Current Loss: 0.100165\n[Training] Epoch 057 | Batch 010/102 | Current Loss: 0.100212\n[Training] Epoch 057 | Batch 020/102 | Current Loss: 0.100137\n[Training] Epoch 057 | Batch 030/102 | Current Loss: 0.100154\n[Training] Epoch 057 | Batch 040/102 | Current Loss: 0.100135\n[Training] Epoch 057 | Batch 050/102 | Current Loss: 0.100043\n[Training] Epoch 057 | Batch 060/102 | Current Loss: 0.100136\n[Training] Epoch 057 | Batch 070/102 | Current Loss: 0.100104\n[Training] Epoch 057 | Batch 080/102 | Current Loss: 0.100123\n[Training] Epoch 057 | Batch 090/102 | Current Loss: 0.100133\n[Training] Epoch 057 | Batch 100/102 | Current Loss: 0.100088\n\n[Training] Epoch 057 Summary:\n  Avg Loss: 0.100132\n  Current LR: 6.01e-05\n[EarlyStopping] Loss improved (0.100142 → 0.100132). Saving model...\n[Training] Epoch 058 | Batch 000/102 | Current Loss: 0.100146\n[Training] Epoch 058 | Batch 010/102 | Current Loss: 0.100138\n[Training] Epoch 058 | Batch 020/102 | Current Loss: 0.100087\n[Training] Epoch 058 | Batch 030/102 | Current Loss: 0.100174\n[Training] Epoch 058 | Batch 040/102 | Current Loss: 0.100155\n[Training] Epoch 058 | Batch 050/102 | Current Loss: 0.100061\n[Training] Epoch 058 | Batch 060/102 | Current Loss: 0.100161\n[Training] Epoch 058 | Batch 070/102 | Current Loss: 0.100183\n[Training] Epoch 058 | Batch 080/102 | Current Loss: 0.100134\n[Training] Epoch 058 | Batch 090/102 | Current Loss: 0.100082\n[Training] Epoch 058 | Batch 100/102 | Current Loss: 0.100181\n\n[Training] Epoch 058 Summary:\n  Avg Loss: 0.100123\n  Current LR: 2.84e-05\n[EarlyStopping] Loss improved (0.100132 → 0.100123). Saving model...\n[Training] Epoch 059 | Batch 000/102 | Current Loss: 0.100163\n[Training] Epoch 059 | Batch 010/102 | Current Loss: 0.100192\n[Training] Epoch 059 | Batch 020/102 | Current Loss: 0.100149\n[Training] Epoch 059 | Batch 030/102 | Current Loss: 0.100149\n[Training] Epoch 059 | Batch 040/102 | Current Loss: 0.100164\n[Training] Epoch 059 | Batch 050/102 | Current Loss: 0.100128\n[Training] Epoch 059 | Batch 060/102 | Current Loss: 0.100091\n[Training] Epoch 059 | Batch 070/102 | Current Loss: 0.100163\n[Training] Epoch 059 | Batch 080/102 | Current Loss: 0.100033\n[Training] Epoch 059 | Batch 090/102 | Current Loss: 0.100197\n[Training] Epoch 059 | Batch 100/102 | Current Loss: 0.100172\n\n[Training] Epoch 059 Summary:\n  Avg Loss: 0.100122\n  Current LR: 8.02e-06\n[EarlyStopping] Loss improved (0.100123 → 0.100122). Saving model...\n[Training] Epoch 060 | Batch 000/102 | Current Loss: 0.100105\n[Training] Epoch 060 | Batch 010/102 | Current Loss: 0.100167\n[Training] Epoch 060 | Batch 020/102 | Current Loss: 0.100178\n[Training] Epoch 060 | Batch 030/102 | Current Loss: 0.100093\n[Training] Epoch 060 | Batch 040/102 | Current Loss: 0.100123\n[Training] Epoch 060 | Batch 050/102 | Current Loss: 0.100085\n[Training] Epoch 060 | Batch 060/102 | Current Loss: 0.100163\n[Training] Epoch 060 | Batch 070/102 | Current Loss: 0.100139\n[Training] Epoch 060 | Batch 080/102 | Current Loss: 0.100088\n[Training] Epoch 060 | Batch 090/102 | Current Loss: 0.100085\n[Training] Epoch 060 | Batch 100/102 | Current Loss: 0.100119\n\n[Training] Epoch 060 Summary:\n  Avg Loss: 0.100120\n  Current LR: 2.88e-04\n[EarlyStopping] Loss improved (0.100122 → 0.100120). Saving model...\n[Training] Epoch 061 | Batch 000/102 | Current Loss: 0.100106\n[Training] Epoch 061 | Batch 010/102 | Current Loss: 0.100203\n[Training] Epoch 061 | Batch 020/102 | Current Loss: 0.100138\n[Training] Epoch 061 | Batch 030/102 | Current Loss: 0.100221\n[Training] Epoch 061 | Batch 040/102 | Current Loss: 0.100203\n[Training] Epoch 061 | Batch 050/102 | Current Loss: 0.100129\n[Training] Epoch 061 | Batch 060/102 | Current Loss: 0.100169\n[Training] Epoch 061 | Batch 070/102 | Current Loss: 0.100248\n[Training] Epoch 061 | Batch 080/102 | Current Loss: 0.100066\n[Training] Epoch 061 | Batch 090/102 | Current Loss: 0.100136\n[Training] Epoch 061 | Batch 100/102 | Current Loss: 0.100134\n\n[Training] Epoch 061 Summary:\n  Avg Loss: 0.100162\n  Current LR: 2.81e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 062 | Batch 000/102 | Current Loss: 0.100174\n[Training] Epoch 062 | Batch 010/102 | Current Loss: 0.100151\n[Training] Epoch 062 | Batch 020/102 | Current Loss: 0.100166\n[Training] Epoch 062 | Batch 030/102 | Current Loss: 0.100084\n[Training] Epoch 062 | Batch 040/102 | Current Loss: 0.100125\n[Training] Epoch 062 | Batch 050/102 | Current Loss: 0.100091\n[Training] Epoch 062 | Batch 060/102 | Current Loss: 0.100180\n[Training] Epoch 062 | Batch 070/102 | Current Loss: 0.100168\n[Training] Epoch 062 | Batch 080/102 | Current Loss: 0.100153\n[Training] Epoch 062 | Batch 090/102 | Current Loss: 0.100199\n[Training] Epoch 062 | Batch 100/102 | Current Loss: 0.100168\n\n[Training] Epoch 062 Summary:\n  Avg Loss: 0.100144\n  Current LR: 2.60e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 063 | Batch 000/102 | Current Loss: 0.100153\n[Training] Epoch 063 | Batch 010/102 | Current Loss: 0.100173\n[Training] Epoch 063 | Batch 020/102 | Current Loss: 0.100178\n[Training] Epoch 063 | Batch 030/102 | Current Loss: 0.100131\n[Training] Epoch 063 | Batch 040/102 | Current Loss: 0.100111\n[Training] Epoch 063 | Batch 050/102 | Current Loss: 0.100160\n[Training] Epoch 063 | Batch 060/102 | Current Loss: 0.100129\n[Training] Epoch 063 | Batch 070/102 | Current Loss: 0.100160\n[Training] Epoch 063 | Batch 080/102 | Current Loss: 0.100188\n[Training] Epoch 063 | Batch 090/102 | Current Loss: 0.100114\n[Training] Epoch 063 | Batch 100/102 | Current Loss: 0.100117\n\n[Training] Epoch 063 Summary:\n  Avg Loss: 0.100135\n  Current LR: 2.29e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 064 | Batch 000/102 | Current Loss: 0.100087\n[Training] Epoch 064 | Batch 010/102 | Current Loss: 0.100131\n[Training] Epoch 064 | Batch 020/102 | Current Loss: 0.100143\n[Training] Epoch 064 | Batch 030/102 | Current Loss: 0.100153\n[Training] Epoch 064 | Batch 040/102 | Current Loss: 0.100126\n[Training] Epoch 064 | Batch 050/102 | Current Loss: 0.100069\n[Training] Epoch 064 | Batch 060/102 | Current Loss: 0.100093\n[Training] Epoch 064 | Batch 070/102 | Current Loss: 0.100079\n[Training] Epoch 064 | Batch 080/102 | Current Loss: 0.100167\n[Training] Epoch 064 | Batch 090/102 | Current Loss: 0.100161\n[Training] Epoch 064 | Batch 100/102 | Current Loss: 0.100110\n\n[Training] Epoch 064 Summary:\n  Avg Loss: 0.100126\n  Current LR: 1.89e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 065 | Batch 000/102 | Current Loss: 0.100076\n[Training] Epoch 065 | Batch 010/102 | Current Loss: 0.100101\n[Training] Epoch 065 | Batch 020/102 | Current Loss: 0.100076\n[Training] Epoch 065 | Batch 030/102 | Current Loss: 0.100117\n[Training] Epoch 065 | Batch 040/102 | Current Loss: 0.100109\n[Training] Epoch 065 | Batch 050/102 | Current Loss: 0.100085\n[Training] Epoch 065 | Batch 060/102 | Current Loss: 0.100193\n[Training] Epoch 065 | Batch 070/102 | Current Loss: 0.100035\n[Training] Epoch 065 | Batch 080/102 | Current Loss: 0.100088\n[Training] Epoch 065 | Batch 090/102 | Current Loss: 0.100129\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 17:07:15,485] Trial 12 finished with value: 0.10011996467616044 and parameters: {'lr': 0.0002877230690836362, 'batch_size': 128, 'bottleneck_width': 1024, 'dropout_rate': 0.2816276958807796, 'alpha': 0.40123484953380534, 'weight_decay': 2.2385618448297324e-05}. Best is trial 8 with value: 0.10009658994043574.\n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 065 | Batch 100/102 | Current Loss: 0.100079\n\n[Training] Epoch 065 Summary:\n  Avg Loss: 0.100122\n  Current LR: 1.44e-04\n[EarlyStopping] Counter: 5/5\n\n[Training] Early stopping triggered\n\n[Optuna] Trial 12 completed with best loss: 0.100120\n\n==================================================\nStarting Optuna Trial 13\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 1.067494040100977e-05\n  batch_size: 128\n  bottleneck_width: 1024\n  dropout_rate: 0.25168624317286553\n  alpha: 0.476890906756914\n  weight_decay: 9.757079467979662e-05\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 128, Total batches: 102\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 1024\n  - Dropout rate: 0.25168624317286553\n[Model] Architecture initialized successfully\n[Model] Total parameters: 2,396,992\n[Optimizer] Initialized with lr=1.07e-05, weight_decay=9.76e-05\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.476890906756914\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/102 | Current Loss: 1.488365\n[Training] Epoch 001 | Batch 010/102 | Current Loss: 1.204084\n[Training] Epoch 001 | Batch 020/102 | Current Loss: 1.019606\n[Training] Epoch 001 | Batch 030/102 | Current Loss: 0.848818\n[Training] Epoch 001 | Batch 040/102 | Current Loss: 0.708636\n[Training] Epoch 001 | Batch 050/102 | Current Loss: 0.587286\n[Training] Epoch 001 | Batch 060/102 | Current Loss: 0.508389\n[Training] Epoch 001 | Batch 070/102 | Current Loss: 0.432447\n[Training] Epoch 001 | Batch 080/102 | Current Loss: 0.381425\n[Training] Epoch 001 | Batch 090/102 | Current Loss: 0.345869\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 17:07:30,809] Trial 13 pruned. \n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 001 | Batch 100/102 | Current Loss: 0.311085\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.690516\n  Current LR: 1.04e-05\n[EarlyStopping] Loss improved (inf → 0.690516). Saving model...\n\n[Optuna] Trial pruned\n\n==================================================\nStarting Optuna Trial 14\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.0002700263092093273\n  batch_size: 64\n  bottleneck_width: 1024\n  dropout_rate: 0.32990691085641666\n  alpha: 0.4744367150124978\n  weight_decay: 1.9592008352031023e-05\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 64, Total batches: 204\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 1024\n  - Dropout rate: 0.32990691085641666\n[Model] Architecture initialized successfully\n[Model] Total parameters: 2,396,992\n[Optimizer] Initialized with lr=2.70e-04, weight_decay=1.96e-05\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.4744367150124978\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/204 | Current Loss: 1.440488\n[Training] Epoch 001 | Batch 010/204 | Current Loss: 0.202726\n[Training] Epoch 001 | Batch 020/204 | Current Loss: 0.142112\n[Training] Epoch 001 | Batch 030/204 | Current Loss: 0.126010\n[Training] Epoch 001 | Batch 040/204 | Current Loss: 0.117906\n[Training] Epoch 001 | Batch 050/204 | Current Loss: 0.114698\n[Training] Epoch 001 | Batch 060/204 | Current Loss: 0.115750\n[Training] Epoch 001 | Batch 070/204 | Current Loss: 0.115871\n[Training] Epoch 001 | Batch 080/204 | Current Loss: 0.108864\n[Training] Epoch 001 | Batch 090/204 | Current Loss: 0.109095\n[Training] Epoch 001 | Batch 100/204 | Current Loss: 0.107351\n[Training] Epoch 001 | Batch 110/204 | Current Loss: 0.107688\n[Training] Epoch 001 | Batch 120/204 | Current Loss: 0.106183\n[Training] Epoch 001 | Batch 130/204 | Current Loss: 0.106990\n[Training] Epoch 001 | Batch 140/204 | Current Loss: 0.109999\n[Training] Epoch 001 | Batch 150/204 | Current Loss: 0.105568\n[Training] Epoch 001 | Batch 160/204 | Current Loss: 0.105526\n[Training] Epoch 001 | Batch 170/204 | Current Loss: 0.105431\n[Training] Epoch 001 | Batch 180/204 | Current Loss: 0.104902\n[Training] Epoch 001 | Batch 190/204 | Current Loss: 0.105742\n[Training] Epoch 001 | Batch 200/204 | Current Loss: 0.104718\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.134376\n  Current LR: 2.63e-04\n[EarlyStopping] Loss improved (inf → 0.134376). Saving model...\n[Training] Epoch 002 | Batch 000/204 | Current Loss: 0.104859\n[Training] Epoch 002 | Batch 010/204 | Current Loss: 0.104281\n[Training] Epoch 002 | Batch 020/204 | Current Loss: 0.103759\n[Training] Epoch 002 | Batch 030/204 | Current Loss: 0.105022\n[Training] Epoch 002 | Batch 040/204 | Current Loss: 0.103814\n[Training] Epoch 002 | Batch 050/204 | Current Loss: 0.103958\n[Training] Epoch 002 | Batch 060/204 | Current Loss: 0.105396\n[Training] Epoch 002 | Batch 070/204 | Current Loss: 0.103393\n[Training] Epoch 002 | Batch 080/204 | Current Loss: 0.103115\n[Training] Epoch 002 | Batch 090/204 | Current Loss: 0.103207\n[Training] Epoch 002 | Batch 100/204 | Current Loss: 0.103436\n[Training] Epoch 002 | Batch 110/204 | Current Loss: 0.103346\n[Training] Epoch 002 | Batch 120/204 | Current Loss: 0.103615\n[Training] Epoch 002 | Batch 130/204 | Current Loss: 0.104869\n[Training] Epoch 002 | Batch 140/204 | Current Loss: 0.102983\n[Training] Epoch 002 | Batch 150/204 | Current Loss: 0.102850\n[Training] Epoch 002 | Batch 160/204 | Current Loss: 0.102806\n[Training] Epoch 002 | Batch 170/204 | Current Loss: 0.102585\n[Training] Epoch 002 | Batch 180/204 | Current Loss: 0.102943\n[Training] Epoch 002 | Batch 190/204 | Current Loss: 0.103357\n[Training] Epoch 002 | Batch 200/204 | Current Loss: 0.104122\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.103636\n  Current LR: 2.44e-04\n[EarlyStopping] Loss improved (0.134376 → 0.103636). Saving model...\n[Training] Epoch 003 | Batch 000/204 | Current Loss: 0.103496\n[Training] Epoch 003 | Batch 010/204 | Current Loss: 0.102679\n[Training] Epoch 003 | Batch 020/204 | Current Loss: 0.102685\n[Training] Epoch 003 | Batch 030/204 | Current Loss: 0.102229\n[Training] Epoch 003 | Batch 040/204 | Current Loss: 0.102176\n[Training] Epoch 003 | Batch 050/204 | Current Loss: 0.102500\n[Training] Epoch 003 | Batch 060/204 | Current Loss: 0.102305\n[Training] Epoch 003 | Batch 070/204 | Current Loss: 0.102251\n[Training] Epoch 003 | Batch 080/204 | Current Loss: 0.102519\n[Training] Epoch 003 | Batch 090/204 | Current Loss: 0.103472\n[Training] Epoch 003 | Batch 100/204 | Current Loss: 0.102356\n[Training] Epoch 003 | Batch 110/204 | Current Loss: 0.101865\n[Training] Epoch 003 | Batch 120/204 | Current Loss: 0.102033\n[Training] Epoch 003 | Batch 130/204 | Current Loss: 0.102209\n[Training] Epoch 003 | Batch 140/204 | Current Loss: 0.101914\n[Training] Epoch 003 | Batch 150/204 | Current Loss: 0.102220\n[Training] Epoch 003 | Batch 160/204 | Current Loss: 0.101796\n[Training] Epoch 003 | Batch 170/204 | Current Loss: 0.101932\n[Training] Epoch 003 | Batch 180/204 | Current Loss: 0.101710\n[Training] Epoch 003 | Batch 190/204 | Current Loss: 0.102203\n[Training] Epoch 003 | Batch 200/204 | Current Loss: 0.101956\n\n[Training] Epoch 003 Summary:\n  Avg Loss: 0.102284\n  Current LR: 2.15e-04\n[EarlyStopping] Loss improved (0.103636 → 0.102284). Saving model...\n[Training] Epoch 004 | Batch 000/204 | Current Loss: 0.101738\n[Training] Epoch 004 | Batch 010/204 | Current Loss: 0.101741\n[Training] Epoch 004 | Batch 020/204 | Current Loss: 0.101966\n[Training] Epoch 004 | Batch 030/204 | Current Loss: 0.101695\n[Training] Epoch 004 | Batch 040/204 | Current Loss: 0.101632\n[Training] Epoch 004 | Batch 050/204 | Current Loss: 0.101860\n[Training] Epoch 004 | Batch 060/204 | Current Loss: 0.101794\n[Training] Epoch 004 | Batch 070/204 | Current Loss: 0.101744\n[Training] Epoch 004 | Batch 080/204 | Current Loss: 0.101480\n[Training] Epoch 004 | Batch 090/204 | Current Loss: 0.103812\n[Training] Epoch 004 | Batch 100/204 | Current Loss: 0.102031\n[Training] Epoch 004 | Batch 110/204 | Current Loss: 0.101367\n[Training] Epoch 004 | Batch 120/204 | Current Loss: 0.101487\n[Training] Epoch 004 | Batch 130/204 | Current Loss: 0.101452\n[Training] Epoch 004 | Batch 140/204 | Current Loss: 0.101579\n[Training] Epoch 004 | Batch 150/204 | Current Loss: 0.101272\n[Training] Epoch 004 | Batch 160/204 | Current Loss: 0.101726\n[Training] Epoch 004 | Batch 170/204 | Current Loss: 0.101699\n[Training] Epoch 004 | Batch 180/204 | Current Loss: 0.101552\n[Training] Epoch 004 | Batch 190/204 | Current Loss: 0.101661\n[Training] Epoch 004 | Batch 200/204 | Current Loss: 0.101223\n\n[Training] Epoch 004 Summary:\n  Avg Loss: 0.101719\n  Current LR: 1.77e-04\n[EarlyStopping] Loss improved (0.102284 → 0.101719). Saving model...\n[Training] Epoch 005 | Batch 000/204 | Current Loss: 0.101247\n[Training] Epoch 005 | Batch 010/204 | Current Loss: 0.101432\n[Training] Epoch 005 | Batch 020/204 | Current Loss: 0.101213\n[Training] Epoch 005 | Batch 030/204 | Current Loss: 0.101480\n[Training] Epoch 005 | Batch 040/204 | Current Loss: 0.101457\n[Training] Epoch 005 | Batch 050/204 | Current Loss: 0.101433\n[Training] Epoch 005 | Batch 060/204 | Current Loss: 0.101524\n[Training] Epoch 005 | Batch 070/204 | Current Loss: 0.101560\n[Training] Epoch 005 | Batch 080/204 | Current Loss: 0.101147\n[Training] Epoch 005 | Batch 090/204 | Current Loss: 0.101593\n[Training] Epoch 005 | Batch 100/204 | Current Loss: 0.101323\n[Training] Epoch 005 | Batch 110/204 | Current Loss: 0.101123\n[Training] Epoch 005 | Batch 120/204 | Current Loss: 0.101615\n[Training] Epoch 005 | Batch 130/204 | Current Loss: 0.101309\n[Training] Epoch 005 | Batch 140/204 | Current Loss: 0.101250\n[Training] Epoch 005 | Batch 150/204 | Current Loss: 0.101424\n[Training] Epoch 005 | Batch 160/204 | Current Loss: 0.101183\n[Training] Epoch 005 | Batch 170/204 | Current Loss: 0.101165\n[Training] Epoch 005 | Batch 180/204 | Current Loss: 0.101116\n[Training] Epoch 005 | Batch 190/204 | Current Loss: 0.101058\n[Training] Epoch 005 | Batch 200/204 | Current Loss: 0.101186\n\n[Training] Epoch 005 Summary:\n  Avg Loss: 0.101331\n  Current LR: 1.36e-04\n[EarlyStopping] Loss improved (0.101719 → 0.101331). Saving model...\n[Training] Epoch 006 | Batch 000/204 | Current Loss: 0.102936\n[Training] Epoch 006 | Batch 010/204 | Current Loss: 0.101137\n[Training] Epoch 006 | Batch 020/204 | Current Loss: 0.101118\n[Training] Epoch 006 | Batch 030/204 | Current Loss: 0.101287\n[Training] Epoch 006 | Batch 040/204 | Current Loss: 0.101043\n[Training] Epoch 006 | Batch 050/204 | Current Loss: 0.101014\n[Training] Epoch 006 | Batch 060/204 | Current Loss: 0.101119\n[Training] Epoch 006 | Batch 070/204 | Current Loss: 0.101073\n[Training] Epoch 006 | Batch 080/204 | Current Loss: 0.101090\n[Training] Epoch 006 | Batch 090/204 | Current Loss: 0.101134\n[Training] Epoch 006 | Batch 100/204 | Current Loss: 0.100838\n[Training] Epoch 006 | Batch 110/204 | Current Loss: 0.101019\n[Training] Epoch 006 | Batch 120/204 | Current Loss: 0.101369\n[Training] Epoch 006 | Batch 130/204 | Current Loss: 0.100926\n[Training] Epoch 006 | Batch 140/204 | Current Loss: 0.100801\n[Training] Epoch 006 | Batch 150/204 | Current Loss: 0.100971\n[Training] Epoch 006 | Batch 160/204 | Current Loss: 0.101096\n[Training] Epoch 006 | Batch 170/204 | Current Loss: 0.100883\n[Training] Epoch 006 | Batch 180/204 | Current Loss: 0.101002\n[Training] Epoch 006 | Batch 190/204 | Current Loss: 0.100803\n[Training] Epoch 006 | Batch 200/204 | Current Loss: 0.100895\n\n[Training] Epoch 006 Summary:\n  Avg Loss: 0.101114\n  Current LR: 9.39e-05\n[EarlyStopping] Loss improved (0.101331 → 0.101114). Saving model...\n[Training] Epoch 007 | Batch 000/204 | Current Loss: 0.100916\n[Training] Epoch 007 | Batch 010/204 | Current Loss: 0.100924\n[Training] Epoch 007 | Batch 020/204 | Current Loss: 0.101181\n[Training] Epoch 007 | Batch 030/204 | Current Loss: 0.100933\n[Training] Epoch 007 | Batch 040/204 | Current Loss: 0.100942\n[Training] Epoch 007 | Batch 050/204 | Current Loss: 0.100825\n[Training] Epoch 007 | Batch 060/204 | Current Loss: 0.100898\n[Training] Epoch 007 | Batch 070/204 | Current Loss: 0.101254\n[Training] Epoch 007 | Batch 080/204 | Current Loss: 0.100992\n[Training] Epoch 007 | Batch 090/204 | Current Loss: 0.101089\n[Training] Epoch 007 | Batch 100/204 | Current Loss: 0.100700\n[Training] Epoch 007 | Batch 110/204 | Current Loss: 0.101020\n[Training] Epoch 007 | Batch 120/204 | Current Loss: 0.100770\n[Training] Epoch 007 | Batch 130/204 | Current Loss: 0.100898\n[Training] Epoch 007 | Batch 140/204 | Current Loss: 0.100921\n[Training] Epoch 007 | Batch 150/204 | Current Loss: 0.100982\n[Training] Epoch 007 | Batch 160/204 | Current Loss: 0.101049\n[Training] Epoch 007 | Batch 170/204 | Current Loss: 0.100992\n[Training] Epoch 007 | Batch 180/204 | Current Loss: 0.100833\n[Training] Epoch 007 | Batch 190/204 | Current Loss: 0.101083\n[Training] Epoch 007 | Batch 200/204 | Current Loss: 0.100922\n\n[Training] Epoch 007 Summary:\n  Avg Loss: 0.100938\n  Current LR: 5.64e-05\n[EarlyStopping] Loss improved (0.101114 → 0.100938). Saving model...\n[Training] Epoch 008 | Batch 000/204 | Current Loss: 0.101025\n[Training] Epoch 008 | Batch 010/204 | Current Loss: 0.100784\n[Training] Epoch 008 | Batch 020/204 | Current Loss: 0.100819\n[Training] Epoch 008 | Batch 030/204 | Current Loss: 0.100807\n[Training] Epoch 008 | Batch 040/204 | Current Loss: 0.100870\n[Training] Epoch 008 | Batch 050/204 | Current Loss: 0.100822\n[Training] Epoch 008 | Batch 060/204 | Current Loss: 0.100782\n[Training] Epoch 008 | Batch 070/204 | Current Loss: 0.100729\n[Training] Epoch 008 | Batch 080/204 | Current Loss: 0.100804\n[Training] Epoch 008 | Batch 090/204 | Current Loss: 0.100969\n[Training] Epoch 008 | Batch 100/204 | Current Loss: 0.101006\n[Training] Epoch 008 | Batch 110/204 | Current Loss: 0.100849\n[Training] Epoch 008 | Batch 120/204 | Current Loss: 0.100788\n[Training] Epoch 008 | Batch 130/204 | Current Loss: 0.100899\n[Training] Epoch 008 | Batch 140/204 | Current Loss: 0.100798\n[Training] Epoch 008 | Batch 150/204 | Current Loss: 0.100738\n[Training] Epoch 008 | Batch 160/204 | Current Loss: 0.100869\n[Training] Epoch 008 | Batch 170/204 | Current Loss: 0.100794\n[Training] Epoch 008 | Batch 180/204 | Current Loss: 0.100744\n[Training] Epoch 008 | Batch 190/204 | Current Loss: 0.100811\n[Training] Epoch 008 | Batch 200/204 | Current Loss: 0.100772\n\n[Training] Epoch 008 Summary:\n  Avg Loss: 0.100867\n  Current LR: 2.67e-05\n[EarlyStopping] Loss improved (0.100938 → 0.100867). Saving model...\n[Training] Epoch 009 | Batch 000/204 | Current Loss: 0.100868\n[Training] Epoch 009 | Batch 010/204 | Current Loss: 0.100784\n[Training] Epoch 009 | Batch 020/204 | Current Loss: 0.100896\n[Training] Epoch 009 | Batch 030/204 | Current Loss: 0.101013\n[Training] Epoch 009 | Batch 040/204 | Current Loss: 0.100906\n[Training] Epoch 009 | Batch 050/204 | Current Loss: 0.100870\n[Training] Epoch 009 | Batch 060/204 | Current Loss: 0.100732\n[Training] Epoch 009 | Batch 070/204 | Current Loss: 0.100626\n[Training] Epoch 009 | Batch 080/204 | Current Loss: 0.100700\n[Training] Epoch 009 | Batch 090/204 | Current Loss: 0.100806\n[Training] Epoch 009 | Batch 100/204 | Current Loss: 0.100817\n[Training] Epoch 009 | Batch 110/204 | Current Loss: 0.100839\n[Training] Epoch 009 | Batch 120/204 | Current Loss: 0.100805\n[Training] Epoch 009 | Batch 130/204 | Current Loss: 0.100669\n[Training] Epoch 009 | Batch 140/204 | Current Loss: 0.100901\n[Training] Epoch 009 | Batch 150/204 | Current Loss: 0.100858\n[Training] Epoch 009 | Batch 160/204 | Current Loss: 0.100790\n[Training] Epoch 009 | Batch 170/204 | Current Loss: 0.100667\n[Training] Epoch 009 | Batch 180/204 | Current Loss: 0.100740\n[Training] Epoch 009 | Batch 190/204 | Current Loss: 0.100739\n[Training] Epoch 009 | Batch 200/204 | Current Loss: 0.100747\n\n[Training] Epoch 009 Summary:\n  Avg Loss: 0.100877\n  Current LR: 7.58e-06\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 010 | Batch 000/204 | Current Loss: 0.100688\n[Training] Epoch 010 | Batch 010/204 | Current Loss: 0.100786\n[Training] Epoch 010 | Batch 020/204 | Current Loss: 0.100806\n[Training] Epoch 010 | Batch 030/204 | Current Loss: 0.100809\n[Training] Epoch 010 | Batch 040/204 | Current Loss: 0.100789\n[Training] Epoch 010 | Batch 050/204 | Current Loss: 0.100811\n[Training] Epoch 010 | Batch 060/204 | Current Loss: 0.100985\n[Training] Epoch 010 | Batch 070/204 | Current Loss: 0.100809\n[Training] Epoch 010 | Batch 080/204 | Current Loss: 0.100880\n[Training] Epoch 010 | Batch 090/204 | Current Loss: 0.100851\n[Training] Epoch 010 | Batch 100/204 | Current Loss: 0.100761\n[Training] Epoch 010 | Batch 110/204 | Current Loss: 0.100916\n[Training] Epoch 010 | Batch 120/204 | Current Loss: 0.100849\n[Training] Epoch 010 | Batch 130/204 | Current Loss: 0.100867\n[Training] Epoch 010 | Batch 140/204 | Current Loss: 0.100681\n[Training] Epoch 010 | Batch 150/204 | Current Loss: 0.100700\n[Training] Epoch 010 | Batch 160/204 | Current Loss: 0.100875\n[Training] Epoch 010 | Batch 170/204 | Current Loss: 0.100649\n[Training] Epoch 010 | Batch 180/204 | Current Loss: 0.100817\n[Training] Epoch 010 | Batch 190/204 | Current Loss: 0.100795\n[Training] Epoch 010 | Batch 200/204 | Current Loss: 0.100730\n\n[Training] Epoch 010 Summary:\n  Avg Loss: 0.100805\n  Current LR: 2.70e-04\n[EarlyStopping] Loss improved (0.100867 → 0.100805). Saving model...\n[Training] Epoch 011 | Batch 000/204 | Current Loss: 0.100835\n[Training] Epoch 011 | Batch 010/204 | Current Loss: 0.100829\n[Training] Epoch 011 | Batch 020/204 | Current Loss: 0.100990\n[Training] Epoch 011 | Batch 030/204 | Current Loss: 0.100854\n[Training] Epoch 011 | Batch 040/204 | Current Loss: 0.100901\n[Training] Epoch 011 | Batch 050/204 | Current Loss: 0.100690\n[Training] Epoch 011 | Batch 060/204 | Current Loss: 0.100741\n[Training] Epoch 011 | Batch 070/204 | Current Loss: 0.100734\n[Training] Epoch 011 | Batch 080/204 | Current Loss: 0.100723\n[Training] Epoch 011 | Batch 090/204 | Current Loss: 0.100798\n[Training] Epoch 011 | Batch 100/204 | Current Loss: 0.100775\n[Training] Epoch 011 | Batch 110/204 | Current Loss: 0.100684\n[Training] Epoch 011 | Batch 120/204 | Current Loss: 0.100733\n[Training] Epoch 011 | Batch 130/204 | Current Loss: 0.100685\n[Training] Epoch 011 | Batch 140/204 | Current Loss: 0.100825\n[Training] Epoch 011 | Batch 150/204 | Current Loss: 0.100705\n[Training] Epoch 011 | Batch 160/204 | Current Loss: 0.100757\n[Training] Epoch 011 | Batch 170/204 | Current Loss: 0.100753\n[Training] Epoch 011 | Batch 180/204 | Current Loss: 0.100756\n[Training] Epoch 011 | Batch 190/204 | Current Loss: 0.100644\n[Training] Epoch 011 | Batch 200/204 | Current Loss: 0.100609\n\n[Training] Epoch 011 Summary:\n  Avg Loss: 0.100764\n  Current LR: 2.63e-04\n[EarlyStopping] Loss improved (0.100805 → 0.100764). Saving model...\n[Training] Epoch 012 | Batch 000/204 | Current Loss: 0.100669\n[Training] Epoch 012 | Batch 010/204 | Current Loss: 0.100653\n[Training] Epoch 012 | Batch 020/204 | Current Loss: 0.100585\n[Training] Epoch 012 | Batch 030/204 | Current Loss: 0.100655\n[Training] Epoch 012 | Batch 040/204 | Current Loss: 0.100582\n[Training] Epoch 012 | Batch 050/204 | Current Loss: 0.100755\n[Training] Epoch 012 | Batch 060/204 | Current Loss: 0.100611\n[Training] Epoch 012 | Batch 070/204 | Current Loss: 0.100429\n[Training] Epoch 012 | Batch 080/204 | Current Loss: 0.100539\n[Training] Epoch 012 | Batch 090/204 | Current Loss: 0.100950\n[Training] Epoch 012 | Batch 100/204 | Current Loss: 0.100537\n[Training] Epoch 012 | Batch 110/204 | Current Loss: 0.100605\n[Training] Epoch 012 | Batch 120/204 | Current Loss: 0.100462\n[Training] Epoch 012 | Batch 130/204 | Current Loss: 0.100472\n[Training] Epoch 012 | Batch 140/204 | Current Loss: 0.100693\n[Training] Epoch 012 | Batch 150/204 | Current Loss: 0.100581\n[Training] Epoch 012 | Batch 160/204 | Current Loss: 0.100657\n[Training] Epoch 012 | Batch 170/204 | Current Loss: 0.100563\n[Training] Epoch 012 | Batch 180/204 | Current Loss: 0.100580\n[Training] Epoch 012 | Batch 190/204 | Current Loss: 0.100434\n[Training] Epoch 012 | Batch 200/204 | Current Loss: 0.100572\n\n[Training] Epoch 012 Summary:\n  Avg Loss: 0.100595\n  Current LR: 2.44e-04\n[EarlyStopping] Loss improved (0.100764 → 0.100595). Saving model...\n[Training] Epoch 013 | Batch 000/204 | Current Loss: 0.100869\n[Training] Epoch 013 | Batch 010/204 | Current Loss: 0.100573\n[Training] Epoch 013 | Batch 020/204 | Current Loss: 0.100478\n[Training] Epoch 013 | Batch 030/204 | Current Loss: 0.100416\n[Training] Epoch 013 | Batch 040/204 | Current Loss: 0.100423\n[Training] Epoch 013 | Batch 050/204 | Current Loss: 0.100534\n[Training] Epoch 013 | Batch 060/204 | Current Loss: 0.100441\n[Training] Epoch 013 | Batch 070/204 | Current Loss: 0.100558\n[Training] Epoch 013 | Batch 080/204 | Current Loss: 0.100622\n[Training] Epoch 013 | Batch 090/204 | Current Loss: 0.100451\n[Training] Epoch 013 | Batch 100/204 | Current Loss: 0.100478\n[Training] Epoch 013 | Batch 110/204 | Current Loss: 0.100604\n[Training] Epoch 013 | Batch 120/204 | Current Loss: 0.100465\n[Training] Epoch 013 | Batch 130/204 | Current Loss: 0.100365\n[Training] Epoch 013 | Batch 140/204 | Current Loss: 0.100477\n[Training] Epoch 013 | Batch 150/204 | Current Loss: 0.100388\n[Training] Epoch 013 | Batch 160/204 | Current Loss: 0.100484\n[Training] Epoch 013 | Batch 170/204 | Current Loss: 0.100497\n[Training] Epoch 013 | Batch 180/204 | Current Loss: 0.100375\n[Training] Epoch 013 | Batch 190/204 | Current Loss: 0.100504\n[Training] Epoch 013 | Batch 200/204 | Current Loss: 0.100634\n\n[Training] Epoch 013 Summary:\n  Avg Loss: 0.100503\n  Current LR: 2.15e-04\n[EarlyStopping] Loss improved (0.100595 → 0.100503). Saving model...\n[Training] Epoch 014 | Batch 000/204 | Current Loss: 0.100383\n[Training] Epoch 014 | Batch 010/204 | Current Loss: 0.100457\n[Training] Epoch 014 | Batch 020/204 | Current Loss: 0.100407\n[Training] Epoch 014 | Batch 030/204 | Current Loss: 0.100335\n[Training] Epoch 014 | Batch 040/204 | Current Loss: 0.100404\n[Training] Epoch 014 | Batch 050/204 | Current Loss: 0.100471\n[Training] Epoch 014 | Batch 060/204 | Current Loss: 0.100495\n[Training] Epoch 014 | Batch 070/204 | Current Loss: 0.100332\n[Training] Epoch 014 | Batch 080/204 | Current Loss: 0.100411\n[Training] Epoch 014 | Batch 090/204 | Current Loss: 0.100412\n[Training] Epoch 014 | Batch 100/204 | Current Loss: 0.100514\n[Training] Epoch 014 | Batch 110/204 | Current Loss: 0.100313\n[Training] Epoch 014 | Batch 120/204 | Current Loss: 0.100395\n[Training] Epoch 014 | Batch 130/204 | Current Loss: 0.100557\n[Training] Epoch 014 | Batch 140/204 | Current Loss: 0.100252\n[Training] Epoch 014 | Batch 150/204 | Current Loss: 0.100345\n[Training] Epoch 014 | Batch 160/204 | Current Loss: 0.100401\n[Training] Epoch 014 | Batch 170/204 | Current Loss: 0.100432\n[Training] Epoch 014 | Batch 180/204 | Current Loss: 0.100377\n[Training] Epoch 014 | Batch 190/204 | Current Loss: 0.100341\n[Training] Epoch 014 | Batch 200/204 | Current Loss: 0.100380\n\n[Training] Epoch 014 Summary:\n  Avg Loss: 0.100439\n  Current LR: 1.77e-04\n[EarlyStopping] Loss improved (0.100503 → 0.100439). Saving model...\n[Training] Epoch 015 | Batch 000/204 | Current Loss: 0.100523\n[Training] Epoch 015 | Batch 010/204 | Current Loss: 0.100542\n[Training] Epoch 015 | Batch 020/204 | Current Loss: 0.100315\n[Training] Epoch 015 | Batch 030/204 | Current Loss: 0.100323\n[Training] Epoch 015 | Batch 040/204 | Current Loss: 0.100290\n[Training] Epoch 015 | Batch 050/204 | Current Loss: 0.100345\n[Training] Epoch 015 | Batch 060/204 | Current Loss: 0.100354\n[Training] Epoch 015 | Batch 070/204 | Current Loss: 0.100338\n[Training] Epoch 015 | Batch 080/204 | Current Loss: 0.100395\n[Training] Epoch 015 | Batch 090/204 | Current Loss: 0.100408\n[Training] Epoch 015 | Batch 100/204 | Current Loss: 0.100389\n[Training] Epoch 015 | Batch 110/204 | Current Loss: 0.100395\n[Training] Epoch 015 | Batch 120/204 | Current Loss: 0.100396\n[Training] Epoch 015 | Batch 130/204 | Current Loss: 0.100362\n[Training] Epoch 015 | Batch 140/204 | Current Loss: 0.100351\n[Training] Epoch 015 | Batch 150/204 | Current Loss: 0.100336\n[Training] Epoch 015 | Batch 160/204 | Current Loss: 0.100352\n[Training] Epoch 015 | Batch 170/204 | Current Loss: 0.100471\n[Training] Epoch 015 | Batch 180/204 | Current Loss: 0.100426\n[Training] Epoch 015 | Batch 190/204 | Current Loss: 0.100304\n[Training] Epoch 015 | Batch 200/204 | Current Loss: 0.100336\n\n[Training] Epoch 015 Summary:\n  Avg Loss: 0.100410\n  Current LR: 1.36e-04\n[EarlyStopping] Loss improved (0.100439 → 0.100410). Saving model...\n[Training] Epoch 016 | Batch 000/204 | Current Loss: 0.100281\n[Training] Epoch 016 | Batch 010/204 | Current Loss: 0.100439\n[Training] Epoch 016 | Batch 020/204 | Current Loss: 0.100324\n[Training] Epoch 016 | Batch 030/204 | Current Loss: 0.100448\n[Training] Epoch 016 | Batch 040/204 | Current Loss: 0.100354\n[Training] Epoch 016 | Batch 050/204 | Current Loss: 0.100288\n[Training] Epoch 016 | Batch 060/204 | Current Loss: 0.100413\n[Training] Epoch 016 | Batch 070/204 | Current Loss: 0.100402\n[Training] Epoch 016 | Batch 080/204 | Current Loss: 0.100449\n[Training] Epoch 016 | Batch 090/204 | Current Loss: 0.100373\n[Training] Epoch 016 | Batch 100/204 | Current Loss: 0.100299\n[Training] Epoch 016 | Batch 110/204 | Current Loss: 0.100372\n[Training] Epoch 016 | Batch 120/204 | Current Loss: 0.100329\n[Training] Epoch 016 | Batch 130/204 | Current Loss: 0.100391\n[Training] Epoch 016 | Batch 140/204 | Current Loss: 0.100386\n[Training] Epoch 016 | Batch 150/204 | Current Loss: 0.100389\n[Training] Epoch 016 | Batch 160/204 | Current Loss: 0.100278\n[Training] Epoch 016 | Batch 170/204 | Current Loss: 0.100289\n[Training] Epoch 016 | Batch 180/204 | Current Loss: 0.100315\n[Training] Epoch 016 | Batch 190/204 | Current Loss: 0.100283\n[Training] Epoch 016 | Batch 200/204 | Current Loss: 0.100300\n\n[Training] Epoch 016 Summary:\n  Avg Loss: 0.100391\n  Current LR: 9.39e-05\n[EarlyStopping] Loss improved (0.100410 → 0.100391). Saving model...\n[Training] Epoch 017 | Batch 000/204 | Current Loss: 0.101050\n[Training] Epoch 017 | Batch 010/204 | Current Loss: 0.100570\n[Training] Epoch 017 | Batch 020/204 | Current Loss: 0.100382\n[Training] Epoch 017 | Batch 030/204 | Current Loss: 0.100341\n[Training] Epoch 017 | Batch 040/204 | Current Loss: 0.100517\n[Training] Epoch 017 | Batch 050/204 | Current Loss: 0.100364\n[Training] Epoch 017 | Batch 060/204 | Current Loss: 0.100356\n[Training] Epoch 017 | Batch 070/204 | Current Loss: 0.100318\n[Training] Epoch 017 | Batch 080/204 | Current Loss: 0.100416\n[Training] Epoch 017 | Batch 090/204 | Current Loss: 0.100211\n[Training] Epoch 017 | Batch 100/204 | Current Loss: 0.100331\n[Training] Epoch 017 | Batch 110/204 | Current Loss: 0.100352\n[Training] Epoch 017 | Batch 120/204 | Current Loss: 0.100275\n[Training] Epoch 017 | Batch 130/204 | Current Loss: 0.100293\n[Training] Epoch 017 | Batch 140/204 | Current Loss: 0.100312\n[Training] Epoch 017 | Batch 150/204 | Current Loss: 0.100323\n[Training] Epoch 017 | Batch 160/204 | Current Loss: 0.100245\n[Training] Epoch 017 | Batch 170/204 | Current Loss: 0.100300\n[Training] Epoch 017 | Batch 180/204 | Current Loss: 0.100273\n[Training] Epoch 017 | Batch 190/204 | Current Loss: 0.100464\n[Training] Epoch 017 | Batch 200/204 | Current Loss: 0.100297\n\n[Training] Epoch 017 Summary:\n  Avg Loss: 0.100413\n  Current LR: 5.64e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 018 | Batch 000/204 | Current Loss: 0.100463\n[Training] Epoch 018 | Batch 010/204 | Current Loss: 0.100363\n[Training] Epoch 018 | Batch 020/204 | Current Loss: 0.100333\n[Training] Epoch 018 | Batch 030/204 | Current Loss: 0.100354\n[Training] Epoch 018 | Batch 040/204 | Current Loss: 0.100256\n[Training] Epoch 018 | Batch 050/204 | Current Loss: 0.100320\n[Training] Epoch 018 | Batch 060/204 | Current Loss: 0.100224\n[Training] Epoch 018 | Batch 070/204 | Current Loss: 0.100308\n[Training] Epoch 018 | Batch 080/204 | Current Loss: 0.100357\n[Training] Epoch 018 | Batch 090/204 | Current Loss: 0.100420\n[Training] Epoch 018 | Batch 100/204 | Current Loss: 0.100322\n[Training] Epoch 018 | Batch 110/204 | Current Loss: 0.100400\n[Training] Epoch 018 | Batch 120/204 | Current Loss: 0.100314\n[Training] Epoch 018 | Batch 130/204 | Current Loss: 0.100322\n[Training] Epoch 018 | Batch 140/204 | Current Loss: 0.100381\n[Training] Epoch 018 | Batch 150/204 | Current Loss: 0.100253\n[Training] Epoch 018 | Batch 160/204 | Current Loss: 0.100437\n[Training] Epoch 018 | Batch 170/204 | Current Loss: 0.100339\n[Training] Epoch 018 | Batch 180/204 | Current Loss: 0.100409\n[Training] Epoch 018 | Batch 190/204 | Current Loss: 0.100331\n[Training] Epoch 018 | Batch 200/204 | Current Loss: 0.100268\n\n[Training] Epoch 018 Summary:\n  Avg Loss: 0.100335\n  Current LR: 2.67e-05\n[EarlyStopping] Loss improved (0.100391 → 0.100335). Saving model...\n[Training] Epoch 019 | Batch 000/204 | Current Loss: 0.100384\n[Training] Epoch 019 | Batch 010/204 | Current Loss: 0.100389\n[Training] Epoch 019 | Batch 020/204 | Current Loss: 0.100396\n[Training] Epoch 019 | Batch 030/204 | Current Loss: 0.100293\n[Training] Epoch 019 | Batch 040/204 | Current Loss: 0.100237\n[Training] Epoch 019 | Batch 050/204 | Current Loss: 0.100326\n[Training] Epoch 019 | Batch 060/204 | Current Loss: 0.100399\n[Training] Epoch 019 | Batch 070/204 | Current Loss: 0.100310\n[Training] Epoch 019 | Batch 080/204 | Current Loss: 0.100431\n[Training] Epoch 019 | Batch 090/204 | Current Loss: 0.100295\n[Training] Epoch 019 | Batch 100/204 | Current Loss: 0.100239\n[Training] Epoch 019 | Batch 110/204 | Current Loss: 0.100263\n[Training] Epoch 019 | Batch 120/204 | Current Loss: 0.100328\n[Training] Epoch 019 | Batch 130/204 | Current Loss: 0.100368\n[Training] Epoch 019 | Batch 140/204 | Current Loss: 0.100463\n[Training] Epoch 019 | Batch 150/204 | Current Loss: 0.100360\n[Training] Epoch 019 | Batch 160/204 | Current Loss: 0.100291\n[Training] Epoch 019 | Batch 170/204 | Current Loss: 0.100261\n[Training] Epoch 019 | Batch 180/204 | Current Loss: 0.100386\n[Training] Epoch 019 | Batch 190/204 | Current Loss: 0.100329\n[Training] Epoch 019 | Batch 200/204 | Current Loss: 0.100218\n\n[Training] Epoch 019 Summary:\n  Avg Loss: 0.100325\n  Current LR: 7.58e-06\n[EarlyStopping] Loss improved (0.100335 → 0.100325). Saving model...\n[Training] Epoch 020 | Batch 000/204 | Current Loss: 0.100317\n[Training] Epoch 020 | Batch 010/204 | Current Loss: 0.100354\n[Training] Epoch 020 | Batch 020/204 | Current Loss: 0.100267\n[Training] Epoch 020 | Batch 030/204 | Current Loss: 0.100205\n[Training] Epoch 020 | Batch 040/204 | Current Loss: 0.100291\n[Training] Epoch 020 | Batch 050/204 | Current Loss: 0.100419\n[Training] Epoch 020 | Batch 060/204 | Current Loss: 0.100546\n[Training] Epoch 020 | Batch 070/204 | Current Loss: 0.100211\n[Training] Epoch 020 | Batch 080/204 | Current Loss: 0.100549\n[Training] Epoch 020 | Batch 090/204 | Current Loss: 0.100389\n[Training] Epoch 020 | Batch 100/204 | Current Loss: 0.100292\n[Training] Epoch 020 | Batch 110/204 | Current Loss: 0.100195\n[Training] Epoch 020 | Batch 120/204 | Current Loss: 0.100364\n[Training] Epoch 020 | Batch 130/204 | Current Loss: 0.100349\n[Training] Epoch 020 | Batch 140/204 | Current Loss: 0.100235\n[Training] Epoch 020 | Batch 150/204 | Current Loss: 0.100185\n[Training] Epoch 020 | Batch 160/204 | Current Loss: 0.100359\n[Training] Epoch 020 | Batch 170/204 | Current Loss: 0.100310\n[Training] Epoch 020 | Batch 180/204 | Current Loss: 0.100290\n[Training] Epoch 020 | Batch 190/204 | Current Loss: 0.100269\n[Training] Epoch 020 | Batch 200/204 | Current Loss: 0.100250\n\n[Training] Epoch 020 Summary:\n  Avg Loss: 0.100310\n  Current LR: 2.70e-04\n[EarlyStopping] Loss improved (0.100325 → 0.100310). Saving model...\n[Training] Epoch 021 | Batch 000/204 | Current Loss: 0.100328\n[Training] Epoch 021 | Batch 010/204 | Current Loss: 0.100396\n[Training] Epoch 021 | Batch 020/204 | Current Loss: 0.100242\n[Training] Epoch 021 | Batch 030/204 | Current Loss: 0.100363\n[Training] Epoch 021 | Batch 040/204 | Current Loss: 0.100308\n[Training] Epoch 021 | Batch 050/204 | Current Loss: 0.100377\n[Training] Epoch 021 | Batch 060/204 | Current Loss: 0.100349\n[Training] Epoch 021 | Batch 070/204 | Current Loss: 0.100241\n[Training] Epoch 021 | Batch 080/204 | Current Loss: 0.100415\n[Training] Epoch 021 | Batch 090/204 | Current Loss: 0.100288\n[Training] Epoch 021 | Batch 100/204 | Current Loss: 0.100320\n[Training] Epoch 021 | Batch 110/204 | Current Loss: 0.100245\n[Training] Epoch 021 | Batch 120/204 | Current Loss: 0.100370\n[Training] Epoch 021 | Batch 130/204 | Current Loss: 0.100342\n[Training] Epoch 021 | Batch 140/204 | Current Loss: 0.100351\n[Training] Epoch 021 | Batch 150/204 | Current Loss: 0.100242\n[Training] Epoch 021 | Batch 160/204 | Current Loss: 0.100276\n[Training] Epoch 021 | Batch 170/204 | Current Loss: 0.100302\n[Training] Epoch 021 | Batch 180/204 | Current Loss: 0.100326\n[Training] Epoch 021 | Batch 190/204 | Current Loss: 0.100356\n[Training] Epoch 021 | Batch 200/204 | Current Loss: 0.100317\n\n[Training] Epoch 021 Summary:\n  Avg Loss: 0.100344\n  Current LR: 2.63e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 022 | Batch 000/204 | Current Loss: 0.100421\n[Training] Epoch 022 | Batch 010/204 | Current Loss: 0.100263\n[Training] Epoch 022 | Batch 020/204 | Current Loss: 0.100395\n[Training] Epoch 022 | Batch 030/204 | Current Loss: 0.100382\n[Training] Epoch 022 | Batch 040/204 | Current Loss: 0.100223\n[Training] Epoch 022 | Batch 050/204 | Current Loss: 0.100333\n[Training] Epoch 022 | Batch 060/204 | Current Loss: 0.100271\n[Training] Epoch 022 | Batch 070/204 | Current Loss: 0.100374\n[Training] Epoch 022 | Batch 080/204 | Current Loss: 0.100259\n[Training] Epoch 022 | Batch 090/204 | Current Loss: 0.100256\n[Training] Epoch 022 | Batch 100/204 | Current Loss: 0.100223\n[Training] Epoch 022 | Batch 110/204 | Current Loss: 0.100201\n[Training] Epoch 022 | Batch 120/204 | Current Loss: 0.100254\n[Training] Epoch 022 | Batch 130/204 | Current Loss: 0.100267\n[Training] Epoch 022 | Batch 140/204 | Current Loss: 0.100266\n[Training] Epoch 022 | Batch 150/204 | Current Loss: 0.100334\n[Training] Epoch 022 | Batch 160/204 | Current Loss: 0.100276\n[Training] Epoch 022 | Batch 170/204 | Current Loss: 0.100201\n[Training] Epoch 022 | Batch 180/204 | Current Loss: 0.100317\n[Training] Epoch 022 | Batch 190/204 | Current Loss: 0.100254\n[Training] Epoch 022 | Batch 200/204 | Current Loss: 0.100226\n\n[Training] Epoch 022 Summary:\n  Avg Loss: 0.100300\n  Current LR: 2.44e-04\n[EarlyStopping] Loss improved (0.100310 → 0.100300). Saving model...\n[Training] Epoch 023 | Batch 000/204 | Current Loss: 0.100227\n[Training] Epoch 023 | Batch 010/204 | Current Loss: 0.100267\n[Training] Epoch 023 | Batch 020/204 | Current Loss: 0.100235\n[Training] Epoch 023 | Batch 030/204 | Current Loss: 0.100163\n[Training] Epoch 023 | Batch 040/204 | Current Loss: 0.100227\n[Training] Epoch 023 | Batch 050/204 | Current Loss: 0.100241\n[Training] Epoch 023 | Batch 060/204 | Current Loss: 0.100211\n[Training] Epoch 023 | Batch 070/204 | Current Loss: 0.100247\n[Training] Epoch 023 | Batch 080/204 | Current Loss: 0.100340\n[Training] Epoch 023 | Batch 090/204 | Current Loss: 0.100164\n[Training] Epoch 023 | Batch 100/204 | Current Loss: 0.100421\n[Training] Epoch 023 | Batch 110/204 | Current Loss: 0.100269\n[Training] Epoch 023 | Batch 120/204 | Current Loss: 0.100297\n[Training] Epoch 023 | Batch 130/204 | Current Loss: 0.100227\n[Training] Epoch 023 | Batch 140/204 | Current Loss: 0.100242\n[Training] Epoch 023 | Batch 150/204 | Current Loss: 0.100189\n[Training] Epoch 023 | Batch 160/204 | Current Loss: 0.100320\n[Training] Epoch 023 | Batch 170/204 | Current Loss: 0.100314\n[Training] Epoch 023 | Batch 180/204 | Current Loss: 0.100242\n[Training] Epoch 023 | Batch 190/204 | Current Loss: 0.100316\n[Training] Epoch 023 | Batch 200/204 | Current Loss: 0.100253\n\n[Training] Epoch 023 Summary:\n  Avg Loss: 0.100267\n  Current LR: 2.15e-04\n[EarlyStopping] Loss improved (0.100300 → 0.100267). Saving model...\n[Training] Epoch 024 | Batch 000/204 | Current Loss: 0.100279\n[Training] Epoch 024 | Batch 010/204 | Current Loss: 0.100165\n[Training] Epoch 024 | Batch 020/204 | Current Loss: 0.100240\n[Training] Epoch 024 | Batch 030/204 | Current Loss: 0.100281\n[Training] Epoch 024 | Batch 040/204 | Current Loss: 0.100188\n[Training] Epoch 024 | Batch 050/204 | Current Loss: 0.100226\n[Training] Epoch 024 | Batch 060/204 | Current Loss: 0.100300\n[Training] Epoch 024 | Batch 070/204 | Current Loss: 0.100232\n[Training] Epoch 024 | Batch 080/204 | Current Loss: 0.100232\n[Training] Epoch 024 | Batch 090/204 | Current Loss: 0.100149\n[Training] Epoch 024 | Batch 100/204 | Current Loss: 0.100147\n[Training] Epoch 024 | Batch 110/204 | Current Loss: 0.100270\n[Training] Epoch 024 | Batch 120/204 | Current Loss: 0.100288\n[Training] Epoch 024 | Batch 130/204 | Current Loss: 0.100311\n[Training] Epoch 024 | Batch 140/204 | Current Loss: 0.100304\n[Training] Epoch 024 | Batch 150/204 | Current Loss: 0.100238\n[Training] Epoch 024 | Batch 160/204 | Current Loss: 0.100218\n[Training] Epoch 024 | Batch 170/204 | Current Loss: 0.100294\n[Training] Epoch 024 | Batch 180/204 | Current Loss: 0.100288\n[Training] Epoch 024 | Batch 190/204 | Current Loss: 0.100246\n[Training] Epoch 024 | Batch 200/204 | Current Loss: 0.100327\n\n[Training] Epoch 024 Summary:\n  Avg Loss: 0.100250\n  Current LR: 1.77e-04\n[EarlyStopping] Loss improved (0.100267 → 0.100250). Saving model...\n[Training] Epoch 025 | Batch 000/204 | Current Loss: 0.100293\n[Training] Epoch 025 | Batch 010/204 | Current Loss: 0.100278\n[Training] Epoch 025 | Batch 020/204 | Current Loss: 0.100196\n[Training] Epoch 025 | Batch 030/204 | Current Loss: 0.100155\n[Training] Epoch 025 | Batch 040/204 | Current Loss: 0.100231\n[Training] Epoch 025 | Batch 050/204 | Current Loss: 0.100242\n[Training] Epoch 025 | Batch 060/204 | Current Loss: 0.100218\n[Training] Epoch 025 | Batch 070/204 | Current Loss: 0.100282\n[Training] Epoch 025 | Batch 080/204 | Current Loss: 0.100240\n[Training] Epoch 025 | Batch 090/204 | Current Loss: 0.100166\n[Training] Epoch 025 | Batch 100/204 | Current Loss: 0.100152\n[Training] Epoch 025 | Batch 110/204 | Current Loss: 0.100141\n[Training] Epoch 025 | Batch 120/204 | Current Loss: 0.100234\n[Training] Epoch 025 | Batch 130/204 | Current Loss: 0.100205\n[Training] Epoch 025 | Batch 140/204 | Current Loss: 0.100277\n[Training] Epoch 025 | Batch 150/204 | Current Loss: 0.100168\n[Training] Epoch 025 | Batch 160/204 | Current Loss: 0.100230\n[Training] Epoch 025 | Batch 170/204 | Current Loss: 0.100247\n[Training] Epoch 025 | Batch 180/204 | Current Loss: 0.100242\n[Training] Epoch 025 | Batch 190/204 | Current Loss: 0.100117\n[Training] Epoch 025 | Batch 200/204 | Current Loss: 0.100204\n\n[Training] Epoch 025 Summary:\n  Avg Loss: 0.100226\n  Current LR: 1.36e-04\n[EarlyStopping] Loss improved (0.100250 → 0.100226). Saving model...\n[Training] Epoch 026 | Batch 000/204 | Current Loss: 0.100121\n[Training] Epoch 026 | Batch 010/204 | Current Loss: 0.100206\n[Training] Epoch 026 | Batch 020/204 | Current Loss: 0.100260\n[Training] Epoch 026 | Batch 030/204 | Current Loss: 0.100144\n[Training] Epoch 026 | Batch 040/204 | Current Loss: 0.100276\n[Training] Epoch 026 | Batch 050/204 | Current Loss: 0.100179\n[Training] Epoch 026 | Batch 060/204 | Current Loss: 0.100260\n[Training] Epoch 026 | Batch 070/204 | Current Loss: 0.100239\n[Training] Epoch 026 | Batch 080/204 | Current Loss: 0.100238\n[Training] Epoch 026 | Batch 090/204 | Current Loss: 0.100204\n[Training] Epoch 026 | Batch 100/204 | Current Loss: 0.100217\n[Training] Epoch 026 | Batch 110/204 | Current Loss: 0.100157\n[Training] Epoch 026 | Batch 120/204 | Current Loss: 0.100208\n[Training] Epoch 026 | Batch 130/204 | Current Loss: 0.100223\n[Training] Epoch 026 | Batch 140/204 | Current Loss: 0.100235\n[Training] Epoch 026 | Batch 150/204 | Current Loss: 0.100241\n[Training] Epoch 026 | Batch 160/204 | Current Loss: 0.100277\n[Training] Epoch 026 | Batch 170/204 | Current Loss: 0.100139\n[Training] Epoch 026 | Batch 180/204 | Current Loss: 0.100203\n[Training] Epoch 026 | Batch 190/204 | Current Loss: 0.100269\n[Training] Epoch 026 | Batch 200/204 | Current Loss: 0.100229\n\n[Training] Epoch 026 Summary:\n  Avg Loss: 0.100207\n  Current LR: 9.39e-05\n[EarlyStopping] Loss improved (0.100226 → 0.100207). Saving model...\n[Training] Epoch 027 | Batch 000/204 | Current Loss: 0.100154\n[Training] Epoch 027 | Batch 010/204 | Current Loss: 0.100207\n[Training] Epoch 027 | Batch 020/204 | Current Loss: 0.100058\n[Training] Epoch 027 | Batch 030/204 | Current Loss: 0.100311\n[Training] Epoch 027 | Batch 040/204 | Current Loss: 0.100273\n[Training] Epoch 027 | Batch 050/204 | Current Loss: 0.100256\n[Training] Epoch 027 | Batch 060/204 | Current Loss: 0.100277\n[Training] Epoch 027 | Batch 070/204 | Current Loss: 0.100230\n[Training] Epoch 027 | Batch 080/204 | Current Loss: 0.100152\n[Training] Epoch 027 | Batch 090/204 | Current Loss: 0.100241\n[Training] Epoch 027 | Batch 100/204 | Current Loss: 0.100284\n[Training] Epoch 027 | Batch 110/204 | Current Loss: 0.100214\n[Training] Epoch 027 | Batch 120/204 | Current Loss: 0.100103\n[Training] Epoch 027 | Batch 130/204 | Current Loss: 0.100212\n[Training] Epoch 027 | Batch 140/204 | Current Loss: 0.100210\n[Training] Epoch 027 | Batch 150/204 | Current Loss: 0.100116\n[Training] Epoch 027 | Batch 160/204 | Current Loss: 0.100222\n[Training] Epoch 027 | Batch 170/204 | Current Loss: 0.100143\n[Training] Epoch 027 | Batch 180/204 | Current Loss: 0.100138\n[Training] Epoch 027 | Batch 190/204 | Current Loss: 0.100164\n[Training] Epoch 027 | Batch 200/204 | Current Loss: 0.100262\n\n[Training] Epoch 027 Summary:\n  Avg Loss: 0.100206\n  Current LR: 5.64e-05\n[EarlyStopping] Loss improved (0.100207 → 0.100206). Saving model...\n[Training] Epoch 028 | Batch 000/204 | Current Loss: 0.100320\n[Training] Epoch 028 | Batch 010/204 | Current Loss: 0.100151\n[Training] Epoch 028 | Batch 020/204 | Current Loss: 0.100117\n[Training] Epoch 028 | Batch 030/204 | Current Loss: 0.100270\n[Training] Epoch 028 | Batch 040/204 | Current Loss: 0.100228\n[Training] Epoch 028 | Batch 050/204 | Current Loss: 0.100247\n[Training] Epoch 028 | Batch 060/204 | Current Loss: 0.100216\n[Training] Epoch 028 | Batch 070/204 | Current Loss: 0.100298\n[Training] Epoch 028 | Batch 080/204 | Current Loss: 0.100081\n[Training] Epoch 028 | Batch 090/204 | Current Loss: 0.100118\n[Training] Epoch 028 | Batch 100/204 | Current Loss: 0.100235\n[Training] Epoch 028 | Batch 110/204 | Current Loss: 0.100171\n[Training] Epoch 028 | Batch 120/204 | Current Loss: 0.100258\n[Training] Epoch 028 | Batch 130/204 | Current Loss: 0.100300\n[Training] Epoch 028 | Batch 140/204 | Current Loss: 0.100101\n[Training] Epoch 028 | Batch 150/204 | Current Loss: 0.100251\n[Training] Epoch 028 | Batch 160/204 | Current Loss: 0.100152\n[Training] Epoch 028 | Batch 170/204 | Current Loss: 0.100203\n[Training] Epoch 028 | Batch 180/204 | Current Loss: 0.100189\n[Training] Epoch 028 | Batch 190/204 | Current Loss: 0.100166\n[Training] Epoch 028 | Batch 200/204 | Current Loss: 0.100231\n\n[Training] Epoch 028 Summary:\n  Avg Loss: 0.100195\n  Current LR: 2.67e-05\n[EarlyStopping] Loss improved (0.100206 → 0.100195). Saving model...\n[Training] Epoch 029 | Batch 000/204 | Current Loss: 0.100271\n[Training] Epoch 029 | Batch 010/204 | Current Loss: 0.100168\n[Training] Epoch 029 | Batch 020/204 | Current Loss: 0.100181\n[Training] Epoch 029 | Batch 030/204 | Current Loss: 0.099990\n[Training] Epoch 029 | Batch 040/204 | Current Loss: 0.100225\n[Training] Epoch 029 | Batch 050/204 | Current Loss: 0.100286\n[Training] Epoch 029 | Batch 060/204 | Current Loss: 0.100278\n[Training] Epoch 029 | Batch 070/204 | Current Loss: 0.100272\n[Training] Epoch 029 | Batch 080/204 | Current Loss: 0.100159\n[Training] Epoch 029 | Batch 090/204 | Current Loss: 0.100161\n[Training] Epoch 029 | Batch 100/204 | Current Loss: 0.100109\n[Training] Epoch 029 | Batch 110/204 | Current Loss: 0.100215\n[Training] Epoch 029 | Batch 120/204 | Current Loss: 0.100149\n[Training] Epoch 029 | Batch 130/204 | Current Loss: 0.100185\n[Training] Epoch 029 | Batch 140/204 | Current Loss: 0.100403\n[Training] Epoch 029 | Batch 150/204 | Current Loss: 0.100148\n[Training] Epoch 029 | Batch 160/204 | Current Loss: 0.100179\n[Training] Epoch 029 | Batch 170/204 | Current Loss: 0.100177\n[Training] Epoch 029 | Batch 180/204 | Current Loss: 0.100179\n[Training] Epoch 029 | Batch 190/204 | Current Loss: 0.100271\n[Training] Epoch 029 | Batch 200/204 | Current Loss: 0.100109\n\n[Training] Epoch 029 Summary:\n  Avg Loss: 0.100181\n  Current LR: 7.58e-06\n[EarlyStopping] Loss improved (0.100195 → 0.100181). Saving model...\n[Training] Epoch 030 | Batch 000/204 | Current Loss: 0.100105\n[Training] Epoch 030 | Batch 010/204 | Current Loss: 0.100281\n[Training] Epoch 030 | Batch 020/204 | Current Loss: 0.100170\n[Training] Epoch 030 | Batch 030/204 | Current Loss: 0.100100\n[Training] Epoch 030 | Batch 040/204 | Current Loss: 0.100115\n[Training] Epoch 030 | Batch 050/204 | Current Loss: 0.100229\n[Training] Epoch 030 | Batch 060/204 | Current Loss: 0.100114\n[Training] Epoch 030 | Batch 070/204 | Current Loss: 0.100299\n[Training] Epoch 030 | Batch 080/204 | Current Loss: 0.100153\n[Training] Epoch 030 | Batch 090/204 | Current Loss: 0.100219\n[Training] Epoch 030 | Batch 100/204 | Current Loss: 0.100313\n[Training] Epoch 030 | Batch 110/204 | Current Loss: 0.100140\n[Training] Epoch 030 | Batch 120/204 | Current Loss: 0.100287\n[Training] Epoch 030 | Batch 130/204 | Current Loss: 0.100154\n[Training] Epoch 030 | Batch 140/204 | Current Loss: 0.100161\n[Training] Epoch 030 | Batch 150/204 | Current Loss: 0.100254\n[Training] Epoch 030 | Batch 160/204 | Current Loss: 0.100250\n[Training] Epoch 030 | Batch 170/204 | Current Loss: 0.100108\n[Training] Epoch 030 | Batch 180/204 | Current Loss: 0.100151\n[Training] Epoch 030 | Batch 190/204 | Current Loss: 0.100160\n[Training] Epoch 030 | Batch 200/204 | Current Loss: 0.100159\n\n[Training] Epoch 030 Summary:\n  Avg Loss: 0.100189\n  Current LR: 2.70e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 031 | Batch 000/204 | Current Loss: 0.100192\n[Training] Epoch 031 | Batch 010/204 | Current Loss: 0.100663\n[Training] Epoch 031 | Batch 020/204 | Current Loss: 0.100380\n[Training] Epoch 031 | Batch 030/204 | Current Loss: 0.100444\n[Training] Epoch 031 | Batch 040/204 | Current Loss: 0.100290\n[Training] Epoch 031 | Batch 050/204 | Current Loss: 0.100330\n[Training] Epoch 031 | Batch 060/204 | Current Loss: 0.100226\n[Training] Epoch 031 | Batch 070/204 | Current Loss: 0.100170\n[Training] Epoch 031 | Batch 080/204 | Current Loss: 0.100096\n[Training] Epoch 031 | Batch 090/204 | Current Loss: 0.100235\n[Training] Epoch 031 | Batch 100/204 | Current Loss: 0.100319\n[Training] Epoch 031 | Batch 110/204 | Current Loss: 0.100180\n[Training] Epoch 031 | Batch 120/204 | Current Loss: 0.100243\n[Training] Epoch 031 | Batch 130/204 | Current Loss: 0.100234\n[Training] Epoch 031 | Batch 140/204 | Current Loss: 0.100150\n[Training] Epoch 031 | Batch 150/204 | Current Loss: 0.100211\n[Training] Epoch 031 | Batch 160/204 | Current Loss: 0.100178\n[Training] Epoch 031 | Batch 170/204 | Current Loss: 0.100243\n[Training] Epoch 031 | Batch 180/204 | Current Loss: 0.100302\n[Training] Epoch 031 | Batch 190/204 | Current Loss: 0.100137\n[Training] Epoch 031 | Batch 200/204 | Current Loss: 0.100300\n\n[Training] Epoch 031 Summary:\n  Avg Loss: 0.100278\n  Current LR: 2.63e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 032 | Batch 000/204 | Current Loss: 0.100377\n[Training] Epoch 032 | Batch 010/204 | Current Loss: 0.100313\n[Training] Epoch 032 | Batch 020/204 | Current Loss: 0.100290\n[Training] Epoch 032 | Batch 030/204 | Current Loss: 0.100216\n[Training] Epoch 032 | Batch 040/204 | Current Loss: 0.100328\n[Training] Epoch 032 | Batch 050/204 | Current Loss: 0.100269\n[Training] Epoch 032 | Batch 060/204 | Current Loss: 0.100167\n[Training] Epoch 032 | Batch 070/204 | Current Loss: 0.100233\n[Training] Epoch 032 | Batch 080/204 | Current Loss: 0.100252\n[Training] Epoch 032 | Batch 090/204 | Current Loss: 0.100081\n[Training] Epoch 032 | Batch 100/204 | Current Loss: 0.100157\n[Training] Epoch 032 | Batch 110/204 | Current Loss: 0.100258\n[Training] Epoch 032 | Batch 120/204 | Current Loss: 0.100142\n[Training] Epoch 032 | Batch 130/204 | Current Loss: 0.100368\n[Training] Epoch 032 | Batch 140/204 | Current Loss: 0.100262\n[Training] Epoch 032 | Batch 150/204 | Current Loss: 0.100121\n[Training] Epoch 032 | Batch 160/204 | Current Loss: 0.100224\n[Training] Epoch 032 | Batch 170/204 | Current Loss: 0.100199\n[Training] Epoch 032 | Batch 180/204 | Current Loss: 0.100084\n[Training] Epoch 032 | Batch 190/204 | Current Loss: 0.100150\n[Training] Epoch 032 | Batch 200/204 | Current Loss: 0.100225\n\n[Training] Epoch 032 Summary:\n  Avg Loss: 0.100217\n  Current LR: 2.44e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 033 | Batch 000/204 | Current Loss: 0.100223\n[Training] Epoch 033 | Batch 010/204 | Current Loss: 0.100121\n[Training] Epoch 033 | Batch 020/204 | Current Loss: 0.100233\n[Training] Epoch 033 | Batch 030/204 | Current Loss: 0.100135\n[Training] Epoch 033 | Batch 040/204 | Current Loss: 0.100198\n[Training] Epoch 033 | Batch 050/204 | Current Loss: 0.100147\n[Training] Epoch 033 | Batch 060/204 | Current Loss: 0.100411\n[Training] Epoch 033 | Batch 070/204 | Current Loss: 0.100157\n[Training] Epoch 033 | Batch 080/204 | Current Loss: 0.100248\n[Training] Epoch 033 | Batch 090/204 | Current Loss: 0.100138\n[Training] Epoch 033 | Batch 100/204 | Current Loss: 0.100153\n[Training] Epoch 033 | Batch 110/204 | Current Loss: 0.100205\n[Training] Epoch 033 | Batch 120/204 | Current Loss: 0.100199\n[Training] Epoch 033 | Batch 130/204 | Current Loss: 0.100178\n[Training] Epoch 033 | Batch 140/204 | Current Loss: 0.100167\n[Training] Epoch 033 | Batch 150/204 | Current Loss: 0.100157\n[Training] Epoch 033 | Batch 160/204 | Current Loss: 0.099999\n[Training] Epoch 033 | Batch 170/204 | Current Loss: 0.100112\n[Training] Epoch 033 | Batch 180/204 | Current Loss: 0.100203\n[Training] Epoch 033 | Batch 190/204 | Current Loss: 0.100116\n[Training] Epoch 033 | Batch 200/204 | Current Loss: 0.100204\n\n[Training] Epoch 033 Summary:\n  Avg Loss: 0.100179\n  Current LR: 2.15e-04\n[EarlyStopping] Loss improved (0.100181 → 0.100179). Saving model...\n[Training] Epoch 034 | Batch 000/204 | Current Loss: 0.100226\n[Training] Epoch 034 | Batch 010/204 | Current Loss: 0.100017\n[Training] Epoch 034 | Batch 020/204 | Current Loss: 0.100082\n[Training] Epoch 034 | Batch 030/204 | Current Loss: 0.100087\n[Training] Epoch 034 | Batch 040/204 | Current Loss: 0.100177\n[Training] Epoch 034 | Batch 050/204 | Current Loss: 0.100145\n[Training] Epoch 034 | Batch 060/204 | Current Loss: 0.100141\n[Training] Epoch 034 | Batch 070/204 | Current Loss: 0.100069\n[Training] Epoch 034 | Batch 080/204 | Current Loss: 0.100109\n[Training] Epoch 034 | Batch 090/204 | Current Loss: 0.100096\n[Training] Epoch 034 | Batch 100/204 | Current Loss: 0.100191\n[Training] Epoch 034 | Batch 110/204 | Current Loss: 0.100135\n[Training] Epoch 034 | Batch 120/204 | Current Loss: 0.100196\n[Training] Epoch 034 | Batch 130/204 | Current Loss: 0.100154\n[Training] Epoch 034 | Batch 140/204 | Current Loss: 0.100188\n[Training] Epoch 034 | Batch 150/204 | Current Loss: 0.100177\n[Training] Epoch 034 | Batch 160/204 | Current Loss: 0.100176\n[Training] Epoch 034 | Batch 170/204 | Current Loss: 0.100184\n[Training] Epoch 034 | Batch 180/204 | Current Loss: 0.100137\n[Training] Epoch 034 | Batch 190/204 | Current Loss: 0.100200\n[Training] Epoch 034 | Batch 200/204 | Current Loss: 0.100203\n\n[Training] Epoch 034 Summary:\n  Avg Loss: 0.100175\n  Current LR: 1.77e-04\n[EarlyStopping] Loss improved (0.100179 → 0.100175). Saving model...\n[Training] Epoch 035 | Batch 000/204 | Current Loss: 0.100266\n[Training] Epoch 035 | Batch 010/204 | Current Loss: 0.100182\n[Training] Epoch 035 | Batch 020/204 | Current Loss: 0.100219\n[Training] Epoch 035 | Batch 030/204 | Current Loss: 0.100102\n[Training] Epoch 035 | Batch 040/204 | Current Loss: 0.100116\n[Training] Epoch 035 | Batch 050/204 | Current Loss: 0.100156\n[Training] Epoch 035 | Batch 060/204 | Current Loss: 0.100130\n[Training] Epoch 035 | Batch 070/204 | Current Loss: 0.100185\n[Training] Epoch 035 | Batch 080/204 | Current Loss: 0.099997\n[Training] Epoch 035 | Batch 090/204 | Current Loss: 0.100230\n[Training] Epoch 035 | Batch 100/204 | Current Loss: 0.100079\n[Training] Epoch 035 | Batch 110/204 | Current Loss: 0.100165\n[Training] Epoch 035 | Batch 120/204 | Current Loss: 0.100064\n[Training] Epoch 035 | Batch 130/204 | Current Loss: 0.100133\n[Training] Epoch 035 | Batch 140/204 | Current Loss: 0.100203\n[Training] Epoch 035 | Batch 150/204 | Current Loss: 0.100057\n[Training] Epoch 035 | Batch 160/204 | Current Loss: 0.100186\n[Training] Epoch 035 | Batch 170/204 | Current Loss: 0.100179\n[Training] Epoch 035 | Batch 180/204 | Current Loss: 0.100080\n[Training] Epoch 035 | Batch 190/204 | Current Loss: 0.100109\n[Training] Epoch 035 | Batch 200/204 | Current Loss: 0.100209\n\n[Training] Epoch 035 Summary:\n  Avg Loss: 0.100171\n  Current LR: 1.36e-04\n[EarlyStopping] Loss improved (0.100175 → 0.100171). Saving model...\n[Training] Epoch 036 | Batch 000/204 | Current Loss: 0.100189\n[Training] Epoch 036 | Batch 010/204 | Current Loss: 0.100067\n[Training] Epoch 036 | Batch 020/204 | Current Loss: 0.100193\n[Training] Epoch 036 | Batch 030/204 | Current Loss: 0.100125\n[Training] Epoch 036 | Batch 040/204 | Current Loss: 0.100146\n[Training] Epoch 036 | Batch 050/204 | Current Loss: 0.100345\n[Training] Epoch 036 | Batch 060/204 | Current Loss: 0.100167\n[Training] Epoch 036 | Batch 070/204 | Current Loss: 0.100287\n[Training] Epoch 036 | Batch 080/204 | Current Loss: 0.100203\n[Training] Epoch 036 | Batch 090/204 | Current Loss: 0.100068\n[Training] Epoch 036 | Batch 100/204 | Current Loss: 0.100181\n[Training] Epoch 036 | Batch 110/204 | Current Loss: 0.100267\n[Training] Epoch 036 | Batch 120/204 | Current Loss: 0.100133\n[Training] Epoch 036 | Batch 130/204 | Current Loss: 0.100251\n[Training] Epoch 036 | Batch 140/204 | Current Loss: 0.100206\n[Training] Epoch 036 | Batch 150/204 | Current Loss: 0.100204\n[Training] Epoch 036 | Batch 160/204 | Current Loss: 0.100094\n[Training] Epoch 036 | Batch 170/204 | Current Loss: 0.100166\n[Training] Epoch 036 | Batch 180/204 | Current Loss: 0.100195\n[Training] Epoch 036 | Batch 190/204 | Current Loss: 0.100146\n[Training] Epoch 036 | Batch 200/204 | Current Loss: 0.100229\n\n[Training] Epoch 036 Summary:\n  Avg Loss: 0.100151\n  Current LR: 9.39e-05\n[EarlyStopping] Loss improved (0.100171 → 0.100151). Saving model...\n[Training] Epoch 037 | Batch 000/204 | Current Loss: 0.100082\n[Training] Epoch 037 | Batch 010/204 | Current Loss: 0.100122\n[Training] Epoch 037 | Batch 020/204 | Current Loss: 0.100209\n[Training] Epoch 037 | Batch 030/204 | Current Loss: 0.100167\n[Training] Epoch 037 | Batch 040/204 | Current Loss: 0.100179\n[Training] Epoch 037 | Batch 050/204 | Current Loss: 0.100170\n[Training] Epoch 037 | Batch 060/204 | Current Loss: 0.100079\n[Training] Epoch 037 | Batch 070/204 | Current Loss: 0.100185\n[Training] Epoch 037 | Batch 080/204 | Current Loss: 0.100099\n[Training] Epoch 037 | Batch 090/204 | Current Loss: 0.100135\n[Training] Epoch 037 | Batch 100/204 | Current Loss: 0.100227\n[Training] Epoch 037 | Batch 110/204 | Current Loss: 0.100149\n[Training] Epoch 037 | Batch 120/204 | Current Loss: 0.100039\n[Training] Epoch 037 | Batch 130/204 | Current Loss: 0.100094\n[Training] Epoch 037 | Batch 140/204 | Current Loss: 0.100067\n[Training] Epoch 037 | Batch 150/204 | Current Loss: 0.100093\n[Training] Epoch 037 | Batch 160/204 | Current Loss: 0.100164\n[Training] Epoch 037 | Batch 170/204 | Current Loss: 0.100133\n[Training] Epoch 037 | Batch 180/204 | Current Loss: 0.100139\n[Training] Epoch 037 | Batch 190/204 | Current Loss: 0.100072\n[Training] Epoch 037 | Batch 200/204 | Current Loss: 0.100216\n\n[Training] Epoch 037 Summary:\n  Avg Loss: 0.100136\n  Current LR: 5.64e-05\n[EarlyStopping] Loss improved (0.100151 → 0.100136). Saving model...\n[Training] Epoch 038 | Batch 000/204 | Current Loss: 0.100186\n[Training] Epoch 038 | Batch 010/204 | Current Loss: 0.100048\n[Training] Epoch 038 | Batch 020/204 | Current Loss: 0.100273\n[Training] Epoch 038 | Batch 030/204 | Current Loss: 0.100169\n[Training] Epoch 038 | Batch 040/204 | Current Loss: 0.100075\n[Training] Epoch 038 | Batch 050/204 | Current Loss: 0.100186\n[Training] Epoch 038 | Batch 060/204 | Current Loss: 0.100081\n[Training] Epoch 038 | Batch 070/204 | Current Loss: 0.100193\n[Training] Epoch 038 | Batch 080/204 | Current Loss: 0.100104\n[Training] Epoch 038 | Batch 090/204 | Current Loss: 0.100077\n[Training] Epoch 038 | Batch 100/204 | Current Loss: 0.100169\n[Training] Epoch 038 | Batch 110/204 | Current Loss: 0.100215\n[Training] Epoch 038 | Batch 120/204 | Current Loss: 0.100115\n[Training] Epoch 038 | Batch 130/204 | Current Loss: 0.100062\n[Training] Epoch 038 | Batch 140/204 | Current Loss: 0.100193\n[Training] Epoch 038 | Batch 150/204 | Current Loss: 0.100048\n[Training] Epoch 038 | Batch 160/204 | Current Loss: 0.100109\n[Training] Epoch 038 | Batch 170/204 | Current Loss: 0.100202\n[Training] Epoch 038 | Batch 180/204 | Current Loss: 0.100093\n[Training] Epoch 038 | Batch 190/204 | Current Loss: 0.100163\n[Training] Epoch 038 | Batch 200/204 | Current Loss: 0.100109\n\n[Training] Epoch 038 Summary:\n  Avg Loss: 0.100128\n  Current LR: 2.67e-05\n[EarlyStopping] Loss improved (0.100136 → 0.100128). Saving model...\n[Training] Epoch 039 | Batch 000/204 | Current Loss: 0.100067\n[Training] Epoch 039 | Batch 010/204 | Current Loss: 0.100072\n[Training] Epoch 039 | Batch 020/204 | Current Loss: 0.100072\n[Training] Epoch 039 | Batch 030/204 | Current Loss: 0.100060\n[Training] Epoch 039 | Batch 040/204 | Current Loss: 0.100125\n[Training] Epoch 039 | Batch 050/204 | Current Loss: 0.100048\n[Training] Epoch 039 | Batch 060/204 | Current Loss: 0.100121\n[Training] Epoch 039 | Batch 070/204 | Current Loss: 0.100080\n[Training] Epoch 039 | Batch 080/204 | Current Loss: 0.100159\n[Training] Epoch 039 | Batch 090/204 | Current Loss: 0.100227\n[Training] Epoch 039 | Batch 100/204 | Current Loss: 0.100101\n[Training] Epoch 039 | Batch 110/204 | Current Loss: 0.100030\n[Training] Epoch 039 | Batch 120/204 | Current Loss: 0.100140\n[Training] Epoch 039 | Batch 130/204 | Current Loss: 0.100074\n[Training] Epoch 039 | Batch 140/204 | Current Loss: 0.100086\n[Training] Epoch 039 | Batch 150/204 | Current Loss: 0.100099\n[Training] Epoch 039 | Batch 160/204 | Current Loss: 0.100062\n[Training] Epoch 039 | Batch 170/204 | Current Loss: 0.100133\n[Training] Epoch 039 | Batch 180/204 | Current Loss: 0.100065\n[Training] Epoch 039 | Batch 190/204 | Current Loss: 0.100145\n[Training] Epoch 039 | Batch 200/204 | Current Loss: 0.100205\n\n[Training] Epoch 039 Summary:\n  Avg Loss: 0.100122\n  Current LR: 7.58e-06\n[EarlyStopping] Loss improved (0.100128 → 0.100122). Saving model...\n[Training] Epoch 040 | Batch 000/204 | Current Loss: 0.100047\n[Training] Epoch 040 | Batch 010/204 | Current Loss: 0.100049\n[Training] Epoch 040 | Batch 020/204 | Current Loss: 0.100082\n[Training] Epoch 040 | Batch 030/204 | Current Loss: 0.100057\n[Training] Epoch 040 | Batch 040/204 | Current Loss: 0.100103\n[Training] Epoch 040 | Batch 050/204 | Current Loss: 0.100108\n[Training] Epoch 040 | Batch 060/204 | Current Loss: 0.100151\n[Training] Epoch 040 | Batch 070/204 | Current Loss: 0.100196\n[Training] Epoch 040 | Batch 080/204 | Current Loss: 0.100120\n[Training] Epoch 040 | Batch 090/204 | Current Loss: 0.100125\n[Training] Epoch 040 | Batch 100/204 | Current Loss: 0.100162\n[Training] Epoch 040 | Batch 110/204 | Current Loss: 0.100162\n[Training] Epoch 040 | Batch 120/204 | Current Loss: 0.100188\n[Training] Epoch 040 | Batch 130/204 | Current Loss: 0.100198\n[Training] Epoch 040 | Batch 140/204 | Current Loss: 0.100131\n[Training] Epoch 040 | Batch 150/204 | Current Loss: 0.100009\n[Training] Epoch 040 | Batch 160/204 | Current Loss: 0.100050\n[Training] Epoch 040 | Batch 170/204 | Current Loss: 0.100131\n[Training] Epoch 040 | Batch 180/204 | Current Loss: 0.100177\n[Training] Epoch 040 | Batch 190/204 | Current Loss: 0.100089\n[Training] Epoch 040 | Batch 200/204 | Current Loss: 0.100079\n\n[Training] Epoch 040 Summary:\n  Avg Loss: 0.100116\n  Current LR: 2.70e-04\n[EarlyStopping] Loss improved (0.100122 → 0.100116). Saving model...\n[Training] Epoch 041 | Batch 000/204 | Current Loss: 0.100104\n[Training] Epoch 041 | Batch 010/204 | Current Loss: 0.100131\n[Training] Epoch 041 | Batch 020/204 | Current Loss: 0.100150\n[Training] Epoch 041 | Batch 030/204 | Current Loss: 0.100145\n[Training] Epoch 041 | Batch 040/204 | Current Loss: 0.100186\n[Training] Epoch 041 | Batch 050/204 | Current Loss: 0.100209\n[Training] Epoch 041 | Batch 060/204 | Current Loss: 0.100239\n[Training] Epoch 041 | Batch 070/204 | Current Loss: 0.100155\n[Training] Epoch 041 | Batch 080/204 | Current Loss: 0.100089\n[Training] Epoch 041 | Batch 090/204 | Current Loss: 0.100255\n[Training] Epoch 041 | Batch 100/204 | Current Loss: 0.100219\n[Training] Epoch 041 | Batch 110/204 | Current Loss: 0.100202\n[Training] Epoch 041 | Batch 120/204 | Current Loss: 0.100233\n[Training] Epoch 041 | Batch 130/204 | Current Loss: 0.100100\n[Training] Epoch 041 | Batch 140/204 | Current Loss: 0.100144\n[Training] Epoch 041 | Batch 150/204 | Current Loss: 0.100075\n[Training] Epoch 041 | Batch 160/204 | Current Loss: 0.100073\n[Training] Epoch 041 | Batch 170/204 | Current Loss: 0.100194\n[Training] Epoch 041 | Batch 180/204 | Current Loss: 0.100239\n[Training] Epoch 041 | Batch 190/204 | Current Loss: 0.100162\n[Training] Epoch 041 | Batch 200/204 | Current Loss: 0.100186\n\n[Training] Epoch 041 Summary:\n  Avg Loss: 0.100175\n  Current LR: 2.63e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 042 | Batch 000/204 | Current Loss: 0.100185\n[Training] Epoch 042 | Batch 010/204 | Current Loss: 0.100209\n[Training] Epoch 042 | Batch 020/204 | Current Loss: 0.100237\n[Training] Epoch 042 | Batch 030/204 | Current Loss: 0.100230\n[Training] Epoch 042 | Batch 040/204 | Current Loss: 0.100156\n[Training] Epoch 042 | Batch 050/204 | Current Loss: 0.100116\n[Training] Epoch 042 | Batch 060/204 | Current Loss: 0.100106\n[Training] Epoch 042 | Batch 070/204 | Current Loss: 0.100213\n[Training] Epoch 042 | Batch 080/204 | Current Loss: 0.100322\n[Training] Epoch 042 | Batch 090/204 | Current Loss: 0.100158\n[Training] Epoch 042 | Batch 100/204 | Current Loss: 0.100204\n[Training] Epoch 042 | Batch 110/204 | Current Loss: 0.100250\n[Training] Epoch 042 | Batch 120/204 | Current Loss: 0.100210\n[Training] Epoch 042 | Batch 130/204 | Current Loss: 0.100146\n[Training] Epoch 042 | Batch 140/204 | Current Loss: 0.100082\n[Training] Epoch 042 | Batch 150/204 | Current Loss: 0.100178\n[Training] Epoch 042 | Batch 160/204 | Current Loss: 0.100259\n[Training] Epoch 042 | Batch 170/204 | Current Loss: 0.100127\n[Training] Epoch 042 | Batch 180/204 | Current Loss: 0.100074\n[Training] Epoch 042 | Batch 190/204 | Current Loss: 0.100087\n[Training] Epoch 042 | Batch 200/204 | Current Loss: 0.100137\n\n[Training] Epoch 042 Summary:\n  Avg Loss: 0.100162\n  Current LR: 2.44e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 043 | Batch 000/204 | Current Loss: 0.100140\n[Training] Epoch 043 | Batch 010/204 | Current Loss: 0.100153\n[Training] Epoch 043 | Batch 020/204 | Current Loss: 0.100114\n[Training] Epoch 043 | Batch 030/204 | Current Loss: 0.100311\n[Training] Epoch 043 | Batch 040/204 | Current Loss: 0.100142\n[Training] Epoch 043 | Batch 050/204 | Current Loss: 0.100174\n[Training] Epoch 043 | Batch 060/204 | Current Loss: 0.100186\n[Training] Epoch 043 | Batch 070/204 | Current Loss: 0.100186\n[Training] Epoch 043 | Batch 080/204 | Current Loss: 0.100105\n[Training] Epoch 043 | Batch 090/204 | Current Loss: 0.100083\n[Training] Epoch 043 | Batch 100/204 | Current Loss: 0.100104\n[Training] Epoch 043 | Batch 110/204 | Current Loss: 0.100184\n[Training] Epoch 043 | Batch 120/204 | Current Loss: 0.100176\n[Training] Epoch 043 | Batch 130/204 | Current Loss: 0.100119\n[Training] Epoch 043 | Batch 140/204 | Current Loss: 0.100145\n[Training] Epoch 043 | Batch 150/204 | Current Loss: 0.100169\n[Training] Epoch 043 | Batch 160/204 | Current Loss: 0.100198\n[Training] Epoch 043 | Batch 170/204 | Current Loss: 0.100118\n[Training] Epoch 043 | Batch 180/204 | Current Loss: 0.100282\n[Training] Epoch 043 | Batch 190/204 | Current Loss: 0.100112\n[Training] Epoch 043 | Batch 200/204 | Current Loss: 0.100093\n\n[Training] Epoch 043 Summary:\n  Avg Loss: 0.100162\n  Current LR: 2.15e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 044 | Batch 000/204 | Current Loss: 0.100120\n[Training] Epoch 044 | Batch 010/204 | Current Loss: 0.100144\n[Training] Epoch 044 | Batch 020/204 | Current Loss: 0.100120\n[Training] Epoch 044 | Batch 030/204 | Current Loss: 0.100196\n[Training] Epoch 044 | Batch 040/204 | Current Loss: 0.100086\n[Training] Epoch 044 | Batch 050/204 | Current Loss: 0.100057\n[Training] Epoch 044 | Batch 060/204 | Current Loss: 0.100119\n[Training] Epoch 044 | Batch 070/204 | Current Loss: 0.100036\n[Training] Epoch 044 | Batch 080/204 | Current Loss: 0.100127\n[Training] Epoch 044 | Batch 090/204 | Current Loss: 0.100107\n[Training] Epoch 044 | Batch 100/204 | Current Loss: 0.100120\n[Training] Epoch 044 | Batch 110/204 | Current Loss: 0.100133\n[Training] Epoch 044 | Batch 120/204 | Current Loss: 0.100076\n[Training] Epoch 044 | Batch 130/204 | Current Loss: 0.100192\n[Training] Epoch 044 | Batch 140/204 | Current Loss: 0.100114\n[Training] Epoch 044 | Batch 150/204 | Current Loss: 0.100156\n[Training] Epoch 044 | Batch 160/204 | Current Loss: 0.100095\n[Training] Epoch 044 | Batch 170/204 | Current Loss: 0.100065\n[Training] Epoch 044 | Batch 180/204 | Current Loss: 0.100172\n[Training] Epoch 044 | Batch 190/204 | Current Loss: 0.100088\n[Training] Epoch 044 | Batch 200/204 | Current Loss: 0.100101\n\n[Training] Epoch 044 Summary:\n  Avg Loss: 0.100128\n  Current LR: 1.77e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 045 | Batch 000/204 | Current Loss: 0.100117\n[Training] Epoch 045 | Batch 010/204 | Current Loss: 0.100047\n[Training] Epoch 045 | Batch 020/204 | Current Loss: 0.100095\n[Training] Epoch 045 | Batch 030/204 | Current Loss: 0.100090\n[Training] Epoch 045 | Batch 040/204 | Current Loss: 0.100009\n[Training] Epoch 045 | Batch 050/204 | Current Loss: 0.100082\n[Training] Epoch 045 | Batch 060/204 | Current Loss: 0.100172\n[Training] Epoch 045 | Batch 070/204 | Current Loss: 0.100089\n[Training] Epoch 045 | Batch 080/204 | Current Loss: 0.100132\n[Training] Epoch 045 | Batch 090/204 | Current Loss: 0.100138\n[Training] Epoch 045 | Batch 100/204 | Current Loss: 0.100064\n[Training] Epoch 045 | Batch 110/204 | Current Loss: 0.100022\n[Training] Epoch 045 | Batch 120/204 | Current Loss: 0.100172\n[Training] Epoch 045 | Batch 130/204 | Current Loss: 0.100125\n[Training] Epoch 045 | Batch 140/204 | Current Loss: 0.100105\n[Training] Epoch 045 | Batch 150/204 | Current Loss: 0.099982\n[Training] Epoch 045 | Batch 160/204 | Current Loss: 0.100167\n[Training] Epoch 045 | Batch 170/204 | Current Loss: 0.100052\n[Training] Epoch 045 | Batch 180/204 | Current Loss: 0.100064\n[Training] Epoch 045 | Batch 190/204 | Current Loss: 0.100156\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 17:18:02,081] Trial 14 finished with value: 0.10011583698146484 and parameters: {'lr': 0.0002700263092093273, 'batch_size': 64, 'bottleneck_width': 1024, 'dropout_rate': 0.32990691085641666, 'alpha': 0.4744367150124978, 'weight_decay': 1.9592008352031023e-05}. Best is trial 8 with value: 0.10009658994043574.\n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 045 | Batch 200/204 | Current Loss: 0.100155\n\n[Training] Epoch 045 Summary:\n  Avg Loss: 0.100119\n  Current LR: 1.36e-04\n[EarlyStopping] Counter: 5/5\n\n[Training] Early stopping triggered\n\n[Optuna] Trial 14 completed with best loss: 0.100116\n\n==================================================\nStarting Optuna Trial 15\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.0004161421207968201\n  batch_size: 128\n  bottleneck_width: 2048\n  dropout_rate: 0.10913518963700314\n  alpha: 0.4761145907143287\n  weight_decay: 4.534515090977433e-05\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 128, Total batches: 102\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 2048\n  - Dropout rate: 0.10913518963700314\n[Model] Architecture initialized successfully\n[Model] Total parameters: 4,495,168\n[Optimizer] Initialized with lr=4.16e-04, weight_decay=4.53e-05\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.4761145907143287\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/102 | Current Loss: 1.185613\n[Training] Epoch 001 | Batch 010/102 | Current Loss: 0.126626\n[Training] Epoch 001 | Batch 020/102 | Current Loss: 0.114629\n[Training] Epoch 001 | Batch 030/102 | Current Loss: 0.106426\n[Training] Epoch 001 | Batch 040/102 | Current Loss: 0.104884\n[Training] Epoch 001 | Batch 050/102 | Current Loss: 0.107800\n[Training] Epoch 001 | Batch 060/102 | Current Loss: 0.103919\n[Training] Epoch 001 | Batch 070/102 | Current Loss: 0.104170\n[Training] Epoch 001 | Batch 080/102 | Current Loss: 0.103382\n[Training] Epoch 001 | Batch 090/102 | Current Loss: 0.103986\n[Training] Epoch 001 | Batch 100/102 | Current Loss: 0.102644\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.125968\n  Current LR: 4.06e-04\n[EarlyStopping] Loss improved (inf → 0.125968). Saving model...\n[Training] Epoch 002 | Batch 000/102 | Current Loss: 0.102633\n[Training] Epoch 002 | Batch 010/102 | Current Loss: 0.102502\n[Training] Epoch 002 | Batch 020/102 | Current Loss: 0.102170\n[Training] Epoch 002 | Batch 030/102 | Current Loss: 0.101910\n[Training] Epoch 002 | Batch 040/102 | Current Loss: 0.101827\n[Training] Epoch 002 | Batch 050/102 | Current Loss: 0.102350\n[Training] Epoch 002 | Batch 060/102 | Current Loss: 0.102826\n[Training] Epoch 002 | Batch 070/102 | Current Loss: 0.101786\n[Training] Epoch 002 | Batch 080/102 | Current Loss: 0.101961\n[Training] Epoch 002 | Batch 090/102 | Current Loss: 0.101619\n[Training] Epoch 002 | Batch 100/102 | Current Loss: 0.101825\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.102216\n  Current LR: 3.76e-04\n[EarlyStopping] Loss improved (0.125968 → 0.102216). Saving model...\n[Training] Epoch 003 | Batch 000/102 | Current Loss: 0.101853\n[Training] Epoch 003 | Batch 010/102 | Current Loss: 0.101735\n[Training] Epoch 003 | Batch 020/102 | Current Loss: 0.101452\n[Training] Epoch 003 | Batch 030/102 | Current Loss: 0.101702\n[Training] Epoch 003 | Batch 040/102 | Current Loss: 0.101501\n[Training] Epoch 003 | Batch 050/102 | Current Loss: 0.101879\n[Training] Epoch 003 | Batch 060/102 | Current Loss: 0.101517\n[Training] Epoch 003 | Batch 070/102 | Current Loss: 0.101516\n[Training] Epoch 003 | Batch 080/102 | Current Loss: 0.101155\n[Training] Epoch 003 | Batch 090/102 | Current Loss: 0.101766\n[Training] Epoch 003 | Batch 100/102 | Current Loss: 0.101377\n\n[Training] Epoch 003 Summary:\n  Avg Loss: 0.101468\n  Current LR: 3.31e-04\n[EarlyStopping] Loss improved (0.102216 → 0.101468). Saving model...\n[Training] Epoch 004 | Batch 000/102 | Current Loss: 0.101119\n[Training] Epoch 004 | Batch 010/102 | Current Loss: 0.100939\n[Training] Epoch 004 | Batch 020/102 | Current Loss: 0.101075\n[Training] Epoch 004 | Batch 030/102 | Current Loss: 0.101132\n[Training] Epoch 004 | Batch 040/102 | Current Loss: 0.101157\n[Training] Epoch 004 | Batch 050/102 | Current Loss: 0.101117\n[Training] Epoch 004 | Batch 060/102 | Current Loss: 0.101062\n[Training] Epoch 004 | Batch 070/102 | Current Loss: 0.101642\n[Training] Epoch 004 | Batch 080/102 | Current Loss: 0.101315\n[Training] Epoch 004 | Batch 090/102 | Current Loss: 0.100981\n[Training] Epoch 004 | Batch 100/102 | Current Loss: 0.100969\n\n[Training] Epoch 004 Summary:\n  Avg Loss: 0.101149\n  Current LR: 2.73e-04\n[EarlyStopping] Loss improved (0.101468 → 0.101149). Saving model...\n[Training] Epoch 005 | Batch 000/102 | Current Loss: 0.101056\n[Training] Epoch 005 | Batch 010/102 | Current Loss: 0.100874\n[Training] Epoch 005 | Batch 020/102 | Current Loss: 0.100939\n[Training] Epoch 005 | Batch 030/102 | Current Loss: 0.101763\n[Training] Epoch 005 | Batch 040/102 | Current Loss: 0.100993\n[Training] Epoch 005 | Batch 050/102 | Current Loss: 0.100943\n[Training] Epoch 005 | Batch 060/102 | Current Loss: 0.101198\n[Training] Epoch 005 | Batch 070/102 | Current Loss: 0.100785\n[Training] Epoch 005 | Batch 080/102 | Current Loss: 0.100851\n[Training] Epoch 005 | Batch 090/102 | Current Loss: 0.100763\n[Training] Epoch 005 | Batch 100/102 | Current Loss: 0.100802\n\n[Training] Epoch 005 Summary:\n  Avg Loss: 0.100998\n  Current LR: 2.09e-04\n[EarlyStopping] Loss improved (0.101149 → 0.100998). Saving model...\n[Training] Epoch 006 | Batch 000/102 | Current Loss: 0.100847\n[Training] Epoch 006 | Batch 010/102 | Current Loss: 0.100763\n[Training] Epoch 006 | Batch 020/102 | Current Loss: 0.100827\n[Training] Epoch 006 | Batch 030/102 | Current Loss: 0.100866\n[Training] Epoch 006 | Batch 040/102 | Current Loss: 0.100853\n[Training] Epoch 006 | Batch 050/102 | Current Loss: 0.100744\n[Training] Epoch 006 | Batch 060/102 | Current Loss: 0.100993\n[Training] Epoch 006 | Batch 070/102 | Current Loss: 0.100805\n[Training] Epoch 006 | Batch 080/102 | Current Loss: 0.100720\n[Training] Epoch 006 | Batch 090/102 | Current Loss: 0.100749\n[Training] Epoch 006 | Batch 100/102 | Current Loss: 0.100888\n\n[Training] Epoch 006 Summary:\n  Avg Loss: 0.100801\n  Current LR: 1.44e-04\n[EarlyStopping] Loss improved (0.100998 → 0.100801). Saving model...\n[Training] Epoch 007 | Batch 000/102 | Current Loss: 0.100998\n[Training] Epoch 007 | Batch 010/102 | Current Loss: 0.100733\n[Training] Epoch 007 | Batch 020/102 | Current Loss: 0.100817\n[Training] Epoch 007 | Batch 030/102 | Current Loss: 0.100731\n[Training] Epoch 007 | Batch 040/102 | Current Loss: 0.100674\n[Training] Epoch 007 | Batch 050/102 | Current Loss: 0.100725\n[Training] Epoch 007 | Batch 060/102 | Current Loss: 0.100678\n[Training] Epoch 007 | Batch 070/102 | Current Loss: 0.100685\n[Training] Epoch 007 | Batch 080/102 | Current Loss: 0.100611\n[Training] Epoch 007 | Batch 090/102 | Current Loss: 0.100593\n[Training] Epoch 007 | Batch 100/102 | Current Loss: 0.100649\n\n[Training] Epoch 007 Summary:\n  Avg Loss: 0.100717\n  Current LR: 8.66e-05\n[EarlyStopping] Loss improved (0.100801 → 0.100717). Saving model...\n[Training] Epoch 008 | Batch 000/102 | Current Loss: 0.100724\n[Training] Epoch 008 | Batch 010/102 | Current Loss: 0.100635\n[Training] Epoch 008 | Batch 020/102 | Current Loss: 0.100872\n[Training] Epoch 008 | Batch 030/102 | Current Loss: 0.100642\n[Training] Epoch 008 | Batch 040/102 | Current Loss: 0.100607\n[Training] Epoch 008 | Batch 050/102 | Current Loss: 0.100708\n[Training] Epoch 008 | Batch 060/102 | Current Loss: 0.100630\n[Training] Epoch 008 | Batch 070/102 | Current Loss: 0.100698\n[Training] Epoch 008 | Batch 080/102 | Current Loss: 0.100862\n[Training] Epoch 008 | Batch 090/102 | Current Loss: 0.100715\n[Training] Epoch 008 | Batch 100/102 | Current Loss: 0.100748\n\n[Training] Epoch 008 Summary:\n  Avg Loss: 0.100677\n  Current LR: 4.06e-05\n[EarlyStopping] Loss improved (0.100717 → 0.100677). Saving model...\n[Training] Epoch 009 | Batch 000/102 | Current Loss: 0.100556\n[Training] Epoch 009 | Batch 010/102 | Current Loss: 0.100762\n[Training] Epoch 009 | Batch 020/102 | Current Loss: 0.100875\n[Training] Epoch 009 | Batch 030/102 | Current Loss: 0.100576\n[Training] Epoch 009 | Batch 040/102 | Current Loss: 0.100620\n[Training] Epoch 009 | Batch 050/102 | Current Loss: 0.100570\n[Training] Epoch 009 | Batch 060/102 | Current Loss: 0.100687\n[Training] Epoch 009 | Batch 070/102 | Current Loss: 0.100612\n[Training] Epoch 009 | Batch 080/102 | Current Loss: 0.100501\n[Training] Epoch 009 | Batch 090/102 | Current Loss: 0.100661\n[Training] Epoch 009 | Batch 100/102 | Current Loss: 0.100653\n\n[Training] Epoch 009 Summary:\n  Avg Loss: 0.100634\n  Current LR: 1.12e-05\n[EarlyStopping] Loss improved (0.100677 → 0.100634). Saving model...\n[Training] Epoch 010 | Batch 000/102 | Current Loss: 0.100700\n[Training] Epoch 010 | Batch 010/102 | Current Loss: 0.100612\n[Training] Epoch 010 | Batch 020/102 | Current Loss: 0.100593\n[Training] Epoch 010 | Batch 030/102 | Current Loss: 0.100613\n[Training] Epoch 010 | Batch 040/102 | Current Loss: 0.100659\n[Training] Epoch 010 | Batch 050/102 | Current Loss: 0.100654\n[Training] Epoch 010 | Batch 060/102 | Current Loss: 0.100660\n[Training] Epoch 010 | Batch 070/102 | Current Loss: 0.100542\n[Training] Epoch 010 | Batch 080/102 | Current Loss: 0.100610\n[Training] Epoch 010 | Batch 090/102 | Current Loss: 0.100528\n[Training] Epoch 010 | Batch 100/102 | Current Loss: 0.100600\n\n[Training] Epoch 010 Summary:\n  Avg Loss: 0.100625\n  Current LR: 4.16e-04\n[EarlyStopping] Loss improved (0.100634 → 0.100625). Saving model...\n[Training] Epoch 011 | Batch 000/102 | Current Loss: 0.100543\n[Training] Epoch 011 | Batch 010/102 | Current Loss: 0.100660\n[Training] Epoch 011 | Batch 020/102 | Current Loss: 0.100721\n[Training] Epoch 011 | Batch 030/102 | Current Loss: 0.101192\n[Training] Epoch 011 | Batch 040/102 | Current Loss: 0.100696\n[Training] Epoch 011 | Batch 050/102 | Current Loss: 0.100541\n[Training] Epoch 011 | Batch 060/102 | Current Loss: 0.101192\n[Training] Epoch 011 | Batch 070/102 | Current Loss: 0.100631\n[Training] Epoch 011 | Batch 080/102 | Current Loss: 0.100638\n[Training] Epoch 011 | Batch 090/102 | Current Loss: 0.100678\n[Training] Epoch 011 | Batch 100/102 | Current Loss: 0.100611\n\n[Training] Epoch 011 Summary:\n  Avg Loss: 0.100714\n  Current LR: 4.06e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 012 | Batch 000/102 | Current Loss: 0.100737\n[Training] Epoch 012 | Batch 010/102 | Current Loss: 0.100481\n[Training] Epoch 012 | Batch 020/102 | Current Loss: 0.100576\n[Training] Epoch 012 | Batch 030/102 | Current Loss: 0.100550\n[Training] Epoch 012 | Batch 040/102 | Current Loss: 0.100758\n[Training] Epoch 012 | Batch 050/102 | Current Loss: 0.100456\n[Training] Epoch 012 | Batch 060/102 | Current Loss: 0.100487\n[Training] Epoch 012 | Batch 070/102 | Current Loss: 0.100461\n[Training] Epoch 012 | Batch 080/102 | Current Loss: 0.100582\n[Training] Epoch 012 | Batch 090/102 | Current Loss: 0.100490\n[Training] Epoch 012 | Batch 100/102 | Current Loss: 0.100486\n\n[Training] Epoch 012 Summary:\n  Avg Loss: 0.100563\n  Current LR: 3.76e-04\n[EarlyStopping] Loss improved (0.100625 → 0.100563). Saving model...\n[Training] Epoch 013 | Batch 000/102 | Current Loss: 0.100528\n[Training] Epoch 013 | Batch 010/102 | Current Loss: 0.100519\n[Training] Epoch 013 | Batch 020/102 | Current Loss: 0.100465\n[Training] Epoch 013 | Batch 030/102 | Current Loss: 0.100510\n[Training] Epoch 013 | Batch 040/102 | Current Loss: 0.100365\n[Training] Epoch 013 | Batch 050/102 | Current Loss: 0.100446\n[Training] Epoch 013 | Batch 060/102 | Current Loss: 0.100452\n[Training] Epoch 013 | Batch 070/102 | Current Loss: 0.100382\n[Training] Epoch 013 | Batch 080/102 | Current Loss: 0.100465\n[Training] Epoch 013 | Batch 090/102 | Current Loss: 0.100433\n[Training] Epoch 013 | Batch 100/102 | Current Loss: 0.100390\n\n[Training] Epoch 013 Summary:\n  Avg Loss: 0.100449\n  Current LR: 3.31e-04\n[EarlyStopping] Loss improved (0.100563 → 0.100449). Saving model...\n[Training] Epoch 014 | Batch 000/102 | Current Loss: 0.100491\n[Training] Epoch 014 | Batch 010/102 | Current Loss: 0.100312\n[Training] Epoch 014 | Batch 020/102 | Current Loss: 0.100339\n[Training] Epoch 014 | Batch 030/102 | Current Loss: 0.100353\n[Training] Epoch 014 | Batch 040/102 | Current Loss: 0.100391\n[Training] Epoch 014 | Batch 050/102 | Current Loss: 0.100434\n[Training] Epoch 014 | Batch 060/102 | Current Loss: 0.100453\n[Training] Epoch 014 | Batch 070/102 | Current Loss: 0.100361\n[Training] Epoch 014 | Batch 080/102 | Current Loss: 0.100428\n[Training] Epoch 014 | Batch 090/102 | Current Loss: 0.100478\n[Training] Epoch 014 | Batch 100/102 | Current Loss: 0.100306\n\n[Training] Epoch 014 Summary:\n  Avg Loss: 0.100399\n  Current LR: 2.73e-04\n[EarlyStopping] Loss improved (0.100449 → 0.100399). Saving model...\n[Training] Epoch 015 | Batch 000/102 | Current Loss: 0.100464\n[Training] Epoch 015 | Batch 010/102 | Current Loss: 0.100386\n[Training] Epoch 015 | Batch 020/102 | Current Loss: 0.100440\n[Training] Epoch 015 | Batch 030/102 | Current Loss: 0.100349\n[Training] Epoch 015 | Batch 040/102 | Current Loss: 0.100389\n[Training] Epoch 015 | Batch 050/102 | Current Loss: 0.100314\n[Training] Epoch 015 | Batch 060/102 | Current Loss: 0.100299\n[Training] Epoch 015 | Batch 070/102 | Current Loss: 0.100283\n[Training] Epoch 015 | Batch 080/102 | Current Loss: 0.100331\n[Training] Epoch 015 | Batch 090/102 | Current Loss: 0.100425\n[Training] Epoch 015 | Batch 100/102 | Current Loss: 0.100282\n\n[Training] Epoch 015 Summary:\n  Avg Loss: 0.100365\n  Current LR: 2.09e-04\n[EarlyStopping] Loss improved (0.100399 → 0.100365). Saving model...\n[Training] Epoch 016 | Batch 000/102 | Current Loss: 0.100342\n[Training] Epoch 016 | Batch 010/102 | Current Loss: 0.100298\n[Training] Epoch 016 | Batch 020/102 | Current Loss: 0.100347\n[Training] Epoch 016 | Batch 030/102 | Current Loss: 0.100267\n[Training] Epoch 016 | Batch 040/102 | Current Loss: 0.100346\n[Training] Epoch 016 | Batch 050/102 | Current Loss: 0.100292\n[Training] Epoch 016 | Batch 060/102 | Current Loss: 0.100344\n[Training] Epoch 016 | Batch 070/102 | Current Loss: 0.100302\n[Training] Epoch 016 | Batch 080/102 | Current Loss: 0.100327\n[Training] Epoch 016 | Batch 090/102 | Current Loss: 0.100319\n[Training] Epoch 016 | Batch 100/102 | Current Loss: 0.100348\n\n[Training] Epoch 016 Summary:\n  Avg Loss: 0.100329\n  Current LR: 1.44e-04\n[EarlyStopping] Loss improved (0.100365 → 0.100329). Saving model...\n[Training] Epoch 017 | Batch 000/102 | Current Loss: 0.100398\n[Training] Epoch 017 | Batch 010/102 | Current Loss: 0.100314\n[Training] Epoch 017 | Batch 020/102 | Current Loss: 0.100316\n[Training] Epoch 017 | Batch 030/102 | Current Loss: 0.100314\n[Training] Epoch 017 | Batch 040/102 | Current Loss: 0.100372\n[Training] Epoch 017 | Batch 050/102 | Current Loss: 0.100275\n[Training] Epoch 017 | Batch 060/102 | Current Loss: 0.100236\n[Training] Epoch 017 | Batch 070/102 | Current Loss: 0.100302\n[Training] Epoch 017 | Batch 080/102 | Current Loss: 0.100333\n[Training] Epoch 017 | Batch 090/102 | Current Loss: 0.100335\n[Training] Epoch 017 | Batch 100/102 | Current Loss: 0.100323\n\n[Training] Epoch 017 Summary:\n  Avg Loss: 0.100304\n  Current LR: 8.66e-05\n[EarlyStopping] Loss improved (0.100329 → 0.100304). Saving model...\n[Training] Epoch 018 | Batch 000/102 | Current Loss: 0.100273\n[Training] Epoch 018 | Batch 010/102 | Current Loss: 0.100260\n[Training] Epoch 018 | Batch 020/102 | Current Loss: 0.100318\n[Training] Epoch 018 | Batch 030/102 | Current Loss: 0.100342\n[Training] Epoch 018 | Batch 040/102 | Current Loss: 0.100283\n[Training] Epoch 018 | Batch 050/102 | Current Loss: 0.100319\n[Training] Epoch 018 | Batch 060/102 | Current Loss: 0.100207\n[Training] Epoch 018 | Batch 070/102 | Current Loss: 0.100272\n[Training] Epoch 018 | Batch 080/102 | Current Loss: 0.100272\n[Training] Epoch 018 | Batch 090/102 | Current Loss: 0.100260\n[Training] Epoch 018 | Batch 100/102 | Current Loss: 0.100366\n\n[Training] Epoch 018 Summary:\n  Avg Loss: 0.100293\n  Current LR: 4.06e-05\n[EarlyStopping] Loss improved (0.100304 → 0.100293). Saving model...\n[Training] Epoch 019 | Batch 000/102 | Current Loss: 0.100353\n[Training] Epoch 019 | Batch 010/102 | Current Loss: 0.100226\n[Training] Epoch 019 | Batch 020/102 | Current Loss: 0.100231\n[Training] Epoch 019 | Batch 030/102 | Current Loss: 0.100285\n[Training] Epoch 019 | Batch 040/102 | Current Loss: 0.100289\n[Training] Epoch 019 | Batch 050/102 | Current Loss: 0.100221\n[Training] Epoch 019 | Batch 060/102 | Current Loss: 0.100366\n[Training] Epoch 019 | Batch 070/102 | Current Loss: 0.100438\n[Training] Epoch 019 | Batch 080/102 | Current Loss: 0.100201\n[Training] Epoch 019 | Batch 090/102 | Current Loss: 0.100269\n[Training] Epoch 019 | Batch 100/102 | Current Loss: 0.100285\n\n[Training] Epoch 019 Summary:\n  Avg Loss: 0.100282\n  Current LR: 1.12e-05\n[EarlyStopping] Loss improved (0.100293 → 0.100282). Saving model...\n[Training] Epoch 020 | Batch 000/102 | Current Loss: 0.100234\n[Training] Epoch 020 | Batch 010/102 | Current Loss: 0.100293\n[Training] Epoch 020 | Batch 020/102 | Current Loss: 0.100284\n[Training] Epoch 020 | Batch 030/102 | Current Loss: 0.100245\n[Training] Epoch 020 | Batch 040/102 | Current Loss: 0.100316\n[Training] Epoch 020 | Batch 050/102 | Current Loss: 0.100211\n[Training] Epoch 020 | Batch 060/102 | Current Loss: 0.100282\n[Training] Epoch 020 | Batch 070/102 | Current Loss: 0.100190\n[Training] Epoch 020 | Batch 080/102 | Current Loss: 0.100330\n[Training] Epoch 020 | Batch 090/102 | Current Loss: 0.100252\n[Training] Epoch 020 | Batch 100/102 | Current Loss: 0.100279\n\n[Training] Epoch 020 Summary:\n  Avg Loss: 0.100278\n  Current LR: 4.16e-04\n[EarlyStopping] Loss improved (0.100282 → 0.100278). Saving model...\n[Training] Epoch 021 | Batch 000/102 | Current Loss: 0.100256\n[Training] Epoch 021 | Batch 010/102 | Current Loss: 0.100335\n[Training] Epoch 021 | Batch 020/102 | Current Loss: 0.100362\n[Training] Epoch 021 | Batch 030/102 | Current Loss: 0.100323\n[Training] Epoch 021 | Batch 040/102 | Current Loss: 0.100360\n[Training] Epoch 021 | Batch 050/102 | Current Loss: 0.100273\n[Training] Epoch 021 | Batch 060/102 | Current Loss: 0.100255\n[Training] Epoch 021 | Batch 070/102 | Current Loss: 0.100286\n[Training] Epoch 021 | Batch 080/102 | Current Loss: 0.100294\n[Training] Epoch 021 | Batch 090/102 | Current Loss: 0.100341\n[Training] Epoch 021 | Batch 100/102 | Current Loss: 0.100291\n\n[Training] Epoch 021 Summary:\n  Avg Loss: 0.100316\n  Current LR: 4.06e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 022 | Batch 000/102 | Current Loss: 0.100334\n[Training] Epoch 022 | Batch 010/102 | Current Loss: 0.100359\n[Training] Epoch 022 | Batch 020/102 | Current Loss: 0.100257\n[Training] Epoch 022 | Batch 030/102 | Current Loss: 0.100266\n[Training] Epoch 022 | Batch 040/102 | Current Loss: 0.100351\n[Training] Epoch 022 | Batch 050/102 | Current Loss: 0.100309\n[Training] Epoch 022 | Batch 060/102 | Current Loss: 0.100314\n[Training] Epoch 022 | Batch 070/102 | Current Loss: 0.100271\n[Training] Epoch 022 | Batch 080/102 | Current Loss: 0.100308\n[Training] Epoch 022 | Batch 090/102 | Current Loss: 0.100256\n[Training] Epoch 022 | Batch 100/102 | Current Loss: 0.100389\n\n[Training] Epoch 022 Summary:\n  Avg Loss: 0.100305\n  Current LR: 3.76e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 023 | Batch 000/102 | Current Loss: 0.100282\n[Training] Epoch 023 | Batch 010/102 | Current Loss: 0.100289\n[Training] Epoch 023 | Batch 020/102 | Current Loss: 0.100298\n[Training] Epoch 023 | Batch 030/102 | Current Loss: 0.100218\n[Training] Epoch 023 | Batch 040/102 | Current Loss: 0.100286\n[Training] Epoch 023 | Batch 050/102 | Current Loss: 0.100231\n[Training] Epoch 023 | Batch 060/102 | Current Loss: 0.100248\n[Training] Epoch 023 | Batch 070/102 | Current Loss: 0.100165\n[Training] Epoch 023 | Batch 080/102 | Current Loss: 0.100203\n[Training] Epoch 023 | Batch 090/102 | Current Loss: 0.100284\n[Training] Epoch 023 | Batch 100/102 | Current Loss: 0.100300\n\n[Training] Epoch 023 Summary:\n  Avg Loss: 0.100270\n  Current LR: 3.31e-04\n[EarlyStopping] Loss improved (0.100278 → 0.100270). Saving model...\n[Training] Epoch 024 | Batch 000/102 | Current Loss: 0.100287\n[Training] Epoch 024 | Batch 010/102 | Current Loss: 0.100260\n[Training] Epoch 024 | Batch 020/102 | Current Loss: 0.100235\n[Training] Epoch 024 | Batch 030/102 | Current Loss: 0.100190\n[Training] Epoch 024 | Batch 040/102 | Current Loss: 0.100169\n[Training] Epoch 024 | Batch 050/102 | Current Loss: 0.100219\n[Training] Epoch 024 | Batch 060/102 | Current Loss: 0.100205\n[Training] Epoch 024 | Batch 070/102 | Current Loss: 0.100197\n[Training] Epoch 024 | Batch 080/102 | Current Loss: 0.100230\n[Training] Epoch 024 | Batch 090/102 | Current Loss: 0.100245\n[Training] Epoch 024 | Batch 100/102 | Current Loss: 0.100229\n\n[Training] Epoch 024 Summary:\n  Avg Loss: 0.100231\n  Current LR: 2.73e-04\n[EarlyStopping] Loss improved (0.100270 → 0.100231). Saving model...\n[Training] Epoch 025 | Batch 000/102 | Current Loss: 0.100222\n[Training] Epoch 025 | Batch 010/102 | Current Loss: 0.100204\n[Training] Epoch 025 | Batch 020/102 | Current Loss: 0.100192\n[Training] Epoch 025 | Batch 030/102 | Current Loss: 0.100212\n[Training] Epoch 025 | Batch 040/102 | Current Loss: 0.100178\n[Training] Epoch 025 | Batch 050/102 | Current Loss: 0.100233\n[Training] Epoch 025 | Batch 060/102 | Current Loss: 0.100210\n[Training] Epoch 025 | Batch 070/102 | Current Loss: 0.100314\n[Training] Epoch 025 | Batch 080/102 | Current Loss: 0.100198\n[Training] Epoch 025 | Batch 090/102 | Current Loss: 0.100136\n[Training] Epoch 025 | Batch 100/102 | Current Loss: 0.100196\n\n[Training] Epoch 025 Summary:\n  Avg Loss: 0.100215\n  Current LR: 2.09e-04\n[EarlyStopping] Loss improved (0.100231 → 0.100215). Saving model...\n[Training] Epoch 026 | Batch 000/102 | Current Loss: 0.100192\n[Training] Epoch 026 | Batch 010/102 | Current Loss: 0.100159\n[Training] Epoch 026 | Batch 020/102 | Current Loss: 0.100192\n[Training] Epoch 026 | Batch 030/102 | Current Loss: 0.100226\n[Training] Epoch 026 | Batch 040/102 | Current Loss: 0.100196\n[Training] Epoch 026 | Batch 050/102 | Current Loss: 0.100234\n[Training] Epoch 026 | Batch 060/102 | Current Loss: 0.100197\n[Training] Epoch 026 | Batch 070/102 | Current Loss: 0.100200\n[Training] Epoch 026 | Batch 080/102 | Current Loss: 0.100169\n[Training] Epoch 026 | Batch 090/102 | Current Loss: 0.100158\n[Training] Epoch 026 | Batch 100/102 | Current Loss: 0.100262\n\n[Training] Epoch 026 Summary:\n  Avg Loss: 0.100201\n  Current LR: 1.44e-04\n[EarlyStopping] Loss improved (0.100215 → 0.100201). Saving model...\n[Training] Epoch 027 | Batch 000/102 | Current Loss: 0.100213\n[Training] Epoch 027 | Batch 010/102 | Current Loss: 0.100105\n[Training] Epoch 027 | Batch 020/102 | Current Loss: 0.100177\n[Training] Epoch 027 | Batch 030/102 | Current Loss: 0.100286\n[Training] Epoch 027 | Batch 040/102 | Current Loss: 0.100209\n[Training] Epoch 027 | Batch 050/102 | Current Loss: 0.100217\n[Training] Epoch 027 | Batch 060/102 | Current Loss: 0.100175\n[Training] Epoch 027 | Batch 070/102 | Current Loss: 0.100126\n[Training] Epoch 027 | Batch 080/102 | Current Loss: 0.100126\n[Training] Epoch 027 | Batch 090/102 | Current Loss: 0.100156\n[Training] Epoch 027 | Batch 100/102 | Current Loss: 0.100197\n\n[Training] Epoch 027 Summary:\n  Avg Loss: 0.100189\n  Current LR: 8.66e-05\n[EarlyStopping] Loss improved (0.100201 → 0.100189). Saving model...\n[Training] Epoch 028 | Batch 000/102 | Current Loss: 0.100210\n[Training] Epoch 028 | Batch 010/102 | Current Loss: 0.100166\n[Training] Epoch 028 | Batch 020/102 | Current Loss: 0.100148\n[Training] Epoch 028 | Batch 030/102 | Current Loss: 0.100186\n[Training] Epoch 028 | Batch 040/102 | Current Loss: 0.100214\n[Training] Epoch 028 | Batch 050/102 | Current Loss: 0.100187\n[Training] Epoch 028 | Batch 060/102 | Current Loss: 0.100143\n[Training] Epoch 028 | Batch 070/102 | Current Loss: 0.100164\n[Training] Epoch 028 | Batch 080/102 | Current Loss: 0.100136\n[Training] Epoch 028 | Batch 090/102 | Current Loss: 0.100186\n[Training] Epoch 028 | Batch 100/102 | Current Loss: 0.100237\n\n[Training] Epoch 028 Summary:\n  Avg Loss: 0.100176\n  Current LR: 4.06e-05\n[EarlyStopping] Loss improved (0.100189 → 0.100176). Saving model...\n[Training] Epoch 029 | Batch 000/102 | Current Loss: 0.100247\n[Training] Epoch 029 | Batch 010/102 | Current Loss: 0.100160\n[Training] Epoch 029 | Batch 020/102 | Current Loss: 0.100232\n[Training] Epoch 029 | Batch 030/102 | Current Loss: 0.100161\n[Training] Epoch 029 | Batch 040/102 | Current Loss: 0.100238\n[Training] Epoch 029 | Batch 050/102 | Current Loss: 0.100175\n[Training] Epoch 029 | Batch 060/102 | Current Loss: 0.100221\n[Training] Epoch 029 | Batch 070/102 | Current Loss: 0.100120\n[Training] Epoch 029 | Batch 080/102 | Current Loss: 0.100195\n[Training] Epoch 029 | Batch 090/102 | Current Loss: 0.100166\n[Training] Epoch 029 | Batch 100/102 | Current Loss: 0.100261\n\n[Training] Epoch 029 Summary:\n  Avg Loss: 0.100177\n  Current LR: 1.12e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 030 | Batch 000/102 | Current Loss: 0.100141\n[Training] Epoch 030 | Batch 010/102 | Current Loss: 0.100116\n[Training] Epoch 030 | Batch 020/102 | Current Loss: 0.100192\n[Training] Epoch 030 | Batch 030/102 | Current Loss: 0.100131\n[Training] Epoch 030 | Batch 040/102 | Current Loss: 0.100247\n[Training] Epoch 030 | Batch 050/102 | Current Loss: 0.100193\n[Training] Epoch 030 | Batch 060/102 | Current Loss: 0.100243\n[Training] Epoch 030 | Batch 070/102 | Current Loss: 0.100176\n[Training] Epoch 030 | Batch 080/102 | Current Loss: 0.100226\n[Training] Epoch 030 | Batch 090/102 | Current Loss: 0.100198\n[Training] Epoch 030 | Batch 100/102 | Current Loss: 0.100206\n\n[Training] Epoch 030 Summary:\n  Avg Loss: 0.100172\n  Current LR: 4.16e-04\n[EarlyStopping] Loss improved (0.100176 → 0.100172). Saving model...\n[Training] Epoch 031 | Batch 000/102 | Current Loss: 0.100235\n[Training] Epoch 031 | Batch 010/102 | Current Loss: 0.100153\n[Training] Epoch 031 | Batch 020/102 | Current Loss: 0.100260\n[Training] Epoch 031 | Batch 030/102 | Current Loss: 0.100269\n[Training] Epoch 031 | Batch 040/102 | Current Loss: 0.100211\n[Training] Epoch 031 | Batch 050/102 | Current Loss: 0.100118\n[Training] Epoch 031 | Batch 060/102 | Current Loss: 0.100303\n[Training] Epoch 031 | Batch 070/102 | Current Loss: 0.100222\n[Training] Epoch 031 | Batch 080/102 | Current Loss: 0.100275\n[Training] Epoch 031 | Batch 090/102 | Current Loss: 0.100302\n[Training] Epoch 031 | Batch 100/102 | Current Loss: 0.100193\n\n[Training] Epoch 031 Summary:\n  Avg Loss: 0.100221\n  Current LR: 4.06e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 032 | Batch 000/102 | Current Loss: 0.100278\n[Training] Epoch 032 | Batch 010/102 | Current Loss: 0.100208\n[Training] Epoch 032 | Batch 020/102 | Current Loss: 0.100280\n[Training] Epoch 032 | Batch 030/102 | Current Loss: 0.100241\n[Training] Epoch 032 | Batch 040/102 | Current Loss: 0.100234\n[Training] Epoch 032 | Batch 050/102 | Current Loss: 0.100174\n[Training] Epoch 032 | Batch 060/102 | Current Loss: 0.100272\n[Training] Epoch 032 | Batch 070/102 | Current Loss: 0.100150\n[Training] Epoch 032 | Batch 080/102 | Current Loss: 0.100133\n[Training] Epoch 032 | Batch 090/102 | Current Loss: 0.100201\n[Training] Epoch 032 | Batch 100/102 | Current Loss: 0.100187\n\n[Training] Epoch 032 Summary:\n  Avg Loss: 0.100215\n  Current LR: 3.76e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 033 | Batch 000/102 | Current Loss: 0.100290\n[Training] Epoch 033 | Batch 010/102 | Current Loss: 0.100211\n[Training] Epoch 033 | Batch 020/102 | Current Loss: 0.100160\n[Training] Epoch 033 | Batch 030/102 | Current Loss: 0.100216\n[Training] Epoch 033 | Batch 040/102 | Current Loss: 0.100170\n[Training] Epoch 033 | Batch 050/102 | Current Loss: 0.100176\n[Training] Epoch 033 | Batch 060/102 | Current Loss: 0.100203\n[Training] Epoch 033 | Batch 070/102 | Current Loss: 0.100251\n[Training] Epoch 033 | Batch 080/102 | Current Loss: 0.100147\n[Training] Epoch 033 | Batch 090/102 | Current Loss: 0.100130\n[Training] Epoch 033 | Batch 100/102 | Current Loss: 0.100179\n\n[Training] Epoch 033 Summary:\n  Avg Loss: 0.100186\n  Current LR: 3.31e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 034 | Batch 000/102 | Current Loss: 0.100103\n[Training] Epoch 034 | Batch 010/102 | Current Loss: 0.100213\n[Training] Epoch 034 | Batch 020/102 | Current Loss: 0.100114\n[Training] Epoch 034 | Batch 030/102 | Current Loss: 0.100155\n[Training] Epoch 034 | Batch 040/102 | Current Loss: 0.100177\n[Training] Epoch 034 | Batch 050/102 | Current Loss: 0.100181\n[Training] Epoch 034 | Batch 060/102 | Current Loss: 0.100235\n[Training] Epoch 034 | Batch 070/102 | Current Loss: 0.100168\n[Training] Epoch 034 | Batch 080/102 | Current Loss: 0.100077\n[Training] Epoch 034 | Batch 090/102 | Current Loss: 0.100053\n[Training] Epoch 034 | Batch 100/102 | Current Loss: 0.100197\n\n[Training] Epoch 034 Summary:\n  Avg Loss: 0.100175\n  Current LR: 2.73e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 035 | Batch 000/102 | Current Loss: 0.100227\n[Training] Epoch 035 | Batch 010/102 | Current Loss: 0.100170\n[Training] Epoch 035 | Batch 020/102 | Current Loss: 0.100182\n[Training] Epoch 035 | Batch 030/102 | Current Loss: 0.100149\n[Training] Epoch 035 | Batch 040/102 | Current Loss: 0.100148\n[Training] Epoch 035 | Batch 050/102 | Current Loss: 0.100151\n[Training] Epoch 035 | Batch 060/102 | Current Loss: 0.100095\n[Training] Epoch 035 | Batch 070/102 | Current Loss: 0.100134\n[Training] Epoch 035 | Batch 080/102 | Current Loss: 0.100177\n[Training] Epoch 035 | Batch 090/102 | Current Loss: 0.100162\n[Training] Epoch 035 | Batch 100/102 | Current Loss: 0.100157\n\n[Training] Epoch 035 Summary:\n  Avg Loss: 0.100165\n  Current LR: 2.09e-04\n[EarlyStopping] Loss improved (0.100172 → 0.100165). Saving model...\n[Training] Epoch 036 | Batch 000/102 | Current Loss: 0.100201\n[Training] Epoch 036 | Batch 010/102 | Current Loss: 0.100197\n[Training] Epoch 036 | Batch 020/102 | Current Loss: 0.100106\n[Training] Epoch 036 | Batch 030/102 | Current Loss: 0.100172\n[Training] Epoch 036 | Batch 040/102 | Current Loss: 0.100113\n[Training] Epoch 036 | Batch 050/102 | Current Loss: 0.100119\n[Training] Epoch 036 | Batch 060/102 | Current Loss: 0.100173\n[Training] Epoch 036 | Batch 070/102 | Current Loss: 0.100129\n[Training] Epoch 036 | Batch 080/102 | Current Loss: 0.100154\n[Training] Epoch 036 | Batch 090/102 | Current Loss: 0.100154\n[Training] Epoch 036 | Batch 100/102 | Current Loss: 0.100139\n\n[Training] Epoch 036 Summary:\n  Avg Loss: 0.100146\n  Current LR: 1.44e-04\n[EarlyStopping] Loss improved (0.100165 → 0.100146). Saving model...\n[Training] Epoch 037 | Batch 000/102 | Current Loss: 0.100161\n[Training] Epoch 037 | Batch 010/102 | Current Loss: 0.100189\n[Training] Epoch 037 | Batch 020/102 | Current Loss: 0.100119\n[Training] Epoch 037 | Batch 030/102 | Current Loss: 0.100086\n[Training] Epoch 037 | Batch 040/102 | Current Loss: 0.100118\n[Training] Epoch 037 | Batch 050/102 | Current Loss: 0.100074\n[Training] Epoch 037 | Batch 060/102 | Current Loss: 0.100224\n[Training] Epoch 037 | Batch 070/102 | Current Loss: 0.100168\n[Training] Epoch 037 | Batch 080/102 | Current Loss: 0.100183\n[Training] Epoch 037 | Batch 090/102 | Current Loss: 0.100133\n[Training] Epoch 037 | Batch 100/102 | Current Loss: 0.100138\n\n[Training] Epoch 037 Summary:\n  Avg Loss: 0.100131\n  Current LR: 8.66e-05\n[EarlyStopping] Loss improved (0.100146 → 0.100131). Saving model...\n[Training] Epoch 038 | Batch 000/102 | Current Loss: 0.100068\n[Training] Epoch 038 | Batch 010/102 | Current Loss: 0.100078\n[Training] Epoch 038 | Batch 020/102 | Current Loss: 0.100109\n[Training] Epoch 038 | Batch 030/102 | Current Loss: 0.100151\n[Training] Epoch 038 | Batch 040/102 | Current Loss: 0.100154\n[Training] Epoch 038 | Batch 050/102 | Current Loss: 0.100182\n[Training] Epoch 038 | Batch 060/102 | Current Loss: 0.100038\n[Training] Epoch 038 | Batch 070/102 | Current Loss: 0.100150\n[Training] Epoch 038 | Batch 080/102 | Current Loss: 0.100156\n[Training] Epoch 038 | Batch 090/102 | Current Loss: 0.100101\n[Training] Epoch 038 | Batch 100/102 | Current Loss: 0.100102\n\n[Training] Epoch 038 Summary:\n  Avg Loss: 0.100127\n  Current LR: 4.06e-05\n[EarlyStopping] Loss improved (0.100131 → 0.100127). Saving model...\n[Training] Epoch 039 | Batch 000/102 | Current Loss: 0.100095\n[Training] Epoch 039 | Batch 010/102 | Current Loss: 0.100167\n[Training] Epoch 039 | Batch 020/102 | Current Loss: 0.100045\n[Training] Epoch 039 | Batch 030/102 | Current Loss: 0.100164\n[Training] Epoch 039 | Batch 040/102 | Current Loss: 0.100113\n[Training] Epoch 039 | Batch 050/102 | Current Loss: 0.100243\n[Training] Epoch 039 | Batch 060/102 | Current Loss: 0.100156\n[Training] Epoch 039 | Batch 070/102 | Current Loss: 0.100185\n[Training] Epoch 039 | Batch 080/102 | Current Loss: 0.100094\n[Training] Epoch 039 | Batch 090/102 | Current Loss: 0.100083\n[Training] Epoch 039 | Batch 100/102 | Current Loss: 0.100071\n\n[Training] Epoch 039 Summary:\n  Avg Loss: 0.100125\n  Current LR: 1.12e-05\n[EarlyStopping] Loss improved (0.100127 → 0.100125). Saving model...\n[Training] Epoch 040 | Batch 000/102 | Current Loss: 0.100093\n[Training] Epoch 040 | Batch 010/102 | Current Loss: 0.100078\n[Training] Epoch 040 | Batch 020/102 | Current Loss: 0.100089\n[Training] Epoch 040 | Batch 030/102 | Current Loss: 0.100111\n[Training] Epoch 040 | Batch 040/102 | Current Loss: 0.100142\n[Training] Epoch 040 | Batch 050/102 | Current Loss: 0.100109\n[Training] Epoch 040 | Batch 060/102 | Current Loss: 0.100128\n[Training] Epoch 040 | Batch 070/102 | Current Loss: 0.100039\n[Training] Epoch 040 | Batch 080/102 | Current Loss: 0.100142\n[Training] Epoch 040 | Batch 090/102 | Current Loss: 0.100218\n[Training] Epoch 040 | Batch 100/102 | Current Loss: 0.100073\n\n[Training] Epoch 040 Summary:\n  Avg Loss: 0.100120\n  Current LR: 4.16e-04\n[EarlyStopping] Loss improved (0.100125 → 0.100120). Saving model...\n[Training] Epoch 041 | Batch 000/102 | Current Loss: 0.100112\n[Training] Epoch 041 | Batch 010/102 | Current Loss: 0.100522\n[Training] Epoch 041 | Batch 020/102 | Current Loss: 0.100629\n[Training] Epoch 041 | Batch 030/102 | Current Loss: 0.100434\n[Training] Epoch 041 | Batch 040/102 | Current Loss: 0.100608\n[Training] Epoch 041 | Batch 050/102 | Current Loss: 0.100260\n[Training] Epoch 041 | Batch 060/102 | Current Loss: 0.100177\n[Training] Epoch 041 | Batch 070/102 | Current Loss: 0.100291\n[Training] Epoch 041 | Batch 080/102 | Current Loss: 0.100222\n[Training] Epoch 041 | Batch 090/102 | Current Loss: 0.100230\n[Training] Epoch 041 | Batch 100/102 | Current Loss: 0.100123\n\n[Training] Epoch 041 Summary:\n  Avg Loss: 0.100354\n  Current LR: 4.06e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 042 | Batch 000/102 | Current Loss: 0.100124\n[Training] Epoch 042 | Batch 010/102 | Current Loss: 0.100095\n[Training] Epoch 042 | Batch 020/102 | Current Loss: 0.100138\n[Training] Epoch 042 | Batch 030/102 | Current Loss: 0.100145\n[Training] Epoch 042 | Batch 040/102 | Current Loss: 0.100158\n[Training] Epoch 042 | Batch 050/102 | Current Loss: 0.100102\n[Training] Epoch 042 | Batch 060/102 | Current Loss: 0.100210\n[Training] Epoch 042 | Batch 070/102 | Current Loss: 0.100174\n[Training] Epoch 042 | Batch 080/102 | Current Loss: 0.100144\n[Training] Epoch 042 | Batch 090/102 | Current Loss: 0.100177\n[Training] Epoch 042 | Batch 100/102 | Current Loss: 0.100138\n\n[Training] Epoch 042 Summary:\n  Avg Loss: 0.100153\n  Current LR: 3.76e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 043 | Batch 000/102 | Current Loss: 0.100168\n[Training] Epoch 043 | Batch 010/102 | Current Loss: 0.100073\n[Training] Epoch 043 | Batch 020/102 | Current Loss: 0.100109\n[Training] Epoch 043 | Batch 030/102 | Current Loss: 0.100112\n[Training] Epoch 043 | Batch 040/102 | Current Loss: 0.100112\n[Training] Epoch 043 | Batch 050/102 | Current Loss: 0.100138\n[Training] Epoch 043 | Batch 060/102 | Current Loss: 0.100110\n[Training] Epoch 043 | Batch 070/102 | Current Loss: 0.100125\n[Training] Epoch 043 | Batch 080/102 | Current Loss: 0.100196\n[Training] Epoch 043 | Batch 090/102 | Current Loss: 0.100109\n[Training] Epoch 043 | Batch 100/102 | Current Loss: 0.100107\n\n[Training] Epoch 043 Summary:\n  Avg Loss: 0.100140\n  Current LR: 3.31e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 044 | Batch 000/102 | Current Loss: 0.100196\n[Training] Epoch 044 | Batch 010/102 | Current Loss: 0.100111\n[Training] Epoch 044 | Batch 020/102 | Current Loss: 0.100136\n[Training] Epoch 044 | Batch 030/102 | Current Loss: 0.100128\n[Training] Epoch 044 | Batch 040/102 | Current Loss: 0.100175\n[Training] Epoch 044 | Batch 050/102 | Current Loss: 0.100152\n[Training] Epoch 044 | Batch 060/102 | Current Loss: 0.100182\n[Training] Epoch 044 | Batch 070/102 | Current Loss: 0.100105\n[Training] Epoch 044 | Batch 080/102 | Current Loss: 0.100147\n[Training] Epoch 044 | Batch 090/102 | Current Loss: 0.100133\n[Training] Epoch 044 | Batch 100/102 | Current Loss: 0.100109\n\n[Training] Epoch 044 Summary:\n  Avg Loss: 0.100134\n  Current LR: 2.73e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 045 | Batch 000/102 | Current Loss: 0.100170\n[Training] Epoch 045 | Batch 010/102 | Current Loss: 0.100133\n[Training] Epoch 045 | Batch 020/102 | Current Loss: 0.100092\n[Training] Epoch 045 | Batch 030/102 | Current Loss: 0.100204\n[Training] Epoch 045 | Batch 040/102 | Current Loss: 0.100126\n[Training] Epoch 045 | Batch 050/102 | Current Loss: 0.100062\n[Training] Epoch 045 | Batch 060/102 | Current Loss: 0.100169\n[Training] Epoch 045 | Batch 070/102 | Current Loss: 0.100088\n[Training] Epoch 045 | Batch 080/102 | Current Loss: 0.100147\n[Training] Epoch 045 | Batch 090/102 | Current Loss: 0.100136\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 17:27:48,790] Trial 15 finished with value: 0.10012004970043313 and parameters: {'lr': 0.0004161421207968201, 'batch_size': 128, 'bottleneck_width': 2048, 'dropout_rate': 0.10913518963700314, 'alpha': 0.4761145907143287, 'weight_decay': 4.534515090977433e-05}. Best is trial 8 with value: 0.10009658994043574.\n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 045 | Batch 100/102 | Current Loss: 0.100160\n\n[Training] Epoch 045 Summary:\n  Avg Loss: 0.100123\n  Current LR: 2.09e-04\n[EarlyStopping] Counter: 5/5\n\n[Training] Early stopping triggered\n\n[Optuna] Trial 15 completed with best loss: 0.100120\n\n==================================================\nStarting Optuna Trial 16\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.00021116852160432329\n  batch_size: 128\n  bottleneck_width: 1024\n  dropout_rate: 0.4113217173328182\n  alpha: 0.448443357102263\n  weight_decay: 1.4953383419721361e-05\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 128, Total batches: 102\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 1024\n  - Dropout rate: 0.4113217173328182\n[Model] Architecture initialized successfully\n[Model] Total parameters: 2,396,992\n[Optimizer] Initialized with lr=2.11e-04, weight_decay=1.50e-05\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.448443357102263\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/102 | Current Loss: 1.541327\n[Training] Epoch 001 | Batch 010/102 | Current Loss: 0.245671\n[Training] Epoch 001 | Batch 020/102 | Current Loss: 0.166471\n[Training] Epoch 001 | Batch 030/102 | Current Loss: 0.135461\n[Training] Epoch 001 | Batch 040/102 | Current Loss: 0.124191\n[Training] Epoch 001 | Batch 050/102 | Current Loss: 0.120877\n[Training] Epoch 001 | Batch 060/102 | Current Loss: 0.120313\n[Training] Epoch 001 | Batch 070/102 | Current Loss: 0.115027\n[Training] Epoch 001 | Batch 080/102 | Current Loss: 0.111533\n[Training] Epoch 001 | Batch 090/102 | Current Loss: 0.111177\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 17:28:03,802] Trial 16 pruned. \n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 001 | Batch 100/102 | Current Loss: 0.109635\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.177752\n  Current LR: 2.06e-04\n[EarlyStopping] Loss improved (inf → 0.177752). Saving model...\n\n[Optuna] Trial pruned\n\n==================================================\nStarting Optuna Trial 17\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.00016646983286589253\n  batch_size: 128\n  bottleneck_width: 1024\n  dropout_rate: 0.4940204733500383\n  alpha: 0.5236893016977584\n  weight_decay: 7.306004493082655e-05\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 128, Total batches: 102\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 1024\n  - Dropout rate: 0.4940204733500383\n[Model] Architecture initialized successfully\n[Model] Total parameters: 2,396,992\n[Optimizer] Initialized with lr=1.66e-04, weight_decay=7.31e-05\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.5236893016977584\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/102 | Current Loss: 1.687163\n[Training] Epoch 001 | Batch 010/102 | Current Loss: 0.331385\n[Training] Epoch 001 | Batch 020/102 | Current Loss: 0.196149\n[Training] Epoch 001 | Batch 030/102 | Current Loss: 0.154074\n[Training] Epoch 001 | Batch 040/102 | Current Loss: 0.137860\n[Training] Epoch 001 | Batch 050/102 | Current Loss: 0.133551\n[Training] Epoch 001 | Batch 060/102 | Current Loss: 0.122793\n[Training] Epoch 001 | Batch 070/102 | Current Loss: 0.119192\n[Training] Epoch 001 | Batch 080/102 | Current Loss: 0.115618\n[Training] Epoch 001 | Batch 090/102 | Current Loss: 0.114343\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 17:28:18,743] Trial 17 pruned. \n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 001 | Batch 100/102 | Current Loss: 0.117746\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.205552\n  Current LR: 1.62e-04\n[EarlyStopping] Loss improved (inf → 0.205552). Saving model...\n\n[Optuna] Trial pruned\n\n==================================================\nStarting Optuna Trial 18\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.0008632097996657894\n  batch_size: 64\n  bottleneck_width: 512\n  dropout_rate: 0.3140554789948301\n  alpha: 0.5913638011196574\n  weight_decay: 3.485896826371103e-05\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 64, Total batches: 204\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 512\n  - Dropout rate: 0.3140554789948301\n[Model] Architecture initialized successfully\n[Model] Total parameters: 1,347,904\n[Optimizer] Initialized with lr=8.63e-04, weight_decay=3.49e-05\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.5913638011196574\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/204 | Current Loss: 1.454491\n[Training] Epoch 001 | Batch 010/204 | Current Loss: 0.160544\n[Training] Epoch 001 | Batch 020/204 | Current Loss: 0.123843\n[Training] Epoch 001 | Batch 030/204 | Current Loss: 0.116442\n[Training] Epoch 001 | Batch 040/204 | Current Loss: 0.110062\n[Training] Epoch 001 | Batch 050/204 | Current Loss: 0.110130\n[Training] Epoch 001 | Batch 060/204 | Current Loss: 0.113908\n[Training] Epoch 001 | Batch 070/204 | Current Loss: 0.106936\n[Training] Epoch 001 | Batch 080/204 | Current Loss: 0.105772\n[Training] Epoch 001 | Batch 090/204 | Current Loss: 0.108621\n[Training] Epoch 001 | Batch 100/204 | Current Loss: 0.104403\n[Training] Epoch 001 | Batch 110/204 | Current Loss: 0.104475\n[Training] Epoch 001 | Batch 120/204 | Current Loss: 0.107272\n[Training] Epoch 001 | Batch 130/204 | Current Loss: 0.104318\n[Training] Epoch 001 | Batch 140/204 | Current Loss: 0.103323\n[Training] Epoch 001 | Batch 150/204 | Current Loss: 0.103242\n[Training] Epoch 001 | Batch 160/204 | Current Loss: 0.104492\n[Training] Epoch 001 | Batch 170/204 | Current Loss: 0.104673\n[Training] Epoch 001 | Batch 180/204 | Current Loss: 0.103393\n[Training] Epoch 001 | Batch 190/204 | Current Loss: 0.103134\n[Training] Epoch 001 | Batch 200/204 | Current Loss: 0.103106\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.126556\n  Current LR: 8.42e-04\n[EarlyStopping] Loss improved (inf → 0.126556). Saving model...\n[Training] Epoch 002 | Batch 000/204 | Current Loss: 0.104166\n[Training] Epoch 002 | Batch 010/204 | Current Loss: 0.102863\n[Training] Epoch 002 | Batch 020/204 | Current Loss: 0.102767\n[Training] Epoch 002 | Batch 030/204 | Current Loss: 0.103137\n[Training] Epoch 002 | Batch 040/204 | Current Loss: 0.102685\n[Training] Epoch 002 | Batch 050/204 | Current Loss: 0.105669\n[Training] Epoch 002 | Batch 060/204 | Current Loss: 0.102851\n[Training] Epoch 002 | Batch 070/204 | Current Loss: 0.102536\n[Training] Epoch 002 | Batch 080/204 | Current Loss: 0.102493\n[Training] Epoch 002 | Batch 090/204 | Current Loss: 0.102156\n[Training] Epoch 002 | Batch 100/204 | Current Loss: 0.101933\n[Training] Epoch 002 | Batch 110/204 | Current Loss: 0.102320\n[Training] Epoch 002 | Batch 120/204 | Current Loss: 0.102877\n[Training] Epoch 002 | Batch 130/204 | Current Loss: 0.102916\n[Training] Epoch 002 | Batch 140/204 | Current Loss: 0.102216\n[Training] Epoch 002 | Batch 150/204 | Current Loss: 0.102997\n[Training] Epoch 002 | Batch 160/204 | Current Loss: 0.102217\n[Training] Epoch 002 | Batch 170/204 | Current Loss: 0.102943\n[Training] Epoch 002 | Batch 180/204 | Current Loss: 0.101859\n[Training] Epoch 002 | Batch 190/204 | Current Loss: 0.101594\n[Training] Epoch 002 | Batch 200/204 | Current Loss: 0.101760\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.102874\n  Current LR: 7.81e-04\n[EarlyStopping] Loss improved (0.126556 → 0.102874). Saving model...\n[Training] Epoch 003 | Batch 000/204 | Current Loss: 0.102241\n[Training] Epoch 003 | Batch 010/204 | Current Loss: 0.101612\n[Training] Epoch 003 | Batch 020/204 | Current Loss: 0.101770\n[Training] Epoch 003 | Batch 030/204 | Current Loss: 0.101559\n[Training] Epoch 003 | Batch 040/204 | Current Loss: 0.101817\n[Training] Epoch 003 | Batch 050/204 | Current Loss: 0.101719\n[Training] Epoch 003 | Batch 060/204 | Current Loss: 0.101463\n[Training] Epoch 003 | Batch 070/204 | Current Loss: 0.101542\n[Training] Epoch 003 | Batch 080/204 | Current Loss: 0.101945\n[Training] Epoch 003 | Batch 090/204 | Current Loss: 0.102057\n[Training] Epoch 003 | Batch 100/204 | Current Loss: 0.102208\n[Training] Epoch 003 | Batch 110/204 | Current Loss: 0.101548\n[Training] Epoch 003 | Batch 120/204 | Current Loss: 0.101168\n[Training] Epoch 003 | Batch 130/204 | Current Loss: 0.102055\n[Training] Epoch 003 | Batch 140/204 | Current Loss: 0.101236\n[Training] Epoch 003 | Batch 150/204 | Current Loss: 0.101614\n[Training] Epoch 003 | Batch 160/204 | Current Loss: 0.101383\n[Training] Epoch 003 | Batch 170/204 | Current Loss: 0.101387\n[Training] Epoch 003 | Batch 180/204 | Current Loss: 0.101210\n[Training] Epoch 003 | Batch 190/204 | Current Loss: 0.101206\n[Training] Epoch 003 | Batch 200/204 | Current Loss: 0.101199\n\n[Training] Epoch 003 Summary:\n  Avg Loss: 0.101673\n  Current LR: 6.86e-04\n[EarlyStopping] Loss improved (0.102874 → 0.101673). Saving model...\n[Training] Epoch 004 | Batch 000/204 | Current Loss: 0.102945\n[Training] Epoch 004 | Batch 010/204 | Current Loss: 0.102290\n[Training] Epoch 004 | Batch 020/204 | Current Loss: 0.101217\n[Training] Epoch 004 | Batch 030/204 | Current Loss: 0.102608\n[Training] Epoch 004 | Batch 040/204 | Current Loss: 0.101367\n[Training] Epoch 004 | Batch 050/204 | Current Loss: 0.101453\n[Training] Epoch 004 | Batch 060/204 | Current Loss: 0.101182\n[Training] Epoch 004 | Batch 070/204 | Current Loss: 0.101357\n[Training] Epoch 004 | Batch 080/204 | Current Loss: 0.101098\n[Training] Epoch 004 | Batch 090/204 | Current Loss: 0.101237\n[Training] Epoch 004 | Batch 100/204 | Current Loss: 0.100837\n[Training] Epoch 004 | Batch 110/204 | Current Loss: 0.100916\n[Training] Epoch 004 | Batch 120/204 | Current Loss: 0.100989\n[Training] Epoch 004 | Batch 130/204 | Current Loss: 0.101036\n[Training] Epoch 004 | Batch 140/204 | Current Loss: 0.101055\n[Training] Epoch 004 | Batch 150/204 | Current Loss: 0.101008\n[Training] Epoch 004 | Batch 160/204 | Current Loss: 0.100912\n[Training] Epoch 004 | Batch 170/204 | Current Loss: 0.100889\n[Training] Epoch 004 | Batch 180/204 | Current Loss: 0.100982\n[Training] Epoch 004 | Batch 190/204 | Current Loss: 0.100909\n[Training] Epoch 004 | Batch 200/204 | Current Loss: 0.100969\n\n[Training] Epoch 004 Summary:\n  Avg Loss: 0.101302\n  Current LR: 5.65e-04\n[EarlyStopping] Loss improved (0.101673 → 0.101302). Saving model...\n[Training] Epoch 005 | Batch 000/204 | Current Loss: 0.102762\n[Training] Epoch 005 | Batch 010/204 | Current Loss: 0.101547\n[Training] Epoch 005 | Batch 020/204 | Current Loss: 0.102445\n[Training] Epoch 005 | Batch 030/204 | Current Loss: 0.102834\n[Training] Epoch 005 | Batch 040/204 | Current Loss: 0.101477\n[Training] Epoch 005 | Batch 050/204 | Current Loss: 0.101023\n[Training] Epoch 005 | Batch 060/204 | Current Loss: 0.101162\n[Training] Epoch 005 | Batch 070/204 | Current Loss: 0.101241\n[Training] Epoch 005 | Batch 080/204 | Current Loss: 0.101000\n[Training] Epoch 005 | Batch 090/204 | Current Loss: 0.100799\n[Training] Epoch 005 | Batch 100/204 | Current Loss: 0.100921\n[Training] Epoch 005 | Batch 110/204 | Current Loss: 0.100991\n[Training] Epoch 005 | Batch 120/204 | Current Loss: 0.100892\n[Training] Epoch 005 | Batch 130/204 | Current Loss: 0.100796\n[Training] Epoch 005 | Batch 140/204 | Current Loss: 0.100747\n[Training] Epoch 005 | Batch 150/204 | Current Loss: 0.100925\n[Training] Epoch 005 | Batch 160/204 | Current Loss: 0.100773\n[Training] Epoch 005 | Batch 170/204 | Current Loss: 0.100716\n[Training] Epoch 005 | Batch 180/204 | Current Loss: 0.100833\n[Training] Epoch 005 | Batch 190/204 | Current Loss: 0.101015\n[Training] Epoch 005 | Batch 200/204 | Current Loss: 0.100920\n\n[Training] Epoch 005 Summary:\n  Avg Loss: 0.101114\n  Current LR: 4.32e-04\n[EarlyStopping] Loss improved (0.101302 → 0.101114). Saving model...\n[Training] Epoch 006 | Batch 000/204 | Current Loss: 0.101192\n[Training] Epoch 006 | Batch 010/204 | Current Loss: 0.100900\n[Training] Epoch 006 | Batch 020/204 | Current Loss: 0.100758\n[Training] Epoch 006 | Batch 030/204 | Current Loss: 0.100773\n[Training] Epoch 006 | Batch 040/204 | Current Loss: 0.100725\n[Training] Epoch 006 | Batch 050/204 | Current Loss: 0.100768\n[Training] Epoch 006 | Batch 060/204 | Current Loss: 0.100781\n[Training] Epoch 006 | Batch 070/204 | Current Loss: 0.100639\n[Training] Epoch 006 | Batch 080/204 | Current Loss: 0.100790\n[Training] Epoch 006 | Batch 090/204 | Current Loss: 0.100640\n[Training] Epoch 006 | Batch 100/204 | Current Loss: 0.100557\n[Training] Epoch 006 | Batch 110/204 | Current Loss: 0.100688\n[Training] Epoch 006 | Batch 120/204 | Current Loss: 0.100633\n[Training] Epoch 006 | Batch 130/204 | Current Loss: 0.100682\n[Training] Epoch 006 | Batch 140/204 | Current Loss: 0.100673\n[Training] Epoch 006 | Batch 150/204 | Current Loss: 0.100732\n[Training] Epoch 006 | Batch 160/204 | Current Loss: 0.100695\n[Training] Epoch 006 | Batch 170/204 | Current Loss: 0.100655\n[Training] Epoch 006 | Batch 180/204 | Current Loss: 0.100608\n[Training] Epoch 006 | Batch 190/204 | Current Loss: 0.100591\n[Training] Epoch 006 | Batch 200/204 | Current Loss: 0.100599\n\n[Training] Epoch 006 Summary:\n  Avg Loss: 0.100691\n  Current LR: 2.99e-04\n[EarlyStopping] Loss improved (0.101114 → 0.100691). Saving model...\n[Training] Epoch 007 | Batch 000/204 | Current Loss: 0.100546\n[Training] Epoch 007 | Batch 010/204 | Current Loss: 0.100620\n[Training] Epoch 007 | Batch 020/204 | Current Loss: 0.100615\n[Training] Epoch 007 | Batch 030/204 | Current Loss: 0.100585\n[Training] Epoch 007 | Batch 040/204 | Current Loss: 0.100519\n[Training] Epoch 007 | Batch 050/204 | Current Loss: 0.100784\n[Training] Epoch 007 | Batch 060/204 | Current Loss: 0.100562\n[Training] Epoch 007 | Batch 070/204 | Current Loss: 0.100612\n[Training] Epoch 007 | Batch 080/204 | Current Loss: 0.100663\n[Training] Epoch 007 | Batch 090/204 | Current Loss: 0.100725\n[Training] Epoch 007 | Batch 100/204 | Current Loss: 0.100621\n[Training] Epoch 007 | Batch 110/204 | Current Loss: 0.100517\n[Training] Epoch 007 | Batch 120/204 | Current Loss: 0.100924\n[Training] Epoch 007 | Batch 130/204 | Current Loss: 0.100502\n[Training] Epoch 007 | Batch 140/204 | Current Loss: 0.100592\n[Training] Epoch 007 | Batch 150/204 | Current Loss: 0.100819\n[Training] Epoch 007 | Batch 160/204 | Current Loss: 0.100682\n[Training] Epoch 007 | Batch 170/204 | Current Loss: 0.100495\n[Training] Epoch 007 | Batch 180/204 | Current Loss: 0.100705\n[Training] Epoch 007 | Batch 190/204 | Current Loss: 0.100607\n[Training] Epoch 007 | Batch 200/204 | Current Loss: 0.100713\n\n[Training] Epoch 007 Summary:\n  Avg Loss: 0.100601\n  Current LR: 1.79e-04\n[EarlyStopping] Loss improved (0.100691 → 0.100601). Saving model...\n[Training] Epoch 008 | Batch 000/204 | Current Loss: 0.100754\n[Training] Epoch 008 | Batch 010/204 | Current Loss: 0.100497\n[Training] Epoch 008 | Batch 020/204 | Current Loss: 0.100560\n[Training] Epoch 008 | Batch 030/204 | Current Loss: 0.100341\n[Training] Epoch 008 | Batch 040/204 | Current Loss: 0.100507\n[Training] Epoch 008 | Batch 050/204 | Current Loss: 0.100579\n[Training] Epoch 008 | Batch 060/204 | Current Loss: 0.100496\n[Training] Epoch 008 | Batch 070/204 | Current Loss: 0.100513\n[Training] Epoch 008 | Batch 080/204 | Current Loss: 0.100459\n[Training] Epoch 008 | Batch 090/204 | Current Loss: 0.100543\n[Training] Epoch 008 | Batch 100/204 | Current Loss: 0.100686\n[Training] Epoch 008 | Batch 110/204 | Current Loss: 0.100530\n[Training] Epoch 008 | Batch 120/204 | Current Loss: 0.100473\n[Training] Epoch 008 | Batch 130/204 | Current Loss: 0.100404\n[Training] Epoch 008 | Batch 140/204 | Current Loss: 0.100531\n[Training] Epoch 008 | Batch 150/204 | Current Loss: 0.100418\n[Training] Epoch 008 | Batch 160/204 | Current Loss: 0.100415\n[Training] Epoch 008 | Batch 170/204 | Current Loss: 0.100453\n[Training] Epoch 008 | Batch 180/204 | Current Loss: 0.100401\n[Training] Epoch 008 | Batch 190/204 | Current Loss: 0.100451\n[Training] Epoch 008 | Batch 200/204 | Current Loss: 0.100562\n\n[Training] Epoch 008 Summary:\n  Avg Loss: 0.100528\n  Current LR: 8.33e-05\n[EarlyStopping] Loss improved (0.100601 → 0.100528). Saving model...\n[Training] Epoch 009 | Batch 000/204 | Current Loss: 0.100385\n[Training] Epoch 009 | Batch 010/204 | Current Loss: 0.100459\n[Training] Epoch 009 | Batch 020/204 | Current Loss: 0.100495\n[Training] Epoch 009 | Batch 030/204 | Current Loss: 0.100401\n[Training] Epoch 009 | Batch 040/204 | Current Loss: 0.100425\n[Training] Epoch 009 | Batch 050/204 | Current Loss: 0.100398\n[Training] Epoch 009 | Batch 060/204 | Current Loss: 0.100433\n[Training] Epoch 009 | Batch 070/204 | Current Loss: 0.100434\n[Training] Epoch 009 | Batch 080/204 | Current Loss: 0.100481\n[Training] Epoch 009 | Batch 090/204 | Current Loss: 0.100516\n[Training] Epoch 009 | Batch 100/204 | Current Loss: 0.100556\n[Training] Epoch 009 | Batch 110/204 | Current Loss: 0.100668\n[Training] Epoch 009 | Batch 120/204 | Current Loss: 0.100480\n[Training] Epoch 009 | Batch 130/204 | Current Loss: 0.100557\n[Training] Epoch 009 | Batch 140/204 | Current Loss: 0.100477\n[Training] Epoch 009 | Batch 150/204 | Current Loss: 0.100462\n[Training] Epoch 009 | Batch 160/204 | Current Loss: 0.100494\n[Training] Epoch 009 | Batch 170/204 | Current Loss: 0.100473\n[Training] Epoch 009 | Batch 180/204 | Current Loss: 0.100495\n[Training] Epoch 009 | Batch 190/204 | Current Loss: 0.100466\n[Training] Epoch 009 | Batch 200/204 | Current Loss: 0.100417\n\n[Training] Epoch 009 Summary:\n  Avg Loss: 0.100507\n  Current LR: 2.21e-05\n[EarlyStopping] Loss improved (0.100528 → 0.100507). Saving model...\n[Training] Epoch 010 | Batch 000/204 | Current Loss: 0.100533\n[Training] Epoch 010 | Batch 010/204 | Current Loss: 0.100400\n[Training] Epoch 010 | Batch 020/204 | Current Loss: 0.100687\n[Training] Epoch 010 | Batch 030/204 | Current Loss: 0.100541\n[Training] Epoch 010 | Batch 040/204 | Current Loss: 0.100501\n[Training] Epoch 010 | Batch 050/204 | Current Loss: 0.100478\n[Training] Epoch 010 | Batch 060/204 | Current Loss: 0.100550\n[Training] Epoch 010 | Batch 070/204 | Current Loss: 0.100579\n[Training] Epoch 010 | Batch 080/204 | Current Loss: 0.100460\n[Training] Epoch 010 | Batch 090/204 | Current Loss: 0.100367\n[Training] Epoch 010 | Batch 100/204 | Current Loss: 0.100539\n[Training] Epoch 010 | Batch 110/204 | Current Loss: 0.100357\n[Training] Epoch 010 | Batch 120/204 | Current Loss: 0.100610\n[Training] Epoch 010 | Batch 130/204 | Current Loss: 0.100459\n[Training] Epoch 010 | Batch 140/204 | Current Loss: 0.100378\n[Training] Epoch 010 | Batch 150/204 | Current Loss: 0.100458\n[Training] Epoch 010 | Batch 160/204 | Current Loss: 0.100334\n[Training] Epoch 010 | Batch 170/204 | Current Loss: 0.100352\n[Training] Epoch 010 | Batch 180/204 | Current Loss: 0.100402\n[Training] Epoch 010 | Batch 190/204 | Current Loss: 0.100359\n[Training] Epoch 010 | Batch 200/204 | Current Loss: 0.100479\n\n[Training] Epoch 010 Summary:\n  Avg Loss: 0.100472\n  Current LR: 8.63e-04\n[EarlyStopping] Loss improved (0.100507 → 0.100472). Saving model...\n[Training] Epoch 011 | Batch 000/204 | Current Loss: 0.100641\n[Training] Epoch 011 | Batch 010/204 | Current Loss: 0.100741\n[Training] Epoch 011 | Batch 020/204 | Current Loss: 0.100671\n[Training] Epoch 011 | Batch 030/204 | Current Loss: 0.100414\n[Training] Epoch 011 | Batch 040/204 | Current Loss: 0.100675\n[Training] Epoch 011 | Batch 050/204 | Current Loss: 0.100466\n[Training] Epoch 011 | Batch 060/204 | Current Loss: 0.100591\n[Training] Epoch 011 | Batch 070/204 | Current Loss: 0.100609\n[Training] Epoch 011 | Batch 080/204 | Current Loss: 0.100566\n[Training] Epoch 011 | Batch 090/204 | Current Loss: 0.100580\n[Training] Epoch 011 | Batch 100/204 | Current Loss: 0.100813\n[Training] Epoch 011 | Batch 110/204 | Current Loss: 0.100540\n[Training] Epoch 011 | Batch 120/204 | Current Loss: 0.100551\n[Training] Epoch 011 | Batch 130/204 | Current Loss: 0.100806\n[Training] Epoch 011 | Batch 140/204 | Current Loss: 0.100704\n[Training] Epoch 011 | Batch 150/204 | Current Loss: 0.100698\n[Training] Epoch 011 | Batch 160/204 | Current Loss: 0.100759\n[Training] Epoch 011 | Batch 170/204 | Current Loss: 0.100570\n[Training] Epoch 011 | Batch 180/204 | Current Loss: 0.100382\n[Training] Epoch 011 | Batch 190/204 | Current Loss: 0.100485\n[Training] Epoch 011 | Batch 200/204 | Current Loss: 0.100417\n\n[Training] Epoch 011 Summary:\n  Avg Loss: 0.100581\n  Current LR: 8.42e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 012 | Batch 000/204 | Current Loss: 0.100607\n[Training] Epoch 012 | Batch 010/204 | Current Loss: 0.100469\n[Training] Epoch 012 | Batch 020/204 | Current Loss: 0.100509\n[Training] Epoch 012 | Batch 030/204 | Current Loss: 0.100527\n[Training] Epoch 012 | Batch 040/204 | Current Loss: 0.100528\n[Training] Epoch 012 | Batch 050/204 | Current Loss: 0.100365\n[Training] Epoch 012 | Batch 060/204 | Current Loss: 0.100284\n[Training] Epoch 012 | Batch 070/204 | Current Loss: 0.100373\n[Training] Epoch 012 | Batch 080/204 | Current Loss: 0.100414\n[Training] Epoch 012 | Batch 090/204 | Current Loss: 0.100250\n[Training] Epoch 012 | Batch 100/204 | Current Loss: 0.100398\n[Training] Epoch 012 | Batch 110/204 | Current Loss: 0.100446\n[Training] Epoch 012 | Batch 120/204 | Current Loss: 0.100305\n[Training] Epoch 012 | Batch 130/204 | Current Loss: 0.100282\n[Training] Epoch 012 | Batch 140/204 | Current Loss: 0.100406\n[Training] Epoch 012 | Batch 150/204 | Current Loss: 0.100414\n[Training] Epoch 012 | Batch 160/204 | Current Loss: 0.100344\n[Training] Epoch 012 | Batch 170/204 | Current Loss: 0.100298\n[Training] Epoch 012 | Batch 180/204 | Current Loss: 0.100306\n[Training] Epoch 012 | Batch 190/204 | Current Loss: 0.100273\n[Training] Epoch 012 | Batch 200/204 | Current Loss: 0.100549\n\n[Training] Epoch 012 Summary:\n  Avg Loss: 0.100416\n  Current LR: 7.81e-04\n[EarlyStopping] Loss improved (0.100472 → 0.100416). Saving model...\n[Training] Epoch 013 | Batch 000/204 | Current Loss: 0.100510\n[Training] Epoch 013 | Batch 010/204 | Current Loss: 0.100474\n[Training] Epoch 013 | Batch 020/204 | Current Loss: 0.100298\n[Training] Epoch 013 | Batch 030/204 | Current Loss: 0.100281\n[Training] Epoch 013 | Batch 040/204 | Current Loss: 0.100323\n[Training] Epoch 013 | Batch 050/204 | Current Loss: 0.100280\n[Training] Epoch 013 | Batch 060/204 | Current Loss: 0.100313\n[Training] Epoch 013 | Batch 070/204 | Current Loss: 0.100321\n[Training] Epoch 013 | Batch 080/204 | Current Loss: 0.100209\n[Training] Epoch 013 | Batch 090/204 | Current Loss: 0.100267\n[Training] Epoch 013 | Batch 100/204 | Current Loss: 0.100487\n[Training] Epoch 013 | Batch 110/204 | Current Loss: 0.100317\n[Training] Epoch 013 | Batch 120/204 | Current Loss: 0.100268\n[Training] Epoch 013 | Batch 130/204 | Current Loss: 0.100387\n[Training] Epoch 013 | Batch 140/204 | Current Loss: 0.100373\n[Training] Epoch 013 | Batch 150/204 | Current Loss: 0.100370\n[Training] Epoch 013 | Batch 160/204 | Current Loss: 0.100333\n[Training] Epoch 013 | Batch 170/204 | Current Loss: 0.100408\n[Training] Epoch 013 | Batch 180/204 | Current Loss: 0.100382\n[Training] Epoch 013 | Batch 190/204 | Current Loss: 0.100334\n[Training] Epoch 013 | Batch 200/204 | Current Loss: 0.100269\n\n[Training] Epoch 013 Summary:\n  Avg Loss: 0.100345\n  Current LR: 6.86e-04\n[EarlyStopping] Loss improved (0.100416 → 0.100345). Saving model...\n[Training] Epoch 014 | Batch 000/204 | Current Loss: 0.100415\n[Training] Epoch 014 | Batch 010/204 | Current Loss: 0.100459\n[Training] Epoch 014 | Batch 020/204 | Current Loss: 0.100572\n[Training] Epoch 014 | Batch 030/204 | Current Loss: 0.100313\n[Training] Epoch 014 | Batch 040/204 | Current Loss: 0.100261\n[Training] Epoch 014 | Batch 050/204 | Current Loss: 0.100223\n[Training] Epoch 014 | Batch 060/204 | Current Loss: 0.100274\n[Training] Epoch 014 | Batch 070/204 | Current Loss: 0.100394\n[Training] Epoch 014 | Batch 080/204 | Current Loss: 0.100360\n[Training] Epoch 014 | Batch 090/204 | Current Loss: 0.100257\n[Training] Epoch 014 | Batch 100/204 | Current Loss: 0.100279\n[Training] Epoch 014 | Batch 110/204 | Current Loss: 0.100404\n[Training] Epoch 014 | Batch 120/204 | Current Loss: 0.100252\n[Training] Epoch 014 | Batch 130/204 | Current Loss: 0.100217\n[Training] Epoch 014 | Batch 140/204 | Current Loss: 0.100220\n[Training] Epoch 014 | Batch 150/204 | Current Loss: 0.100224\n[Training] Epoch 014 | Batch 160/204 | Current Loss: 0.100267\n[Training] Epoch 014 | Batch 170/204 | Current Loss: 0.100238\n[Training] Epoch 014 | Batch 180/204 | Current Loss: 0.100182\n[Training] Epoch 014 | Batch 190/204 | Current Loss: 0.100371\n[Training] Epoch 014 | Batch 200/204 | Current Loss: 0.100330\n\n[Training] Epoch 014 Summary:\n  Avg Loss: 0.100300\n  Current LR: 5.65e-04\n[EarlyStopping] Loss improved (0.100345 → 0.100300). Saving model...\n[Training] Epoch 015 | Batch 000/204 | Current Loss: 0.100388\n[Training] Epoch 015 | Batch 010/204 | Current Loss: 0.100347\n[Training] Epoch 015 | Batch 020/204 | Current Loss: 0.100263\n[Training] Epoch 015 | Batch 030/204 | Current Loss: 0.100251\n[Training] Epoch 015 | Batch 040/204 | Current Loss: 0.100221\n[Training] Epoch 015 | Batch 050/204 | Current Loss: 0.100199\n[Training] Epoch 015 | Batch 060/204 | Current Loss: 0.100249\n[Training] Epoch 015 | Batch 070/204 | Current Loss: 0.100340\n[Training] Epoch 015 | Batch 080/204 | Current Loss: 0.100273\n[Training] Epoch 015 | Batch 090/204 | Current Loss: 0.100263\n[Training] Epoch 015 | Batch 100/204 | Current Loss: 0.100241\n[Training] Epoch 015 | Batch 110/204 | Current Loss: 0.100233\n[Training] Epoch 015 | Batch 120/204 | Current Loss: 0.100289\n[Training] Epoch 015 | Batch 130/204 | Current Loss: 0.100200\n[Training] Epoch 015 | Batch 140/204 | Current Loss: 0.100314\n[Training] Epoch 015 | Batch 150/204 | Current Loss: 0.100196\n[Training] Epoch 015 | Batch 160/204 | Current Loss: 0.100316\n[Training] Epoch 015 | Batch 170/204 | Current Loss: 0.100279\n[Training] Epoch 015 | Batch 180/204 | Current Loss: 0.100293\n[Training] Epoch 015 | Batch 190/204 | Current Loss: 0.100310\n[Training] Epoch 015 | Batch 200/204 | Current Loss: 0.100113\n\n[Training] Epoch 015 Summary:\n  Avg Loss: 0.100273\n  Current LR: 4.32e-04\n[EarlyStopping] Loss improved (0.100300 → 0.100273). Saving model...\n[Training] Epoch 016 | Batch 000/204 | Current Loss: 0.100225\n[Training] Epoch 016 | Batch 010/204 | Current Loss: 0.100267\n[Training] Epoch 016 | Batch 020/204 | Current Loss: 0.100261\n[Training] Epoch 016 | Batch 030/204 | Current Loss: 0.100390\n[Training] Epoch 016 | Batch 040/204 | Current Loss: 0.100281\n[Training] Epoch 016 | Batch 050/204 | Current Loss: 0.100368\n[Training] Epoch 016 | Batch 060/204 | Current Loss: 0.100326\n[Training] Epoch 016 | Batch 070/204 | Current Loss: 0.100223\n[Training] Epoch 016 | Batch 080/204 | Current Loss: 0.100268\n[Training] Epoch 016 | Batch 090/204 | Current Loss: 0.100251\n[Training] Epoch 016 | Batch 100/204 | Current Loss: 0.100231\n[Training] Epoch 016 | Batch 110/204 | Current Loss: 0.100358\n[Training] Epoch 016 | Batch 120/204 | Current Loss: 0.100361\n[Training] Epoch 016 | Batch 130/204 | Current Loss: 0.100270\n[Training] Epoch 016 | Batch 140/204 | Current Loss: 0.100187\n[Training] Epoch 016 | Batch 150/204 | Current Loss: 0.100220\n[Training] Epoch 016 | Batch 160/204 | Current Loss: 0.100385\n[Training] Epoch 016 | Batch 170/204 | Current Loss: 0.100236\n[Training] Epoch 016 | Batch 180/204 | Current Loss: 0.100255\n[Training] Epoch 016 | Batch 190/204 | Current Loss: 0.100226\n[Training] Epoch 016 | Batch 200/204 | Current Loss: 0.100252\n\n[Training] Epoch 016 Summary:\n  Avg Loss: 0.100245\n  Current LR: 2.99e-04\n[EarlyStopping] Loss improved (0.100273 → 0.100245). Saving model...\n[Training] Epoch 017 | Batch 000/204 | Current Loss: 0.100325\n[Training] Epoch 017 | Batch 010/204 | Current Loss: 0.100204\n[Training] Epoch 017 | Batch 020/204 | Current Loss: 0.100291\n[Training] Epoch 017 | Batch 030/204 | Current Loss: 0.100219\n[Training] Epoch 017 | Batch 040/204 | Current Loss: 0.100182\n[Training] Epoch 017 | Batch 050/204 | Current Loss: 0.100250\n[Training] Epoch 017 | Batch 060/204 | Current Loss: 0.100237\n[Training] Epoch 017 | Batch 070/204 | Current Loss: 0.100345\n[Training] Epoch 017 | Batch 080/204 | Current Loss: 0.100127\n[Training] Epoch 017 | Batch 090/204 | Current Loss: 0.100180\n[Training] Epoch 017 | Batch 100/204 | Current Loss: 0.100147\n[Training] Epoch 017 | Batch 110/204 | Current Loss: 0.100138\n[Training] Epoch 017 | Batch 120/204 | Current Loss: 0.100266\n[Training] Epoch 017 | Batch 130/204 | Current Loss: 0.100271\n[Training] Epoch 017 | Batch 140/204 | Current Loss: 0.100138\n[Training] Epoch 017 | Batch 150/204 | Current Loss: 0.100223\n[Training] Epoch 017 | Batch 160/204 | Current Loss: 0.100236\n[Training] Epoch 017 | Batch 170/204 | Current Loss: 0.100188\n[Training] Epoch 017 | Batch 180/204 | Current Loss: 0.100114\n[Training] Epoch 017 | Batch 190/204 | Current Loss: 0.100242\n[Training] Epoch 017 | Batch 200/204 | Current Loss: 0.100192\n\n[Training] Epoch 017 Summary:\n  Avg Loss: 0.100231\n  Current LR: 1.79e-04\n[EarlyStopping] Loss improved (0.100245 → 0.100231). Saving model...\n[Training] Epoch 018 | Batch 000/204 | Current Loss: 0.100207\n[Training] Epoch 018 | Batch 010/204 | Current Loss: 0.100193\n[Training] Epoch 018 | Batch 020/204 | Current Loss: 0.100374\n[Training] Epoch 018 | Batch 030/204 | Current Loss: 0.100270\n[Training] Epoch 018 | Batch 040/204 | Current Loss: 0.100342\n[Training] Epoch 018 | Batch 050/204 | Current Loss: 0.100273\n[Training] Epoch 018 | Batch 060/204 | Current Loss: 0.100259\n[Training] Epoch 018 | Batch 070/204 | Current Loss: 0.100145\n[Training] Epoch 018 | Batch 080/204 | Current Loss: 0.100231\n[Training] Epoch 018 | Batch 090/204 | Current Loss: 0.100103\n[Training] Epoch 018 | Batch 100/204 | Current Loss: 0.100335\n[Training] Epoch 018 | Batch 110/204 | Current Loss: 0.100266\n[Training] Epoch 018 | Batch 120/204 | Current Loss: 0.100178\n[Training] Epoch 018 | Batch 130/204 | Current Loss: 0.100291\n[Training] Epoch 018 | Batch 140/204 | Current Loss: 0.100314\n[Training] Epoch 018 | Batch 150/204 | Current Loss: 0.100238\n[Training] Epoch 018 | Batch 160/204 | Current Loss: 0.100182\n[Training] Epoch 018 | Batch 170/204 | Current Loss: 0.100103\n[Training] Epoch 018 | Batch 180/204 | Current Loss: 0.100328\n[Training] Epoch 018 | Batch 190/204 | Current Loss: 0.100205\n[Training] Epoch 018 | Batch 200/204 | Current Loss: 0.100267\n\n[Training] Epoch 018 Summary:\n  Avg Loss: 0.100215\n  Current LR: 8.33e-05\n[EarlyStopping] Loss improved (0.100231 → 0.100215). Saving model...\n[Training] Epoch 019 | Batch 000/204 | Current Loss: 0.100279\n[Training] Epoch 019 | Batch 010/204 | Current Loss: 0.100300\n[Training] Epoch 019 | Batch 020/204 | Current Loss: 0.100227\n[Training] Epoch 019 | Batch 030/204 | Current Loss: 0.100222\n[Training] Epoch 019 | Batch 040/204 | Current Loss: 0.100181\n[Training] Epoch 019 | Batch 050/204 | Current Loss: 0.100183\n[Training] Epoch 019 | Batch 060/204 | Current Loss: 0.100144\n[Training] Epoch 019 | Batch 070/204 | Current Loss: 0.100214\n[Training] Epoch 019 | Batch 080/204 | Current Loss: 0.100284\n[Training] Epoch 019 | Batch 090/204 | Current Loss: 0.100145\n[Training] Epoch 019 | Batch 100/204 | Current Loss: 0.100215\n[Training] Epoch 019 | Batch 110/204 | Current Loss: 0.100233\n[Training] Epoch 019 | Batch 120/204 | Current Loss: 0.100243\n[Training] Epoch 019 | Batch 130/204 | Current Loss: 0.100210\n[Training] Epoch 019 | Batch 140/204 | Current Loss: 0.100186\n[Training] Epoch 019 | Batch 150/204 | Current Loss: 0.100207\n[Training] Epoch 019 | Batch 160/204 | Current Loss: 0.100156\n[Training] Epoch 019 | Batch 170/204 | Current Loss: 0.100258\n[Training] Epoch 019 | Batch 180/204 | Current Loss: 0.100193\n[Training] Epoch 019 | Batch 190/204 | Current Loss: 0.100312\n[Training] Epoch 019 | Batch 200/204 | Current Loss: 0.100192\n\n[Training] Epoch 019 Summary:\n  Avg Loss: 0.100222\n  Current LR: 2.21e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 020 | Batch 000/204 | Current Loss: 0.100092\n[Training] Epoch 020 | Batch 010/204 | Current Loss: 0.100191\n[Training] Epoch 020 | Batch 020/204 | Current Loss: 0.100218\n[Training] Epoch 020 | Batch 030/204 | Current Loss: 0.100226\n[Training] Epoch 020 | Batch 040/204 | Current Loss: 0.100177\n[Training] Epoch 020 | Batch 050/204 | Current Loss: 0.100178\n[Training] Epoch 020 | Batch 060/204 | Current Loss: 0.100175\n[Training] Epoch 020 | Batch 070/204 | Current Loss: 0.100163\n[Training] Epoch 020 | Batch 080/204 | Current Loss: 0.100288\n[Training] Epoch 020 | Batch 090/204 | Current Loss: 0.100237\n[Training] Epoch 020 | Batch 100/204 | Current Loss: 0.100229\n[Training] Epoch 020 | Batch 110/204 | Current Loss: 0.100288\n[Training] Epoch 020 | Batch 120/204 | Current Loss: 0.100114\n[Training] Epoch 020 | Batch 130/204 | Current Loss: 0.100246\n[Training] Epoch 020 | Batch 140/204 | Current Loss: 0.100109\n[Training] Epoch 020 | Batch 150/204 | Current Loss: 0.100166\n[Training] Epoch 020 | Batch 160/204 | Current Loss: 0.100099\n[Training] Epoch 020 | Batch 170/204 | Current Loss: 0.100229\n[Training] Epoch 020 | Batch 180/204 | Current Loss: 0.100250\n[Training] Epoch 020 | Batch 190/204 | Current Loss: 0.100243\n[Training] Epoch 020 | Batch 200/204 | Current Loss: 0.100122\n\n[Training] Epoch 020 Summary:\n  Avg Loss: 0.100203\n  Current LR: 8.63e-04\n[EarlyStopping] Loss improved (0.100215 → 0.100203). Saving model...\n[Training] Epoch 021 | Batch 000/204 | Current Loss: 0.100244\n[Training] Epoch 021 | Batch 010/204 | Current Loss: 0.100370\n[Training] Epoch 021 | Batch 020/204 | Current Loss: 0.100220\n[Training] Epoch 021 | Batch 030/204 | Current Loss: 0.100345\n[Training] Epoch 021 | Batch 040/204 | Current Loss: 0.100340\n[Training] Epoch 021 | Batch 050/204 | Current Loss: 0.100328\n[Training] Epoch 021 | Batch 060/204 | Current Loss: 0.100225\n[Training] Epoch 021 | Batch 070/204 | Current Loss: 0.100255\n[Training] Epoch 021 | Batch 080/204 | Current Loss: 0.100358\n[Training] Epoch 021 | Batch 090/204 | Current Loss: 0.100219\n[Training] Epoch 021 | Batch 100/204 | Current Loss: 0.100412\n[Training] Epoch 021 | Batch 110/204 | Current Loss: 0.100307\n[Training] Epoch 021 | Batch 120/204 | Current Loss: 0.100269\n[Training] Epoch 021 | Batch 130/204 | Current Loss: 0.100526\n[Training] Epoch 021 | Batch 140/204 | Current Loss: 0.100428\n[Training] Epoch 021 | Batch 150/204 | Current Loss: 0.100224\n[Training] Epoch 021 | Batch 160/204 | Current Loss: 0.100230\n[Training] Epoch 021 | Batch 170/204 | Current Loss: 0.100362\n[Training] Epoch 021 | Batch 180/204 | Current Loss: 0.100396\n[Training] Epoch 021 | Batch 190/204 | Current Loss: 0.100343\n[Training] Epoch 021 | Batch 200/204 | Current Loss: 0.100158\n\n[Training] Epoch 021 Summary:\n  Avg Loss: 0.100310\n  Current LR: 8.42e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 022 | Batch 000/204 | Current Loss: 0.100285\n[Training] Epoch 022 | Batch 010/204 | Current Loss: 0.100317\n[Training] Epoch 022 | Batch 020/204 | Current Loss: 0.100278\n[Training] Epoch 022 | Batch 030/204 | Current Loss: 0.100320\n[Training] Epoch 022 | Batch 040/204 | Current Loss: 0.100345\n[Training] Epoch 022 | Batch 050/204 | Current Loss: 0.100234\n[Training] Epoch 022 | Batch 060/204 | Current Loss: 0.100243\n[Training] Epoch 022 | Batch 070/204 | Current Loss: 0.100265\n[Training] Epoch 022 | Batch 080/204 | Current Loss: 0.100240\n[Training] Epoch 022 | Batch 090/204 | Current Loss: 0.100240\n[Training] Epoch 022 | Batch 100/204 | Current Loss: 0.100290\n[Training] Epoch 022 | Batch 110/204 | Current Loss: 0.100189\n[Training] Epoch 022 | Batch 120/204 | Current Loss: 0.100331\n[Training] Epoch 022 | Batch 130/204 | Current Loss: 0.100306\n[Training] Epoch 022 | Batch 140/204 | Current Loss: 0.100287\n[Training] Epoch 022 | Batch 150/204 | Current Loss: 0.100289\n[Training] Epoch 022 | Batch 160/204 | Current Loss: 0.100260\n[Training] Epoch 022 | Batch 170/204 | Current Loss: 0.100252\n[Training] Epoch 022 | Batch 180/204 | Current Loss: 0.100232\n[Training] Epoch 022 | Batch 190/204 | Current Loss: 0.100118\n[Training] Epoch 022 | Batch 200/204 | Current Loss: 0.100222\n\n[Training] Epoch 022 Summary:\n  Avg Loss: 0.100254\n  Current LR: 7.81e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 023 | Batch 000/204 | Current Loss: 0.100216\n[Training] Epoch 023 | Batch 010/204 | Current Loss: 0.100222\n[Training] Epoch 023 | Batch 020/204 | Current Loss: 0.100205\n[Training] Epoch 023 | Batch 030/204 | Current Loss: 0.100144\n[Training] Epoch 023 | Batch 040/204 | Current Loss: 0.100259\n[Training] Epoch 023 | Batch 050/204 | Current Loss: 0.100213\n[Training] Epoch 023 | Batch 060/204 | Current Loss: 0.100199\n[Training] Epoch 023 | Batch 070/204 | Current Loss: 0.100139\n[Training] Epoch 023 | Batch 080/204 | Current Loss: 0.100231\n[Training] Epoch 023 | Batch 090/204 | Current Loss: 0.100200\n[Training] Epoch 023 | Batch 100/204 | Current Loss: 0.100158\n[Training] Epoch 023 | Batch 110/204 | Current Loss: 0.100140\n[Training] Epoch 023 | Batch 120/204 | Current Loss: 0.100237\n[Training] Epoch 023 | Batch 130/204 | Current Loss: 0.100216\n[Training] Epoch 023 | Batch 140/204 | Current Loss: 0.100245\n[Training] Epoch 023 | Batch 150/204 | Current Loss: 0.100319\n[Training] Epoch 023 | Batch 160/204 | Current Loss: 0.100248\n[Training] Epoch 023 | Batch 170/204 | Current Loss: 0.100183\n[Training] Epoch 023 | Batch 180/204 | Current Loss: 0.100110\n[Training] Epoch 023 | Batch 190/204 | Current Loss: 0.100264\n[Training] Epoch 023 | Batch 200/204 | Current Loss: 0.100229\n\n[Training] Epoch 023 Summary:\n  Avg Loss: 0.100220\n  Current LR: 6.86e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 024 | Batch 000/204 | Current Loss: 0.100206\n[Training] Epoch 024 | Batch 010/204 | Current Loss: 0.100287\n[Training] Epoch 024 | Batch 020/204 | Current Loss: 0.100353\n[Training] Epoch 024 | Batch 030/204 | Current Loss: 0.100201\n[Training] Epoch 024 | Batch 040/204 | Current Loss: 0.100261\n[Training] Epoch 024 | Batch 050/204 | Current Loss: 0.100302\n[Training] Epoch 024 | Batch 060/204 | Current Loss: 0.100200\n[Training] Epoch 024 | Batch 070/204 | Current Loss: 0.100237\n[Training] Epoch 024 | Batch 080/204 | Current Loss: 0.100222\n[Training] Epoch 024 | Batch 090/204 | Current Loss: 0.100110\n[Training] Epoch 024 | Batch 100/204 | Current Loss: 0.100099\n[Training] Epoch 024 | Batch 110/204 | Current Loss: 0.100124\n[Training] Epoch 024 | Batch 120/204 | Current Loss: 0.100162\n[Training] Epoch 024 | Batch 130/204 | Current Loss: 0.100279\n[Training] Epoch 024 | Batch 140/204 | Current Loss: 0.100224\n[Training] Epoch 024 | Batch 150/204 | Current Loss: 0.100267\n[Training] Epoch 024 | Batch 160/204 | Current Loss: 0.100329\n[Training] Epoch 024 | Batch 170/204 | Current Loss: 0.100292\n[Training] Epoch 024 | Batch 180/204 | Current Loss: 0.100144\n[Training] Epoch 024 | Batch 190/204 | Current Loss: 0.100310\n[Training] Epoch 024 | Batch 200/204 | Current Loss: 0.100314\n\n[Training] Epoch 024 Summary:\n  Avg Loss: 0.100214\n  Current LR: 5.65e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 025 | Batch 000/204 | Current Loss: 0.100223\n[Training] Epoch 025 | Batch 010/204 | Current Loss: 0.100227\n[Training] Epoch 025 | Batch 020/204 | Current Loss: 0.100195\n[Training] Epoch 025 | Batch 030/204 | Current Loss: 0.100250\n[Training] Epoch 025 | Batch 040/204 | Current Loss: 0.100221\n[Training] Epoch 025 | Batch 050/204 | Current Loss: 0.100299\n[Training] Epoch 025 | Batch 060/204 | Current Loss: 0.100247\n[Training] Epoch 025 | Batch 070/204 | Current Loss: 0.100139\n[Training] Epoch 025 | Batch 080/204 | Current Loss: 0.100253\n[Training] Epoch 025 | Batch 090/204 | Current Loss: 0.100220\n[Training] Epoch 025 | Batch 100/204 | Current Loss: 0.100204\n[Training] Epoch 025 | Batch 110/204 | Current Loss: 0.100309\n[Training] Epoch 025 | Batch 120/204 | Current Loss: 0.100169\n[Training] Epoch 025 | Batch 130/204 | Current Loss: 0.100259\n[Training] Epoch 025 | Batch 140/204 | Current Loss: 0.100202\n[Training] Epoch 025 | Batch 150/204 | Current Loss: 0.100220\n[Training] Epoch 025 | Batch 160/204 | Current Loss: 0.100129\n[Training] Epoch 025 | Batch 170/204 | Current Loss: 0.100145\n[Training] Epoch 025 | Batch 180/204 | Current Loss: 0.100195\n[Training] Epoch 025 | Batch 190/204 | Current Loss: 0.100206\n[Training] Epoch 025 | Batch 200/204 | Current Loss: 0.100152\n\n[Training] Epoch 025 Summary:\n  Avg Loss: 0.100193\n  Current LR: 4.32e-04\n[EarlyStopping] Loss improved (0.100203 → 0.100193). Saving model...\n[Training] Epoch 026 | Batch 000/204 | Current Loss: 0.100198\n[Training] Epoch 026 | Batch 010/204 | Current Loss: 0.100143\n[Training] Epoch 026 | Batch 020/204 | Current Loss: 0.100177\n[Training] Epoch 026 | Batch 030/204 | Current Loss: 0.100104\n[Training] Epoch 026 | Batch 040/204 | Current Loss: 0.100174\n[Training] Epoch 026 | Batch 050/204 | Current Loss: 0.100113\n[Training] Epoch 026 | Batch 060/204 | Current Loss: 0.100200\n[Training] Epoch 026 | Batch 070/204 | Current Loss: 0.100190\n[Training] Epoch 026 | Batch 080/204 | Current Loss: 0.100231\n[Training] Epoch 026 | Batch 090/204 | Current Loss: 0.100154\n[Training] Epoch 026 | Batch 100/204 | Current Loss: 0.100201\n[Training] Epoch 026 | Batch 110/204 | Current Loss: 0.100130\n[Training] Epoch 026 | Batch 120/204 | Current Loss: 0.100226\n[Training] Epoch 026 | Batch 130/204 | Current Loss: 0.100264\n[Training] Epoch 026 | Batch 140/204 | Current Loss: 0.100212\n[Training] Epoch 026 | Batch 150/204 | Current Loss: 0.100181\n[Training] Epoch 026 | Batch 160/204 | Current Loss: 0.100157\n[Training] Epoch 026 | Batch 170/204 | Current Loss: 0.100191\n[Training] Epoch 026 | Batch 180/204 | Current Loss: 0.100213\n[Training] Epoch 026 | Batch 190/204 | Current Loss: 0.100109\n[Training] Epoch 026 | Batch 200/204 | Current Loss: 0.100234\n\n[Training] Epoch 026 Summary:\n  Avg Loss: 0.100168\n  Current LR: 2.99e-04\n[EarlyStopping] Loss improved (0.100193 → 0.100168). Saving model...\n[Training] Epoch 027 | Batch 000/204 | Current Loss: 0.100215\n[Training] Epoch 027 | Batch 010/204 | Current Loss: 0.100150\n[Training] Epoch 027 | Batch 020/204 | Current Loss: 0.100123\n[Training] Epoch 027 | Batch 030/204 | Current Loss: 0.100035\n[Training] Epoch 027 | Batch 040/204 | Current Loss: 0.100057\n[Training] Epoch 027 | Batch 050/204 | Current Loss: 0.100025\n[Training] Epoch 027 | Batch 060/204 | Current Loss: 0.100223\n[Training] Epoch 027 | Batch 070/204 | Current Loss: 0.100245\n[Training] Epoch 027 | Batch 080/204 | Current Loss: 0.100185\n[Training] Epoch 027 | Batch 090/204 | Current Loss: 0.100085\n[Training] Epoch 027 | Batch 100/204 | Current Loss: 0.100154\n[Training] Epoch 027 | Batch 110/204 | Current Loss: 0.100110\n[Training] Epoch 027 | Batch 120/204 | Current Loss: 0.100039\n[Training] Epoch 027 | Batch 130/204 | Current Loss: 0.100105\n[Training] Epoch 027 | Batch 140/204 | Current Loss: 0.100164\n[Training] Epoch 027 | Batch 150/204 | Current Loss: 0.100225\n[Training] Epoch 027 | Batch 160/204 | Current Loss: 0.100268\n[Training] Epoch 027 | Batch 170/204 | Current Loss: 0.100156\n[Training] Epoch 027 | Batch 180/204 | Current Loss: 0.100141\n[Training] Epoch 027 | Batch 190/204 | Current Loss: 0.100186\n[Training] Epoch 027 | Batch 200/204 | Current Loss: 0.100195\n\n[Training] Epoch 027 Summary:\n  Avg Loss: 0.100151\n  Current LR: 1.79e-04\n[EarlyStopping] Loss improved (0.100168 → 0.100151). Saving model...\n[Training] Epoch 028 | Batch 000/204 | Current Loss: 0.100129\n[Training] Epoch 028 | Batch 010/204 | Current Loss: 0.100155\n[Training] Epoch 028 | Batch 020/204 | Current Loss: 0.100194\n[Training] Epoch 028 | Batch 030/204 | Current Loss: 0.100215\n[Training] Epoch 028 | Batch 040/204 | Current Loss: 0.100206\n[Training] Epoch 028 | Batch 050/204 | Current Loss: 0.100102\n[Training] Epoch 028 | Batch 060/204 | Current Loss: 0.100229\n[Training] Epoch 028 | Batch 070/204 | Current Loss: 0.100267\n[Training] Epoch 028 | Batch 080/204 | Current Loss: 0.100147\n[Training] Epoch 028 | Batch 090/204 | Current Loss: 0.100026\n[Training] Epoch 028 | Batch 100/204 | Current Loss: 0.100137\n[Training] Epoch 028 | Batch 110/204 | Current Loss: 0.100104\n[Training] Epoch 028 | Batch 120/204 | Current Loss: 0.100080\n[Training] Epoch 028 | Batch 130/204 | Current Loss: 0.100200\n[Training] Epoch 028 | Batch 140/204 | Current Loss: 0.100210\n[Training] Epoch 028 | Batch 150/204 | Current Loss: 0.100202\n[Training] Epoch 028 | Batch 160/204 | Current Loss: 0.100069\n[Training] Epoch 028 | Batch 170/204 | Current Loss: 0.100083\n[Training] Epoch 028 | Batch 180/204 | Current Loss: 0.100064\n[Training] Epoch 028 | Batch 190/204 | Current Loss: 0.100106\n[Training] Epoch 028 | Batch 200/204 | Current Loss: 0.100144\n\n[Training] Epoch 028 Summary:\n  Avg Loss: 0.100150\n  Current LR: 8.33e-05\n[EarlyStopping] Loss improved (0.100151 → 0.100150). Saving model...\n[Training] Epoch 029 | Batch 000/204 | Current Loss: 0.100206\n[Training] Epoch 029 | Batch 010/204 | Current Loss: 0.100160\n[Training] Epoch 029 | Batch 020/204 | Current Loss: 0.100223\n[Training] Epoch 029 | Batch 030/204 | Current Loss: 0.100063\n[Training] Epoch 029 | Batch 040/204 | Current Loss: 0.100131\n[Training] Epoch 029 | Batch 050/204 | Current Loss: 0.100090\n[Training] Epoch 029 | Batch 060/204 | Current Loss: 0.100108\n[Training] Epoch 029 | Batch 070/204 | Current Loss: 0.100124\n[Training] Epoch 029 | Batch 080/204 | Current Loss: 0.100073\n[Training] Epoch 029 | Batch 090/204 | Current Loss: 0.100060\n[Training] Epoch 029 | Batch 100/204 | Current Loss: 0.100061\n[Training] Epoch 029 | Batch 110/204 | Current Loss: 0.100153\n[Training] Epoch 029 | Batch 120/204 | Current Loss: 0.100151\n[Training] Epoch 029 | Batch 130/204 | Current Loss: 0.100073\n[Training] Epoch 029 | Batch 140/204 | Current Loss: 0.100140\n[Training] Epoch 029 | Batch 150/204 | Current Loss: 0.100116\n[Training] Epoch 029 | Batch 160/204 | Current Loss: 0.100059\n[Training] Epoch 029 | Batch 170/204 | Current Loss: 0.100109\n[Training] Epoch 029 | Batch 180/204 | Current Loss: 0.100094\n[Training] Epoch 029 | Batch 190/204 | Current Loss: 0.100151\n[Training] Epoch 029 | Batch 200/204 | Current Loss: 0.100109\n\n[Training] Epoch 029 Summary:\n  Avg Loss: 0.100149\n  Current LR: 2.21e-05\n[EarlyStopping] Loss improved (0.100150 → 0.100149). Saving model...\n[Training] Epoch 030 | Batch 000/204 | Current Loss: 0.100154\n[Training] Epoch 030 | Batch 010/204 | Current Loss: 0.100175\n[Training] Epoch 030 | Batch 020/204 | Current Loss: 0.100177\n[Training] Epoch 030 | Batch 030/204 | Current Loss: 0.100204\n[Training] Epoch 030 | Batch 040/204 | Current Loss: 0.100084\n[Training] Epoch 030 | Batch 050/204 | Current Loss: 0.100037\n[Training] Epoch 030 | Batch 060/204 | Current Loss: 0.100156\n[Training] Epoch 030 | Batch 070/204 | Current Loss: 0.100085\n[Training] Epoch 030 | Batch 080/204 | Current Loss: 0.100143\n[Training] Epoch 030 | Batch 090/204 | Current Loss: 0.100116\n[Training] Epoch 030 | Batch 100/204 | Current Loss: 0.100245\n[Training] Epoch 030 | Batch 110/204 | Current Loss: 0.100115\n[Training] Epoch 030 | Batch 120/204 | Current Loss: 0.100055\n[Training] Epoch 030 | Batch 130/204 | Current Loss: 0.100135\n[Training] Epoch 030 | Batch 140/204 | Current Loss: 0.100156\n[Training] Epoch 030 | Batch 150/204 | Current Loss: 0.100078\n[Training] Epoch 030 | Batch 160/204 | Current Loss: 0.100133\n[Training] Epoch 030 | Batch 170/204 | Current Loss: 0.100137\n[Training] Epoch 030 | Batch 180/204 | Current Loss: 0.100102\n[Training] Epoch 030 | Batch 190/204 | Current Loss: 0.100019\n[Training] Epoch 030 | Batch 200/204 | Current Loss: 0.100214\n\n[Training] Epoch 030 Summary:\n  Avg Loss: 0.100138\n  Current LR: 8.63e-04\n[EarlyStopping] Loss improved (0.100149 → 0.100138). Saving model...\n[Training] Epoch 031 | Batch 000/204 | Current Loss: 0.100110\n[Training] Epoch 031 | Batch 010/204 | Current Loss: 0.100287\n[Training] Epoch 031 | Batch 020/204 | Current Loss: 0.100605\n[Training] Epoch 031 | Batch 030/204 | Current Loss: 0.100311\n[Training] Epoch 031 | Batch 040/204 | Current Loss: 0.100337\n[Training] Epoch 031 | Batch 050/204 | Current Loss: 0.100318\n[Training] Epoch 031 | Batch 060/204 | Current Loss: 0.100751\n[Training] Epoch 031 | Batch 070/204 | Current Loss: 0.100576\n[Training] Epoch 031 | Batch 080/204 | Current Loss: 0.100298\n[Training] Epoch 031 | Batch 090/204 | Current Loss: 0.100253\n[Training] Epoch 031 | Batch 100/204 | Current Loss: 0.100254\n[Training] Epoch 031 | Batch 110/204 | Current Loss: 0.100137\n[Training] Epoch 031 | Batch 120/204 | Current Loss: 0.100367\n[Training] Epoch 031 | Batch 130/204 | Current Loss: 0.100270\n[Training] Epoch 031 | Batch 140/204 | Current Loss: 0.100135\n[Training] Epoch 031 | Batch 150/204 | Current Loss: 0.100214\n[Training] Epoch 031 | Batch 160/204 | Current Loss: 0.100144\n[Training] Epoch 031 | Batch 170/204 | Current Loss: 0.100203\n[Training] Epoch 031 | Batch 180/204 | Current Loss: 0.100278\n[Training] Epoch 031 | Batch 190/204 | Current Loss: 0.100260\n[Training] Epoch 031 | Batch 200/204 | Current Loss: 0.100212\n\n[Training] Epoch 031 Summary:\n  Avg Loss: 0.100372\n  Current LR: 8.42e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 032 | Batch 000/204 | Current Loss: 0.100519\n[Training] Epoch 032 | Batch 010/204 | Current Loss: 0.100432\n[Training] Epoch 032 | Batch 020/204 | Current Loss: 0.100276\n[Training] Epoch 032 | Batch 030/204 | Current Loss: 0.100312\n[Training] Epoch 032 | Batch 040/204 | Current Loss: 0.100375\n[Training] Epoch 032 | Batch 050/204 | Current Loss: 0.100291\n[Training] Epoch 032 | Batch 060/204 | Current Loss: 0.100212\n[Training] Epoch 032 | Batch 070/204 | Current Loss: 0.100165\n[Training] Epoch 032 | Batch 080/204 | Current Loss: 0.100210\n[Training] Epoch 032 | Batch 090/204 | Current Loss: 0.100260\n[Training] Epoch 032 | Batch 100/204 | Current Loss: 0.100188\n[Training] Epoch 032 | Batch 110/204 | Current Loss: 0.100240\n[Training] Epoch 032 | Batch 120/204 | Current Loss: 0.100058\n[Training] Epoch 032 | Batch 130/204 | Current Loss: 0.100152\n[Training] Epoch 032 | Batch 140/204 | Current Loss: 0.100205\n[Training] Epoch 032 | Batch 150/204 | Current Loss: 0.100195\n[Training] Epoch 032 | Batch 160/204 | Current Loss: 0.100175\n[Training] Epoch 032 | Batch 170/204 | Current Loss: 0.100163\n[Training] Epoch 032 | Batch 180/204 | Current Loss: 0.100245\n[Training] Epoch 032 | Batch 190/204 | Current Loss: 0.100274\n[Training] Epoch 032 | Batch 200/204 | Current Loss: 0.100181\n\n[Training] Epoch 032 Summary:\n  Avg Loss: 0.100230\n  Current LR: 7.81e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 033 | Batch 000/204 | Current Loss: 0.100231\n[Training] Epoch 033 | Batch 010/204 | Current Loss: 0.100221\n[Training] Epoch 033 | Batch 020/204 | Current Loss: 0.100120\n[Training] Epoch 033 | Batch 030/204 | Current Loss: 0.100243\n[Training] Epoch 033 | Batch 040/204 | Current Loss: 0.100101\n[Training] Epoch 033 | Batch 050/204 | Current Loss: 0.100214\n[Training] Epoch 033 | Batch 060/204 | Current Loss: 0.100176\n[Training] Epoch 033 | Batch 070/204 | Current Loss: 0.100201\n[Training] Epoch 033 | Batch 080/204 | Current Loss: 0.100173\n[Training] Epoch 033 | Batch 090/204 | Current Loss: 0.100126\n[Training] Epoch 033 | Batch 100/204 | Current Loss: 0.100240\n[Training] Epoch 033 | Batch 110/204 | Current Loss: 0.100194\n[Training] Epoch 033 | Batch 120/204 | Current Loss: 0.100218\n[Training] Epoch 033 | Batch 130/204 | Current Loss: 0.100149\n[Training] Epoch 033 | Batch 140/204 | Current Loss: 0.100121\n[Training] Epoch 033 | Batch 150/204 | Current Loss: 0.100216\n[Training] Epoch 033 | Batch 160/204 | Current Loss: 0.100133\n[Training] Epoch 033 | Batch 170/204 | Current Loss: 0.100224\n[Training] Epoch 033 | Batch 180/204 | Current Loss: 0.100183\n[Training] Epoch 033 | Batch 190/204 | Current Loss: 0.100297\n[Training] Epoch 033 | Batch 200/204 | Current Loss: 0.100137\n\n[Training] Epoch 033 Summary:\n  Avg Loss: 0.100169\n  Current LR: 6.86e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 034 | Batch 000/204 | Current Loss: 0.100205\n[Training] Epoch 034 | Batch 010/204 | Current Loss: 0.100236\n[Training] Epoch 034 | Batch 020/204 | Current Loss: 0.100191\n[Training] Epoch 034 | Batch 030/204 | Current Loss: 0.100173\n[Training] Epoch 034 | Batch 040/204 | Current Loss: 0.100111\n[Training] Epoch 034 | Batch 050/204 | Current Loss: 0.100110\n[Training] Epoch 034 | Batch 060/204 | Current Loss: 0.100106\n[Training] Epoch 034 | Batch 070/204 | Current Loss: 0.100257\n[Training] Epoch 034 | Batch 080/204 | Current Loss: 0.100257\n[Training] Epoch 034 | Batch 090/204 | Current Loss: 0.100114\n[Training] Epoch 034 | Batch 100/204 | Current Loss: 0.100186\n[Training] Epoch 034 | Batch 110/204 | Current Loss: 0.100221\n[Training] Epoch 034 | Batch 120/204 | Current Loss: 0.100116\n[Training] Epoch 034 | Batch 130/204 | Current Loss: 0.100105\n[Training] Epoch 034 | Batch 140/204 | Current Loss: 0.100188\n[Training] Epoch 034 | Batch 150/204 | Current Loss: 0.100199\n[Training] Epoch 034 | Batch 160/204 | Current Loss: 0.100160\n[Training] Epoch 034 | Batch 170/204 | Current Loss: 0.100107\n[Training] Epoch 034 | Batch 180/204 | Current Loss: 0.100235\n[Training] Epoch 034 | Batch 190/204 | Current Loss: 0.100116\n[Training] Epoch 034 | Batch 200/204 | Current Loss: 0.100195\n\n[Training] Epoch 034 Summary:\n  Avg Loss: 0.100147\n  Current LR: 5.65e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 035 | Batch 000/204 | Current Loss: 0.100193\n[Training] Epoch 035 | Batch 010/204 | Current Loss: 0.100214\n[Training] Epoch 035 | Batch 020/204 | Current Loss: 0.100128\n[Training] Epoch 035 | Batch 030/204 | Current Loss: 0.100175\n[Training] Epoch 035 | Batch 040/204 | Current Loss: 0.100209\n[Training] Epoch 035 | Batch 050/204 | Current Loss: 0.100127\n[Training] Epoch 035 | Batch 060/204 | Current Loss: 0.100131\n[Training] Epoch 035 | Batch 070/204 | Current Loss: 0.100149\n[Training] Epoch 035 | Batch 080/204 | Current Loss: 0.099985\n[Training] Epoch 035 | Batch 090/204 | Current Loss: 0.100098\n[Training] Epoch 035 | Batch 100/204 | Current Loss: 0.100113\n[Training] Epoch 035 | Batch 110/204 | Current Loss: 0.100039\n[Training] Epoch 035 | Batch 120/204 | Current Loss: 0.100237\n[Training] Epoch 035 | Batch 130/204 | Current Loss: 0.100186\n[Training] Epoch 035 | Batch 140/204 | Current Loss: 0.100147\n[Training] Epoch 035 | Batch 150/204 | Current Loss: 0.100138\n[Training] Epoch 035 | Batch 160/204 | Current Loss: 0.100142\n[Training] Epoch 035 | Batch 170/204 | Current Loss: 0.100041\n[Training] Epoch 035 | Batch 180/204 | Current Loss: 0.100110\n[Training] Epoch 035 | Batch 190/204 | Current Loss: 0.100093\n[Training] Epoch 035 | Batch 200/204 | Current Loss: 0.100194\n\n[Training] Epoch 035 Summary:\n  Avg Loss: 0.100133\n  Current LR: 4.32e-04\n[EarlyStopping] Loss improved (0.100138 → 0.100133). Saving model...\n[Training] Epoch 036 | Batch 000/204 | Current Loss: 0.100173\n[Training] Epoch 036 | Batch 010/204 | Current Loss: 0.100158\n[Training] Epoch 036 | Batch 020/204 | Current Loss: 0.100129\n[Training] Epoch 036 | Batch 030/204 | Current Loss: 0.100118\n[Training] Epoch 036 | Batch 040/204 | Current Loss: 0.100109\n[Training] Epoch 036 | Batch 050/204 | Current Loss: 0.100119\n[Training] Epoch 036 | Batch 060/204 | Current Loss: 0.100095\n[Training] Epoch 036 | Batch 070/204 | Current Loss: 0.100093\n[Training] Epoch 036 | Batch 080/204 | Current Loss: 0.100100\n[Training] Epoch 036 | Batch 090/204 | Current Loss: 0.100082\n[Training] Epoch 036 | Batch 100/204 | Current Loss: 0.100148\n[Training] Epoch 036 | Batch 110/204 | Current Loss: 0.100173\n[Training] Epoch 036 | Batch 120/204 | Current Loss: 0.100174\n[Training] Epoch 036 | Batch 130/204 | Current Loss: 0.100012\n[Training] Epoch 036 | Batch 140/204 | Current Loss: 0.100055\n[Training] Epoch 036 | Batch 150/204 | Current Loss: 0.100046\n[Training] Epoch 036 | Batch 160/204 | Current Loss: 0.099995\n[Training] Epoch 036 | Batch 170/204 | Current Loss: 0.100098\n[Training] Epoch 036 | Batch 180/204 | Current Loss: 0.100197\n[Training] Epoch 036 | Batch 190/204 | Current Loss: 0.100116\n[Training] Epoch 036 | Batch 200/204 | Current Loss: 0.100239\n\n[Training] Epoch 036 Summary:\n  Avg Loss: 0.100122\n  Current LR: 2.99e-04\n[EarlyStopping] Loss improved (0.100133 → 0.100122). Saving model...\n[Training] Epoch 037 | Batch 000/204 | Current Loss: 0.100133\n[Training] Epoch 037 | Batch 010/204 | Current Loss: 0.100237\n[Training] Epoch 037 | Batch 020/204 | Current Loss: 0.100069\n[Training] Epoch 037 | Batch 030/204 | Current Loss: 0.100057\n[Training] Epoch 037 | Batch 040/204 | Current Loss: 0.099983\n[Training] Epoch 037 | Batch 050/204 | Current Loss: 0.100220\n[Training] Epoch 037 | Batch 060/204 | Current Loss: 0.100191\n[Training] Epoch 037 | Batch 070/204 | Current Loss: 0.100098\n[Training] Epoch 037 | Batch 080/204 | Current Loss: 0.100246\n[Training] Epoch 037 | Batch 090/204 | Current Loss: 0.100169\n[Training] Epoch 037 | Batch 100/204 | Current Loss: 0.100101\n[Training] Epoch 037 | Batch 110/204 | Current Loss: 0.100136\n[Training] Epoch 037 | Batch 120/204 | Current Loss: 0.100266\n[Training] Epoch 037 | Batch 130/204 | Current Loss: 0.100084\n[Training] Epoch 037 | Batch 140/204 | Current Loss: 0.100060\n[Training] Epoch 037 | Batch 150/204 | Current Loss: 0.100126\n[Training] Epoch 037 | Batch 160/204 | Current Loss: 0.100127\n[Training] Epoch 037 | Batch 170/204 | Current Loss: 0.100106\n[Training] Epoch 037 | Batch 180/204 | Current Loss: 0.100084\n[Training] Epoch 037 | Batch 190/204 | Current Loss: 0.100228\n[Training] Epoch 037 | Batch 200/204 | Current Loss: 0.100146\n\n[Training] Epoch 037 Summary:\n  Avg Loss: 0.100115\n  Current LR: 1.79e-04\n[EarlyStopping] Loss improved (0.100122 → 0.100115). Saving model...\n[Training] Epoch 038 | Batch 000/204 | Current Loss: 0.100106\n[Training] Epoch 038 | Batch 010/204 | Current Loss: 0.100181\n[Training] Epoch 038 | Batch 020/204 | Current Loss: 0.100074\n[Training] Epoch 038 | Batch 030/204 | Current Loss: 0.100112\n[Training] Epoch 038 | Batch 040/204 | Current Loss: 0.100166\n[Training] Epoch 038 | Batch 050/204 | Current Loss: 0.100057\n[Training] Epoch 038 | Batch 060/204 | Current Loss: 0.100054\n[Training] Epoch 038 | Batch 070/204 | Current Loss: 0.100191\n[Training] Epoch 038 | Batch 080/204 | Current Loss: 0.100105\n[Training] Epoch 038 | Batch 090/204 | Current Loss: 0.100144\n[Training] Epoch 038 | Batch 100/204 | Current Loss: 0.100146\n[Training] Epoch 038 | Batch 110/204 | Current Loss: 0.100196\n[Training] Epoch 038 | Batch 120/204 | Current Loss: 0.100114\n[Training] Epoch 038 | Batch 130/204 | Current Loss: 0.100211\n[Training] Epoch 038 | Batch 140/204 | Current Loss: 0.100143\n[Training] Epoch 038 | Batch 150/204 | Current Loss: 0.100104\n[Training] Epoch 038 | Batch 160/204 | Current Loss: 0.100014\n[Training] Epoch 038 | Batch 170/204 | Current Loss: 0.100057\n[Training] Epoch 038 | Batch 180/204 | Current Loss: 0.100062\n[Training] Epoch 038 | Batch 190/204 | Current Loss: 0.100067\n[Training] Epoch 038 | Batch 200/204 | Current Loss: 0.100166\n\n[Training] Epoch 038 Summary:\n  Avg Loss: 0.100104\n  Current LR: 8.33e-05\n[EarlyStopping] Loss improved (0.100115 → 0.100104). Saving model...\n[Training] Epoch 039 | Batch 000/204 | Current Loss: 0.100083\n[Training] Epoch 039 | Batch 010/204 | Current Loss: 0.100110\n[Training] Epoch 039 | Batch 020/204 | Current Loss: 0.100125\n[Training] Epoch 039 | Batch 030/204 | Current Loss: 0.100077\n[Training] Epoch 039 | Batch 040/204 | Current Loss: 0.100127\n[Training] Epoch 039 | Batch 050/204 | Current Loss: 0.100053\n[Training] Epoch 039 | Batch 060/204 | Current Loss: 0.100076\n[Training] Epoch 039 | Batch 070/204 | Current Loss: 0.100170\n[Training] Epoch 039 | Batch 080/204 | Current Loss: 0.100103\n[Training] Epoch 039 | Batch 090/204 | Current Loss: 0.100185\n[Training] Epoch 039 | Batch 100/204 | Current Loss: 0.100121\n[Training] Epoch 039 | Batch 110/204 | Current Loss: 0.100055\n[Training] Epoch 039 | Batch 120/204 | Current Loss: 0.100023\n[Training] Epoch 039 | Batch 130/204 | Current Loss: 0.100076\n[Training] Epoch 039 | Batch 140/204 | Current Loss: 0.100080\n[Training] Epoch 039 | Batch 150/204 | Current Loss: 0.100089\n[Training] Epoch 039 | Batch 160/204 | Current Loss: 0.100102\n[Training] Epoch 039 | Batch 170/204 | Current Loss: 0.100198\n[Training] Epoch 039 | Batch 180/204 | Current Loss: 0.100141\n[Training] Epoch 039 | Batch 190/204 | Current Loss: 0.100065\n[Training] Epoch 039 | Batch 200/204 | Current Loss: 0.100160\n\n[Training] Epoch 039 Summary:\n  Avg Loss: 0.100104\n  Current LR: 2.21e-05\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 040 | Batch 000/204 | Current Loss: 0.100004\n[Training] Epoch 040 | Batch 010/204 | Current Loss: 0.100175\n[Training] Epoch 040 | Batch 020/204 | Current Loss: 0.100049\n[Training] Epoch 040 | Batch 030/204 | Current Loss: 0.100035\n[Training] Epoch 040 | Batch 040/204 | Current Loss: 0.100054\n[Training] Epoch 040 | Batch 050/204 | Current Loss: 0.100074\n[Training] Epoch 040 | Batch 060/204 | Current Loss: 0.100121\n[Training] Epoch 040 | Batch 070/204 | Current Loss: 0.100064\n[Training] Epoch 040 | Batch 080/204 | Current Loss: 0.100087\n[Training] Epoch 040 | Batch 090/204 | Current Loss: 0.100060\n[Training] Epoch 040 | Batch 100/204 | Current Loss: 0.100141\n[Training] Epoch 040 | Batch 110/204 | Current Loss: 0.100041\n[Training] Epoch 040 | Batch 120/204 | Current Loss: 0.100006\n[Training] Epoch 040 | Batch 130/204 | Current Loss: 0.099999\n[Training] Epoch 040 | Batch 140/204 | Current Loss: 0.100061\n[Training] Epoch 040 | Batch 150/204 | Current Loss: 0.100082\n[Training] Epoch 040 | Batch 160/204 | Current Loss: 0.100180\n[Training] Epoch 040 | Batch 170/204 | Current Loss: 0.099997\n[Training] Epoch 040 | Batch 180/204 | Current Loss: 0.100167\n[Training] Epoch 040 | Batch 190/204 | Current Loss: 0.100146\n[Training] Epoch 040 | Batch 200/204 | Current Loss: 0.100022\n\n[Training] Epoch 040 Summary:\n  Avg Loss: 0.100096\n  Current LR: 8.63e-04\n[EarlyStopping] Loss improved (0.100104 → 0.100096). Saving model...\n[Training] Epoch 041 | Batch 000/204 | Current Loss: 0.100069\n[Training] Epoch 041 | Batch 010/204 | Current Loss: 0.100855\n[Training] Epoch 041 | Batch 020/204 | Current Loss: 0.100613\n[Training] Epoch 041 | Batch 030/204 | Current Loss: 0.100888\n[Training] Epoch 041 | Batch 040/204 | Current Loss: 0.100989\n[Training] Epoch 041 | Batch 050/204 | Current Loss: 0.101455\n[Training] Epoch 041 | Batch 060/204 | Current Loss: 0.100558\n[Training] Epoch 041 | Batch 070/204 | Current Loss: 0.100764\n[Training] Epoch 041 | Batch 080/204 | Current Loss: 0.100712\n[Training] Epoch 041 | Batch 090/204 | Current Loss: 0.100425\n[Training] Epoch 041 | Batch 100/204 | Current Loss: 0.100319\n[Training] Epoch 041 | Batch 110/204 | Current Loss: 0.100427\n[Training] Epoch 041 | Batch 120/204 | Current Loss: 0.100335\n[Training] Epoch 041 | Batch 130/204 | Current Loss: 0.100327\n[Training] Epoch 041 | Batch 140/204 | Current Loss: 0.100326\n[Training] Epoch 041 | Batch 150/204 | Current Loss: 0.100229\n[Training] Epoch 041 | Batch 160/204 | Current Loss: 0.100288\n[Training] Epoch 041 | Batch 170/204 | Current Loss: 0.100245\n[Training] Epoch 041 | Batch 180/204 | Current Loss: 0.100197\n[Training] Epoch 041 | Batch 190/204 | Current Loss: 0.100249\n[Training] Epoch 041 | Batch 200/204 | Current Loss: 0.100192\n\n[Training] Epoch 041 Summary:\n  Avg Loss: 0.100483\n  Current LR: 8.42e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 042 | Batch 000/204 | Current Loss: 0.100165\n[Training] Epoch 042 | Batch 010/204 | Current Loss: 0.100207\n[Training] Epoch 042 | Batch 020/204 | Current Loss: 0.100205\n[Training] Epoch 042 | Batch 030/204 | Current Loss: 0.100146\n[Training] Epoch 042 | Batch 040/204 | Current Loss: 0.100169\n[Training] Epoch 042 | Batch 050/204 | Current Loss: 0.100216\n[Training] Epoch 042 | Batch 060/204 | Current Loss: 0.100203\n[Training] Epoch 042 | Batch 070/204 | Current Loss: 0.100143\n[Training] Epoch 042 | Batch 080/204 | Current Loss: 0.100194\n[Training] Epoch 042 | Batch 090/204 | Current Loss: 0.100004\n[Training] Epoch 042 | Batch 100/204 | Current Loss: 0.100068\n[Training] Epoch 042 | Batch 110/204 | Current Loss: 0.100115\n[Training] Epoch 042 | Batch 120/204 | Current Loss: 0.100172\n[Training] Epoch 042 | Batch 130/204 | Current Loss: 0.100179\n[Training] Epoch 042 | Batch 140/204 | Current Loss: 0.100182\n[Training] Epoch 042 | Batch 150/204 | Current Loss: 0.100038\n[Training] Epoch 042 | Batch 160/204 | Current Loss: 0.100079\n[Training] Epoch 042 | Batch 170/204 | Current Loss: 0.100053\n[Training] Epoch 042 | Batch 180/204 | Current Loss: 0.100167\n[Training] Epoch 042 | Batch 190/204 | Current Loss: 0.100186\n[Training] Epoch 042 | Batch 200/204 | Current Loss: 0.100177\n\n[Training] Epoch 042 Summary:\n  Avg Loss: 0.100170\n  Current LR: 7.81e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 043 | Batch 000/204 | Current Loss: 0.100116\n[Training] Epoch 043 | Batch 010/204 | Current Loss: 0.100159\n[Training] Epoch 043 | Batch 020/204 | Current Loss: 0.100174\n[Training] Epoch 043 | Batch 030/204 | Current Loss: 0.100162\n[Training] Epoch 043 | Batch 040/204 | Current Loss: 0.100171\n[Training] Epoch 043 | Batch 050/204 | Current Loss: 0.100108\n[Training] Epoch 043 | Batch 060/204 | Current Loss: 0.100109\n[Training] Epoch 043 | Batch 070/204 | Current Loss: 0.100147\n[Training] Epoch 043 | Batch 080/204 | Current Loss: 0.100072\n[Training] Epoch 043 | Batch 090/204 | Current Loss: 0.099966\n[Training] Epoch 043 | Batch 100/204 | Current Loss: 0.100021\n[Training] Epoch 043 | Batch 110/204 | Current Loss: 0.100152\n[Training] Epoch 043 | Batch 120/204 | Current Loss: 0.100164\n[Training] Epoch 043 | Batch 130/204 | Current Loss: 0.100158\n[Training] Epoch 043 | Batch 140/204 | Current Loss: 0.100147\n[Training] Epoch 043 | Batch 150/204 | Current Loss: 0.100230\n[Training] Epoch 043 | Batch 160/204 | Current Loss: 0.100232\n[Training] Epoch 043 | Batch 170/204 | Current Loss: 0.100171\n[Training] Epoch 043 | Batch 180/204 | Current Loss: 0.100225\n[Training] Epoch 043 | Batch 190/204 | Current Loss: 0.100099\n[Training] Epoch 043 | Batch 200/204 | Current Loss: 0.100102\n\n[Training] Epoch 043 Summary:\n  Avg Loss: 0.100150\n  Current LR: 6.86e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 044 | Batch 000/204 | Current Loss: 0.100115\n[Training] Epoch 044 | Batch 010/204 | Current Loss: 0.100043\n[Training] Epoch 044 | Batch 020/204 | Current Loss: 0.100115\n[Training] Epoch 044 | Batch 030/204 | Current Loss: 0.100142\n[Training] Epoch 044 | Batch 040/204 | Current Loss: 0.100259\n[Training] Epoch 044 | Batch 050/204 | Current Loss: 0.100103\n[Training] Epoch 044 | Batch 060/204 | Current Loss: 0.100150\n[Training] Epoch 044 | Batch 070/204 | Current Loss: 0.100114\n[Training] Epoch 044 | Batch 080/204 | Current Loss: 0.100053\n[Training] Epoch 044 | Batch 090/204 | Current Loss: 0.100140\n[Training] Epoch 044 | Batch 100/204 | Current Loss: 0.100105\n[Training] Epoch 044 | Batch 110/204 | Current Loss: 0.100209\n[Training] Epoch 044 | Batch 120/204 | Current Loss: 0.100173\n[Training] Epoch 044 | Batch 130/204 | Current Loss: 0.100193\n[Training] Epoch 044 | Batch 140/204 | Current Loss: 0.099981\n[Training] Epoch 044 | Batch 150/204 | Current Loss: 0.100093\n[Training] Epoch 044 | Batch 160/204 | Current Loss: 0.100063\n[Training] Epoch 044 | Batch 170/204 | Current Loss: 0.100088\n[Training] Epoch 044 | Batch 180/204 | Current Loss: 0.100103\n[Training] Epoch 044 | Batch 190/204 | Current Loss: 0.100065\n[Training] Epoch 044 | Batch 200/204 | Current Loss: 0.100079\n\n[Training] Epoch 044 Summary:\n  Avg Loss: 0.100131\n  Current LR: 5.65e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 045 | Batch 000/204 | Current Loss: 0.100180\n[Training] Epoch 045 | Batch 010/204 | Current Loss: 0.100127\n[Training] Epoch 045 | Batch 020/204 | Current Loss: 0.100144\n[Training] Epoch 045 | Batch 030/204 | Current Loss: 0.100085\n[Training] Epoch 045 | Batch 040/204 | Current Loss: 0.100170\n[Training] Epoch 045 | Batch 050/204 | Current Loss: 0.100140\n[Training] Epoch 045 | Batch 060/204 | Current Loss: 0.100127\n[Training] Epoch 045 | Batch 070/204 | Current Loss: 0.100159\n[Training] Epoch 045 | Batch 080/204 | Current Loss: 0.100090\n[Training] Epoch 045 | Batch 090/204 | Current Loss: 0.099976\n[Training] Epoch 045 | Batch 100/204 | Current Loss: 0.100067\n[Training] Epoch 045 | Batch 110/204 | Current Loss: 0.100180\n[Training] Epoch 045 | Batch 120/204 | Current Loss: 0.100079\n[Training] Epoch 045 | Batch 130/204 | Current Loss: 0.100142\n[Training] Epoch 045 | Batch 140/204 | Current Loss: 0.100166\n[Training] Epoch 045 | Batch 150/204 | Current Loss: 0.100049\n[Training] Epoch 045 | Batch 160/204 | Current Loss: 0.100117\n[Training] Epoch 045 | Batch 170/204 | Current Loss: 0.100023\n[Training] Epoch 045 | Batch 180/204 | Current Loss: 0.100141\n[Training] Epoch 045 | Batch 190/204 | Current Loss: 0.100045\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 17:39:06,770] Trial 18 finished with value: 0.1000964782547717 and parameters: {'lr': 0.0008632097996657894, 'batch_size': 64, 'bottleneck_width': 512, 'dropout_rate': 0.3140554789948301, 'alpha': 0.5913638011196574, 'weight_decay': 3.485896826371103e-05}. Best is trial 18 with value: 0.1000964782547717.\n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 045 | Batch 200/204 | Current Loss: 0.100013\n\n[Training] Epoch 045 Summary:\n  Avg Loss: 0.100116\n  Current LR: 4.32e-04\n[EarlyStopping] Counter: 5/5\n\n[Training] Early stopping triggered\n\n[Optuna] Trial 18 completed with best loss: 0.100096\n\n==================================================\nStarting Optuna Trial 19\n==================================================\n\n[Optuna] Suggested hyperparameters:\n  lr: 0.0009587140972521137\n  batch_size: 64\n  bottleneck_width: 512\n  dropout_rate: 0.31858741775550603\n  alpha: 0.6130990679467545\n  weight_decay: 1.467896189415769e-06\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 64, Total batches: 204\n\n[Model] Initializing model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 512\n  - Dropout rate: 0.31858741775550603\n[Model] Architecture initialized successfully\n[Model] Total parameters: 1,347,904\n[Optimizer] Initialized with lr=9.59e-04, weight_decay=1.47e-06\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.6130990679467545\n\n[Training] Starting training...\n[Training] Epoch 001 | Batch 000/204 | Current Loss: 1.521923\n[Training] Epoch 001 | Batch 010/204 | Current Loss: 0.155725\n[Training] Epoch 001 | Batch 020/204 | Current Loss: 0.124093\n[Training] Epoch 001 | Batch 030/204 | Current Loss: 0.112283\n[Training] Epoch 001 | Batch 040/204 | Current Loss: 0.109735\n[Training] Epoch 001 | Batch 050/204 | Current Loss: 0.114116\n[Training] Epoch 001 | Batch 060/204 | Current Loss: 0.108212\n[Training] Epoch 001 | Batch 070/204 | Current Loss: 0.105938\n[Training] Epoch 001 | Batch 080/204 | Current Loss: 0.105974\n[Training] Epoch 001 | Batch 090/204 | Current Loss: 0.110828\n[Training] Epoch 001 | Batch 100/204 | Current Loss: 0.105641\n[Training] Epoch 001 | Batch 110/204 | Current Loss: 0.104184\n[Training] Epoch 001 | Batch 120/204 | Current Loss: 0.106308\n[Training] Epoch 001 | Batch 130/204 | Current Loss: 0.104033\n[Training] Epoch 001 | Batch 140/204 | Current Loss: 0.104192\n[Training] Epoch 001 | Batch 150/204 | Current Loss: 0.104727\n[Training] Epoch 001 | Batch 160/204 | Current Loss: 0.106329\n[Training] Epoch 001 | Batch 170/204 | Current Loss: 0.105694\n[Training] Epoch 001 | Batch 180/204 | Current Loss: 0.108341\n[Training] Epoch 001 | Batch 190/204 | Current Loss: 0.103697\n[Training] Epoch 001 | Batch 200/204 | Current Loss: 0.102533\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.127396\n  Current LR: 9.35e-04\n[EarlyStopping] Loss improved (inf → 0.127396). Saving model...\n[Training] Epoch 002 | Batch 000/204 | Current Loss: 0.103888\n[Training] Epoch 002 | Batch 010/204 | Current Loss: 0.103710\n[Training] Epoch 002 | Batch 020/204 | Current Loss: 0.103749\n[Training] Epoch 002 | Batch 030/204 | Current Loss: 0.102974\n[Training] Epoch 002 | Batch 040/204 | Current Loss: 0.108864\n[Training] Epoch 002 | Batch 050/204 | Current Loss: 0.102568\n[Training] Epoch 002 | Batch 060/204 | Current Loss: 0.104519\n[Training] Epoch 002 | Batch 070/204 | Current Loss: 0.103250\n[Training] Epoch 002 | Batch 080/204 | Current Loss: 0.102359\n[Training] Epoch 002 | Batch 090/204 | Current Loss: 0.103865\n[Training] Epoch 002 | Batch 100/204 | Current Loss: 0.102607\n[Training] Epoch 002 | Batch 110/204 | Current Loss: 0.103912\n[Training] Epoch 002 | Batch 120/204 | Current Loss: 0.103254\n[Training] Epoch 002 | Batch 130/204 | Current Loss: 0.103592\n[Training] Epoch 002 | Batch 140/204 | Current Loss: 0.102652\n[Training] Epoch 002 | Batch 150/204 | Current Loss: 0.102192\n[Training] Epoch 002 | Batch 160/204 | Current Loss: 0.101860\n[Training] Epoch 002 | Batch 170/204 | Current Loss: 0.101998\n[Training] Epoch 002 | Batch 180/204 | Current Loss: 0.101909\n[Training] Epoch 002 | Batch 190/204 | Current Loss: 0.107523\n[Training] Epoch 002 | Batch 200/204 | Current Loss: 0.102809\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.103524\n  Current LR: 8.67e-04\n[EarlyStopping] Loss improved (0.127396 → 0.103524). Saving model...\n[Training] Epoch 003 | Batch 000/204 | Current Loss: 0.105535\n[Training] Epoch 003 | Batch 010/204 | Current Loss: 0.103045\n[Training] Epoch 003 | Batch 020/204 | Current Loss: 0.104302\n[Training] Epoch 003 | Batch 030/204 | Current Loss: 0.102138\n[Training] Epoch 003 | Batch 040/204 | Current Loss: 0.102117\n[Training] Epoch 003 | Batch 050/204 | Current Loss: 0.101969\n[Training] Epoch 003 | Batch 060/204 | Current Loss: 0.102726\n[Training] Epoch 003 | Batch 070/204 | Current Loss: 0.103322\n[Training] Epoch 003 | Batch 080/204 | Current Loss: 0.103473\n[Training] Epoch 003 | Batch 090/204 | Current Loss: 0.102838\n[Training] Epoch 003 | Batch 100/204 | Current Loss: 0.101903\n[Training] Epoch 003 | Batch 110/204 | Current Loss: 0.104591\n[Training] Epoch 003 | Batch 120/204 | Current Loss: 0.102515\n[Training] Epoch 003 | Batch 130/204 | Current Loss: 0.103719\n[Training] Epoch 003 | Batch 140/204 | Current Loss: 0.101734\n[Training] Epoch 003 | Batch 150/204 | Current Loss: 0.102660\n[Training] Epoch 003 | Batch 160/204 | Current Loss: 0.101284\n[Training] Epoch 003 | Batch 170/204 | Current Loss: 0.101407\n[Training] Epoch 003 | Batch 180/204 | Current Loss: 0.102582\n[Training] Epoch 003 | Batch 190/204 | Current Loss: 0.101630\n[Training] Epoch 003 | Batch 200/204 | Current Loss: 0.101721\n\n[Training] Epoch 003 Summary:\n  Avg Loss: 0.102679\n  Current LR: 7.61e-04\n[EarlyStopping] Loss improved (0.103524 → 0.102679). Saving model...\n[Training] Epoch 004 | Batch 000/204 | Current Loss: 0.101379\n[Training] Epoch 004 | Batch 010/204 | Current Loss: 0.101709\n[Training] Epoch 004 | Batch 020/204 | Current Loss: 0.101286\n[Training] Epoch 004 | Batch 030/204 | Current Loss: 0.101976\n[Training] Epoch 004 | Batch 040/204 | Current Loss: 0.101346\n[Training] Epoch 004 | Batch 050/204 | Current Loss: 0.101296\n[Training] Epoch 004 | Batch 060/204 | Current Loss: 0.102102\n[Training] Epoch 004 | Batch 070/204 | Current Loss: 0.101287\n[Training] Epoch 004 | Batch 080/204 | Current Loss: 0.100982\n[Training] Epoch 004 | Batch 090/204 | Current Loss: 0.101045\n[Training] Epoch 004 | Batch 100/204 | Current Loss: 0.101306\n[Training] Epoch 004 | Batch 110/204 | Current Loss: 0.100975\n[Training] Epoch 004 | Batch 120/204 | Current Loss: 0.101012\n[Training] Epoch 004 | Batch 130/204 | Current Loss: 0.101333\n[Training] Epoch 004 | Batch 140/204 | Current Loss: 0.101463\n[Training] Epoch 004 | Batch 150/204 | Current Loss: 0.100924\n[Training] Epoch 004 | Batch 160/204 | Current Loss: 0.101240\n[Training] Epoch 004 | Batch 170/204 | Current Loss: 0.101090\n[Training] Epoch 004 | Batch 180/204 | Current Loss: 0.100955\n[Training] Epoch 004 | Batch 190/204 | Current Loss: 0.100891\n[Training] Epoch 004 | Batch 200/204 | Current Loss: 0.101194\n\n[Training] Epoch 004 Summary:\n  Avg Loss: 0.101447\n  Current LR: 6.28e-04\n[EarlyStopping] Loss improved (0.102679 → 0.101447). Saving model...\n[Training] Epoch 005 | Batch 000/204 | Current Loss: 0.102268\n[Training] Epoch 005 | Batch 010/204 | Current Loss: 0.101268\n[Training] Epoch 005 | Batch 020/204 | Current Loss: 0.101341\n[Training] Epoch 005 | Batch 030/204 | Current Loss: 0.101227\n[Training] Epoch 005 | Batch 040/204 | Current Loss: 0.101092\n[Training] Epoch 005 | Batch 050/204 | Current Loss: 0.101146\n[Training] Epoch 005 | Batch 060/204 | Current Loss: 0.100865\n[Training] Epoch 005 | Batch 070/204 | Current Loss: 0.101135\n[Training] Epoch 005 | Batch 080/204 | Current Loss: 0.100927\n[Training] Epoch 005 | Batch 090/204 | Current Loss: 0.100842\n[Training] Epoch 005 | Batch 100/204 | Current Loss: 0.101070\n[Training] Epoch 005 | Batch 110/204 | Current Loss: 0.101044\n[Training] Epoch 005 | Batch 120/204 | Current Loss: 0.100940\n[Training] Epoch 005 | Batch 130/204 | Current Loss: 0.100870\n[Training] Epoch 005 | Batch 140/204 | Current Loss: 0.100812\n[Training] Epoch 005 | Batch 150/204 | Current Loss: 0.100954\n[Training] Epoch 005 | Batch 160/204 | Current Loss: 0.100798\n[Training] Epoch 005 | Batch 170/204 | Current Loss: 0.100811\n[Training] Epoch 005 | Batch 180/204 | Current Loss: 0.100714\n[Training] Epoch 005 | Batch 190/204 | Current Loss: 0.100877\n[Training] Epoch 005 | Batch 200/204 | Current Loss: 0.100843\n\n[Training] Epoch 005 Summary:\n  Avg Loss: 0.101049\n  Current LR: 4.80e-04\n[EarlyStopping] Loss improved (0.101447 → 0.101049). Saving model...\n[Training] Epoch 006 | Batch 000/204 | Current Loss: 0.100906\n[Training] Epoch 006 | Batch 010/204 | Current Loss: 0.100676\n[Training] Epoch 006 | Batch 020/204 | Current Loss: 0.100920\n[Training] Epoch 006 | Batch 030/204 | Current Loss: 0.100664\n[Training] Epoch 006 | Batch 040/204 | Current Loss: 0.100627\n[Training] Epoch 006 | Batch 050/204 | Current Loss: 0.100701\n[Training] Epoch 006 | Batch 060/204 | Current Loss: 0.100801\n[Training] Epoch 006 | Batch 070/204 | Current Loss: 0.100728\n[Training] Epoch 006 | Batch 080/204 | Current Loss: 0.100681\n[Training] Epoch 006 | Batch 090/204 | Current Loss: 0.100629\n[Training] Epoch 006 | Batch 100/204 | Current Loss: 0.100751\n[Training] Epoch 006 | Batch 110/204 | Current Loss: 0.100856\n[Training] Epoch 006 | Batch 120/204 | Current Loss: 0.100558\n[Training] Epoch 006 | Batch 130/204 | Current Loss: 0.100704\n[Training] Epoch 006 | Batch 140/204 | Current Loss: 0.100565\n[Training] Epoch 006 | Batch 150/204 | Current Loss: 0.100584\n[Training] Epoch 006 | Batch 160/204 | Current Loss: 0.100798\n[Training] Epoch 006 | Batch 170/204 | Current Loss: 0.100820\n[Training] Epoch 006 | Batch 180/204 | Current Loss: 0.100667\n[Training] Epoch 006 | Batch 190/204 | Current Loss: 0.100574\n[Training] Epoch 006 | Batch 200/204 | Current Loss: 0.100770\n\n[Training] Epoch 006 Summary:\n  Avg Loss: 0.100812\n  Current LR: 3.32e-04\n[EarlyStopping] Loss improved (0.101049 → 0.100812). Saving model...\n[Training] Epoch 007 | Batch 000/204 | Current Loss: 0.102876\n[Training] Epoch 007 | Batch 010/204 | Current Loss: 0.101622\n[Training] Epoch 007 | Batch 020/204 | Current Loss: 0.100964\n[Training] Epoch 007 | Batch 030/204 | Current Loss: 0.101029\n[Training] Epoch 007 | Batch 040/204 | Current Loss: 0.100883\n[Training] Epoch 007 | Batch 050/204 | Current Loss: 0.100932\n[Training] Epoch 007 | Batch 060/204 | Current Loss: 0.101064\n[Training] Epoch 007 | Batch 070/204 | Current Loss: 0.100714\n[Training] Epoch 007 | Batch 080/204 | Current Loss: 0.100700\n[Training] Epoch 007 | Batch 090/204 | Current Loss: 0.100973\n[Training] Epoch 007 | Batch 100/204 | Current Loss: 0.100786\n[Training] Epoch 007 | Batch 110/204 | Current Loss: 0.100643\n[Training] Epoch 007 | Batch 120/204 | Current Loss: 0.100640\n[Training] Epoch 007 | Batch 130/204 | Current Loss: 0.100564\n[Training] Epoch 007 | Batch 140/204 | Current Loss: 0.100547\n[Training] Epoch 007 | Batch 150/204 | Current Loss: 0.100629\n[Training] Epoch 007 | Batch 160/204 | Current Loss: 0.100906\n[Training] Epoch 007 | Batch 170/204 | Current Loss: 0.100665\n[Training] Epoch 007 | Batch 180/204 | Current Loss: 0.100851\n[Training] Epoch 007 | Batch 190/204 | Current Loss: 0.100639\n[Training] Epoch 007 | Batch 200/204 | Current Loss: 0.100586\n\n[Training] Epoch 007 Summary:\n  Avg Loss: 0.100812\n  Current LR: 1.98e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 008 | Batch 000/204 | Current Loss: 0.100601\n[Training] Epoch 008 | Batch 010/204 | Current Loss: 0.100630\n[Training] Epoch 008 | Batch 020/204 | Current Loss: 0.100536\n[Training] Epoch 008 | Batch 030/204 | Current Loss: 0.100693\n[Training] Epoch 008 | Batch 040/204 | Current Loss: 0.100660\n[Training] Epoch 008 | Batch 050/204 | Current Loss: 0.100710\n[Training] Epoch 008 | Batch 060/204 | Current Loss: 0.100708\n[Training] Epoch 008 | Batch 070/204 | Current Loss: 0.100683\n[Training] Epoch 008 | Batch 080/204 | Current Loss: 0.100535\n[Training] Epoch 008 | Batch 090/204 | Current Loss: 0.100495\n[Training] Epoch 008 | Batch 100/204 | Current Loss: 0.100664\n[Training] Epoch 008 | Batch 110/204 | Current Loss: 0.100464\n[Training] Epoch 008 | Batch 120/204 | Current Loss: 0.100586\n[Training] Epoch 008 | Batch 130/204 | Current Loss: 0.100547\n[Training] Epoch 008 | Batch 140/204 | Current Loss: 0.100773\n[Training] Epoch 008 | Batch 150/204 | Current Loss: 0.100627\n[Training] Epoch 008 | Batch 160/204 | Current Loss: 0.101020\n[Training] Epoch 008 | Batch 170/204 | Current Loss: 0.100630\n[Training] Epoch 008 | Batch 180/204 | Current Loss: 0.100757\n[Training] Epoch 008 | Batch 190/204 | Current Loss: 0.100852\n[Training] Epoch 008 | Batch 200/204 | Current Loss: 0.100626\n\n[Training] Epoch 008 Summary:\n  Avg Loss: 0.100613\n  Current LR: 9.25e-05\n[EarlyStopping] Loss improved (0.100812 → 0.100613). Saving model...\n[Training] Epoch 009 | Batch 000/204 | Current Loss: 0.100608\n[Training] Epoch 009 | Batch 010/204 | Current Loss: 0.100618\n[Training] Epoch 009 | Batch 020/204 | Current Loss: 0.100759\n[Training] Epoch 009 | Batch 030/204 | Current Loss: 0.100600\n[Training] Epoch 009 | Batch 040/204 | Current Loss: 0.100490\n[Training] Epoch 009 | Batch 050/204 | Current Loss: 0.100589\n[Training] Epoch 009 | Batch 060/204 | Current Loss: 0.100660\n[Training] Epoch 009 | Batch 070/204 | Current Loss: 0.100610\n[Training] Epoch 009 | Batch 080/204 | Current Loss: 0.100750\n[Training] Epoch 009 | Batch 090/204 | Current Loss: 0.100456\n[Training] Epoch 009 | Batch 100/204 | Current Loss: 0.100580\n[Training] Epoch 009 | Batch 110/204 | Current Loss: 0.100628\n[Training] Epoch 009 | Batch 120/204 | Current Loss: 0.100592\n[Training] Epoch 009 | Batch 130/204 | Current Loss: 0.100466\n[Training] Epoch 009 | Batch 140/204 | Current Loss: 0.100474\n[Training] Epoch 009 | Batch 150/204 | Current Loss: 0.100693\n[Training] Epoch 009 | Batch 160/204 | Current Loss: 0.100494\n[Training] Epoch 009 | Batch 170/204 | Current Loss: 0.100522\n[Training] Epoch 009 | Batch 180/204 | Current Loss: 0.100688\n[Training] Epoch 009 | Batch 190/204 | Current Loss: 0.100536\n[Training] Epoch 009 | Batch 200/204 | Current Loss: 0.100563\n\n[Training] Epoch 009 Summary:\n  Avg Loss: 0.100572\n  Current LR: 2.44e-05\n[EarlyStopping] Loss improved (0.100613 → 0.100572). Saving model...\n[Training] Epoch 010 | Batch 000/204 | Current Loss: 0.100596\n[Training] Epoch 010 | Batch 010/204 | Current Loss: 0.100542\n[Training] Epoch 010 | Batch 020/204 | Current Loss: 0.100498\n[Training] Epoch 010 | Batch 030/204 | Current Loss: 0.100551\n[Training] Epoch 010 | Batch 040/204 | Current Loss: 0.100480\n[Training] Epoch 010 | Batch 050/204 | Current Loss: 0.100557\n[Training] Epoch 010 | Batch 060/204 | Current Loss: 0.100541\n[Training] Epoch 010 | Batch 070/204 | Current Loss: 0.100490\n[Training] Epoch 010 | Batch 080/204 | Current Loss: 0.100574\n[Training] Epoch 010 | Batch 090/204 | Current Loss: 0.100498\n[Training] Epoch 010 | Batch 100/204 | Current Loss: 0.100465\n[Training] Epoch 010 | Batch 110/204 | Current Loss: 0.100512\n[Training] Epoch 010 | Batch 120/204 | Current Loss: 0.100465\n[Training] Epoch 010 | Batch 130/204 | Current Loss: 0.100593\n[Training] Epoch 010 | Batch 140/204 | Current Loss: 0.100459\n[Training] Epoch 010 | Batch 150/204 | Current Loss: 0.100441\n[Training] Epoch 010 | Batch 160/204 | Current Loss: 0.100553\n[Training] Epoch 010 | Batch 170/204 | Current Loss: 0.100612\n[Training] Epoch 010 | Batch 180/204 | Current Loss: 0.100489\n[Training] Epoch 010 | Batch 190/204 | Current Loss: 0.100479\n[Training] Epoch 010 | Batch 200/204 | Current Loss: 0.100450\n\n[Training] Epoch 010 Summary:\n  Avg Loss: 0.100540\n  Current LR: 9.59e-04\n[EarlyStopping] Loss improved (0.100572 → 0.100540). Saving model...\n[Training] Epoch 011 | Batch 000/204 | Current Loss: 0.100578\n[Training] Epoch 011 | Batch 010/204 | Current Loss: 0.100865\n[Training] Epoch 011 | Batch 020/204 | Current Loss: 0.100794\n[Training] Epoch 011 | Batch 030/204 | Current Loss: 0.100789\n[Training] Epoch 011 | Batch 040/204 | Current Loss: 0.100538\n[Training] Epoch 011 | Batch 050/204 | Current Loss: 0.100705\n[Training] Epoch 011 | Batch 060/204 | Current Loss: 0.100735\n[Training] Epoch 011 | Batch 070/204 | Current Loss: 0.100529\n[Training] Epoch 011 | Batch 080/204 | Current Loss: 0.100602\n[Training] Epoch 011 | Batch 090/204 | Current Loss: 0.100641\n[Training] Epoch 011 | Batch 100/204 | Current Loss: 0.100422\n[Training] Epoch 011 | Batch 110/204 | Current Loss: 0.100468\n[Training] Epoch 011 | Batch 120/204 | Current Loss: 0.100625\n[Training] Epoch 011 | Batch 130/204 | Current Loss: 0.100543\n[Training] Epoch 011 | Batch 140/204 | Current Loss: 0.100592\n[Training] Epoch 011 | Batch 150/204 | Current Loss: 0.100523\n[Training] Epoch 011 | Batch 160/204 | Current Loss: 0.100477\n[Training] Epoch 011 | Batch 170/204 | Current Loss: 0.100637\n[Training] Epoch 011 | Batch 180/204 | Current Loss: 0.100558\n[Training] Epoch 011 | Batch 190/204 | Current Loss: 0.100640\n[Training] Epoch 011 | Batch 200/204 | Current Loss: 0.100357\n\n[Training] Epoch 011 Summary:\n  Avg Loss: 0.100621\n  Current LR: 9.35e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 012 | Batch 000/204 | Current Loss: 0.100504\n[Training] Epoch 012 | Batch 010/204 | Current Loss: 0.100599\n[Training] Epoch 012 | Batch 020/204 | Current Loss: 0.100521\n[Training] Epoch 012 | Batch 030/204 | Current Loss: 0.100464\n[Training] Epoch 012 | Batch 040/204 | Current Loss: 0.100462\n[Training] Epoch 012 | Batch 050/204 | Current Loss: 0.100472\n[Training] Epoch 012 | Batch 060/204 | Current Loss: 0.100480\n[Training] Epoch 012 | Batch 070/204 | Current Loss: 0.100531\n[Training] Epoch 012 | Batch 080/204 | Current Loss: 0.100417\n[Training] Epoch 012 | Batch 090/204 | Current Loss: 0.100431\n[Training] Epoch 012 | Batch 100/204 | Current Loss: 0.100387\n[Training] Epoch 012 | Batch 110/204 | Current Loss: 0.100420\n[Training] Epoch 012 | Batch 120/204 | Current Loss: 0.100463\n[Training] Epoch 012 | Batch 130/204 | Current Loss: 0.100401\n[Training] Epoch 012 | Batch 140/204 | Current Loss: 0.100303\n[Training] Epoch 012 | Batch 150/204 | Current Loss: 0.100330\n[Training] Epoch 012 | Batch 160/204 | Current Loss: 0.100413\n[Training] Epoch 012 | Batch 170/204 | Current Loss: 0.100364\n[Training] Epoch 012 | Batch 180/204 | Current Loss: 0.100407\n[Training] Epoch 012 | Batch 190/204 | Current Loss: 0.100192\n[Training] Epoch 012 | Batch 200/204 | Current Loss: 0.100516\n\n[Training] Epoch 012 Summary:\n  Avg Loss: 0.100436\n  Current LR: 8.67e-04\n[EarlyStopping] Loss improved (0.100540 → 0.100436). Saving model...\n[Training] Epoch 013 | Batch 000/204 | Current Loss: 0.100714\n[Training] Epoch 013 | Batch 010/204 | Current Loss: 0.100402\n[Training] Epoch 013 | Batch 020/204 | Current Loss: 0.100241\n[Training] Epoch 013 | Batch 030/204 | Current Loss: 0.100445\n[Training] Epoch 013 | Batch 040/204 | Current Loss: 0.100397\n[Training] Epoch 013 | Batch 050/204 | Current Loss: 0.100395\n[Training] Epoch 013 | Batch 060/204 | Current Loss: 0.100267\n[Training] Epoch 013 | Batch 070/204 | Current Loss: 0.100328\n[Training] Epoch 013 | Batch 080/204 | Current Loss: 0.100311\n[Training] Epoch 013 | Batch 090/204 | Current Loss: 0.100419\n[Training] Epoch 013 | Batch 100/204 | Current Loss: 0.100295\n[Training] Epoch 013 | Batch 110/204 | Current Loss: 0.100452\n[Training] Epoch 013 | Batch 120/204 | Current Loss: 0.100273\n[Training] Epoch 013 | Batch 130/204 | Current Loss: 0.100335\n[Training] Epoch 013 | Batch 140/204 | Current Loss: 0.100316\n[Training] Epoch 013 | Batch 150/204 | Current Loss: 0.100279\n[Training] Epoch 013 | Batch 160/204 | Current Loss: 0.100412\n[Training] Epoch 013 | Batch 170/204 | Current Loss: 0.100292\n[Training] Epoch 013 | Batch 180/204 | Current Loss: 0.100409\n[Training] Epoch 013 | Batch 190/204 | Current Loss: 0.100281\n[Training] Epoch 013 | Batch 200/204 | Current Loss: 0.100285\n\n[Training] Epoch 013 Summary:\n  Avg Loss: 0.100372\n  Current LR: 7.61e-04\n[EarlyStopping] Loss improved (0.100436 → 0.100372). Saving model...\n[Training] Epoch 014 | Batch 000/204 | Current Loss: 0.101866\n[Training] Epoch 014 | Batch 010/204 | Current Loss: 0.100877\n[Training] Epoch 014 | Batch 020/204 | Current Loss: 0.100542\n[Training] Epoch 014 | Batch 030/204 | Current Loss: 0.100407\n[Training] Epoch 014 | Batch 040/204 | Current Loss: 0.100483\n[Training] Epoch 014 | Batch 050/204 | Current Loss: 0.100379\n[Training] Epoch 014 | Batch 060/204 | Current Loss: 0.100321\n[Training] Epoch 014 | Batch 070/204 | Current Loss: 0.100368\n[Training] Epoch 014 | Batch 080/204 | Current Loss: 0.100352\n[Training] Epoch 014 | Batch 090/204 | Current Loss: 0.100280\n[Training] Epoch 014 | Batch 100/204 | Current Loss: 0.100240\n[Training] Epoch 014 | Batch 110/204 | Current Loss: 0.100312\n[Training] Epoch 014 | Batch 120/204 | Current Loss: 0.100282\n[Training] Epoch 014 | Batch 130/204 | Current Loss: 0.100307\n[Training] Epoch 014 | Batch 140/204 | Current Loss: 0.100333\n[Training] Epoch 014 | Batch 150/204 | Current Loss: 0.100365\n[Training] Epoch 014 | Batch 160/204 | Current Loss: 0.100476\n[Training] Epoch 014 | Batch 170/204 | Current Loss: 0.100294\n[Training] Epoch 014 | Batch 180/204 | Current Loss: 0.100226\n[Training] Epoch 014 | Batch 190/204 | Current Loss: 0.100230\n[Training] Epoch 014 | Batch 200/204 | Current Loss: 0.100304\n\n[Training] Epoch 014 Summary:\n  Avg Loss: 0.100385\n  Current LR: 6.28e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 015 | Batch 000/204 | Current Loss: 0.100581\n[Training] Epoch 015 | Batch 010/204 | Current Loss: 0.100400\n[Training] Epoch 015 | Batch 020/204 | Current Loss: 0.100373\n[Training] Epoch 015 | Batch 030/204 | Current Loss: 0.100306\n[Training] Epoch 015 | Batch 040/204 | Current Loss: 0.100251\n[Training] Epoch 015 | Batch 050/204 | Current Loss: 0.100342\n[Training] Epoch 015 | Batch 060/204 | Current Loss: 0.100404\n[Training] Epoch 015 | Batch 070/204 | Current Loss: 0.100313\n[Training] Epoch 015 | Batch 080/204 | Current Loss: 0.100235\n[Training] Epoch 015 | Batch 090/204 | Current Loss: 0.100294\n[Training] Epoch 015 | Batch 100/204 | Current Loss: 0.100385\n[Training] Epoch 015 | Batch 110/204 | Current Loss: 0.100322\n[Training] Epoch 015 | Batch 120/204 | Current Loss: 0.100428\n[Training] Epoch 015 | Batch 130/204 | Current Loss: 0.100196\n[Training] Epoch 015 | Batch 140/204 | Current Loss: 0.100444\n[Training] Epoch 015 | Batch 150/204 | Current Loss: 0.100330\n[Training] Epoch 015 | Batch 160/204 | Current Loss: 0.100342\n[Training] Epoch 015 | Batch 170/204 | Current Loss: 0.100374\n[Training] Epoch 015 | Batch 180/204 | Current Loss: 0.100483\n[Training] Epoch 015 | Batch 190/204 | Current Loss: 0.100324\n[Training] Epoch 015 | Batch 200/204 | Current Loss: 0.100401\n\n[Training] Epoch 015 Summary:\n  Avg Loss: 0.100336\n  Current LR: 4.80e-04\n[EarlyStopping] Loss improved (0.100372 → 0.100336). Saving model...\n[Training] Epoch 016 | Batch 000/204 | Current Loss: 0.102793\n[Training] Epoch 016 | Batch 010/204 | Current Loss: 0.100644\n[Training] Epoch 016 | Batch 020/204 | Current Loss: 0.100545\n[Training] Epoch 016 | Batch 030/204 | Current Loss: 0.100337\n[Training] Epoch 016 | Batch 040/204 | Current Loss: 0.100313\n[Training] Epoch 016 | Batch 050/204 | Current Loss: 0.100215\n[Training] Epoch 016 | Batch 060/204 | Current Loss: 0.100330\n[Training] Epoch 016 | Batch 070/204 | Current Loss: 0.100275\n[Training] Epoch 016 | Batch 080/204 | Current Loss: 0.100311\n[Training] Epoch 016 | Batch 090/204 | Current Loss: 0.100309\n[Training] Epoch 016 | Batch 100/204 | Current Loss: 0.100303\n[Training] Epoch 016 | Batch 110/204 | Current Loss: 0.100268\n[Training] Epoch 016 | Batch 120/204 | Current Loss: 0.100256\n[Training] Epoch 016 | Batch 130/204 | Current Loss: 0.100347\n[Training] Epoch 016 | Batch 140/204 | Current Loss: 0.100211\n[Training] Epoch 016 | Batch 150/204 | Current Loss: 0.100289\n[Training] Epoch 016 | Batch 160/204 | Current Loss: 0.100241\n[Training] Epoch 016 | Batch 170/204 | Current Loss: 0.100200\n[Training] Epoch 016 | Batch 180/204 | Current Loss: 0.100265\n[Training] Epoch 016 | Batch 190/204 | Current Loss: 0.100235\n[Training] Epoch 016 | Batch 200/204 | Current Loss: 0.100339\n\n[Training] Epoch 016 Summary:\n  Avg Loss: 0.100345\n  Current LR: 3.32e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 017 | Batch 000/204 | Current Loss: 0.100319\n[Training] Epoch 017 | Batch 010/204 | Current Loss: 0.100111\n[Training] Epoch 017 | Batch 020/204 | Current Loss: 0.100252\n[Training] Epoch 017 | Batch 030/204 | Current Loss: 0.100348\n[Training] Epoch 017 | Batch 040/204 | Current Loss: 0.100178\n[Training] Epoch 017 | Batch 050/204 | Current Loss: 0.100191\n[Training] Epoch 017 | Batch 060/204 | Current Loss: 0.100303\n[Training] Epoch 017 | Batch 070/204 | Current Loss: 0.100243\n[Training] Epoch 017 | Batch 080/204 | Current Loss: 0.100197\n[Training] Epoch 017 | Batch 090/204 | Current Loss: 0.100282\n[Training] Epoch 017 | Batch 100/204 | Current Loss: 0.100254\n[Training] Epoch 017 | Batch 110/204 | Current Loss: 0.100162\n[Training] Epoch 017 | Batch 120/204 | Current Loss: 0.100290\n[Training] Epoch 017 | Batch 130/204 | Current Loss: 0.100211\n[Training] Epoch 017 | Batch 140/204 | Current Loss: 0.100300\n[Training] Epoch 017 | Batch 150/204 | Current Loss: 0.100229\n[Training] Epoch 017 | Batch 160/204 | Current Loss: 0.100259\n[Training] Epoch 017 | Batch 170/204 | Current Loss: 0.100234\n[Training] Epoch 017 | Batch 180/204 | Current Loss: 0.100329\n[Training] Epoch 017 | Batch 190/204 | Current Loss: 0.100250\n[Training] Epoch 017 | Batch 200/204 | Current Loss: 0.100334\n\n[Training] Epoch 017 Summary:\n  Avg Loss: 0.100246\n  Current LR: 1.98e-04\n[EarlyStopping] Loss improved (0.100336 → 0.100246). Saving model...\n[Training] Epoch 018 | Batch 000/204 | Current Loss: 0.100218\n[Training] Epoch 018 | Batch 010/204 | Current Loss: 0.100202\n[Training] Epoch 018 | Batch 020/204 | Current Loss: 0.100261\n[Training] Epoch 018 | Batch 030/204 | Current Loss: 0.100250\n[Training] Epoch 018 | Batch 040/204 | Current Loss: 0.100190\n[Training] Epoch 018 | Batch 050/204 | Current Loss: 0.100134\n[Training] Epoch 018 | Batch 060/204 | Current Loss: 0.100183\n[Training] Epoch 018 | Batch 070/204 | Current Loss: 0.100305\n[Training] Epoch 018 | Batch 080/204 | Current Loss: 0.100256\n[Training] Epoch 018 | Batch 090/204 | Current Loss: 0.100167\n[Training] Epoch 018 | Batch 100/204 | Current Loss: 0.100249\n[Training] Epoch 018 | Batch 110/204 | Current Loss: 0.100174\n[Training] Epoch 018 | Batch 120/204 | Current Loss: 0.100244\n[Training] Epoch 018 | Batch 130/204 | Current Loss: 0.100162\n[Training] Epoch 018 | Batch 140/204 | Current Loss: 0.100284\n[Training] Epoch 018 | Batch 150/204 | Current Loss: 0.100232\n[Training] Epoch 018 | Batch 160/204 | Current Loss: 0.100324\n[Training] Epoch 018 | Batch 170/204 | Current Loss: 0.100186\n[Training] Epoch 018 | Batch 180/204 | Current Loss: 0.100158\n[Training] Epoch 018 | Batch 190/204 | Current Loss: 0.100200\n[Training] Epoch 018 | Batch 200/204 | Current Loss: 0.100179\n\n[Training] Epoch 018 Summary:\n  Avg Loss: 0.100224\n  Current LR: 9.25e-05\n[EarlyStopping] Loss improved (0.100246 → 0.100224). Saving model...\n[Training] Epoch 019 | Batch 000/204 | Current Loss: 0.100296\n[Training] Epoch 019 | Batch 010/204 | Current Loss: 0.100285\n[Training] Epoch 019 | Batch 020/204 | Current Loss: 0.100288\n[Training] Epoch 019 | Batch 030/204 | Current Loss: 0.100294\n[Training] Epoch 019 | Batch 040/204 | Current Loss: 0.100338\n[Training] Epoch 019 | Batch 050/204 | Current Loss: 0.100190\n[Training] Epoch 019 | Batch 060/204 | Current Loss: 0.100091\n[Training] Epoch 019 | Batch 070/204 | Current Loss: 0.100197\n[Training] Epoch 019 | Batch 080/204 | Current Loss: 0.100256\n[Training] Epoch 019 | Batch 090/204 | Current Loss: 0.100165\n[Training] Epoch 019 | Batch 100/204 | Current Loss: 0.100254\n[Training] Epoch 019 | Batch 110/204 | Current Loss: 0.100171\n[Training] Epoch 019 | Batch 120/204 | Current Loss: 0.100272\n[Training] Epoch 019 | Batch 130/204 | Current Loss: 0.100162\n[Training] Epoch 019 | Batch 140/204 | Current Loss: 0.100233\n[Training] Epoch 019 | Batch 150/204 | Current Loss: 0.100087\n[Training] Epoch 019 | Batch 160/204 | Current Loss: 0.100212\n[Training] Epoch 019 | Batch 170/204 | Current Loss: 0.100241\n[Training] Epoch 019 | Batch 180/204 | Current Loss: 0.100285\n[Training] Epoch 019 | Batch 190/204 | Current Loss: 0.100314\n[Training] Epoch 019 | Batch 200/204 | Current Loss: 0.100253\n\n[Training] Epoch 019 Summary:\n  Avg Loss: 0.100215\n  Current LR: 2.44e-05\n[EarlyStopping] Loss improved (0.100224 → 0.100215). Saving model...\n[Training] Epoch 020 | Batch 000/204 | Current Loss: 0.100148\n[Training] Epoch 020 | Batch 010/204 | Current Loss: 0.100209\n[Training] Epoch 020 | Batch 020/204 | Current Loss: 0.100381\n[Training] Epoch 020 | Batch 030/204 | Current Loss: 0.100135\n[Training] Epoch 020 | Batch 040/204 | Current Loss: 0.100268\n[Training] Epoch 020 | Batch 050/204 | Current Loss: 0.100168\n[Training] Epoch 020 | Batch 060/204 | Current Loss: 0.100277\n[Training] Epoch 020 | Batch 070/204 | Current Loss: 0.100253\n[Training] Epoch 020 | Batch 080/204 | Current Loss: 0.100187\n[Training] Epoch 020 | Batch 090/204 | Current Loss: 0.100432\n[Training] Epoch 020 | Batch 100/204 | Current Loss: 0.100219\n[Training] Epoch 020 | Batch 110/204 | Current Loss: 0.100107\n[Training] Epoch 020 | Batch 120/204 | Current Loss: 0.100170\n[Training] Epoch 020 | Batch 130/204 | Current Loss: 0.100194\n[Training] Epoch 020 | Batch 140/204 | Current Loss: 0.100165\n[Training] Epoch 020 | Batch 150/204 | Current Loss: 0.100146\n[Training] Epoch 020 | Batch 160/204 | Current Loss: 0.100308\n[Training] Epoch 020 | Batch 170/204 | Current Loss: 0.100205\n[Training] Epoch 020 | Batch 180/204 | Current Loss: 0.100235\n[Training] Epoch 020 | Batch 190/204 | Current Loss: 0.100191\n[Training] Epoch 020 | Batch 200/204 | Current Loss: 0.100196\n\n[Training] Epoch 020 Summary:\n  Avg Loss: 0.100206\n  Current LR: 9.59e-04\n[EarlyStopping] Loss improved (0.100215 → 0.100206). Saving model...\n[Training] Epoch 021 | Batch 000/204 | Current Loss: 0.100245\n[Training] Epoch 021 | Batch 010/204 | Current Loss: 0.100265\n[Training] Epoch 021 | Batch 020/204 | Current Loss: 0.100350\n[Training] Epoch 021 | Batch 030/204 | Current Loss: 0.100410\n[Training] Epoch 021 | Batch 040/204 | Current Loss: 0.100391\n[Training] Epoch 021 | Batch 050/204 | Current Loss: 0.100251\n[Training] Epoch 021 | Batch 060/204 | Current Loss: 0.100348\n[Training] Epoch 021 | Batch 070/204 | Current Loss: 0.100412\n[Training] Epoch 021 | Batch 080/204 | Current Loss: 0.100339\n[Training] Epoch 021 | Batch 090/204 | Current Loss: 0.100171\n[Training] Epoch 021 | Batch 100/204 | Current Loss: 0.100282\n[Training] Epoch 021 | Batch 110/204 | Current Loss: 0.100333\n[Training] Epoch 021 | Batch 120/204 | Current Loss: 0.100381\n[Training] Epoch 021 | Batch 130/204 | Current Loss: 0.100277\n[Training] Epoch 021 | Batch 140/204 | Current Loss: 0.100293\n[Training] Epoch 021 | Batch 150/204 | Current Loss: 0.100253\n[Training] Epoch 021 | Batch 160/204 | Current Loss: 0.100224\n[Training] Epoch 021 | Batch 170/204 | Current Loss: 0.100353\n[Training] Epoch 021 | Batch 180/204 | Current Loss: 0.100255\n[Training] Epoch 021 | Batch 190/204 | Current Loss: 0.100300\n[Training] Epoch 021 | Batch 200/204 | Current Loss: 0.100222\n\n[Training] Epoch 021 Summary:\n  Avg Loss: 0.100296\n  Current LR: 9.35e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 022 | Batch 000/204 | Current Loss: 0.100305\n[Training] Epoch 022 | Batch 010/204 | Current Loss: 0.100408\n[Training] Epoch 022 | Batch 020/204 | Current Loss: 0.100191\n[Training] Epoch 022 | Batch 030/204 | Current Loss: 0.100257\n[Training] Epoch 022 | Batch 040/204 | Current Loss: 0.100182\n[Training] Epoch 022 | Batch 050/204 | Current Loss: 0.100207\n[Training] Epoch 022 | Batch 060/204 | Current Loss: 0.100212\n[Training] Epoch 022 | Batch 070/204 | Current Loss: 0.100296\n[Training] Epoch 022 | Batch 080/204 | Current Loss: 0.100248\n[Training] Epoch 022 | Batch 090/204 | Current Loss: 0.100375\n[Training] Epoch 022 | Batch 100/204 | Current Loss: 0.100237\n[Training] Epoch 022 | Batch 110/204 | Current Loss: 0.100200\n[Training] Epoch 022 | Batch 120/204 | Current Loss: 0.100259\n[Training] Epoch 022 | Batch 130/204 | Current Loss: 0.100353\n[Training] Epoch 022 | Batch 140/204 | Current Loss: 0.100222\n[Training] Epoch 022 | Batch 150/204 | Current Loss: 0.100189\n[Training] Epoch 022 | Batch 160/204 | Current Loss: 0.100250\n[Training] Epoch 022 | Batch 170/204 | Current Loss: 0.100331\n[Training] Epoch 022 | Batch 180/204 | Current Loss: 0.100208\n[Training] Epoch 022 | Batch 190/204 | Current Loss: 0.100260\n[Training] Epoch 022 | Batch 200/204 | Current Loss: 0.100126\n\n[Training] Epoch 022 Summary:\n  Avg Loss: 0.100253\n  Current LR: 8.67e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 023 | Batch 000/204 | Current Loss: 0.100198\n[Training] Epoch 023 | Batch 010/204 | Current Loss: 0.100370\n[Training] Epoch 023 | Batch 020/204 | Current Loss: 0.100271\n[Training] Epoch 023 | Batch 030/204 | Current Loss: 0.100235\n[Training] Epoch 023 | Batch 040/204 | Current Loss: 0.100142\n[Training] Epoch 023 | Batch 050/204 | Current Loss: 0.100143\n[Training] Epoch 023 | Batch 060/204 | Current Loss: 0.100218\n[Training] Epoch 023 | Batch 070/204 | Current Loss: 0.100297\n[Training] Epoch 023 | Batch 080/204 | Current Loss: 0.100266\n[Training] Epoch 023 | Batch 090/204 | Current Loss: 0.100251\n[Training] Epoch 023 | Batch 100/204 | Current Loss: 0.100173\n[Training] Epoch 023 | Batch 110/204 | Current Loss: 0.100239\n[Training] Epoch 023 | Batch 120/204 | Current Loss: 0.100231\n[Training] Epoch 023 | Batch 130/204 | Current Loss: 0.100338\n[Training] Epoch 023 | Batch 140/204 | Current Loss: 0.100434\n[Training] Epoch 023 | Batch 150/204 | Current Loss: 0.100360\n[Training] Epoch 023 | Batch 160/204 | Current Loss: 0.100254\n[Training] Epoch 023 | Batch 170/204 | Current Loss: 0.100214\n[Training] Epoch 023 | Batch 180/204 | Current Loss: 0.100253\n[Training] Epoch 023 | Batch 190/204 | Current Loss: 0.100300\n[Training] Epoch 023 | Batch 200/204 | Current Loss: 0.100219\n\n[Training] Epoch 023 Summary:\n  Avg Loss: 0.100253\n  Current LR: 7.61e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 024 | Batch 000/204 | Current Loss: 0.100279\n[Training] Epoch 024 | Batch 010/204 | Current Loss: 0.100324\n[Training] Epoch 024 | Batch 020/204 | Current Loss: 0.100243\n[Training] Epoch 024 | Batch 030/204 | Current Loss: 0.100202\n[Training] Epoch 024 | Batch 040/204 | Current Loss: 0.100195\n[Training] Epoch 024 | Batch 050/204 | Current Loss: 0.100123\n[Training] Epoch 024 | Batch 060/204 | Current Loss: 0.100171\n[Training] Epoch 024 | Batch 070/204 | Current Loss: 0.100144\n[Training] Epoch 024 | Batch 080/204 | Current Loss: 0.100253\n[Training] Epoch 024 | Batch 090/204 | Current Loss: 0.100234\n[Training] Epoch 024 | Batch 100/204 | Current Loss: 0.100387\n[Training] Epoch 024 | Batch 110/204 | Current Loss: 0.100136\n[Training] Epoch 024 | Batch 120/204 | Current Loss: 0.100135\n[Training] Epoch 024 | Batch 130/204 | Current Loss: 0.100080\n[Training] Epoch 024 | Batch 140/204 | Current Loss: 0.100183\n[Training] Epoch 024 | Batch 150/204 | Current Loss: 0.100261\n[Training] Epoch 024 | Batch 160/204 | Current Loss: 0.100200\n[Training] Epoch 024 | Batch 170/204 | Current Loss: 0.100123\n[Training] Epoch 024 | Batch 180/204 | Current Loss: 0.100283\n[Training] Epoch 024 | Batch 190/204 | Current Loss: 0.100229\n[Training] Epoch 024 | Batch 200/204 | Current Loss: 0.100237\n\n[Training] Epoch 024 Summary:\n  Avg Loss: 0.100210\n  Current LR: 6.28e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 025 | Batch 000/204 | Current Loss: 0.100104\n[Training] Epoch 025 | Batch 010/204 | Current Loss: 0.100241\n[Training] Epoch 025 | Batch 020/204 | Current Loss: 0.100122\n[Training] Epoch 025 | Batch 030/204 | Current Loss: 0.100147\n[Training] Epoch 025 | Batch 040/204 | Current Loss: 0.100170\n[Training] Epoch 025 | Batch 050/204 | Current Loss: 0.100252\n[Training] Epoch 025 | Batch 060/204 | Current Loss: 0.100200\n[Training] Epoch 025 | Batch 070/204 | Current Loss: 0.100298\n[Training] Epoch 025 | Batch 080/204 | Current Loss: 0.100276\n[Training] Epoch 025 | Batch 090/204 | Current Loss: 0.100266\n[Training] Epoch 025 | Batch 100/204 | Current Loss: 0.100193\n[Training] Epoch 025 | Batch 110/204 | Current Loss: 0.100203\n[Training] Epoch 025 | Batch 120/204 | Current Loss: 0.100251\n[Training] Epoch 025 | Batch 130/204 | Current Loss: 0.100234\n[Training] Epoch 025 | Batch 140/204 | Current Loss: 0.100130\n[Training] Epoch 025 | Batch 150/204 | Current Loss: 0.100140\n[Training] Epoch 025 | Batch 160/204 | Current Loss: 0.100092\n[Training] Epoch 025 | Batch 170/204 | Current Loss: 0.100116\n[Training] Epoch 025 | Batch 180/204 | Current Loss: 0.100205\n[Training] Epoch 025 | Batch 190/204 | Current Loss: 0.100177\n[Training] Epoch 025 | Batch 200/204 | Current Loss: 0.100129\n\n[Training] Epoch 025 Summary:\n  Avg Loss: 0.100191\n  Current LR: 4.80e-04\n[EarlyStopping] Loss improved (0.100206 → 0.100191). Saving model...\n[Training] Epoch 026 | Batch 000/204 | Current Loss: 0.100282\n[Training] Epoch 026 | Batch 010/204 | Current Loss: 0.100211\n[Training] Epoch 026 | Batch 020/204 | Current Loss: 0.100294\n[Training] Epoch 026 | Batch 030/204 | Current Loss: 0.100177\n[Training] Epoch 026 | Batch 040/204 | Current Loss: 0.100133\n[Training] Epoch 026 | Batch 050/204 | Current Loss: 0.100149\n[Training] Epoch 026 | Batch 060/204 | Current Loss: 0.100233\n[Training] Epoch 026 | Batch 070/204 | Current Loss: 0.100055\n[Training] Epoch 026 | Batch 080/204 | Current Loss: 0.100050\n[Training] Epoch 026 | Batch 090/204 | Current Loss: 0.100132\n[Training] Epoch 026 | Batch 100/204 | Current Loss: 0.100184\n[Training] Epoch 026 | Batch 110/204 | Current Loss: 0.100143\n[Training] Epoch 026 | Batch 120/204 | Current Loss: 0.100125\n[Training] Epoch 026 | Batch 130/204 | Current Loss: 0.100152\n[Training] Epoch 026 | Batch 140/204 | Current Loss: 0.100218\n[Training] Epoch 026 | Batch 150/204 | Current Loss: 0.100163\n[Training] Epoch 026 | Batch 160/204 | Current Loss: 0.100141\n[Training] Epoch 026 | Batch 170/204 | Current Loss: 0.100190\n[Training] Epoch 026 | Batch 180/204 | Current Loss: 0.100131\n[Training] Epoch 026 | Batch 190/204 | Current Loss: 0.100217\n[Training] Epoch 026 | Batch 200/204 | Current Loss: 0.100108\n\n[Training] Epoch 026 Summary:\n  Avg Loss: 0.100170\n  Current LR: 3.32e-04\n[EarlyStopping] Loss improved (0.100191 → 0.100170). Saving model...\n[Training] Epoch 027 | Batch 000/204 | Current Loss: 0.100242\n[Training] Epoch 027 | Batch 010/204 | Current Loss: 0.100222\n[Training] Epoch 027 | Batch 020/204 | Current Loss: 0.100193\n[Training] Epoch 027 | Batch 030/204 | Current Loss: 0.100124\n[Training] Epoch 027 | Batch 040/204 | Current Loss: 0.100116\n[Training] Epoch 027 | Batch 050/204 | Current Loss: 0.100022\n[Training] Epoch 027 | Batch 060/204 | Current Loss: 0.100172\n[Training] Epoch 027 | Batch 070/204 | Current Loss: 0.100179\n[Training] Epoch 027 | Batch 080/204 | Current Loss: 0.100097\n[Training] Epoch 027 | Batch 090/204 | Current Loss: 0.100095\n[Training] Epoch 027 | Batch 100/204 | Current Loss: 0.100166\n[Training] Epoch 027 | Batch 110/204 | Current Loss: 0.100139\n[Training] Epoch 027 | Batch 120/204 | Current Loss: 0.100107\n[Training] Epoch 027 | Batch 130/204 | Current Loss: 0.100160\n[Training] Epoch 027 | Batch 140/204 | Current Loss: 0.100130\n[Training] Epoch 027 | Batch 150/204 | Current Loss: 0.100193\n[Training] Epoch 027 | Batch 160/204 | Current Loss: 0.100153\n[Training] Epoch 027 | Batch 170/204 | Current Loss: 0.100094\n[Training] Epoch 027 | Batch 180/204 | Current Loss: 0.100121\n[Training] Epoch 027 | Batch 190/204 | Current Loss: 0.100204\n[Training] Epoch 027 | Batch 200/204 | Current Loss: 0.100332\n\n[Training] Epoch 027 Summary:\n  Avg Loss: 0.100155\n  Current LR: 1.98e-04\n[EarlyStopping] Loss improved (0.100170 → 0.100155). Saving model...\n[Training] Epoch 028 | Batch 000/204 | Current Loss: 0.100111\n[Training] Epoch 028 | Batch 010/204 | Current Loss: 0.100150\n[Training] Epoch 028 | Batch 020/204 | Current Loss: 0.100244\n[Training] Epoch 028 | Batch 030/204 | Current Loss: 0.100112\n[Training] Epoch 028 | Batch 040/204 | Current Loss: 0.100175\n[Training] Epoch 028 | Batch 050/204 | Current Loss: 0.100212\n[Training] Epoch 028 | Batch 060/204 | Current Loss: 0.100074\n[Training] Epoch 028 | Batch 070/204 | Current Loss: 0.100229\n[Training] Epoch 028 | Batch 080/204 | Current Loss: 0.100186\n[Training] Epoch 028 | Batch 090/204 | Current Loss: 0.100183\n[Training] Epoch 028 | Batch 100/204 | Current Loss: 0.100059\n[Training] Epoch 028 | Batch 110/204 | Current Loss: 0.100056\n[Training] Epoch 028 | Batch 120/204 | Current Loss: 0.100036\n[Training] Epoch 028 | Batch 130/204 | Current Loss: 0.100185\n[Training] Epoch 028 | Batch 140/204 | Current Loss: 0.100059\n[Training] Epoch 028 | Batch 150/204 | Current Loss: 0.100198\n[Training] Epoch 028 | Batch 160/204 | Current Loss: 0.100070\n[Training] Epoch 028 | Batch 170/204 | Current Loss: 0.100121\n[Training] Epoch 028 | Batch 180/204 | Current Loss: 0.100118\n[Training] Epoch 028 | Batch 190/204 | Current Loss: 0.100170\n[Training] Epoch 028 | Batch 200/204 | Current Loss: 0.100076\n\n[Training] Epoch 028 Summary:\n  Avg Loss: 0.100146\n  Current LR: 9.25e-05\n[EarlyStopping] Loss improved (0.100155 → 0.100146). Saving model...\n[Training] Epoch 029 | Batch 000/204 | Current Loss: 0.100230\n[Training] Epoch 029 | Batch 010/204 | Current Loss: 0.100199\n[Training] Epoch 029 | Batch 020/204 | Current Loss: 0.100193\n[Training] Epoch 029 | Batch 030/204 | Current Loss: 0.100171\n[Training] Epoch 029 | Batch 040/204 | Current Loss: 0.100060\n[Training] Epoch 029 | Batch 050/204 | Current Loss: 0.100134\n[Training] Epoch 029 | Batch 060/204 | Current Loss: 0.100166\n[Training] Epoch 029 | Batch 070/204 | Current Loss: 0.100104\n[Training] Epoch 029 | Batch 080/204 | Current Loss: 0.100151\n[Training] Epoch 029 | Batch 090/204 | Current Loss: 0.100185\n[Training] Epoch 029 | Batch 100/204 | Current Loss: 0.100150\n[Training] Epoch 029 | Batch 110/204 | Current Loss: 0.100020\n[Training] Epoch 029 | Batch 120/204 | Current Loss: 0.100121\n[Training] Epoch 029 | Batch 130/204 | Current Loss: 0.100171\n[Training] Epoch 029 | Batch 140/204 | Current Loss: 0.100177\n[Training] Epoch 029 | Batch 150/204 | Current Loss: 0.100162\n[Training] Epoch 029 | Batch 160/204 | Current Loss: 0.100189\n[Training] Epoch 029 | Batch 170/204 | Current Loss: 0.100143\n[Training] Epoch 029 | Batch 180/204 | Current Loss: 0.100132\n[Training] Epoch 029 | Batch 190/204 | Current Loss: 0.100209\n[Training] Epoch 029 | Batch 200/204 | Current Loss: 0.100093\n\n[Training] Epoch 029 Summary:\n  Avg Loss: 0.100136\n  Current LR: 2.44e-05\n[EarlyStopping] Loss improved (0.100146 → 0.100136). Saving model...\n[Training] Epoch 030 | Batch 000/204 | Current Loss: 0.100073\n[Training] Epoch 030 | Batch 010/204 | Current Loss: 0.100068\n[Training] Epoch 030 | Batch 020/204 | Current Loss: 0.100140\n[Training] Epoch 030 | Batch 030/204 | Current Loss: 0.100087\n[Training] Epoch 030 | Batch 040/204 | Current Loss: 0.100127\n[Training] Epoch 030 | Batch 050/204 | Current Loss: 0.100122\n[Training] Epoch 030 | Batch 060/204 | Current Loss: 0.100246\n[Training] Epoch 030 | Batch 070/204 | Current Loss: 0.100100\n[Training] Epoch 030 | Batch 080/204 | Current Loss: 0.100173\n[Training] Epoch 030 | Batch 090/204 | Current Loss: 0.100101\n[Training] Epoch 030 | Batch 100/204 | Current Loss: 0.100154\n[Training] Epoch 030 | Batch 110/204 | Current Loss: 0.100310\n[Training] Epoch 030 | Batch 120/204 | Current Loss: 0.100163\n[Training] Epoch 030 | Batch 130/204 | Current Loss: 0.100126\n[Training] Epoch 030 | Batch 140/204 | Current Loss: 0.100126\n[Training] Epoch 030 | Batch 150/204 | Current Loss: 0.100031\n[Training] Epoch 030 | Batch 160/204 | Current Loss: 0.100133\n[Training] Epoch 030 | Batch 170/204 | Current Loss: 0.100161\n[Training] Epoch 030 | Batch 180/204 | Current Loss: 0.100203\n[Training] Epoch 030 | Batch 190/204 | Current Loss: 0.100110\n[Training] Epoch 030 | Batch 200/204 | Current Loss: 0.100148\n\n[Training] Epoch 030 Summary:\n  Avg Loss: 0.100124\n  Current LR: 9.59e-04\n[EarlyStopping] Loss improved (0.100136 → 0.100124). Saving model...\n[Training] Epoch 031 | Batch 000/204 | Current Loss: 0.100169\n[Training] Epoch 031 | Batch 010/204 | Current Loss: 0.100279\n[Training] Epoch 031 | Batch 020/204 | Current Loss: 0.100199\n[Training] Epoch 031 | Batch 030/204 | Current Loss: 0.100235\n[Training] Epoch 031 | Batch 040/204 | Current Loss: 0.100216\n[Training] Epoch 031 | Batch 050/204 | Current Loss: 0.100242\n[Training] Epoch 031 | Batch 060/204 | Current Loss: 0.100663\n[Training] Epoch 031 | Batch 070/204 | Current Loss: 0.100610\n[Training] Epoch 031 | Batch 080/204 | Current Loss: 0.100458\n[Training] Epoch 031 | Batch 090/204 | Current Loss: 0.101306\n[Training] Epoch 031 | Batch 100/204 | Current Loss: 0.100756\n[Training] Epoch 031 | Batch 110/204 | Current Loss: 0.100847\n[Training] Epoch 031 | Batch 120/204 | Current Loss: 0.100511\n[Training] Epoch 031 | Batch 130/204 | Current Loss: 0.100500\n[Training] Epoch 031 | Batch 140/204 | Current Loss: 0.100406\n[Training] Epoch 031 | Batch 150/204 | Current Loss: 0.100401\n[Training] Epoch 031 | Batch 160/204 | Current Loss: 0.100529\n[Training] Epoch 031 | Batch 170/204 | Current Loss: 0.100321\n[Training] Epoch 031 | Batch 180/204 | Current Loss: 0.100488\n[Training] Epoch 031 | Batch 190/204 | Current Loss: 0.100939\n[Training] Epoch 031 | Batch 200/204 | Current Loss: 0.100501\n\n[Training] Epoch 031 Summary:\n  Avg Loss: 0.100648\n  Current LR: 9.35e-04\n[EarlyStopping] Counter: 1/5\n[Training] Epoch 032 | Batch 000/204 | Current Loss: 0.100692\n[Training] Epoch 032 | Batch 010/204 | Current Loss: 0.100642\n[Training] Epoch 032 | Batch 020/204 | Current Loss: 0.100582\n[Training] Epoch 032 | Batch 030/204 | Current Loss: 0.100478\n[Training] Epoch 032 | Batch 040/204 | Current Loss: 0.100288\n[Training] Epoch 032 | Batch 050/204 | Current Loss: 0.100261\n[Training] Epoch 032 | Batch 060/204 | Current Loss: 0.100292\n[Training] Epoch 032 | Batch 070/204 | Current Loss: 0.100232\n[Training] Epoch 032 | Batch 080/204 | Current Loss: 0.100211\n[Training] Epoch 032 | Batch 090/204 | Current Loss: 0.100304\n[Training] Epoch 032 | Batch 100/204 | Current Loss: 0.100190\n[Training] Epoch 032 | Batch 110/204 | Current Loss: 0.100213\n[Training] Epoch 032 | Batch 120/204 | Current Loss: 0.100406\n[Training] Epoch 032 | Batch 130/204 | Current Loss: 0.100151\n[Training] Epoch 032 | Batch 140/204 | Current Loss: 0.100204\n[Training] Epoch 032 | Batch 150/204 | Current Loss: 0.100191\n[Training] Epoch 032 | Batch 160/204 | Current Loss: 0.100273\n[Training] Epoch 032 | Batch 170/204 | Current Loss: 0.100243\n[Training] Epoch 032 | Batch 180/204 | Current Loss: 0.100179\n[Training] Epoch 032 | Batch 190/204 | Current Loss: 0.100265\n[Training] Epoch 032 | Batch 200/204 | Current Loss: 0.100190\n\n[Training] Epoch 032 Summary:\n  Avg Loss: 0.100272\n  Current LR: 8.67e-04\n[EarlyStopping] Counter: 2/5\n[Training] Epoch 033 | Batch 000/204 | Current Loss: 0.100209\n[Training] Epoch 033 | Batch 010/204 | Current Loss: 0.100214\n[Training] Epoch 033 | Batch 020/204 | Current Loss: 0.100256\n[Training] Epoch 033 | Batch 030/204 | Current Loss: 0.100144\n[Training] Epoch 033 | Batch 040/204 | Current Loss: 0.100297\n[Training] Epoch 033 | Batch 050/204 | Current Loss: 0.100133\n[Training] Epoch 033 | Batch 060/204 | Current Loss: 0.100217\n[Training] Epoch 033 | Batch 070/204 | Current Loss: 0.100142\n[Training] Epoch 033 | Batch 080/204 | Current Loss: 0.100180\n[Training] Epoch 033 | Batch 090/204 | Current Loss: 0.100162\n[Training] Epoch 033 | Batch 100/204 | Current Loss: 0.100245\n[Training] Epoch 033 | Batch 110/204 | Current Loss: 0.100219\n[Training] Epoch 033 | Batch 120/204 | Current Loss: 0.100221\n[Training] Epoch 033 | Batch 130/204 | Current Loss: 0.100220\n[Training] Epoch 033 | Batch 140/204 | Current Loss: 0.100278\n[Training] Epoch 033 | Batch 150/204 | Current Loss: 0.100093\n[Training] Epoch 033 | Batch 160/204 | Current Loss: 0.100206\n[Training] Epoch 033 | Batch 170/204 | Current Loss: 0.100160\n[Training] Epoch 033 | Batch 180/204 | Current Loss: 0.100219\n[Training] Epoch 033 | Batch 190/204 | Current Loss: 0.100122\n[Training] Epoch 033 | Batch 200/204 | Current Loss: 0.100191\n\n[Training] Epoch 033 Summary:\n  Avg Loss: 0.100187\n  Current LR: 7.61e-04\n[EarlyStopping] Counter: 3/5\n[Training] Epoch 034 | Batch 000/204 | Current Loss: 0.100667\n[Training] Epoch 034 | Batch 010/204 | Current Loss: 0.100520\n[Training] Epoch 034 | Batch 020/204 | Current Loss: 0.100385\n[Training] Epoch 034 | Batch 030/204 | Current Loss: 0.100245\n[Training] Epoch 034 | Batch 040/204 | Current Loss: 0.100270\n[Training] Epoch 034 | Batch 050/204 | Current Loss: 0.100243\n[Training] Epoch 034 | Batch 060/204 | Current Loss: 0.100194\n[Training] Epoch 034 | Batch 070/204 | Current Loss: 0.100212\n[Training] Epoch 034 | Batch 080/204 | Current Loss: 0.100144\n[Training] Epoch 034 | Batch 090/204 | Current Loss: 0.100099\n[Training] Epoch 034 | Batch 100/204 | Current Loss: 0.100188\n[Training] Epoch 034 | Batch 110/204 | Current Loss: 0.100167\n[Training] Epoch 034 | Batch 120/204 | Current Loss: 0.100279\n[Training] Epoch 034 | Batch 130/204 | Current Loss: 0.100131\n[Training] Epoch 034 | Batch 140/204 | Current Loss: 0.100220\n[Training] Epoch 034 | Batch 150/204 | Current Loss: 0.100132\n[Training] Epoch 034 | Batch 160/204 | Current Loss: 0.100208\n[Training] Epoch 034 | Batch 170/204 | Current Loss: 0.100208\n[Training] Epoch 034 | Batch 180/204 | Current Loss: 0.100097\n[Training] Epoch 034 | Batch 190/204 | Current Loss: 0.100267\n[Training] Epoch 034 | Batch 200/204 | Current Loss: 0.100143\n\n[Training] Epoch 034 Summary:\n  Avg Loss: 0.100264\n  Current LR: 6.28e-04\n[EarlyStopping] Counter: 4/5\n[Training] Epoch 035 | Batch 000/204 | Current Loss: 0.100150\n[Training] Epoch 035 | Batch 010/204 | Current Loss: 0.100142\n[Training] Epoch 035 | Batch 020/204 | Current Loss: 0.100136\n[Training] Epoch 035 | Batch 030/204 | Current Loss: 0.100265\n[Training] Epoch 035 | Batch 040/204 | Current Loss: 0.100183\n[Training] Epoch 035 | Batch 050/204 | Current Loss: 0.100344\n[Training] Epoch 035 | Batch 060/204 | Current Loss: 0.100200\n[Training] Epoch 035 | Batch 070/204 | Current Loss: 0.100201\n[Training] Epoch 035 | Batch 080/204 | Current Loss: 0.100213\n[Training] Epoch 035 | Batch 090/204 | Current Loss: 0.100275\n[Training] Epoch 035 | Batch 100/204 | Current Loss: 0.100118\n[Training] Epoch 035 | Batch 110/204 | Current Loss: 0.100191\n[Training] Epoch 035 | Batch 120/204 | Current Loss: 0.100071\n[Training] Epoch 035 | Batch 130/204 | Current Loss: 0.100226\n[Training] Epoch 035 | Batch 140/204 | Current Loss: 0.100051\n[Training] Epoch 035 | Batch 150/204 | Current Loss: 0.100109\n[Training] Epoch 035 | Batch 160/204 | Current Loss: 0.100223\n[Training] Epoch 035 | Batch 170/204 | Current Loss: 0.100144\n[Training] Epoch 035 | Batch 180/204 | Current Loss: 0.100274\n[Training] Epoch 035 | Batch 190/204 | Current Loss: 0.100089\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-06-21 17:47:23,611] Trial 19 finished with value: 0.10012369365522675 and parameters: {'lr': 0.0009587140972521137, 'batch_size': 64, 'bottleneck_width': 512, 'dropout_rate': 0.31858741775550603, 'alpha': 0.6130990679467545, 'weight_decay': 1.467896189415769e-06}. Best is trial 18 with value: 0.1000964782547717.\n","output_type":"stream"},{"name":"stdout","text":"[Training] Epoch 035 | Batch 200/204 | Current Loss: 0.100205\n\n[Training] Epoch 035 Summary:\n  Avg Loss: 0.100160\n  Current LR: 4.80e-04\n[EarlyStopping] Counter: 5/5\n\n[Training] Early stopping triggered\n\n[Optuna] Trial 19 completed with best loss: 0.100124\n\n[Optuna] Optimization completed\nBest trial:\n  Value (loss): 0.100096\n  Params: \n    lr: 0.0008632097996657894\n    batch_size: 64\n    bottleneck_width: 512\n    dropout_rate: 0.3140554789948301\n    alpha: 0.5913638011196574\n    weight_decay: 3.485896826371103e-05\n\n==================================================\nStarting Final Model Training\nUsing parameters:\n  lr: 0.0008632097996657894\n  batch_size: 64\n  bottleneck_width: 512\n  dropout_rate: 0.3140554789948301\n  alpha: 0.5913638011196574\n  weight_decay: 3.485896826371103e-05\n==================================================\n\n[Data] Loading dataset...\n\n[Data] Initializing dataset from /kaggle/input/embeddings-dataset\n[Data] Applying transforms: Resize(32), CenterCrop(32), ColorJitter, Normalize\n[Data] Found 13 pickle files\n[Data] Loaded 1000 samples from out1.pickle (1/13)\n[Data] Loaded 1000 samples from out10.pickle (2/13)\n[Data] Loaded 1000 samples from out11.pickle (3/13)\n[Data] Loaded 1000 samples from out12.pickle (4/13)\n[Data] Loaded 1000 samples from out13.pickle (5/13)\n[Data] Loaded 1000 samples from out2.pickle (6/13)\n[Data] Loaded 1000 samples from out3.pickle (7/13)\n[Data] Loaded 1000 samples from out4.pickle (8/13)\n[Data] Loaded 1000 samples from out5.pickle (9/13)\n[Data] Loaded 1000 samples from out6.pickle (10/13)\n[Data] Loaded 1000 samples from out7.pickle (11/13)\n[Data] Loaded 1000 samples from out8.pickle (12/13)\n[Data] Loaded 1000 samples from out9.pickle (13/13)\n[Data] Total samples: 13000\n[Data] Batch size: 64, Total batches: 204\n\n[Model] Initializing final model...\n\n[Model] Initializing encoder with:\n  - Bottleneck width: 512\n  - Dropout rate: 0.3140554789948301\n[Model] Architecture initialized successfully\n[Model] Total parameters: 1,347,904\n[Optimizer] Initialized with lr=8.63e-04, weight_decay=3.49e-05\n[Scheduler] CosineAnnealingWarmRestarts (T_0=10, eta_min=1e-6)\n\n[Loss] Initializing hybrid loss with alpha=0.5913638011196574\n\n[Training] Starting final training...\n[Training] Epoch 001 | Batch 000/204 | Current Loss: 1.499505\n[Training] Epoch 001 | Batch 010/204 | Current Loss: 0.185561\n[Training] Epoch 001 | Batch 020/204 | Current Loss: 0.123503\n[Training] Epoch 001 | Batch 030/204 | Current Loss: 0.113534\n[Training] Epoch 001 | Batch 040/204 | Current Loss: 0.113072\n[Training] Epoch 001 | Batch 050/204 | Current Loss: 0.107399\n[Training] Epoch 001 | Batch 060/204 | Current Loss: 0.113848\n[Training] Epoch 001 | Batch 070/204 | Current Loss: 0.108363\n[Training] Epoch 001 | Batch 080/204 | Current Loss: 0.105048\n[Training] Epoch 001 | Batch 090/204 | Current Loss: 0.104730\n[Training] Epoch 001 | Batch 100/204 | Current Loss: 0.117451\n[Training] Epoch 001 | Batch 110/204 | Current Loss: 0.104358\n[Training] Epoch 001 | Batch 120/204 | Current Loss: 0.104780\n[Training] Epoch 001 | Batch 130/204 | Current Loss: 0.104453\n[Training] Epoch 001 | Batch 140/204 | Current Loss: 0.102779\n[Training] Epoch 001 | Batch 150/204 | Current Loss: 0.102942\n[Training] Epoch 001 | Batch 160/204 | Current Loss: 0.113696\n[Training] Epoch 001 | Batch 170/204 | Current Loss: 0.104148\n[Training] Epoch 001 | Batch 180/204 | Current Loss: 0.103696\n[Training] Epoch 001 | Batch 190/204 | Current Loss: 0.103964\n[Training] Epoch 001 | Batch 200/204 | Current Loss: 0.102532\n\n[Training] Epoch 001 Summary:\n  Avg Loss: 0.126737\n  Current LR: 8.42e-04\n[EarlyStopping] Loss improved (inf → 0.126737). Saving model...\n[Training] Epoch 002 | Batch 000/204 | Current Loss: 0.105199\n[Training] Epoch 002 | Batch 010/204 | Current Loss: 0.109259\n[Training] Epoch 002 | Batch 020/204 | Current Loss: 0.104179\n[Training] Epoch 002 | Batch 030/204 | Current Loss: 0.104042\n[Training] Epoch 002 | Batch 040/204 | Current Loss: 0.105614\n[Training] Epoch 002 | Batch 050/204 | Current Loss: 0.105807\n[Training] Epoch 002 | Batch 060/204 | Current Loss: 0.102964\n[Training] Epoch 002 | Batch 070/204 | Current Loss: 0.105468\n[Training] Epoch 002 | Batch 080/204 | Current Loss: 0.103345\n[Training] Epoch 002 | Batch 090/204 | Current Loss: 0.105157\n[Training] Epoch 002 | Batch 100/204 | Current Loss: 0.103288\n[Training] Epoch 002 | Batch 110/204 | Current Loss: 0.101912\n[Training] Epoch 002 | Batch 120/204 | Current Loss: 0.105130\n[Training] Epoch 002 | Batch 130/204 | Current Loss: 0.102056\n[Training] Epoch 002 | Batch 140/204 | Current Loss: 0.101716\n[Training] Epoch 002 | Batch 150/204 | Current Loss: 0.103599\n[Training] Epoch 002 | Batch 160/204 | Current Loss: 0.102003\n[Training] Epoch 002 | Batch 170/204 | Current Loss: 0.102190\n[Training] Epoch 002 | Batch 180/204 | Current Loss: 0.102190\n[Training] Epoch 002 | Batch 190/204 | Current Loss: 0.101759\n[Training] Epoch 002 | Batch 200/204 | Current Loss: 0.101745\n\n[Training] Epoch 002 Summary:\n  Avg Loss: 0.103393\n  Current LR: 7.81e-04\n[EarlyStopping] Loss improved (0.126737 → 0.103393). Saving model...\n[Training] Epoch 003 | Batch 000/204 | Current Loss: 0.101681\n[Training] Epoch 003 | Batch 010/204 | Current Loss: 0.101605\n[Training] Epoch 003 | Batch 020/204 | Current Loss: 0.101523\n[Training] Epoch 003 | Batch 030/204 | Current Loss: 0.101769\n[Training] Epoch 003 | Batch 040/204 | Current Loss: 0.101903\n[Training] Epoch 003 | Batch 050/204 | Current Loss: 0.101594\n[Training] Epoch 003 | Batch 060/204 | Current Loss: 0.101473\n[Training] Epoch 003 | Batch 070/204 | Current Loss: 0.101645\n[Training] Epoch 003 | Batch 080/204 | Current Loss: 0.101339\n[Training] Epoch 003 | Batch 090/204 | Current Loss: 0.101364\n[Training] Epoch 003 | Batch 100/204 | Current Loss: 0.101242\n[Training] Epoch 003 | Batch 110/204 | Current Loss: 0.101228\n[Training] Epoch 003 | Batch 120/204 | Current Loss: 0.102403\n[Training] Epoch 003 | Batch 130/204 | Current Loss: 0.101077\n[Training] Epoch 003 | Batch 140/204 | Current Loss: 0.102065\n[Training] Epoch 003 | Batch 150/204 | Current Loss: 0.101225\n[Training] Epoch 003 | Batch 160/204 | Current Loss: 0.101238\n[Training] Epoch 003 | Batch 170/204 | Current Loss: 0.101718\n[Training] Epoch 003 | Batch 180/204 | Current Loss: 0.101843\n[Training] Epoch 003 | Batch 190/204 | Current Loss: 0.101308\n[Training] Epoch 003 | Batch 200/204 | Current Loss: 0.101334\n\n[Training] Epoch 003 Summary:\n  Avg Loss: 0.101715\n  Current LR: 6.86e-04\n[EarlyStopping] Loss improved (0.103393 → 0.101715). Saving model...\n[Training] Epoch 004 | Batch 000/204 | Current Loss: 0.101730\n[Training] Epoch 004 | Batch 010/204 | Current Loss: 0.103144\n[Training] Epoch 004 | Batch 020/204 | Current Loss: 0.101260\n[Training] Epoch 004 | Batch 030/204 | Current Loss: 0.101596\n[Training] Epoch 004 | Batch 040/204 | Current Loss: 0.101187\n[Training] Epoch 004 | Batch 050/204 | Current Loss: 0.101323\n[Training] Epoch 004 | Batch 060/204 | Current Loss: 0.101291\n[Training] Epoch 004 | Batch 070/204 | Current Loss: 0.101553\n[Training] Epoch 004 | Batch 080/204 | Current Loss: 0.100906\n[Training] Epoch 004 | Batch 090/204 | Current Loss: 0.101146\n[Training] Epoch 004 | Batch 100/204 | Current Loss: 0.100999\n[Training] Epoch 004 | Batch 110/204 | Current Loss: 0.101319\n[Training] Epoch 004 | Batch 120/204 | Current Loss: 0.100981\n[Training] Epoch 004 | Batch 130/204 | Current Loss: 0.100962\n[Training] Epoch 004 | Batch 140/204 | Current Loss: 0.100728\n[Training] Epoch 004 | Batch 150/204 | Current Loss: 0.101027\n[Training] Epoch 004 | Batch 160/204 | Current Loss: 0.100786\n[Training] Epoch 004 | Batch 170/204 | Current Loss: 0.100919\n[Training] Epoch 004 | Batch 180/204 | Current Loss: 0.100949\n[Training] Epoch 004 | Batch 190/204 | Current Loss: 0.100857\n[Training] Epoch 004 | Batch 200/204 | Current Loss: 0.100803\n\n[Training] Epoch 004 Summary:\n  Avg Loss: 0.101313\n  Current LR: 5.65e-04\n[EarlyStopping] Loss improved (0.101715 → 0.101313). Saving model...\n[Training] Epoch 005 | Batch 000/204 | Current Loss: 0.101012\n[Training] Epoch 005 | Batch 010/204 | Current Loss: 0.100746\n[Training] Epoch 005 | Batch 020/204 | Current Loss: 0.100907\n[Training] Epoch 005 | Batch 030/204 | Current Loss: 0.100802\n[Training] Epoch 005 | Batch 040/204 | Current Loss: 0.100903\n[Training] Epoch 005 | Batch 050/204 | Current Loss: 0.101094\n[Training] Epoch 005 | Batch 060/204 | Current Loss: 0.100960\n[Training] Epoch 005 | Batch 070/204 | Current Loss: 0.100950\n[Training] Epoch 005 | Batch 080/204 | Current Loss: 0.101094\n[Training] Epoch 005 | Batch 090/204 | Current Loss: 0.100746\n[Training] Epoch 005 | Batch 100/204 | Current Loss: 0.100702\n[Training] Epoch 005 | Batch 110/204 | Current Loss: 0.100883\n[Training] Epoch 005 | Batch 120/204 | Current Loss: 0.100810\n[Training] Epoch 005 | Batch 130/204 | Current Loss: 0.100638\n[Training] Epoch 005 | Batch 140/204 | Current Loss: 0.100732\n[Training] Epoch 005 | Batch 150/204 | Current Loss: 0.100721\n[Training] Epoch 005 | Batch 160/204 | Current Loss: 0.100839\n[Training] Epoch 005 | Batch 170/204 | Current Loss: 0.100701\n[Training] Epoch 005 | Batch 180/204 | Current Loss: 0.100784\n[Training] Epoch 005 | Batch 190/204 | Current Loss: 0.100710\n[Training] Epoch 005 | Batch 200/204 | Current Loss: 0.100816\n\n[Training] Epoch 005 Summary:\n  Avg Loss: 0.100798\n  Current LR: 4.32e-04\n[EarlyStopping] Loss improved (0.101313 → 0.100798). Saving model...\n[Training] Epoch 006 | Batch 000/204 | Current Loss: 0.100672\n[Training] Epoch 006 | Batch 010/204 | Current Loss: 0.100930\n[Training] Epoch 006 | Batch 020/204 | Current Loss: 0.100756\n[Training] Epoch 006 | Batch 030/204 | Current Loss: 0.100603\n[Training] Epoch 006 | Batch 040/204 | Current Loss: 0.100752\n[Training] Epoch 006 | Batch 050/204 | Current Loss: 0.100571\n[Training] Epoch 006 | Batch 060/204 | Current Loss: 0.100759\n[Training] Epoch 006 | Batch 070/204 | Current Loss: 0.100595\n[Training] Epoch 006 | Batch 080/204 | Current Loss: 0.100604\n[Training] Epoch 006 | Batch 090/204 | Current Loss: 0.100437\n[Training] Epoch 006 | Batch 100/204 | Current Loss: 0.100574\n[Training] Epoch 006 | Batch 110/204 | Current Loss: 0.100509\n[Training] Epoch 006 | Batch 120/204 | Current Loss: 0.100503\n[Training] Epoch 006 | Batch 130/204 | Current Loss: 0.100467\n[Training] Epoch 006 | Batch 140/204 | Current Loss: 0.100412\n[Training] Epoch 006 | Batch 150/204 | Current Loss: 0.100568\n[Training] Epoch 006 | Batch 160/204 | Current Loss: 0.100553\n[Training] Epoch 006 | Batch 170/204 | Current Loss: 0.100518\n[Training] Epoch 006 | Batch 180/204 | Current Loss: 0.100583\n[Training] Epoch 006 | Batch 190/204 | Current Loss: 0.100642\n[Training] Epoch 006 | Batch 200/204 | Current Loss: 0.100547\n\n[Training] Epoch 006 Summary:\n  Avg Loss: 0.100639\n  Current LR: 2.99e-04\n[EarlyStopping] Loss improved (0.100798 → 0.100639). Saving model...\n[Training] Epoch 007 | Batch 000/204 | Current Loss: 0.100662\n[Training] Epoch 007 | Batch 010/204 | Current Loss: 0.100607\n[Training] Epoch 007 | Batch 020/204 | Current Loss: 0.100602\n[Training] Epoch 007 | Batch 030/204 | Current Loss: 0.100594\n[Training] Epoch 007 | Batch 040/204 | Current Loss: 0.100645\n[Training] Epoch 007 | Batch 050/204 | Current Loss: 0.100581\n[Training] Epoch 007 | Batch 060/204 | Current Loss: 0.100455\n[Training] Epoch 007 | Batch 070/204 | Current Loss: 0.100601\n[Training] Epoch 007 | Batch 080/204 | Current Loss: 0.100499\n[Training] Epoch 007 | Batch 090/204 | Current Loss: 0.100508\n[Training] Epoch 007 | Batch 100/204 | Current Loss: 0.100577\n[Training] Epoch 007 | Batch 110/204 | Current Loss: 0.100448\n[Training] Epoch 007 | Batch 120/204 | Current Loss: 0.100416\n[Training] Epoch 007 | Batch 130/204 | Current Loss: 0.100580\n[Training] Epoch 007 | Batch 140/204 | Current Loss: 0.100517\n[Training] Epoch 007 | Batch 150/204 | Current Loss: 0.100612\n[Training] Epoch 007 | Batch 160/204 | Current Loss: 0.100539\n[Training] Epoch 007 | Batch 170/204 | Current Loss: 0.100478\n[Training] Epoch 007 | Batch 180/204 | Current Loss: 0.100462\n[Training] Epoch 007 | Batch 190/204 | Current Loss: 0.100572\n[Training] Epoch 007 | Batch 200/204 | Current Loss: 0.100419\n\n[Training] Epoch 007 Summary:\n  Avg Loss: 0.100541\n  Current LR: 1.79e-04\n[EarlyStopping] Loss improved (0.100639 → 0.100541). Saving model...\n[Training] Epoch 008 | Batch 000/204 | Current Loss: 0.100384\n[Training] Epoch 008 | Batch 010/204 | Current Loss: 0.100601\n[Training] Epoch 008 | Batch 020/204 | Current Loss: 0.100604\n[Training] Epoch 008 | Batch 030/204 | Current Loss: 0.100672\n[Training] Epoch 008 | Batch 040/204 | Current Loss: 0.100465\n[Training] Epoch 008 | Batch 050/204 | Current Loss: 0.100468\n[Training] Epoch 008 | Batch 060/204 | Current Loss: 0.100353\n[Training] Epoch 008 | Batch 070/204 | Current Loss: 0.100546\n[Training] Epoch 008 | Batch 080/204 | Current Loss: 0.100723\n[Training] Epoch 008 | Batch 090/204 | Current Loss: 0.100475\n[Training] Epoch 008 | Batch 100/204 | Current Loss: 0.100521\n[Training] Epoch 008 | Batch 110/204 | Current Loss: 0.100341\n[Training] Epoch 008 | Batch 120/204 | Current Loss: 0.100425\n[Training] Epoch 008 | Batch 130/204 | Current Loss: 0.100419\n[Training] Epoch 008 | Batch 140/204 | Current Loss: 0.100576\n[Training] Epoch 008 | Batch 150/204 | Current Loss: 0.100377\n[Training] Epoch 008 | Batch 160/204 | Current Loss: 0.100489\n[Training] Epoch 008 | Batch 170/204 | Current Loss: 0.100642\n[Training] Epoch 008 | Batch 180/204 | Current Loss: 0.100498\n[Training] Epoch 008 | Batch 190/204 | Current Loss: 0.100467\n[Training] Epoch 008 | Batch 200/204 | Current Loss: 0.100580\n\n[Training] Epoch 008 Summary:\n  Avg Loss: 0.100499\n  Current LR: 8.33e-05\n[EarlyStopping] Loss improved (0.100541 → 0.100499). Saving model...\n[Training] Epoch 009 | Batch 000/204 | Current Loss: 0.100543\n[Training] Epoch 009 | Batch 010/204 | Current Loss: 0.100567\n[Training] Epoch 009 | Batch 020/204 | Current Loss: 0.100628\n[Training] Epoch 009 | Batch 030/204 | Current Loss: 0.100366\n[Training] Epoch 009 | Batch 040/204 | Current Loss: 0.100582\n[Training] Epoch 009 | Batch 050/204 | Current Loss: 0.100434\n[Training] Epoch 009 | Batch 060/204 | Current Loss: 0.100576\n[Training] Epoch 009 | Batch 070/204 | Current Loss: 0.100451\n[Training] Epoch 009 | Batch 080/204 | Current Loss: 0.100368\n[Training] Epoch 009 | Batch 090/204 | Current Loss: 0.100426\n[Training] Epoch 009 | Batch 100/204 | Current Loss: 0.100421\n[Training] Epoch 009 | Batch 110/204 | Current Loss: 0.100411\n[Training] Epoch 009 | Batch 120/204 | Current Loss: 0.100412\n[Training] Epoch 009 | Batch 130/204 | Current Loss: 0.100366\n[Training] Epoch 009 | Batch 140/204 | Current Loss: 0.100238\n[Training] Epoch 009 | Batch 150/204 | Current Loss: 0.100374\n[Training] Epoch 009 | Batch 160/204 | Current Loss: 0.100437\n[Training] Epoch 009 | Batch 170/204 | Current Loss: 0.100536\n[Training] Epoch 009 | Batch 180/204 | Current Loss: 0.100556\n[Training] Epoch 009 | Batch 190/204 | Current Loss: 0.100471\n[Training] Epoch 009 | Batch 200/204 | Current Loss: 0.100525\n\n[Training] Epoch 009 Summary:\n  Avg Loss: 0.100488\n  Current LR: 2.21e-05\n[EarlyStopping] Loss improved (0.100499 → 0.100488). Saving model...\n[Training] Epoch 010 | Batch 000/204 | Current Loss: 0.100507\n[Training] Epoch 010 | Batch 010/204 | Current Loss: 0.100480\n[Training] Epoch 010 | Batch 020/204 | Current Loss: 0.100507\n[Training] Epoch 010 | Batch 030/204 | Current Loss: 0.100443\n[Training] Epoch 010 | Batch 040/204 | Current Loss: 0.100411\n[Training] Epoch 010 | Batch 050/204 | Current Loss: 0.100514\n[Training] Epoch 010 | Batch 060/204 | Current Loss: 0.100481\n[Training] Epoch 010 | Batch 070/204 | Current Loss: 0.100518\n[Training] Epoch 010 | Batch 080/204 | Current Loss: 0.100576\n[Training] Epoch 010 | Batch 090/204 | Current Loss: 0.100379\n[Training] Epoch 010 | Batch 100/204 | Current Loss: 0.100442\n[Training] Epoch 010 | Batch 110/204 | Current Loss: 0.100469\n[Training] Epoch 010 | Batch 120/204 | Current Loss: 0.100460\n[Training] Epoch 010 | Batch 130/204 | Current Loss: 0.100485\n[Training] Epoch 010 | Batch 140/204 | Current Loss: 0.100454\n[Training] Epoch 010 | Batch 150/204 | Current Loss: 0.100485\n[Training] Epoch 010 | Batch 160/204 | Current Loss: 0.100384\n[Training] Epoch 010 | Batch 170/204 | Current Loss: 0.100437\n[Training] Epoch 010 | Batch 180/204 | Current Loss: 0.100484\n[Training] Epoch 010 | Batch 190/204 | Current Loss: 0.100430\n[Training] Epoch 010 | Batch 200/204 | Current Loss: 0.100469\n\n[Training] Epoch 010 Summary:\n  Avg Loss: 0.100464\n  Current LR: 8.63e-04\n[EarlyStopping] Loss improved (0.100488 → 0.100464). Saving model...\n[Training] Epoch 011 | Batch 000/204 | Current Loss: 0.100369\n[Training] Epoch 011 | Batch 010/204 | Current Loss: 0.100419\n[Training] Epoch 011 | Batch 020/204 | Current Loss: 0.100599\n[Training] Epoch 011 | Batch 030/204 | Current Loss: 0.100510\n[Training] Epoch 011 | Batch 040/204 | Current Loss: 0.100485\n[Training] Epoch 011 | Batch 050/204 | Current Loss: 0.100507\n[Training] Epoch 011 | Batch 060/204 | Current Loss: 0.100524\n[Training] Epoch 011 | Batch 070/204 | Current Loss: 0.100609\n[Training] Epoch 011 | Batch 080/204 | Current Loss: 0.100571\n[Training] Epoch 011 | Batch 090/204 | Current Loss: 0.100456\n[Training] Epoch 011 | Batch 100/204 | Current Loss: 0.100436\n[Training] Epoch 011 | Batch 110/204 | Current Loss: 0.100509\n[Training] Epoch 011 | Batch 120/204 | Current Loss: 0.100451\n[Training] Epoch 011 | Batch 130/204 | Current Loss: 0.100509\n[Training] Epoch 011 | Batch 140/204 | Current Loss: 0.100385\n[Training] Epoch 011 | Batch 150/204 | Current Loss: 0.100509\n[Training] Epoch 011 | Batch 160/204 | Current Loss: 0.100477\n[Training] Epoch 011 | Batch 170/204 | Current Loss: 0.100518\n[Training] Epoch 011 | Batch 180/204 | Current Loss: 0.100421\n[Training] Epoch 011 | Batch 190/204 | Current Loss: 0.100376\n[Training] Epoch 011 | Batch 200/204 | Current Loss: 0.100435\n\n[Training] Epoch 011 Summary:\n  Avg Loss: 0.100484\n  Current LR: 8.42e-04\n[EarlyStopping] Counter: 1/10\n[Training] Epoch 012 | Batch 000/204 | Current Loss: 0.100535\n[Training] Epoch 012 | Batch 010/204 | Current Loss: 0.100400\n[Training] Epoch 012 | Batch 020/204 | Current Loss: 0.100649\n[Training] Epoch 012 | Batch 030/204 | Current Loss: 0.100361\n[Training] Epoch 012 | Batch 040/204 | Current Loss: 0.100294\n[Training] Epoch 012 | Batch 050/204 | Current Loss: 0.100498\n[Training] Epoch 012 | Batch 060/204 | Current Loss: 0.100389\n[Training] Epoch 012 | Batch 070/204 | Current Loss: 0.100308\n[Training] Epoch 012 | Batch 080/204 | Current Loss: 0.100420\n[Training] Epoch 012 | Batch 090/204 | Current Loss: 0.100416\n[Training] Epoch 012 | Batch 100/204 | Current Loss: 0.100405\n[Training] Epoch 012 | Batch 110/204 | Current Loss: 0.100404\n[Training] Epoch 012 | Batch 120/204 | Current Loss: 0.100437\n[Training] Epoch 012 | Batch 130/204 | Current Loss: 0.100432\n[Training] Epoch 012 | Batch 140/204 | Current Loss: 0.100436\n[Training] Epoch 012 | Batch 150/204 | Current Loss: 0.100264\n[Training] Epoch 012 | Batch 160/204 | Current Loss: 0.100390\n[Training] Epoch 012 | Batch 170/204 | Current Loss: 0.100488\n[Training] Epoch 012 | Batch 180/204 | Current Loss: 0.100399\n[Training] Epoch 012 | Batch 190/204 | Current Loss: 0.100325\n[Training] Epoch 012 | Batch 200/204 | Current Loss: 0.100317\n\n[Training] Epoch 012 Summary:\n  Avg Loss: 0.100394\n  Current LR: 7.81e-04\n[EarlyStopping] Loss improved (0.100464 → 0.100394). Saving model...\n[Training] Epoch 013 | Batch 000/204 | Current Loss: 0.100491\n[Training] Epoch 013 | Batch 010/204 | Current Loss: 0.100396\n[Training] Epoch 013 | Batch 020/204 | Current Loss: 0.100423\n[Training] Epoch 013 | Batch 030/204 | Current Loss: 0.100430\n[Training] Epoch 013 | Batch 040/204 | Current Loss: 0.100454\n[Training] Epoch 013 | Batch 050/204 | Current Loss: 0.100328\n[Training] Epoch 013 | Batch 060/204 | Current Loss: 0.100368\n[Training] Epoch 013 | Batch 070/204 | Current Loss: 0.100241\n[Training] Epoch 013 | Batch 080/204 | Current Loss: 0.100352\n[Training] Epoch 013 | Batch 090/204 | Current Loss: 0.100384\n[Training] Epoch 013 | Batch 100/204 | Current Loss: 0.100255\n[Training] Epoch 013 | Batch 110/204 | Current Loss: 0.100325\n[Training] Epoch 013 | Batch 120/204 | Current Loss: 0.100280\n[Training] Epoch 013 | Batch 130/204 | Current Loss: 0.100328\n[Training] Epoch 013 | Batch 140/204 | Current Loss: 0.100361\n[Training] Epoch 013 | Batch 150/204 | Current Loss: 0.100361\n[Training] Epoch 013 | Batch 160/204 | Current Loss: 0.100246\n[Training] Epoch 013 | Batch 170/204 | Current Loss: 0.100363\n[Training] Epoch 013 | Batch 180/204 | Current Loss: 0.100256\n[Training] Epoch 013 | Batch 190/204 | Current Loss: 0.100383\n[Training] Epoch 013 | Batch 200/204 | Current Loss: 0.100281\n\n[Training] Epoch 013 Summary:\n  Avg Loss: 0.100349\n  Current LR: 6.86e-04\n[EarlyStopping] Loss improved (0.100394 → 0.100349). Saving model...\n[Training] Epoch 014 | Batch 000/204 | Current Loss: 0.100860\n[Training] Epoch 014 | Batch 010/204 | Current Loss: 0.100456\n[Training] Epoch 014 | Batch 020/204 | Current Loss: 0.100436\n[Training] Epoch 014 | Batch 030/204 | Current Loss: 0.100316\n[Training] Epoch 014 | Batch 040/204 | Current Loss: 0.100250\n[Training] Epoch 014 | Batch 050/204 | Current Loss: 0.100302\n[Training] Epoch 014 | Batch 060/204 | Current Loss: 0.100317\n[Training] Epoch 014 | Batch 070/204 | Current Loss: 0.100320\n[Training] Epoch 014 | Batch 080/204 | Current Loss: 0.100254\n[Training] Epoch 014 | Batch 090/204 | Current Loss: 0.100307\n[Training] Epoch 014 | Batch 100/204 | Current Loss: 0.100319\n[Training] Epoch 014 | Batch 110/204 | Current Loss: 0.100310\n[Training] Epoch 014 | Batch 120/204 | Current Loss: 0.100283\n[Training] Epoch 014 | Batch 130/204 | Current Loss: 0.100278\n[Training] Epoch 014 | Batch 140/204 | Current Loss: 0.100322\n[Training] Epoch 014 | Batch 150/204 | Current Loss: 0.100360\n[Training] Epoch 014 | Batch 160/204 | Current Loss: 0.100307\n[Training] Epoch 014 | Batch 170/204 | Current Loss: 0.100191\n[Training] Epoch 014 | Batch 180/204 | Current Loss: 0.100207\n[Training] Epoch 014 | Batch 190/204 | Current Loss: 0.100169\n[Training] Epoch 014 | Batch 200/204 | Current Loss: 0.100192\n\n[Training] Epoch 014 Summary:\n  Avg Loss: 0.100319\n  Current LR: 5.65e-04\n[EarlyStopping] Loss improved (0.100349 → 0.100319). Saving model...\n[Training] Epoch 015 | Batch 000/204 | Current Loss: 0.100264\n[Training] Epoch 015 | Batch 010/204 | Current Loss: 0.100371\n[Training] Epoch 015 | Batch 020/204 | Current Loss: 0.100318\n[Training] Epoch 015 | Batch 030/204 | Current Loss: 0.100190\n[Training] Epoch 015 | Batch 040/204 | Current Loss: 0.100277\n[Training] Epoch 015 | Batch 050/204 | Current Loss: 0.100272\n[Training] Epoch 015 | Batch 060/204 | Current Loss: 0.100277\n[Training] Epoch 015 | Batch 070/204 | Current Loss: 0.100262\n[Training] Epoch 015 | Batch 080/204 | Current Loss: 0.100201\n[Training] Epoch 015 | Batch 090/204 | Current Loss: 0.100151\n[Training] Epoch 015 | Batch 100/204 | Current Loss: 0.100177\n[Training] Epoch 015 | Batch 110/204 | Current Loss: 0.100245\n[Training] Epoch 015 | Batch 120/204 | Current Loss: 0.100259\n[Training] Epoch 015 | Batch 130/204 | Current Loss: 0.100040\n[Training] Epoch 015 | Batch 140/204 | Current Loss: 0.100279\n[Training] Epoch 015 | Batch 150/204 | Current Loss: 0.100210\n[Training] Epoch 015 | Batch 160/204 | Current Loss: 0.100180\n[Training] Epoch 015 | Batch 170/204 | Current Loss: 0.100193\n[Training] Epoch 015 | Batch 180/204 | Current Loss: 0.100155\n[Training] Epoch 015 | Batch 190/204 | Current Loss: 0.100249\n[Training] Epoch 015 | Batch 200/204 | Current Loss: 0.100215\n\n[Training] Epoch 015 Summary:\n  Avg Loss: 0.100265\n  Current LR: 4.32e-04\n[EarlyStopping] Loss improved (0.100319 → 0.100265). Saving model...\n[Training] Epoch 016 | Batch 000/204 | Current Loss: 0.100193\n[Training] Epoch 016 | Batch 010/204 | Current Loss: 0.100208\n[Training] Epoch 016 | Batch 020/204 | Current Loss: 0.100254\n[Training] Epoch 016 | Batch 030/204 | Current Loss: 0.100243\n[Training] Epoch 016 | Batch 040/204 | Current Loss: 0.100253\n[Training] Epoch 016 | Batch 050/204 | Current Loss: 0.100154\n[Training] Epoch 016 | Batch 060/204 | Current Loss: 0.100222\n[Training] Epoch 016 | Batch 070/204 | Current Loss: 0.100190\n[Training] Epoch 016 | Batch 080/204 | Current Loss: 0.100302\n[Training] Epoch 016 | Batch 090/204 | Current Loss: 0.100210\n[Training] Epoch 016 | Batch 100/204 | Current Loss: 0.100252\n[Training] Epoch 016 | Batch 110/204 | Current Loss: 0.100256\n[Training] Epoch 016 | Batch 120/204 | Current Loss: 0.100298\n[Training] Epoch 016 | Batch 130/204 | Current Loss: 0.100289\n[Training] Epoch 016 | Batch 140/204 | Current Loss: 0.100168\n[Training] Epoch 016 | Batch 150/204 | Current Loss: 0.100330\n[Training] Epoch 016 | Batch 160/204 | Current Loss: 0.100308\n[Training] Epoch 016 | Batch 170/204 | Current Loss: 0.100195\n[Training] Epoch 016 | Batch 180/204 | Current Loss: 0.100303\n[Training] Epoch 016 | Batch 190/204 | Current Loss: 0.100307\n[Training] Epoch 016 | Batch 200/204 | Current Loss: 0.100174\n\n[Training] Epoch 016 Summary:\n  Avg Loss: 0.100245\n  Current LR: 2.99e-04\n[EarlyStopping] Loss improved (0.100265 → 0.100245). Saving model...\n[Training] Epoch 017 | Batch 000/204 | Current Loss: 0.100387\n[Training] Epoch 017 | Batch 010/204 | Current Loss: 0.100260\n[Training] Epoch 017 | Batch 020/204 | Current Loss: 0.100150\n[Training] Epoch 017 | Batch 030/204 | Current Loss: 0.100252\n[Training] Epoch 017 | Batch 040/204 | Current Loss: 0.100174\n[Training] Epoch 017 | Batch 050/204 | Current Loss: 0.100251\n[Training] Epoch 017 | Batch 060/204 | Current Loss: 0.100176\n[Training] Epoch 017 | Batch 070/204 | Current Loss: 0.100170\n[Training] Epoch 017 | Batch 080/204 | Current Loss: 0.100130\n[Training] Epoch 017 | Batch 090/204 | Current Loss: 0.100172\n[Training] Epoch 017 | Batch 100/204 | Current Loss: 0.100257\n[Training] Epoch 017 | Batch 110/204 | Current Loss: 0.100150\n[Training] Epoch 017 | Batch 120/204 | Current Loss: 0.100300\n[Training] Epoch 017 | Batch 130/204 | Current Loss: 0.100156\n[Training] Epoch 017 | Batch 140/204 | Current Loss: 0.100177\n[Training] Epoch 017 | Batch 150/204 | Current Loss: 0.100182\n[Training] Epoch 017 | Batch 160/204 | Current Loss: 0.100217\n[Training] Epoch 017 | Batch 170/204 | Current Loss: 0.100380\n[Training] Epoch 017 | Batch 180/204 | Current Loss: 0.100288\n[Training] Epoch 017 | Batch 190/204 | Current Loss: 0.100268\n[Training] Epoch 017 | Batch 200/204 | Current Loss: 0.100223\n\n[Training] Epoch 017 Summary:\n  Avg Loss: 0.100241\n  Current LR: 1.79e-04\n[EarlyStopping] Loss improved (0.100245 → 0.100241). Saving model...\n[Training] Epoch 018 | Batch 000/204 | Current Loss: 0.100188\n[Training] Epoch 018 | Batch 010/204 | Current Loss: 0.100268\n[Training] Epoch 018 | Batch 020/204 | Current Loss: 0.100340\n[Training] Epoch 018 | Batch 030/204 | Current Loss: 0.100257\n[Training] Epoch 018 | Batch 040/204 | Current Loss: 0.100176\n[Training] Epoch 018 | Batch 050/204 | Current Loss: 0.100207\n[Training] Epoch 018 | Batch 060/204 | Current Loss: 0.100186\n[Training] Epoch 018 | Batch 070/204 | Current Loss: 0.100237\n[Training] Epoch 018 | Batch 080/204 | Current Loss: 0.100088\n[Training] Epoch 018 | Batch 090/204 | Current Loss: 0.100121\n[Training] Epoch 018 | Batch 100/204 | Current Loss: 0.100222\n[Training] Epoch 018 | Batch 110/204 | Current Loss: 0.100177\n[Training] Epoch 018 | Batch 120/204 | Current Loss: 0.100182\n[Training] Epoch 018 | Batch 130/204 | Current Loss: 0.100220\n[Training] Epoch 018 | Batch 140/204 | Current Loss: 0.100269\n[Training] Epoch 018 | Batch 150/204 | Current Loss: 0.100374\n[Training] Epoch 018 | Batch 160/204 | Current Loss: 0.100177\n[Training] Epoch 018 | Batch 170/204 | Current Loss: 0.100199\n[Training] Epoch 018 | Batch 180/204 | Current Loss: 0.100219\n[Training] Epoch 018 | Batch 190/204 | Current Loss: 0.100356\n[Training] Epoch 018 | Batch 200/204 | Current Loss: 0.100181\n\n[Training] Epoch 018 Summary:\n  Avg Loss: 0.100216\n  Current LR: 8.33e-05\n[EarlyStopping] Loss improved (0.100241 → 0.100216). Saving model...\n[Training] Epoch 019 | Batch 000/204 | Current Loss: 0.100152\n[Training] Epoch 019 | Batch 010/204 | Current Loss: 0.100114\n[Training] Epoch 019 | Batch 020/204 | Current Loss: 0.100210\n[Training] Epoch 019 | Batch 030/204 | Current Loss: 0.100263\n[Training] Epoch 019 | Batch 040/204 | Current Loss: 0.100208\n[Training] Epoch 019 | Batch 050/204 | Current Loss: 0.100161\n[Training] Epoch 019 | Batch 060/204 | Current Loss: 0.100177\n[Training] Epoch 019 | Batch 070/204 | Current Loss: 0.100206\n[Training] Epoch 019 | Batch 080/204 | Current Loss: 0.100293\n[Training] Epoch 019 | Batch 090/204 | Current Loss: 0.100202\n[Training] Epoch 019 | Batch 100/204 | Current Loss: 0.100147\n[Training] Epoch 019 | Batch 110/204 | Current Loss: 0.100168\n[Training] Epoch 019 | Batch 120/204 | Current Loss: 0.100278\n[Training] Epoch 019 | Batch 130/204 | Current Loss: 0.100226\n[Training] Epoch 019 | Batch 140/204 | Current Loss: 0.100243\n[Training] Epoch 019 | Batch 150/204 | Current Loss: 0.100244\n[Training] Epoch 019 | Batch 160/204 | Current Loss: 0.100178\n[Training] Epoch 019 | Batch 170/204 | Current Loss: 0.100291\n[Training] Epoch 019 | Batch 180/204 | Current Loss: 0.100148\n[Training] Epoch 019 | Batch 190/204 | Current Loss: 0.100213\n[Training] Epoch 019 | Batch 200/204 | Current Loss: 0.100285\n\n[Training] Epoch 019 Summary:\n  Avg Loss: 0.100206\n  Current LR: 2.21e-05\n[EarlyStopping] Loss improved (0.100216 → 0.100206). Saving model...\n[Training] Epoch 020 | Batch 000/204 | Current Loss: 0.100241\n[Training] Epoch 020 | Batch 010/204 | Current Loss: 0.100198\n[Training] Epoch 020 | Batch 020/204 | Current Loss: 0.100314\n[Training] Epoch 020 | Batch 030/204 | Current Loss: 0.100265\n[Training] Epoch 020 | Batch 040/204 | Current Loss: 0.100223\n[Training] Epoch 020 | Batch 050/204 | Current Loss: 0.100217\n[Training] Epoch 020 | Batch 060/204 | Current Loss: 0.100211\n[Training] Epoch 020 | Batch 070/204 | Current Loss: 0.100102\n[Training] Epoch 020 | Batch 080/204 | Current Loss: 0.100199\n[Training] Epoch 020 | Batch 090/204 | Current Loss: 0.100169\n[Training] Epoch 020 | Batch 100/204 | Current Loss: 0.100125\n[Training] Epoch 020 | Batch 110/204 | Current Loss: 0.100167\n[Training] Epoch 020 | Batch 120/204 | Current Loss: 0.100176\n[Training] Epoch 020 | Batch 130/204 | Current Loss: 0.100221\n[Training] Epoch 020 | Batch 140/204 | Current Loss: 0.100261\n[Training] Epoch 020 | Batch 150/204 | Current Loss: 0.100238\n[Training] Epoch 020 | Batch 160/204 | Current Loss: 0.100128\n[Training] Epoch 020 | Batch 170/204 | Current Loss: 0.100296\n[Training] Epoch 020 | Batch 180/204 | Current Loss: 0.100145\n[Training] Epoch 020 | Batch 190/204 | Current Loss: 0.100212\n[Training] Epoch 020 | Batch 200/204 | Current Loss: 0.100184\n\n[Training] Epoch 020 Summary:\n  Avg Loss: 0.100203\n  Current LR: 8.63e-04\n[EarlyStopping] Loss improved (0.100206 → 0.100203). Saving model...\n[Training] Epoch 021 | Batch 000/204 | Current Loss: 0.100226\n[Training] Epoch 021 | Batch 010/204 | Current Loss: 0.100662\n[Training] Epoch 021 | Batch 020/204 | Current Loss: 0.100267\n[Training] Epoch 021 | Batch 030/204 | Current Loss: 0.100361\n[Training] Epoch 021 | Batch 040/204 | Current Loss: 0.100348\n[Training] Epoch 021 | Batch 050/204 | Current Loss: 0.100292\n[Training] Epoch 021 | Batch 060/204 | Current Loss: 0.100318\n[Training] Epoch 021 | Batch 070/204 | Current Loss: 0.100352\n[Training] Epoch 021 | Batch 080/204 | Current Loss: 0.100426\n[Training] Epoch 021 | Batch 090/204 | Current Loss: 0.100273\n[Training] Epoch 021 | Batch 100/204 | Current Loss: 0.100289\n[Training] Epoch 021 | Batch 110/204 | Current Loss: 0.100362\n[Training] Epoch 021 | Batch 120/204 | Current Loss: 0.100323\n[Training] Epoch 021 | Batch 130/204 | Current Loss: 0.100212\n[Training] Epoch 021 | Batch 140/204 | Current Loss: 0.100241\n[Training] Epoch 021 | Batch 150/204 | Current Loss: 0.100268\n[Training] Epoch 021 | Batch 160/204 | Current Loss: 0.100230\n[Training] Epoch 021 | Batch 170/204 | Current Loss: 0.100357\n[Training] Epoch 021 | Batch 180/204 | Current Loss: 0.100269\n[Training] Epoch 021 | Batch 190/204 | Current Loss: 0.100248\n[Training] Epoch 021 | Batch 200/204 | Current Loss: 0.100377\n\n[Training] Epoch 021 Summary:\n  Avg Loss: 0.100349\n  Current LR: 8.42e-04\n[EarlyStopping] Counter: 1/10\n[Training] Epoch 022 | Batch 000/204 | Current Loss: 0.100327\n[Training] Epoch 022 | Batch 010/204 | Current Loss: 0.100277\n[Training] Epoch 022 | Batch 020/204 | Current Loss: 0.100262\n[Training] Epoch 022 | Batch 030/204 | Current Loss: 0.100245\n[Training] Epoch 022 | Batch 040/204 | Current Loss: 0.100331\n[Training] Epoch 022 | Batch 050/204 | Current Loss: 0.100214\n[Training] Epoch 022 | Batch 060/204 | Current Loss: 0.100381\n[Training] Epoch 022 | Batch 070/204 | Current Loss: 0.100265\n[Training] Epoch 022 | Batch 080/204 | Current Loss: 0.100224\n[Training] Epoch 022 | Batch 090/204 | Current Loss: 0.100312\n[Training] Epoch 022 | Batch 100/204 | Current Loss: 0.100238\n[Training] Epoch 022 | Batch 110/204 | Current Loss: 0.100353\n[Training] Epoch 022 | Batch 120/204 | Current Loss: 0.100236\n[Training] Epoch 022 | Batch 130/204 | Current Loss: 0.100246\n[Training] Epoch 022 | Batch 140/204 | Current Loss: 0.100340\n[Training] Epoch 022 | Batch 150/204 | Current Loss: 0.100232\n[Training] Epoch 022 | Batch 160/204 | Current Loss: 0.100273\n[Training] Epoch 022 | Batch 170/204 | Current Loss: 0.100200\n[Training] Epoch 022 | Batch 180/204 | Current Loss: 0.100395\n[Training] Epoch 022 | Batch 190/204 | Current Loss: 0.100142\n[Training] Epoch 022 | Batch 200/204 | Current Loss: 0.100156\n\n[Training] Epoch 022 Summary:\n  Avg Loss: 0.100251\n  Current LR: 7.81e-04\n[EarlyStopping] Counter: 2/10\n[Training] Epoch 023 | Batch 000/204 | Current Loss: 0.100188\n[Training] Epoch 023 | Batch 010/204 | Current Loss: 0.100171\n[Training] Epoch 023 | Batch 020/204 | Current Loss: 0.100154\n[Training] Epoch 023 | Batch 030/204 | Current Loss: 0.100264\n[Training] Epoch 023 | Batch 040/204 | Current Loss: 0.100201\n[Training] Epoch 023 | Batch 050/204 | Current Loss: 0.100138\n[Training] Epoch 023 | Batch 060/204 | Current Loss: 0.100174\n[Training] Epoch 023 | Batch 070/204 | Current Loss: 0.100219\n[Training] Epoch 023 | Batch 080/204 | Current Loss: 0.100159\n[Training] Epoch 023 | Batch 090/204 | Current Loss: 0.100198\n[Training] Epoch 023 | Batch 100/204 | Current Loss: 0.100284\n[Training] Epoch 023 | Batch 110/204 | Current Loss: 0.100151\n[Training] Epoch 023 | Batch 120/204 | Current Loss: 0.100249\n[Training] Epoch 023 | Batch 130/204 | Current Loss: 0.100244\n[Training] Epoch 023 | Batch 140/204 | Current Loss: 0.100166\n[Training] Epoch 023 | Batch 150/204 | Current Loss: 0.100090\n[Training] Epoch 023 | Batch 160/204 | Current Loss: 0.100236\n[Training] Epoch 023 | Batch 170/204 | Current Loss: 0.100107\n[Training] Epoch 023 | Batch 180/204 | Current Loss: 0.100106\n[Training] Epoch 023 | Batch 190/204 | Current Loss: 0.100238\n[Training] Epoch 023 | Batch 200/204 | Current Loss: 0.100152\n\n[Training] Epoch 023 Summary:\n  Avg Loss: 0.100219\n  Current LR: 6.86e-04\n[EarlyStopping] Counter: 3/10\n[Training] Epoch 024 | Batch 000/204 | Current Loss: 0.100200\n[Training] Epoch 024 | Batch 010/204 | Current Loss: 0.100226\n[Training] Epoch 024 | Batch 020/204 | Current Loss: 0.100234\n[Training] Epoch 024 | Batch 030/204 | Current Loss: 0.100320\n[Training] Epoch 024 | Batch 040/204 | Current Loss: 0.100134\n[Training] Epoch 024 | Batch 050/204 | Current Loss: 0.100320\n[Training] Epoch 024 | Batch 060/204 | Current Loss: 0.100148\n[Training] Epoch 024 | Batch 070/204 | Current Loss: 0.100191\n[Training] Epoch 024 | Batch 080/204 | Current Loss: 0.100063\n[Training] Epoch 024 | Batch 090/204 | Current Loss: 0.100198\n[Training] Epoch 024 | Batch 100/204 | Current Loss: 0.100272\n[Training] Epoch 024 | Batch 110/204 | Current Loss: 0.100250\n[Training] Epoch 024 | Batch 120/204 | Current Loss: 0.100219\n[Training] Epoch 024 | Batch 130/204 | Current Loss: 0.100187\n[Training] Epoch 024 | Batch 140/204 | Current Loss: 0.100157\n[Training] Epoch 024 | Batch 150/204 | Current Loss: 0.100239\n[Training] Epoch 024 | Batch 160/204 | Current Loss: 0.100021\n[Training] Epoch 024 | Batch 170/204 | Current Loss: 0.100255\n[Training] Epoch 024 | Batch 180/204 | Current Loss: 0.100205\n[Training] Epoch 024 | Batch 190/204 | Current Loss: 0.100292\n[Training] Epoch 024 | Batch 200/204 | Current Loss: 0.100201\n\n[Training] Epoch 024 Summary:\n  Avg Loss: 0.100193\n  Current LR: 5.65e-04\n[EarlyStopping] Loss improved (0.100203 → 0.100193). Saving model...\n[Training] Epoch 025 | Batch 000/204 | Current Loss: 0.100208\n[Training] Epoch 025 | Batch 010/204 | Current Loss: 0.100179\n[Training] Epoch 025 | Batch 020/204 | Current Loss: 0.100241\n[Training] Epoch 025 | Batch 030/204 | Current Loss: 0.100226\n[Training] Epoch 025 | Batch 040/204 | Current Loss: 0.100221\n[Training] Epoch 025 | Batch 050/204 | Current Loss: 0.100199\n[Training] Epoch 025 | Batch 060/204 | Current Loss: 0.100195\n[Training] Epoch 025 | Batch 070/204 | Current Loss: 0.100137\n[Training] Epoch 025 | Batch 080/204 | Current Loss: 0.100161\n[Training] Epoch 025 | Batch 090/204 | Current Loss: 0.100249\n[Training] Epoch 025 | Batch 100/204 | Current Loss: 0.100161\n[Training] Epoch 025 | Batch 110/204 | Current Loss: 0.100097\n[Training] Epoch 025 | Batch 120/204 | Current Loss: 0.100212\n[Training] Epoch 025 | Batch 130/204 | Current Loss: 0.100193\n[Training] Epoch 025 | Batch 140/204 | Current Loss: 0.100252\n[Training] Epoch 025 | Batch 150/204 | Current Loss: 0.100178\n[Training] Epoch 025 | Batch 160/204 | Current Loss: 0.100153\n[Training] Epoch 025 | Batch 170/204 | Current Loss: 0.100227\n[Training] Epoch 025 | Batch 180/204 | Current Loss: 0.100096\n[Training] Epoch 025 | Batch 190/204 | Current Loss: 0.100151\n[Training] Epoch 025 | Batch 200/204 | Current Loss: 0.100188\n\n[Training] Epoch 025 Summary:\n  Avg Loss: 0.100177\n  Current LR: 4.32e-04\n[EarlyStopping] Loss improved (0.100193 → 0.100177). Saving model...\n[Training] Epoch 026 | Batch 000/204 | Current Loss: 0.100157\n[Training] Epoch 026 | Batch 010/204 | Current Loss: 0.100149\n[Training] Epoch 026 | Batch 020/204 | Current Loss: 0.100301\n[Training] Epoch 026 | Batch 030/204 | Current Loss: 0.100201\n[Training] Epoch 026 | Batch 040/204 | Current Loss: 0.100185\n[Training] Epoch 026 | Batch 050/204 | Current Loss: 0.100217\n[Training] Epoch 026 | Batch 060/204 | Current Loss: 0.100188\n[Training] Epoch 026 | Batch 070/204 | Current Loss: 0.100180\n[Training] Epoch 026 | Batch 080/204 | Current Loss: 0.100060\n[Training] Epoch 026 | Batch 090/204 | Current Loss: 0.100148\n[Training] Epoch 026 | Batch 100/204 | Current Loss: 0.100146\n[Training] Epoch 026 | Batch 110/204 | Current Loss: 0.100199\n[Training] Epoch 026 | Batch 120/204 | Current Loss: 0.100134\n[Training] Epoch 026 | Batch 130/204 | Current Loss: 0.100078\n[Training] Epoch 026 | Batch 140/204 | Current Loss: 0.100238\n[Training] Epoch 026 | Batch 150/204 | Current Loss: 0.100158\n[Training] Epoch 026 | Batch 160/204 | Current Loss: 0.100187\n[Training] Epoch 026 | Batch 170/204 | Current Loss: 0.100143\n[Training] Epoch 026 | Batch 180/204 | Current Loss: 0.100068\n[Training] Epoch 026 | Batch 190/204 | Current Loss: 0.100192\n[Training] Epoch 026 | Batch 200/204 | Current Loss: 0.100272\n\n[Training] Epoch 026 Summary:\n  Avg Loss: 0.100159\n  Current LR: 2.99e-04\n[EarlyStopping] Loss improved (0.100177 → 0.100159). Saving model...\n[Training] Epoch 027 | Batch 000/204 | Current Loss: 0.100206\n[Training] Epoch 027 | Batch 010/204 | Current Loss: 0.100161\n[Training] Epoch 027 | Batch 020/204 | Current Loss: 0.100225\n[Training] Epoch 027 | Batch 030/204 | Current Loss: 0.100105\n[Training] Epoch 027 | Batch 040/204 | Current Loss: 0.100210\n[Training] Epoch 027 | Batch 050/204 | Current Loss: 0.100185\n[Training] Epoch 027 | Batch 060/204 | Current Loss: 0.100129\n[Training] Epoch 027 | Batch 070/204 | Current Loss: 0.100189\n[Training] Epoch 027 | Batch 080/204 | Current Loss: 0.100163\n[Training] Epoch 027 | Batch 090/204 | Current Loss: 0.100111\n[Training] Epoch 027 | Batch 100/204 | Current Loss: 0.100170\n[Training] Epoch 027 | Batch 110/204 | Current Loss: 0.100111\n[Training] Epoch 027 | Batch 120/204 | Current Loss: 0.100211\n[Training] Epoch 027 | Batch 130/204 | Current Loss: 0.100179\n[Training] Epoch 027 | Batch 140/204 | Current Loss: 0.100173\n[Training] Epoch 027 | Batch 150/204 | Current Loss: 0.100084\n[Training] Epoch 027 | Batch 160/204 | Current Loss: 0.100111\n[Training] Epoch 027 | Batch 170/204 | Current Loss: 0.100125\n[Training] Epoch 027 | Batch 180/204 | Current Loss: 0.100081\n[Training] Epoch 027 | Batch 190/204 | Current Loss: 0.100075\n[Training] Epoch 027 | Batch 200/204 | Current Loss: 0.100122\n\n[Training] Epoch 027 Summary:\n  Avg Loss: 0.100152\n  Current LR: 1.79e-04\n[EarlyStopping] Loss improved (0.100159 → 0.100152). Saving model...\n[Training] Epoch 028 | Batch 000/204 | Current Loss: 0.100022\n[Training] Epoch 028 | Batch 010/204 | Current Loss: 0.100157\n[Training] Epoch 028 | Batch 020/204 | Current Loss: 0.100095\n[Training] Epoch 028 | Batch 030/204 | Current Loss: 0.100187\n[Training] Epoch 028 | Batch 040/204 | Current Loss: 0.100094\n[Training] Epoch 028 | Batch 050/204 | Current Loss: 0.100026\n[Training] Epoch 028 | Batch 060/204 | Current Loss: 0.100174\n[Training] Epoch 028 | Batch 070/204 | Current Loss: 0.100155\n[Training] Epoch 028 | Batch 080/204 | Current Loss: 0.100139\n[Training] Epoch 028 | Batch 090/204 | Current Loss: 0.100076\n[Training] Epoch 028 | Batch 100/204 | Current Loss: 0.100084\n[Training] Epoch 028 | Batch 110/204 | Current Loss: 0.100107\n[Training] Epoch 028 | Batch 120/204 | Current Loss: 0.100145\n[Training] Epoch 028 | Batch 130/204 | Current Loss: 0.100155\n[Training] Epoch 028 | Batch 140/204 | Current Loss: 0.100147\n[Training] Epoch 028 | Batch 150/204 | Current Loss: 0.100187\n[Training] Epoch 028 | Batch 160/204 | Current Loss: 0.100145\n[Training] Epoch 028 | Batch 170/204 | Current Loss: 0.100148\n[Training] Epoch 028 | Batch 180/204 | Current Loss: 0.100123\n[Training] Epoch 028 | Batch 190/204 | Current Loss: 0.100090\n[Training] Epoch 028 | Batch 200/204 | Current Loss: 0.100143\n\n[Training] Epoch 028 Summary:\n  Avg Loss: 0.100136\n  Current LR: 8.33e-05\n[EarlyStopping] Loss improved (0.100152 → 0.100136). Saving model...\n[Training] Epoch 029 | Batch 000/204 | Current Loss: 0.100126\n[Training] Epoch 029 | Batch 010/204 | Current Loss: 0.100139\n[Training] Epoch 029 | Batch 020/204 | Current Loss: 0.100108\n[Training] Epoch 029 | Batch 030/204 | Current Loss: 0.100016\n[Training] Epoch 029 | Batch 040/204 | Current Loss: 0.100120\n[Training] Epoch 029 | Batch 050/204 | Current Loss: 0.100084\n[Training] Epoch 029 | Batch 060/204 | Current Loss: 0.100117\n[Training] Epoch 029 | Batch 070/204 | Current Loss: 0.100121\n[Training] Epoch 029 | Batch 080/204 | Current Loss: 0.100092\n[Training] Epoch 029 | Batch 090/204 | Current Loss: 0.100111\n[Training] Epoch 029 | Batch 100/204 | Current Loss: 0.100129\n[Training] Epoch 029 | Batch 110/204 | Current Loss: 0.100119\n[Training] Epoch 029 | Batch 120/204 | Current Loss: 0.100120\n[Training] Epoch 029 | Batch 130/204 | Current Loss: 0.100215\n[Training] Epoch 029 | Batch 140/204 | Current Loss: 0.100081\n[Training] Epoch 029 | Batch 150/204 | Current Loss: 0.100097\n[Training] Epoch 029 | Batch 160/204 | Current Loss: 0.100053\n[Training] Epoch 029 | Batch 170/204 | Current Loss: 0.100122\n[Training] Epoch 029 | Batch 180/204 | Current Loss: 0.100165\n[Training] Epoch 029 | Batch 190/204 | Current Loss: 0.100073\n[Training] Epoch 029 | Batch 200/204 | Current Loss: 0.100062\n\n[Training] Epoch 029 Summary:\n  Avg Loss: 0.100123\n  Current LR: 2.21e-05\n[EarlyStopping] Loss improved (0.100136 → 0.100123). Saving model...\n[Training] Epoch 030 | Batch 000/204 | Current Loss: 0.100131\n[Training] Epoch 030 | Batch 010/204 | Current Loss: 0.100116\n[Training] Epoch 030 | Batch 020/204 | Current Loss: 0.100195\n[Training] Epoch 030 | Batch 030/204 | Current Loss: 0.100101\n[Training] Epoch 030 | Batch 040/204 | Current Loss: 0.100185\n[Training] Epoch 030 | Batch 050/204 | Current Loss: 0.100095\n[Training] Epoch 030 | Batch 060/204 | Current Loss: 0.100197\n[Training] Epoch 030 | Batch 070/204 | Current Loss: 0.100101\n[Training] Epoch 030 | Batch 080/204 | Current Loss: 0.100102\n[Training] Epoch 030 | Batch 090/204 | Current Loss: 0.100027\n[Training] Epoch 030 | Batch 100/204 | Current Loss: 0.100262\n[Training] Epoch 030 | Batch 110/204 | Current Loss: 0.100092\n[Training] Epoch 030 | Batch 120/204 | Current Loss: 0.100133\n[Training] Epoch 030 | Batch 130/204 | Current Loss: 0.100053\n[Training] Epoch 030 | Batch 140/204 | Current Loss: 0.100065\n[Training] Epoch 030 | Batch 150/204 | Current Loss: 0.100098\n[Training] Epoch 030 | Batch 160/204 | Current Loss: 0.100110\n[Training] Epoch 030 | Batch 170/204 | Current Loss: 0.100107\n[Training] Epoch 030 | Batch 180/204 | Current Loss: 0.100081\n[Training] Epoch 030 | Batch 190/204 | Current Loss: 0.100148\n[Training] Epoch 030 | Batch 200/204 | Current Loss: 0.100099\n\n[Training] Epoch 030 Summary:\n  Avg Loss: 0.100120\n  Current LR: 8.63e-04\n[EarlyStopping] Loss improved (0.100123 → 0.100120). Saving model...\n[Training] Epoch 031 | Batch 000/204 | Current Loss: 0.100053\n[Training] Epoch 031 | Batch 010/204 | Current Loss: 0.100522\n[Training] Epoch 031 | Batch 020/204 | Current Loss: 0.100807\n[Training] Epoch 031 | Batch 030/204 | Current Loss: 0.100751\n[Training] Epoch 031 | Batch 040/204 | Current Loss: 0.100342\n[Training] Epoch 031 | Batch 050/204 | Current Loss: 0.100450\n[Training] Epoch 031 | Batch 060/204 | Current Loss: 0.100359\n[Training] Epoch 031 | Batch 070/204 | Current Loss: 0.100287\n[Training] Epoch 031 | Batch 080/204 | Current Loss: 0.100852\n[Training] Epoch 031 | Batch 090/204 | Current Loss: 0.101597\n[Training] Epoch 031 | Batch 100/204 | Current Loss: 0.101633\n[Training] Epoch 031 | Batch 110/204 | Current Loss: 0.101920\n[Training] Epoch 031 | Batch 120/204 | Current Loss: 0.100606\n[Training] Epoch 031 | Batch 130/204 | Current Loss: 0.100611\n[Training] Epoch 031 | Batch 140/204 | Current Loss: 0.101820\n[Training] Epoch 031 | Batch 150/204 | Current Loss: 0.100686\n[Training] Epoch 031 | Batch 160/204 | Current Loss: 0.100531\n[Training] Epoch 031 | Batch 170/204 | Current Loss: 0.100691\n[Training] Epoch 031 | Batch 180/204 | Current Loss: 0.100505\n[Training] Epoch 031 | Batch 190/204 | Current Loss: 0.100521\n[Training] Epoch 031 | Batch 200/204 | Current Loss: 0.100372\n\n[Training] Epoch 031 Summary:\n  Avg Loss: 0.100903\n  Current LR: 8.42e-04\n[EarlyStopping] Counter: 1/10\n[Training] Epoch 032 | Batch 000/204 | Current Loss: 0.103402\n[Training] Epoch 032 | Batch 010/204 | Current Loss: 0.100855\n[Training] Epoch 032 | Batch 020/204 | Current Loss: 0.100914\n[Training] Epoch 032 | Batch 030/204 | Current Loss: 0.100664\n[Training] Epoch 032 | Batch 040/204 | Current Loss: 0.100443\n[Training] Epoch 032 | Batch 050/204 | Current Loss: 0.100369\n[Training] Epoch 032 | Batch 060/204 | Current Loss: 0.100317\n[Training] Epoch 032 | Batch 070/204 | Current Loss: 0.100272\n[Training] Epoch 032 | Batch 080/204 | Current Loss: 0.100319\n[Training] Epoch 032 | Batch 090/204 | Current Loss: 0.100266\n[Training] Epoch 032 | Batch 100/204 | Current Loss: 0.100267\n[Training] Epoch 032 | Batch 110/204 | Current Loss: 0.100191\n[Training] Epoch 032 | Batch 120/204 | Current Loss: 0.100223\n[Training] Epoch 032 | Batch 130/204 | Current Loss: 0.100232\n[Training] Epoch 032 | Batch 140/204 | Current Loss: 0.100242\n[Training] Epoch 032 | Batch 150/204 | Current Loss: 0.100167\n[Training] Epoch 032 | Batch 160/204 | Current Loss: 0.100246\n[Training] Epoch 032 | Batch 170/204 | Current Loss: 0.100306\n[Training] Epoch 032 | Batch 180/204 | Current Loss: 0.100278\n[Training] Epoch 032 | Batch 190/204 | Current Loss: 0.100222\n[Training] Epoch 032 | Batch 200/204 | Current Loss: 0.100182\n\n[Training] Epoch 032 Summary:\n  Avg Loss: 0.100455\n  Current LR: 7.81e-04\n[EarlyStopping] Counter: 2/10\n[Training] Epoch 033 | Batch 000/204 | Current Loss: 0.100208\n[Training] Epoch 033 | Batch 010/204 | Current Loss: 0.100199\n[Training] Epoch 033 | Batch 020/204 | Current Loss: 0.100152\n[Training] Epoch 033 | Batch 030/204 | Current Loss: 0.100178\n[Training] Epoch 033 | Batch 040/204 | Current Loss: 0.100183\n[Training] Epoch 033 | Batch 050/204 | Current Loss: 0.100218\n[Training] Epoch 033 | Batch 060/204 | Current Loss: 0.100175\n[Training] Epoch 033 | Batch 070/204 | Current Loss: 0.100172\n[Training] Epoch 033 | Batch 080/204 | Current Loss: 0.100096\n[Training] Epoch 033 | Batch 090/204 | Current Loss: 0.100284\n[Training] Epoch 033 | Batch 100/204 | Current Loss: 0.100102\n[Training] Epoch 033 | Batch 110/204 | Current Loss: 0.100160\n[Training] Epoch 033 | Batch 120/204 | Current Loss: 0.100196\n[Training] Epoch 033 | Batch 130/204 | Current Loss: 0.100193\n[Training] Epoch 033 | Batch 140/204 | Current Loss: 0.100217\n[Training] Epoch 033 | Batch 150/204 | Current Loss: 0.100246\n[Training] Epoch 033 | Batch 160/204 | Current Loss: 0.100279\n[Training] Epoch 033 | Batch 170/204 | Current Loss: 0.100021\n[Training] Epoch 033 | Batch 180/204 | Current Loss: 0.100218\n[Training] Epoch 033 | Batch 190/204 | Current Loss: 0.100191\n[Training] Epoch 033 | Batch 200/204 | Current Loss: 0.100143\n\n[Training] Epoch 033 Summary:\n  Avg Loss: 0.100188\n  Current LR: 6.86e-04\n[EarlyStopping] Counter: 3/10\n[Training] Epoch 034 | Batch 000/204 | Current Loss: 0.100282\n[Training] Epoch 034 | Batch 010/204 | Current Loss: 0.100163\n[Training] Epoch 034 | Batch 020/204 | Current Loss: 0.100154\n[Training] Epoch 034 | Batch 030/204 | Current Loss: 0.100261\n[Training] Epoch 034 | Batch 040/204 | Current Loss: 0.100214\n[Training] Epoch 034 | Batch 050/204 | Current Loss: 0.100173\n[Training] Epoch 034 | Batch 060/204 | Current Loss: 0.100185\n[Training] Epoch 034 | Batch 070/204 | Current Loss: 0.100158\n[Training] Epoch 034 | Batch 080/204 | Current Loss: 0.100122\n[Training] Epoch 034 | Batch 090/204 | Current Loss: 0.100162\n[Training] Epoch 034 | Batch 100/204 | Current Loss: 0.100216\n[Training] Epoch 034 | Batch 110/204 | Current Loss: 0.100057\n[Training] Epoch 034 | Batch 120/204 | Current Loss: 0.100144\n[Training] Epoch 034 | Batch 130/204 | Current Loss: 0.100249\n[Training] Epoch 034 | Batch 140/204 | Current Loss: 0.100135\n[Training] Epoch 034 | Batch 150/204 | Current Loss: 0.100260\n[Training] Epoch 034 | Batch 160/204 | Current Loss: 0.100244\n[Training] Epoch 034 | Batch 170/204 | Current Loss: 0.100114\n[Training] Epoch 034 | Batch 180/204 | Current Loss: 0.100284\n[Training] Epoch 034 | Batch 190/204 | Current Loss: 0.100218\n[Training] Epoch 034 | Batch 200/204 | Current Loss: 0.100129\n\n[Training] Epoch 034 Summary:\n  Avg Loss: 0.100183\n  Current LR: 5.65e-04\n[EarlyStopping] Counter: 4/10\n[Training] Epoch 035 | Batch 000/204 | Current Loss: 0.100065\n[Training] Epoch 035 | Batch 010/204 | Current Loss: 0.100214\n[Training] Epoch 035 | Batch 020/204 | Current Loss: 0.100045\n[Training] Epoch 035 | Batch 030/204 | Current Loss: 0.100036\n[Training] Epoch 035 | Batch 040/204 | Current Loss: 0.100175\n[Training] Epoch 035 | Batch 050/204 | Current Loss: 0.100105\n[Training] Epoch 035 | Batch 060/204 | Current Loss: 0.100164\n[Training] Epoch 035 | Batch 070/204 | Current Loss: 0.100218\n[Training] Epoch 035 | Batch 080/204 | Current Loss: 0.100197\n[Training] Epoch 035 | Batch 090/204 | Current Loss: 0.100139\n[Training] Epoch 035 | Batch 100/204 | Current Loss: 0.100045\n[Training] Epoch 035 | Batch 110/204 | Current Loss: 0.100181\n[Training] Epoch 035 | Batch 120/204 | Current Loss: 0.100151\n[Training] Epoch 035 | Batch 130/204 | Current Loss: 0.100135\n[Training] Epoch 035 | Batch 140/204 | Current Loss: 0.100051\n[Training] Epoch 035 | Batch 150/204 | Current Loss: 0.100139\n[Training] Epoch 035 | Batch 160/204 | Current Loss: 0.100053\n[Training] Epoch 035 | Batch 170/204 | Current Loss: 0.100155\n[Training] Epoch 035 | Batch 180/204 | Current Loss: 0.100084\n[Training] Epoch 035 | Batch 190/204 | Current Loss: 0.100279\n[Training] Epoch 035 | Batch 200/204 | Current Loss: 0.100261\n\n[Training] Epoch 035 Summary:\n  Avg Loss: 0.100151\n  Current LR: 4.32e-04\n[EarlyStopping] Counter: 5/10\n[Training] Epoch 036 | Batch 000/204 | Current Loss: 0.100141\n[Training] Epoch 036 | Batch 010/204 | Current Loss: 0.100262\n[Training] Epoch 036 | Batch 020/204 | Current Loss: 0.100017\n[Training] Epoch 036 | Batch 030/204 | Current Loss: 0.100254\n[Training] Epoch 036 | Batch 040/204 | Current Loss: 0.100149\n[Training] Epoch 036 | Batch 050/204 | Current Loss: 0.100184\n[Training] Epoch 036 | Batch 060/204 | Current Loss: 0.100218\n[Training] Epoch 036 | Batch 070/204 | Current Loss: 0.100099\n[Training] Epoch 036 | Batch 080/204 | Current Loss: 0.100106\n[Training] Epoch 036 | Batch 090/204 | Current Loss: 0.100112\n[Training] Epoch 036 | Batch 100/204 | Current Loss: 0.100233\n[Training] Epoch 036 | Batch 110/204 | Current Loss: 0.100015\n[Training] Epoch 036 | Batch 120/204 | Current Loss: 0.100064\n[Training] Epoch 036 | Batch 130/204 | Current Loss: 0.100120\n[Training] Epoch 036 | Batch 140/204 | Current Loss: 0.100056\n[Training] Epoch 036 | Batch 150/204 | Current Loss: 0.100170\n[Training] Epoch 036 | Batch 160/204 | Current Loss: 0.100134\n[Training] Epoch 036 | Batch 170/204 | Current Loss: 0.100299\n[Training] Epoch 036 | Batch 180/204 | Current Loss: 0.100097\n[Training] Epoch 036 | Batch 190/204 | Current Loss: 0.100215\n[Training] Epoch 036 | Batch 200/204 | Current Loss: 0.100127\n\n[Training] Epoch 036 Summary:\n  Avg Loss: 0.100148\n  Current LR: 2.99e-04\n[EarlyStopping] Counter: 6/10\n[Training] Epoch 037 | Batch 000/204 | Current Loss: 0.100177\n[Training] Epoch 037 | Batch 010/204 | Current Loss: 0.100149\n[Training] Epoch 037 | Batch 020/204 | Current Loss: 0.100124\n[Training] Epoch 037 | Batch 030/204 | Current Loss: 0.100160\n[Training] Epoch 037 | Batch 040/204 | Current Loss: 0.100152\n[Training] Epoch 037 | Batch 050/204 | Current Loss: 0.100158\n[Training] Epoch 037 | Batch 060/204 | Current Loss: 0.100066\n[Training] Epoch 037 | Batch 070/204 | Current Loss: 0.100222\n[Training] Epoch 037 | Batch 080/204 | Current Loss: 0.099999\n[Training] Epoch 037 | Batch 090/204 | Current Loss: 0.100110\n[Training] Epoch 037 | Batch 100/204 | Current Loss: 0.100099\n[Training] Epoch 037 | Batch 110/204 | Current Loss: 0.100129\n[Training] Epoch 037 | Batch 120/204 | Current Loss: 0.100076\n[Training] Epoch 037 | Batch 130/204 | Current Loss: 0.100240\n[Training] Epoch 037 | Batch 140/204 | Current Loss: 0.100192\n[Training] Epoch 037 | Batch 150/204 | Current Loss: 0.100151\n[Training] Epoch 037 | Batch 160/204 | Current Loss: 0.100038\n[Training] Epoch 037 | Batch 170/204 | Current Loss: 0.100131\n[Training] Epoch 037 | Batch 180/204 | Current Loss: 0.100076\n[Training] Epoch 037 | Batch 190/204 | Current Loss: 0.100073\n[Training] Epoch 037 | Batch 200/204 | Current Loss: 0.100259\n\n[Training] Epoch 037 Summary:\n  Avg Loss: 0.100130\n  Current LR: 1.79e-04\n[EarlyStopping] Counter: 7/10\n[Training] Epoch 038 | Batch 000/204 | Current Loss: 0.100220\n[Training] Epoch 038 | Batch 010/204 | Current Loss: 0.100118\n[Training] Epoch 038 | Batch 020/204 | Current Loss: 0.100131\n[Training] Epoch 038 | Batch 030/204 | Current Loss: 0.100166\n[Training] Epoch 038 | Batch 040/204 | Current Loss: 0.100173\n[Training] Epoch 038 | Batch 050/204 | Current Loss: 0.100013\n[Training] Epoch 038 | Batch 060/204 | Current Loss: 0.100120\n[Training] Epoch 038 | Batch 070/204 | Current Loss: 0.100095\n[Training] Epoch 038 | Batch 080/204 | Current Loss: 0.100150\n[Training] Epoch 038 | Batch 090/204 | Current Loss: 0.100050\n[Training] Epoch 038 | Batch 100/204 | Current Loss: 0.100171\n[Training] Epoch 038 | Batch 110/204 | Current Loss: 0.100116\n[Training] Epoch 038 | Batch 120/204 | Current Loss: 0.100177\n[Training] Epoch 038 | Batch 130/204 | Current Loss: 0.100194\n[Training] Epoch 038 | Batch 140/204 | Current Loss: 0.100189\n[Training] Epoch 038 | Batch 150/204 | Current Loss: 0.100260\n[Training] Epoch 038 | Batch 160/204 | Current Loss: 0.100148\n[Training] Epoch 038 | Batch 170/204 | Current Loss: 0.100101\n[Training] Epoch 038 | Batch 180/204 | Current Loss: 0.100039\n[Training] Epoch 038 | Batch 190/204 | Current Loss: 0.100133\n[Training] Epoch 038 | Batch 200/204 | Current Loss: 0.100071\n\n[Training] Epoch 038 Summary:\n  Avg Loss: 0.100125\n  Current LR: 8.33e-05\n[EarlyStopping] Counter: 8/10\n[Training] Epoch 039 | Batch 000/204 | Current Loss: 0.100120\n[Training] Epoch 039 | Batch 010/204 | Current Loss: 0.100197\n[Training] Epoch 039 | Batch 020/204 | Current Loss: 0.100169\n[Training] Epoch 039 | Batch 030/204 | Current Loss: 0.100083\n[Training] Epoch 039 | Batch 040/204 | Current Loss: 0.100108\n[Training] Epoch 039 | Batch 050/204 | Current Loss: 0.100030\n[Training] Epoch 039 | Batch 060/204 | Current Loss: 0.100102\n[Training] Epoch 039 | Batch 070/204 | Current Loss: 0.100201\n[Training] Epoch 039 | Batch 080/204 | Current Loss: 0.100182\n[Training] Epoch 039 | Batch 090/204 | Current Loss: 0.100132\n[Training] Epoch 039 | Batch 100/204 | Current Loss: 0.100063\n[Training] Epoch 039 | Batch 110/204 | Current Loss: 0.100124\n[Training] Epoch 039 | Batch 120/204 | Current Loss: 0.100112\n[Training] Epoch 039 | Batch 130/204 | Current Loss: 0.100072\n[Training] Epoch 039 | Batch 140/204 | Current Loss: 0.100229\n[Training] Epoch 039 | Batch 150/204 | Current Loss: 0.100137\n[Training] Epoch 039 | Batch 160/204 | Current Loss: 0.100186\n[Training] Epoch 039 | Batch 170/204 | Current Loss: 0.100064\n[Training] Epoch 039 | Batch 180/204 | Current Loss: 0.100139\n[Training] Epoch 039 | Batch 190/204 | Current Loss: 0.100145\n[Training] Epoch 039 | Batch 200/204 | Current Loss: 0.100167\n\n[Training] Epoch 039 Summary:\n  Avg Loss: 0.100117\n  Current LR: 2.21e-05\n[EarlyStopping] Loss improved (0.100120 → 0.100117). Saving model...\n[Training] Epoch 040 | Batch 000/204 | Current Loss: 0.100202\n[Training] Epoch 040 | Batch 010/204 | Current Loss: 0.100134\n[Training] Epoch 040 | Batch 020/204 | Current Loss: 0.100137\n[Training] Epoch 040 | Batch 030/204 | Current Loss: 0.100038\n[Training] Epoch 040 | Batch 040/204 | Current Loss: 0.100103\n[Training] Epoch 040 | Batch 050/204 | Current Loss: 0.100141\n[Training] Epoch 040 | Batch 060/204 | Current Loss: 0.100110\n[Training] Epoch 040 | Batch 070/204 | Current Loss: 0.100090\n[Training] Epoch 040 | Batch 080/204 | Current Loss: 0.100084\n[Training] Epoch 040 | Batch 090/204 | Current Loss: 0.100020\n[Training] Epoch 040 | Batch 100/204 | Current Loss: 0.100134\n[Training] Epoch 040 | Batch 110/204 | Current Loss: 0.100067\n[Training] Epoch 040 | Batch 120/204 | Current Loss: 0.100099\n[Training] Epoch 040 | Batch 130/204 | Current Loss: 0.100160\n[Training] Epoch 040 | Batch 140/204 | Current Loss: 0.100129\n[Training] Epoch 040 | Batch 150/204 | Current Loss: 0.100173\n[Training] Epoch 040 | Batch 160/204 | Current Loss: 0.100129\n[Training] Epoch 040 | Batch 170/204 | Current Loss: 0.100088\n[Training] Epoch 040 | Batch 180/204 | Current Loss: 0.100165\n[Training] Epoch 040 | Batch 190/204 | Current Loss: 0.100151\n[Training] Epoch 040 | Batch 200/204 | Current Loss: 0.100086\n\n[Training] Epoch 040 Summary:\n  Avg Loss: 0.100111\n  Current LR: 8.63e-04\n[EarlyStopping] Loss improved (0.100117 → 0.100111). Saving model...\n[Training] Epoch 041 | Batch 000/204 | Current Loss: 0.100131\n[Training] Epoch 041 | Batch 010/204 | Current Loss: 0.100313\n[Training] Epoch 041 | Batch 020/204 | Current Loss: 0.100186\n[Training] Epoch 041 | Batch 030/204 | Current Loss: 0.100153\n[Training] Epoch 041 | Batch 040/204 | Current Loss: 0.100119\n[Training] Epoch 041 | Batch 050/204 | Current Loss: 0.100118\n[Training] Epoch 041 | Batch 060/204 | Current Loss: 0.100342\n[Training] Epoch 041 | Batch 070/204 | Current Loss: 0.100266\n[Training] Epoch 041 | Batch 080/204 | Current Loss: 0.100212\n[Training] Epoch 041 | Batch 090/204 | Current Loss: 0.100264\n[Training] Epoch 041 | Batch 100/204 | Current Loss: 0.100134\n[Training] Epoch 041 | Batch 110/204 | Current Loss: 0.100246\n[Training] Epoch 041 | Batch 120/204 | Current Loss: 0.100226\n[Training] Epoch 041 | Batch 130/204 | Current Loss: 0.100129\n[Training] Epoch 041 | Batch 140/204 | Current Loss: 0.100155\n[Training] Epoch 041 | Batch 150/204 | Current Loss: 0.100142\n[Training] Epoch 041 | Batch 160/204 | Current Loss: 0.100260\n[Training] Epoch 041 | Batch 170/204 | Current Loss: 0.100239\n[Training] Epoch 041 | Batch 180/204 | Current Loss: 0.100053\n[Training] Epoch 041 | Batch 190/204 | Current Loss: 0.100084\n[Training] Epoch 041 | Batch 200/204 | Current Loss: 0.100079\n\n[Training] Epoch 041 Summary:\n  Avg Loss: 0.100186\n  Current LR: 8.42e-04\n[EarlyStopping] Counter: 1/10\n[Training] Epoch 042 | Batch 000/204 | Current Loss: 0.100093\n[Training] Epoch 042 | Batch 010/204 | Current Loss: 0.100163\n[Training] Epoch 042 | Batch 020/204 | Current Loss: 0.100102\n[Training] Epoch 042 | Batch 030/204 | Current Loss: 0.100139\n[Training] Epoch 042 | Batch 040/204 | Current Loss: 0.100075\n[Training] Epoch 042 | Batch 050/204 | Current Loss: 0.100115\n[Training] Epoch 042 | Batch 060/204 | Current Loss: 0.100144\n[Training] Epoch 042 | Batch 070/204 | Current Loss: 0.100186\n[Training] Epoch 042 | Batch 080/204 | Current Loss: 0.100230\n[Training] Epoch 042 | Batch 090/204 | Current Loss: 0.100216\n[Training] Epoch 042 | Batch 100/204 | Current Loss: 0.100133\n[Training] Epoch 042 | Batch 110/204 | Current Loss: 0.100114\n[Training] Epoch 042 | Batch 120/204 | Current Loss: 0.100148\n[Training] Epoch 042 | Batch 130/204 | Current Loss: 0.100210\n[Training] Epoch 042 | Batch 140/204 | Current Loss: 0.100100\n[Training] Epoch 042 | Batch 150/204 | Current Loss: 0.100147\n[Training] Epoch 042 | Batch 160/204 | Current Loss: 0.100086\n[Training] Epoch 042 | Batch 170/204 | Current Loss: 0.100120\n[Training] Epoch 042 | Batch 180/204 | Current Loss: 0.100176\n[Training] Epoch 042 | Batch 190/204 | Current Loss: 0.100157\n[Training] Epoch 042 | Batch 200/204 | Current Loss: 0.100181\n\n[Training] Epoch 042 Summary:\n  Avg Loss: 0.100150\n  Current LR: 7.81e-04\n[EarlyStopping] Counter: 2/10\n[Training] Epoch 043 | Batch 000/204 | Current Loss: 0.100151\n[Training] Epoch 043 | Batch 010/204 | Current Loss: 0.100187\n[Training] Epoch 043 | Batch 020/204 | Current Loss: 0.100066\n[Training] Epoch 043 | Batch 030/204 | Current Loss: 0.100090\n[Training] Epoch 043 | Batch 040/204 | Current Loss: 0.100199\n[Training] Epoch 043 | Batch 050/204 | Current Loss: 0.100090\n[Training] Epoch 043 | Batch 060/204 | Current Loss: 0.100170\n[Training] Epoch 043 | Batch 070/204 | Current Loss: 0.100059\n[Training] Epoch 043 | Batch 080/204 | Current Loss: 0.100097\n[Training] Epoch 043 | Batch 090/204 | Current Loss: 0.100278\n[Training] Epoch 043 | Batch 100/204 | Current Loss: 0.100102\n[Training] Epoch 043 | Batch 110/204 | Current Loss: 0.100144\n[Training] Epoch 043 | Batch 120/204 | Current Loss: 0.100199\n[Training] Epoch 043 | Batch 130/204 | Current Loss: 0.100117\n[Training] Epoch 043 | Batch 140/204 | Current Loss: 0.100126\n[Training] Epoch 043 | Batch 150/204 | Current Loss: 0.100160\n[Training] Epoch 043 | Batch 160/204 | Current Loss: 0.100181\n[Training] Epoch 043 | Batch 170/204 | Current Loss: 0.100221\n[Training] Epoch 043 | Batch 180/204 | Current Loss: 0.100174\n[Training] Epoch 043 | Batch 190/204 | Current Loss: 0.100123\n[Training] Epoch 043 | Batch 200/204 | Current Loss: 0.100227\n\n[Training] Epoch 043 Summary:\n  Avg Loss: 0.100151\n  Current LR: 6.86e-04\n[EarlyStopping] Counter: 3/10\n[Training] Epoch 044 | Batch 000/204 | Current Loss: 0.100185\n[Training] Epoch 044 | Batch 010/204 | Current Loss: 0.100384\n[Training] Epoch 044 | Batch 020/204 | Current Loss: 0.100165\n[Training] Epoch 044 | Batch 030/204 | Current Loss: 0.100169\n[Training] Epoch 044 | Batch 040/204 | Current Loss: 0.100185\n[Training] Epoch 044 | Batch 050/204 | Current Loss: 0.100180\n[Training] Epoch 044 | Batch 060/204 | Current Loss: 0.100093\n[Training] Epoch 044 | Batch 070/204 | Current Loss: 0.100167\n[Training] Epoch 044 | Batch 080/204 | Current Loss: 0.100211\n[Training] Epoch 044 | Batch 090/204 | Current Loss: 0.100201\n[Training] Epoch 044 | Batch 100/204 | Current Loss: 0.100110\n[Training] Epoch 044 | Batch 110/204 | Current Loss: 0.100096\n[Training] Epoch 044 | Batch 120/204 | Current Loss: 0.100170\n[Training] Epoch 044 | Batch 130/204 | Current Loss: 0.100120\n[Training] Epoch 044 | Batch 140/204 | Current Loss: 0.100106\n[Training] Epoch 044 | Batch 150/204 | Current Loss: 0.100146\n[Training] Epoch 044 | Batch 160/204 | Current Loss: 0.100074\n[Training] Epoch 044 | Batch 170/204 | Current Loss: 0.100089\n[Training] Epoch 044 | Batch 180/204 | Current Loss: 0.100085\n[Training] Epoch 044 | Batch 190/204 | Current Loss: 0.100143\n[Training] Epoch 044 | Batch 200/204 | Current Loss: 0.100121\n\n[Training] Epoch 044 Summary:\n  Avg Loss: 0.100173\n  Current LR: 5.65e-04\n[EarlyStopping] Counter: 4/10\n[Training] Epoch 045 | Batch 000/204 | Current Loss: 0.100200\n[Training] Epoch 045 | Batch 010/204 | Current Loss: 0.100121\n[Training] Epoch 045 | Batch 020/204 | Current Loss: 0.100183\n[Training] Epoch 045 | Batch 030/204 | Current Loss: 0.100088\n[Training] Epoch 045 | Batch 040/204 | Current Loss: 0.100153\n[Training] Epoch 045 | Batch 050/204 | Current Loss: 0.100110\n[Training] Epoch 045 | Batch 060/204 | Current Loss: 0.100078\n[Training] Epoch 045 | Batch 070/204 | Current Loss: 0.100133\n[Training] Epoch 045 | Batch 080/204 | Current Loss: 0.100021\n[Training] Epoch 045 | Batch 090/204 | Current Loss: 0.100136\n[Training] Epoch 045 | Batch 100/204 | Current Loss: 0.100090\n[Training] Epoch 045 | Batch 110/204 | Current Loss: 0.100133\n[Training] Epoch 045 | Batch 120/204 | Current Loss: 0.100077\n[Training] Epoch 045 | Batch 130/204 | Current Loss: 0.100111\n[Training] Epoch 045 | Batch 140/204 | Current Loss: 0.100050\n[Training] Epoch 045 | Batch 150/204 | Current Loss: 0.100013\n[Training] Epoch 045 | Batch 160/204 | Current Loss: 0.100166\n[Training] Epoch 045 | Batch 170/204 | Current Loss: 0.100204\n[Training] Epoch 045 | Batch 180/204 | Current Loss: 0.100170\n[Training] Epoch 045 | Batch 190/204 | Current Loss: 0.100147\n[Training] Epoch 045 | Batch 200/204 | Current Loss: 0.100107\n\n[Training] Epoch 045 Summary:\n  Avg Loss: 0.100112\n  Current LR: 4.32e-04\n[EarlyStopping] Counter: 5/10\n[Training] Epoch 046 | Batch 000/204 | Current Loss: 0.100115\n[Training] Epoch 046 | Batch 010/204 | Current Loss: 0.100052\n[Training] Epoch 046 | Batch 020/204 | Current Loss: 0.099993\n[Training] Epoch 046 | Batch 030/204 | Current Loss: 0.099984\n[Training] Epoch 046 | Batch 040/204 | Current Loss: 0.100064\n[Training] Epoch 046 | Batch 050/204 | Current Loss: 0.100154\n[Training] Epoch 046 | Batch 060/204 | Current Loss: 0.100109\n[Training] Epoch 046 | Batch 070/204 | Current Loss: 0.100159\n[Training] Epoch 046 | Batch 080/204 | Current Loss: 0.100099\n[Training] Epoch 046 | Batch 090/204 | Current Loss: 0.100238\n[Training] Epoch 046 | Batch 100/204 | Current Loss: 0.100124\n[Training] Epoch 046 | Batch 110/204 | Current Loss: 0.100101\n[Training] Epoch 046 | Batch 120/204 | Current Loss: 0.100157\n[Training] Epoch 046 | Batch 130/204 | Current Loss: 0.100001\n[Training] Epoch 046 | Batch 140/204 | Current Loss: 0.100069\n[Training] Epoch 046 | Batch 150/204 | Current Loss: 0.100097\n[Training] Epoch 046 | Batch 160/204 | Current Loss: 0.100175\n[Training] Epoch 046 | Batch 170/204 | Current Loss: 0.100215\n[Training] Epoch 046 | Batch 180/204 | Current Loss: 0.100090\n[Training] Epoch 046 | Batch 190/204 | Current Loss: 0.100210\n[Training] Epoch 046 | Batch 200/204 | Current Loss: 0.100176\n\n[Training] Epoch 046 Summary:\n  Avg Loss: 0.100105\n  Current LR: 2.99e-04\n[EarlyStopping] Loss improved (0.100111 → 0.100105). Saving model...\n[Training] Epoch 047 | Batch 000/204 | Current Loss: 0.100225\n[Training] Epoch 047 | Batch 010/204 | Current Loss: 0.099980\n[Training] Epoch 047 | Batch 020/204 | Current Loss: 0.100126\n[Training] Epoch 047 | Batch 030/204 | Current Loss: 0.100090\n[Training] Epoch 047 | Batch 040/204 | Current Loss: 0.100059\n[Training] Epoch 047 | Batch 050/204 | Current Loss: 0.100141\n[Training] Epoch 047 | Batch 060/204 | Current Loss: 0.100104\n[Training] Epoch 047 | Batch 070/204 | Current Loss: 0.100078\n[Training] Epoch 047 | Batch 080/204 | Current Loss: 0.100115\n[Training] Epoch 047 | Batch 090/204 | Current Loss: 0.100151\n[Training] Epoch 047 | Batch 100/204 | Current Loss: 0.100132\n[Training] Epoch 047 | Batch 110/204 | Current Loss: 0.100184\n[Training] Epoch 047 | Batch 120/204 | Current Loss: 0.100013\n[Training] Epoch 047 | Batch 130/204 | Current Loss: 0.100210\n[Training] Epoch 047 | Batch 140/204 | Current Loss: 0.099979\n[Training] Epoch 047 | Batch 150/204 | Current Loss: 0.100079\n[Training] Epoch 047 | Batch 160/204 | Current Loss: 0.100104\n[Training] Epoch 047 | Batch 170/204 | Current Loss: 0.100109\n[Training] Epoch 047 | Batch 180/204 | Current Loss: 0.100012\n[Training] Epoch 047 | Batch 190/204 | Current Loss: 0.100139\n[Training] Epoch 047 | Batch 200/204 | Current Loss: 0.100214\n\n[Training] Epoch 047 Summary:\n  Avg Loss: 0.100101\n  Current LR: 1.79e-04\n[EarlyStopping] Loss improved (0.100105 → 0.100101). Saving model...\n[Training] Epoch 048 | Batch 000/204 | Current Loss: 0.100125\n[Training] Epoch 048 | Batch 010/204 | Current Loss: 0.100336\n[Training] Epoch 048 | Batch 020/204 | Current Loss: 0.100012\n[Training] Epoch 048 | Batch 030/204 | Current Loss: 0.100108\n[Training] Epoch 048 | Batch 040/204 | Current Loss: 0.100138\n[Training] Epoch 048 | Batch 050/204 | Current Loss: 0.100108\n[Training] Epoch 048 | Batch 060/204 | Current Loss: 0.100109\n[Training] Epoch 048 | Batch 070/204 | Current Loss: 0.100178\n[Training] Epoch 048 | Batch 080/204 | Current Loss: 0.100082\n[Training] Epoch 048 | Batch 090/204 | Current Loss: 0.100082\n[Training] Epoch 048 | Batch 100/204 | Current Loss: 0.100076\n[Training] Epoch 048 | Batch 110/204 | Current Loss: 0.100097\n[Training] Epoch 048 | Batch 120/204 | Current Loss: 0.100076\n[Training] Epoch 048 | Batch 130/204 | Current Loss: 0.100075\n[Training] Epoch 048 | Batch 140/204 | Current Loss: 0.100104\n[Training] Epoch 048 | Batch 150/204 | Current Loss: 0.100183\n[Training] Epoch 048 | Batch 160/204 | Current Loss: 0.100082\n[Training] Epoch 048 | Batch 170/204 | Current Loss: 0.100098\n[Training] Epoch 048 | Batch 180/204 | Current Loss: 0.100155\n[Training] Epoch 048 | Batch 190/204 | Current Loss: 0.099971\n[Training] Epoch 048 | Batch 200/204 | Current Loss: 0.100044\n\n[Training] Epoch 048 Summary:\n  Avg Loss: 0.100111\n  Current LR: 8.33e-05\n[EarlyStopping] Counter: 1/10\n[Training] Epoch 049 | Batch 000/204 | Current Loss: 0.100099\n[Training] Epoch 049 | Batch 010/204 | Current Loss: 0.100221\n[Training] Epoch 049 | Batch 020/204 | Current Loss: 0.100064\n[Training] Epoch 049 | Batch 030/204 | Current Loss: 0.100065\n[Training] Epoch 049 | Batch 040/204 | Current Loss: 0.100130\n[Training] Epoch 049 | Batch 050/204 | Current Loss: 0.100079\n[Training] Epoch 049 | Batch 060/204 | Current Loss: 0.100053\n[Training] Epoch 049 | Batch 070/204 | Current Loss: 0.100049\n[Training] Epoch 049 | Batch 080/204 | Current Loss: 0.100120\n[Training] Epoch 049 | Batch 090/204 | Current Loss: 0.100099\n[Training] Epoch 049 | Batch 100/204 | Current Loss: 0.100053\n[Training] Epoch 049 | Batch 110/204 | Current Loss: 0.100037\n[Training] Epoch 049 | Batch 120/204 | Current Loss: 0.100057\n[Training] Epoch 049 | Batch 130/204 | Current Loss: 0.100096\n[Training] Epoch 049 | Batch 140/204 | Current Loss: 0.100052\n[Training] Epoch 049 | Batch 150/204 | Current Loss: 0.100099\n[Training] Epoch 049 | Batch 160/204 | Current Loss: 0.100108\n[Training] Epoch 049 | Batch 170/204 | Current Loss: 0.100142\n[Training] Epoch 049 | Batch 180/204 | Current Loss: 0.100059\n[Training] Epoch 049 | Batch 190/204 | Current Loss: 0.100135\n[Training] Epoch 049 | Batch 200/204 | Current Loss: 0.100047\n\n[Training] Epoch 049 Summary:\n  Avg Loss: 0.100093\n  Current LR: 2.21e-05\n[EarlyStopping] Loss improved (0.100101 → 0.100093). Saving model...\n[Training] Epoch 050 | Batch 000/204 | Current Loss: 0.100047\n[Training] Epoch 050 | Batch 010/204 | Current Loss: 0.100091\n[Training] Epoch 050 | Batch 020/204 | Current Loss: 0.100020\n[Training] Epoch 050 | Batch 030/204 | Current Loss: 0.100095\n[Training] Epoch 050 | Batch 040/204 | Current Loss: 0.100051\n[Training] Epoch 050 | Batch 050/204 | Current Loss: 0.100156\n[Training] Epoch 050 | Batch 060/204 | Current Loss: 0.100175\n[Training] Epoch 050 | Batch 070/204 | Current Loss: 0.100165\n[Training] Epoch 050 | Batch 080/204 | Current Loss: 0.100151\n[Training] Epoch 050 | Batch 090/204 | Current Loss: 0.100106\n[Training] Epoch 050 | Batch 100/204 | Current Loss: 0.100156\n[Training] Epoch 050 | Batch 110/204 | Current Loss: 0.100062\n[Training] Epoch 050 | Batch 120/204 | Current Loss: 0.100099\n[Training] Epoch 050 | Batch 130/204 | Current Loss: 0.100146\n[Training] Epoch 050 | Batch 140/204 | Current Loss: 0.100118\n[Training] Epoch 050 | Batch 150/204 | Current Loss: 0.099980\n[Training] Epoch 050 | Batch 160/204 | Current Loss: 0.100101\n[Training] Epoch 050 | Batch 170/204 | Current Loss: 0.100129\n[Training] Epoch 050 | Batch 180/204 | Current Loss: 0.100041\n[Training] Epoch 050 | Batch 190/204 | Current Loss: 0.100026\n[Training] Epoch 050 | Batch 200/204 | Current Loss: 0.100049\n\n[Training] Epoch 050 Summary:\n  Avg Loss: 0.100088\n  Current LR: 8.63e-04\n[EarlyStopping] Loss improved (0.100093 → 0.100088). Saving model...\n[Training] Epoch 051 | Batch 000/204 | Current Loss: 0.100119\n[Training] Epoch 051 | Batch 010/204 | Current Loss: 0.100182\n[Training] Epoch 051 | Batch 020/204 | Current Loss: 0.100243\n[Training] Epoch 051 | Batch 030/204 | Current Loss: 0.100193\n[Training] Epoch 051 | Batch 040/204 | Current Loss: 0.100147\n[Training] Epoch 051 | Batch 050/204 | Current Loss: 0.100204\n[Training] Epoch 051 | Batch 060/204 | Current Loss: 0.100159\n[Training] Epoch 051 | Batch 070/204 | Current Loss: 0.101246\n[Training] Epoch 051 | Batch 080/204 | Current Loss: 0.104462\n[Training] Epoch 051 | Batch 090/204 | Current Loss: 0.101754\n[Training] Epoch 051 | Batch 100/204 | Current Loss: 0.101040\n[Training] Epoch 051 | Batch 110/204 | Current Loss: 0.101736\n[Training] Epoch 051 | Batch 120/204 | Current Loss: 0.101160\n[Training] Epoch 051 | Batch 130/204 | Current Loss: 0.100393\n[Training] Epoch 051 | Batch 140/204 | Current Loss: 0.100328\n[Training] Epoch 051 | Batch 150/204 | Current Loss: 0.100318\n[Training] Epoch 051 | Batch 160/204 | Current Loss: 0.100447\n[Training] Epoch 051 | Batch 170/204 | Current Loss: 0.100246\n[Training] Epoch 051 | Batch 180/204 | Current Loss: 0.100167\n[Training] Epoch 051 | Batch 190/204 | Current Loss: 0.100211\n[Training] Epoch 051 | Batch 200/204 | Current Loss: 0.100195\n\n[Training] Epoch 051 Summary:\n  Avg Loss: 0.100741\n  Current LR: 8.42e-04\n[EarlyStopping] Counter: 1/10\n[Training] Epoch 052 | Batch 000/204 | Current Loss: 0.100202\n[Training] Epoch 052 | Batch 010/204 | Current Loss: 0.100258\n[Training] Epoch 052 | Batch 020/204 | Current Loss: 0.100143\n[Training] Epoch 052 | Batch 030/204 | Current Loss: 0.100217\n[Training] Epoch 052 | Batch 040/204 | Current Loss: 0.100244\n[Training] Epoch 052 | Batch 050/204 | Current Loss: 0.100220\n[Training] Epoch 052 | Batch 060/204 | Current Loss: 0.100277\n[Training] Epoch 052 | Batch 070/204 | Current Loss: 0.100268\n[Training] Epoch 052 | Batch 080/204 | Current Loss: 0.100128\n[Training] Epoch 052 | Batch 090/204 | Current Loss: 0.100205\n[Training] Epoch 052 | Batch 100/204 | Current Loss: 0.100241\n[Training] Epoch 052 | Batch 110/204 | Current Loss: 0.100186\n[Training] Epoch 052 | Batch 120/204 | Current Loss: 0.100223\n[Training] Epoch 052 | Batch 130/204 | Current Loss: 0.100223\n[Training] Epoch 052 | Batch 140/204 | Current Loss: 0.100159\n[Training] Epoch 052 | Batch 150/204 | Current Loss: 0.100135\n[Training] Epoch 052 | Batch 160/204 | Current Loss: 0.100191\n[Training] Epoch 052 | Batch 170/204 | Current Loss: 0.100076\n[Training] Epoch 052 | Batch 180/204 | Current Loss: 0.100198\n[Training] Epoch 052 | Batch 190/204 | Current Loss: 0.100200\n[Training] Epoch 052 | Batch 200/204 | Current Loss: 0.100046\n\n[Training] Epoch 052 Summary:\n  Avg Loss: 0.100190\n  Current LR: 7.81e-04\n[EarlyStopping] Counter: 2/10\n[Training] Epoch 053 | Batch 000/204 | Current Loss: 0.100232\n[Training] Epoch 053 | Batch 010/204 | Current Loss: 0.100245\n[Training] Epoch 053 | Batch 020/204 | Current Loss: 0.100097\n[Training] Epoch 053 | Batch 030/204 | Current Loss: 0.100175\n[Training] Epoch 053 | Batch 040/204 | Current Loss: 0.100134\n[Training] Epoch 053 | Batch 050/204 | Current Loss: 0.100103\n[Training] Epoch 053 | Batch 060/204 | Current Loss: 0.100130\n[Training] Epoch 053 | Batch 070/204 | Current Loss: 0.100040\n[Training] Epoch 053 | Batch 080/204 | Current Loss: 0.100151\n[Training] Epoch 053 | Batch 090/204 | Current Loss: 0.100062\n[Training] Epoch 053 | Batch 100/204 | Current Loss: 0.100073\n[Training] Epoch 053 | Batch 110/204 | Current Loss: 0.100135\n[Training] Epoch 053 | Batch 120/204 | Current Loss: 0.100102\n[Training] Epoch 053 | Batch 130/204 | Current Loss: 0.100076\n[Training] Epoch 053 | Batch 140/204 | Current Loss: 0.100204\n[Training] Epoch 053 | Batch 150/204 | Current Loss: 0.100160\n[Training] Epoch 053 | Batch 160/204 | Current Loss: 0.100151\n[Training] Epoch 053 | Batch 170/204 | Current Loss: 0.100212\n[Training] Epoch 053 | Batch 180/204 | Current Loss: 0.100181\n[Training] Epoch 053 | Batch 190/204 | Current Loss: 0.100106\n[Training] Epoch 053 | Batch 200/204 | Current Loss: 0.100172\n\n[Training] Epoch 053 Summary:\n  Avg Loss: 0.100153\n  Current LR: 6.86e-04\n[EarlyStopping] Counter: 3/10\n[Training] Epoch 054 | Batch 000/204 | Current Loss: 0.100213\n[Training] Epoch 054 | Batch 010/204 | Current Loss: 0.100215\n[Training] Epoch 054 | Batch 020/204 | Current Loss: 0.100204\n[Training] Epoch 054 | Batch 030/204 | Current Loss: 0.100222\n[Training] Epoch 054 | Batch 040/204 | Current Loss: 0.100052\n[Training] Epoch 054 | Batch 050/204 | Current Loss: 0.100044\n[Training] Epoch 054 | Batch 060/204 | Current Loss: 0.100109\n[Training] Epoch 054 | Batch 070/204 | Current Loss: 0.099996\n[Training] Epoch 054 | Batch 080/204 | Current Loss: 0.100118\n[Training] Epoch 054 | Batch 090/204 | Current Loss: 0.100141\n[Training] Epoch 054 | Batch 100/204 | Current Loss: 0.100160\n[Training] Epoch 054 | Batch 110/204 | Current Loss: 0.100114\n[Training] Epoch 054 | Batch 120/204 | Current Loss: 0.100135\n[Training] Epoch 054 | Batch 130/204 | Current Loss: 0.100140\n[Training] Epoch 054 | Batch 140/204 | Current Loss: 0.100065\n[Training] Epoch 054 | Batch 150/204 | Current Loss: 0.100004\n[Training] Epoch 054 | Batch 160/204 | Current Loss: 0.100072\n[Training] Epoch 054 | Batch 170/204 | Current Loss: 0.100155\n[Training] Epoch 054 | Batch 180/204 | Current Loss: 0.100041\n[Training] Epoch 054 | Batch 190/204 | Current Loss: 0.100069\n[Training] Epoch 054 | Batch 200/204 | Current Loss: 0.100071\n\n[Training] Epoch 054 Summary:\n  Avg Loss: 0.100131\n  Current LR: 5.65e-04\n[EarlyStopping] Counter: 4/10\n[Training] Epoch 055 | Batch 000/204 | Current Loss: 0.100157\n[Training] Epoch 055 | Batch 010/204 | Current Loss: 0.100090\n[Training] Epoch 055 | Batch 020/204 | Current Loss: 0.100138\n[Training] Epoch 055 | Batch 030/204 | Current Loss: 0.100238\n[Training] Epoch 055 | Batch 040/204 | Current Loss: 0.100141\n[Training] Epoch 055 | Batch 050/204 | Current Loss: 0.100105\n[Training] Epoch 055 | Batch 060/204 | Current Loss: 0.100137\n[Training] Epoch 055 | Batch 070/204 | Current Loss: 0.100120\n[Training] Epoch 055 | Batch 080/204 | Current Loss: 0.100253\n[Training] Epoch 055 | Batch 090/204 | Current Loss: 0.100064\n[Training] Epoch 055 | Batch 100/204 | Current Loss: 0.100045\n[Training] Epoch 055 | Batch 110/204 | Current Loss: 0.100156\n[Training] Epoch 055 | Batch 120/204 | Current Loss: 0.100067\n[Training] Epoch 055 | Batch 130/204 | Current Loss: 0.100036\n[Training] Epoch 055 | Batch 140/204 | Current Loss: 0.100069\n[Training] Epoch 055 | Batch 150/204 | Current Loss: 0.100041\n[Training] Epoch 055 | Batch 160/204 | Current Loss: 0.100076\n[Training] Epoch 055 | Batch 170/204 | Current Loss: 0.100070\n[Training] Epoch 055 | Batch 180/204 | Current Loss: 0.100027\n[Training] Epoch 055 | Batch 190/204 | Current Loss: 0.100118\n[Training] Epoch 055 | Batch 200/204 | Current Loss: 0.100101\n\n[Training] Epoch 055 Summary:\n  Avg Loss: 0.100129\n  Current LR: 4.32e-04\n[EarlyStopping] Counter: 5/10\n[Training] Epoch 056 | Batch 000/204 | Current Loss: 0.100067\n[Training] Epoch 056 | Batch 010/204 | Current Loss: 0.100180\n[Training] Epoch 056 | Batch 020/204 | Current Loss: 0.100116\n[Training] Epoch 056 | Batch 030/204 | Current Loss: 0.100151\n[Training] Epoch 056 | Batch 040/204 | Current Loss: 0.100131\n[Training] Epoch 056 | Batch 050/204 | Current Loss: 0.100099\n[Training] Epoch 056 | Batch 060/204 | Current Loss: 0.100154\n[Training] Epoch 056 | Batch 070/204 | Current Loss: 0.100056\n[Training] Epoch 056 | Batch 080/204 | Current Loss: 0.100011\n[Training] Epoch 056 | Batch 090/204 | Current Loss: 0.100153\n[Training] Epoch 056 | Batch 100/204 | Current Loss: 0.100029\n[Training] Epoch 056 | Batch 110/204 | Current Loss: 0.100132\n[Training] Epoch 056 | Batch 120/204 | Current Loss: 0.100135\n[Training] Epoch 056 | Batch 130/204 | Current Loss: 0.100100\n[Training] Epoch 056 | Batch 140/204 | Current Loss: 0.100211\n[Training] Epoch 056 | Batch 150/204 | Current Loss: 0.100044\n[Training] Epoch 056 | Batch 160/204 | Current Loss: 0.100115\n[Training] Epoch 056 | Batch 170/204 | Current Loss: 0.100099\n[Training] Epoch 056 | Batch 180/204 | Current Loss: 0.100061\n[Training] Epoch 056 | Batch 190/204 | Current Loss: 0.100113\n[Training] Epoch 056 | Batch 200/204 | Current Loss: 0.100030\n\n[Training] Epoch 056 Summary:\n  Avg Loss: 0.100107\n  Current LR: 2.99e-04\n[EarlyStopping] Counter: 6/10\n[Training] Epoch 057 | Batch 000/204 | Current Loss: 0.100108\n[Training] Epoch 057 | Batch 010/204 | Current Loss: 0.100092\n[Training] Epoch 057 | Batch 020/204 | Current Loss: 0.100140\n[Training] Epoch 057 | Batch 030/204 | Current Loss: 0.100171\n[Training] Epoch 057 | Batch 040/204 | Current Loss: 0.100213\n[Training] Epoch 057 | Batch 050/204 | Current Loss: 0.100142\n[Training] Epoch 057 | Batch 060/204 | Current Loss: 0.100057\n[Training] Epoch 057 | Batch 070/204 | Current Loss: 0.100134\n[Training] Epoch 057 | Batch 080/204 | Current Loss: 0.100081\n[Training] Epoch 057 | Batch 090/204 | Current Loss: 0.100209\n[Training] Epoch 057 | Batch 100/204 | Current Loss: 0.100130\n[Training] Epoch 057 | Batch 110/204 | Current Loss: 0.100066\n[Training] Epoch 057 | Batch 120/204 | Current Loss: 0.100114\n[Training] Epoch 057 | Batch 130/204 | Current Loss: 0.100147\n[Training] Epoch 057 | Batch 140/204 | Current Loss: 0.100114\n[Training] Epoch 057 | Batch 150/204 | Current Loss: 0.100052\n[Training] Epoch 057 | Batch 160/204 | Current Loss: 0.100078\n[Training] Epoch 057 | Batch 170/204 | Current Loss: 0.100046\n[Training] Epoch 057 | Batch 180/204 | Current Loss: 0.100055\n[Training] Epoch 057 | Batch 190/204 | Current Loss: 0.100012\n[Training] Epoch 057 | Batch 200/204 | Current Loss: 0.100102\n\n[Training] Epoch 057 Summary:\n  Avg Loss: 0.100099\n  Current LR: 1.79e-04\n[EarlyStopping] Counter: 7/10\n[Training] Epoch 058 | Batch 000/204 | Current Loss: 0.100075\n[Training] Epoch 058 | Batch 010/204 | Current Loss: 0.100061\n[Training] Epoch 058 | Batch 020/204 | Current Loss: 0.100091\n[Training] Epoch 058 | Batch 030/204 | Current Loss: 0.100046\n[Training] Epoch 058 | Batch 040/204 | Current Loss: 0.100088\n[Training] Epoch 058 | Batch 050/204 | Current Loss: 0.100063\n[Training] Epoch 058 | Batch 060/204 | Current Loss: 0.100214\n[Training] Epoch 058 | Batch 070/204 | Current Loss: 0.100132\n[Training] Epoch 058 | Batch 080/204 | Current Loss: 0.100087\n[Training] Epoch 058 | Batch 090/204 | Current Loss: 0.100179\n[Training] Epoch 058 | Batch 100/204 | Current Loss: 0.100129\n[Training] Epoch 058 | Batch 110/204 | Current Loss: 0.100036\n[Training] Epoch 058 | Batch 120/204 | Current Loss: 0.100128\n[Training] Epoch 058 | Batch 130/204 | Current Loss: 0.100141\n[Training] Epoch 058 | Batch 140/204 | Current Loss: 0.100088\n[Training] Epoch 058 | Batch 150/204 | Current Loss: 0.100111\n[Training] Epoch 058 | Batch 160/204 | Current Loss: 0.100015\n[Training] Epoch 058 | Batch 170/204 | Current Loss: 0.100040\n[Training] Epoch 058 | Batch 180/204 | Current Loss: 0.100079\n[Training] Epoch 058 | Batch 190/204 | Current Loss: 0.100171\n[Training] Epoch 058 | Batch 200/204 | Current Loss: 0.100131\n\n[Training] Epoch 058 Summary:\n  Avg Loss: 0.100090\n  Current LR: 8.33e-05\n[EarlyStopping] Counter: 8/10\n[Training] Epoch 059 | Batch 000/204 | Current Loss: 0.100084\n[Training] Epoch 059 | Batch 010/204 | Current Loss: 0.100029\n[Training] Epoch 059 | Batch 020/204 | Current Loss: 0.100106\n[Training] Epoch 059 | Batch 030/204 | Current Loss: 0.100035\n[Training] Epoch 059 | Batch 040/204 | Current Loss: 0.100031\n[Training] Epoch 059 | Batch 050/204 | Current Loss: 0.100064\n[Training] Epoch 059 | Batch 060/204 | Current Loss: 0.100006\n[Training] Epoch 059 | Batch 070/204 | Current Loss: 0.099930\n[Training] Epoch 059 | Batch 080/204 | Current Loss: 0.100085\n[Training] Epoch 059 | Batch 090/204 | Current Loss: 0.100032\n[Training] Epoch 059 | Batch 100/204 | Current Loss: 0.100101\n[Training] Epoch 059 | Batch 110/204 | Current Loss: 0.100062\n[Training] Epoch 059 | Batch 120/204 | Current Loss: 0.100080\n[Training] Epoch 059 | Batch 130/204 | Current Loss: 0.100070\n[Training] Epoch 059 | Batch 140/204 | Current Loss: 0.100073\n[Training] Epoch 059 | Batch 150/204 | Current Loss: 0.100068\n[Training] Epoch 059 | Batch 160/204 | Current Loss: 0.100115\n[Training] Epoch 059 | Batch 170/204 | Current Loss: 0.100112\n[Training] Epoch 059 | Batch 180/204 | Current Loss: 0.100100\n[Training] Epoch 059 | Batch 190/204 | Current Loss: 0.100011\n[Training] Epoch 059 | Batch 200/204 | Current Loss: 0.100069\n\n[Training] Epoch 059 Summary:\n  Avg Loss: 0.100090\n  Current LR: 2.21e-05\n[EarlyStopping] Counter: 9/10\n[Training] Epoch 060 | Batch 000/204 | Current Loss: 0.100134\n[Training] Epoch 060 | Batch 010/204 | Current Loss: 0.100092\n[Training] Epoch 060 | Batch 020/204 | Current Loss: 0.100058\n[Training] Epoch 060 | Batch 030/204 | Current Loss: 0.100104\n[Training] Epoch 060 | Batch 040/204 | Current Loss: 0.100158\n[Training] Epoch 060 | Batch 050/204 | Current Loss: 0.099980\n[Training] Epoch 060 | Batch 060/204 | Current Loss: 0.100065\n[Training] Epoch 060 | Batch 070/204 | Current Loss: 0.100110\n[Training] Epoch 060 | Batch 080/204 | Current Loss: 0.100053\n[Training] Epoch 060 | Batch 090/204 | Current Loss: 0.100043\n[Training] Epoch 060 | Batch 100/204 | Current Loss: 0.100101\n[Training] Epoch 060 | Batch 110/204 | Current Loss: 0.100131\n[Training] Epoch 060 | Batch 120/204 | Current Loss: 0.100177\n[Training] Epoch 060 | Batch 130/204 | Current Loss: 0.100098\n[Training] Epoch 060 | Batch 140/204 | Current Loss: 0.100119\n[Training] Epoch 060 | Batch 150/204 | Current Loss: 0.100225\n[Training] Epoch 060 | Batch 160/204 | Current Loss: 0.099977\n[Training] Epoch 060 | Batch 170/204 | Current Loss: 0.100014\n[Training] Epoch 060 | Batch 180/204 | Current Loss: 0.100159\n[Training] Epoch 060 | Batch 190/204 | Current Loss: 0.100077\n[Training] Epoch 060 | Batch 200/204 | Current Loss: 0.100068\n\n[Training] Epoch 060 Summary:\n  Avg Loss: 0.100087\n  Current LR: 8.63e-04\n[EarlyStopping] Loss improved (0.100088 → 0.100087). Saving model...\n[Training] Epoch 061 | Batch 000/204 | Current Loss: 0.100055\n[Training] Epoch 061 | Batch 010/204 | Current Loss: 0.100029\n[Training] Epoch 061 | Batch 020/204 | Current Loss: 0.100088\n[Training] Epoch 061 | Batch 030/204 | Current Loss: 0.100186\n[Training] Epoch 061 | Batch 040/204 | Current Loss: 0.100288\n[Training] Epoch 061 | Batch 050/204 | Current Loss: 0.100156\n[Training] Epoch 061 | Batch 060/204 | Current Loss: 0.100336\n[Training] Epoch 061 | Batch 070/204 | Current Loss: 0.100458\n[Training] Epoch 061 | Batch 080/204 | Current Loss: 0.100498\n[Training] Epoch 061 | Batch 090/204 | Current Loss: 0.100505\n[Training] Epoch 061 | Batch 100/204 | Current Loss: 0.100127\n[Training] Epoch 061 | Batch 110/204 | Current Loss: 0.100187\n[Training] Epoch 061 | Batch 120/204 | Current Loss: 0.100149\n[Training] Epoch 061 | Batch 130/204 | Current Loss: 0.100116\n[Training] Epoch 061 | Batch 140/204 | Current Loss: 0.100173\n[Training] Epoch 061 | Batch 150/204 | Current Loss: 0.100269\n[Training] Epoch 061 | Batch 160/204 | Current Loss: 0.100215\n[Training] Epoch 061 | Batch 170/204 | Current Loss: 0.100136\n[Training] Epoch 061 | Batch 180/204 | Current Loss: 0.100225\n[Training] Epoch 061 | Batch 190/204 | Current Loss: 0.100134\n[Training] Epoch 061 | Batch 200/204 | Current Loss: 0.100201\n\n[Training] Epoch 061 Summary:\n  Avg Loss: 0.100227\n  Current LR: 8.42e-04\n[EarlyStopping] Counter: 1/10\n[Training] Epoch 062 | Batch 000/204 | Current Loss: 0.100158\n[Training] Epoch 062 | Batch 010/204 | Current Loss: 0.100382\n[Training] Epoch 062 | Batch 020/204 | Current Loss: 0.100152\n[Training] Epoch 062 | Batch 030/204 | Current Loss: 0.100143\n[Training] Epoch 062 | Batch 040/204 | Current Loss: 0.100154\n[Training] Epoch 062 | Batch 050/204 | Current Loss: 0.100162\n[Training] Epoch 062 | Batch 060/204 | Current Loss: 0.100172\n[Training] Epoch 062 | Batch 070/204 | Current Loss: 0.100098\n[Training] Epoch 062 | Batch 080/204 | Current Loss: 0.100097\n[Training] Epoch 062 | Batch 090/204 | Current Loss: 0.100041\n[Training] Epoch 062 | Batch 100/204 | Current Loss: 0.100251\n[Training] Epoch 062 | Batch 110/204 | Current Loss: 0.100127\n[Training] Epoch 062 | Batch 120/204 | Current Loss: 0.100175\n[Training] Epoch 062 | Batch 130/204 | Current Loss: 0.100160\n[Training] Epoch 062 | Batch 140/204 | Current Loss: 0.100043\n[Training] Epoch 062 | Batch 150/204 | Current Loss: 0.100013\n[Training] Epoch 062 | Batch 160/204 | Current Loss: 0.100191\n[Training] Epoch 062 | Batch 170/204 | Current Loss: 0.100149\n[Training] Epoch 062 | Batch 180/204 | Current Loss: 0.100136\n[Training] Epoch 062 | Batch 190/204 | Current Loss: 0.100340\n[Training] Epoch 062 | Batch 200/204 | Current Loss: 0.100116\n\n[Training] Epoch 062 Summary:\n  Avg Loss: 0.100151\n  Current LR: 7.81e-04\n[EarlyStopping] Counter: 2/10\n[Training] Epoch 063 | Batch 000/204 | Current Loss: 0.100132\n[Training] Epoch 063 | Batch 010/204 | Current Loss: 0.100163\n[Training] Epoch 063 | Batch 020/204 | Current Loss: 0.100039\n[Training] Epoch 063 | Batch 030/204 | Current Loss: 0.100174\n[Training] Epoch 063 | Batch 040/204 | Current Loss: 0.100084\n[Training] Epoch 063 | Batch 050/204 | Current Loss: 0.100129\n[Training] Epoch 063 | Batch 060/204 | Current Loss: 0.100080\n[Training] Epoch 063 | Batch 070/204 | Current Loss: 0.100047\n[Training] Epoch 063 | Batch 080/204 | Current Loss: 0.100088\n[Training] Epoch 063 | Batch 090/204 | Current Loss: 0.100234\n[Training] Epoch 063 | Batch 100/204 | Current Loss: 0.100176\n[Training] Epoch 063 | Batch 110/204 | Current Loss: 0.100135\n[Training] Epoch 063 | Batch 120/204 | Current Loss: 0.100087\n[Training] Epoch 063 | Batch 130/204 | Current Loss: 0.100041\n[Training] Epoch 063 | Batch 140/204 | Current Loss: 0.100060\n[Training] Epoch 063 | Batch 150/204 | Current Loss: 0.100102\n[Training] Epoch 063 | Batch 160/204 | Current Loss: 0.100127\n[Training] Epoch 063 | Batch 170/204 | Current Loss: 0.100109\n[Training] Epoch 063 | Batch 180/204 | Current Loss: 0.100060\n[Training] Epoch 063 | Batch 190/204 | Current Loss: 0.100035\n[Training] Epoch 063 | Batch 200/204 | Current Loss: 0.100140\n\n[Training] Epoch 063 Summary:\n  Avg Loss: 0.100121\n  Current LR: 6.86e-04\n[EarlyStopping] Counter: 3/10\n[Training] Epoch 064 | Batch 000/204 | Current Loss: 0.100182\n[Training] Epoch 064 | Batch 010/204 | Current Loss: 0.100148\n[Training] Epoch 064 | Batch 020/204 | Current Loss: 0.100218\n[Training] Epoch 064 | Batch 030/204 | Current Loss: 0.100125\n[Training] Epoch 064 | Batch 040/204 | Current Loss: 0.100148\n[Training] Epoch 064 | Batch 050/204 | Current Loss: 0.100233\n[Training] Epoch 064 | Batch 060/204 | Current Loss: 0.100131\n[Training] Epoch 064 | Batch 070/204 | Current Loss: 0.100169\n[Training] Epoch 064 | Batch 080/204 | Current Loss: 0.100058\n[Training] Epoch 064 | Batch 090/204 | Current Loss: 0.100095\n[Training] Epoch 064 | Batch 100/204 | Current Loss: 0.100185\n[Training] Epoch 064 | Batch 110/204 | Current Loss: 0.100145\n[Training] Epoch 064 | Batch 120/204 | Current Loss: 0.100195\n[Training] Epoch 064 | Batch 130/204 | Current Loss: 0.100033\n[Training] Epoch 064 | Batch 140/204 | Current Loss: 0.100081\n[Training] Epoch 064 | Batch 150/204 | Current Loss: 0.100238\n[Training] Epoch 064 | Batch 160/204 | Current Loss: 0.100020\n[Training] Epoch 064 | Batch 170/204 | Current Loss: 0.099958\n[Training] Epoch 064 | Batch 180/204 | Current Loss: 0.099982\n[Training] Epoch 064 | Batch 190/204 | Current Loss: 0.100125\n[Training] Epoch 064 | Batch 200/204 | Current Loss: 0.100075\n\n[Training] Epoch 064 Summary:\n  Avg Loss: 0.100127\n  Current LR: 5.65e-04\n[EarlyStopping] Counter: 4/10\n[Training] Epoch 065 | Batch 000/204 | Current Loss: 0.100227\n[Training] Epoch 065 | Batch 010/204 | Current Loss: 0.100875\n[Training] Epoch 065 | Batch 020/204 | Current Loss: 0.100366\n[Training] Epoch 065 | Batch 030/204 | Current Loss: 0.100286\n[Training] Epoch 065 | Batch 040/204 | Current Loss: 0.100158\n[Training] Epoch 065 | Batch 050/204 | Current Loss: 0.100130\n[Training] Epoch 065 | Batch 060/204 | Current Loss: 0.100208\n[Training] Epoch 065 | Batch 070/204 | Current Loss: 0.100140\n[Training] Epoch 065 | Batch 080/204 | Current Loss: 0.100220\n[Training] Epoch 065 | Batch 090/204 | Current Loss: 0.100139\n[Training] Epoch 065 | Batch 100/204 | Current Loss: 0.100072\n[Training] Epoch 065 | Batch 110/204 | Current Loss: 0.100208\n[Training] Epoch 065 | Batch 120/204 | Current Loss: 0.100113\n[Training] Epoch 065 | Batch 130/204 | Current Loss: 0.100061\n[Training] Epoch 065 | Batch 140/204 | Current Loss: 0.100136\n[Training] Epoch 065 | Batch 150/204 | Current Loss: 0.100034\n[Training] Epoch 065 | Batch 160/204 | Current Loss: 0.100067\n[Training] Epoch 065 | Batch 170/204 | Current Loss: 0.100158\n[Training] Epoch 065 | Batch 180/204 | Current Loss: 0.099970\n[Training] Epoch 065 | Batch 190/204 | Current Loss: 0.100167\n[Training] Epoch 065 | Batch 200/204 | Current Loss: 0.100148\n\n[Training] Epoch 065 Summary:\n  Avg Loss: 0.100147\n  Current LR: 4.32e-04\n[EarlyStopping] Counter: 5/10\n[Training] Epoch 066 | Batch 000/204 | Current Loss: 0.100126\n[Training] Epoch 066 | Batch 010/204 | Current Loss: 0.100132\n[Training] Epoch 066 | Batch 020/204 | Current Loss: 0.100081\n[Training] Epoch 066 | Batch 030/204 | Current Loss: 0.100078\n[Training] Epoch 066 | Batch 040/204 | Current Loss: 0.100123\n[Training] Epoch 066 | Batch 050/204 | Current Loss: 0.100156\n[Training] Epoch 066 | Batch 060/204 | Current Loss: 0.100072\n[Training] Epoch 066 | Batch 070/204 | Current Loss: 0.100107\n[Training] Epoch 066 | Batch 080/204 | Current Loss: 0.099992\n[Training] Epoch 066 | Batch 090/204 | Current Loss: 0.100093\n[Training] Epoch 066 | Batch 100/204 | Current Loss: 0.100067\n[Training] Epoch 066 | Batch 110/204 | Current Loss: 0.100091\n[Training] Epoch 066 | Batch 120/204 | Current Loss: 0.100147\n[Training] Epoch 066 | Batch 130/204 | Current Loss: 0.100159\n[Training] Epoch 066 | Batch 140/204 | Current Loss: 0.100122\n[Training] Epoch 066 | Batch 150/204 | Current Loss: 0.100070\n[Training] Epoch 066 | Batch 160/204 | Current Loss: 0.099908\n[Training] Epoch 066 | Batch 170/204 | Current Loss: 0.100059\n[Training] Epoch 066 | Batch 180/204 | Current Loss: 0.100238\n[Training] Epoch 066 | Batch 190/204 | Current Loss: 0.100127\n[Training] Epoch 066 | Batch 200/204 | Current Loss: 0.100075\n\n[Training] Epoch 066 Summary:\n  Avg Loss: 0.100099\n  Current LR: 2.99e-04\n[EarlyStopping] Counter: 6/10\n[Training] Epoch 067 | Batch 000/204 | Current Loss: 0.100158\n[Training] Epoch 067 | Batch 010/204 | Current Loss: 0.100050\n[Training] Epoch 067 | Batch 020/204 | Current Loss: 0.100109\n[Training] Epoch 067 | Batch 030/204 | Current Loss: 0.100007\n[Training] Epoch 067 | Batch 040/204 | Current Loss: 0.100062\n[Training] Epoch 067 | Batch 050/204 | Current Loss: 0.099979\n[Training] Epoch 067 | Batch 060/204 | Current Loss: 0.100101\n[Training] Epoch 067 | Batch 070/204 | Current Loss: 0.100089\n[Training] Epoch 067 | Batch 080/204 | Current Loss: 0.100010\n[Training] Epoch 067 | Batch 090/204 | Current Loss: 0.100015\n[Training] Epoch 067 | Batch 100/204 | Current Loss: 0.100078\n[Training] Epoch 067 | Batch 110/204 | Current Loss: 0.100149\n[Training] Epoch 067 | Batch 120/204 | Current Loss: 0.100076\n[Training] Epoch 067 | Batch 130/204 | Current Loss: 0.100061\n[Training] Epoch 067 | Batch 140/204 | Current Loss: 0.100088\n[Training] Epoch 067 | Batch 150/204 | Current Loss: 0.100053\n[Training] Epoch 067 | Batch 160/204 | Current Loss: 0.100059\n[Training] Epoch 067 | Batch 170/204 | Current Loss: 0.100032\n[Training] Epoch 067 | Batch 180/204 | Current Loss: 0.100139\n[Training] Epoch 067 | Batch 190/204 | Current Loss: 0.100035\n[Training] Epoch 067 | Batch 200/204 | Current Loss: 0.100149\n\n[Training] Epoch 067 Summary:\n  Avg Loss: 0.100082\n  Current LR: 1.79e-04\n[EarlyStopping] Loss improved (0.100087 → 0.100082). Saving model...\n[Training] Epoch 068 | Batch 000/204 | Current Loss: 0.100002\n[Training] Epoch 068 | Batch 010/204 | Current Loss: 0.100073\n[Training] Epoch 068 | Batch 020/204 | Current Loss: 0.100119\n[Training] Epoch 068 | Batch 030/204 | Current Loss: 0.100138\n[Training] Epoch 068 | Batch 040/204 | Current Loss: 0.100060\n[Training] Epoch 068 | Batch 050/204 | Current Loss: 0.100162\n[Training] Epoch 068 | Batch 060/204 | Current Loss: 0.100055\n[Training] Epoch 068 | Batch 070/204 | Current Loss: 0.099941\n[Training] Epoch 068 | Batch 080/204 | Current Loss: 0.100143\n[Training] Epoch 068 | Batch 090/204 | Current Loss: 0.100027\n[Training] Epoch 068 | Batch 100/204 | Current Loss: 0.100071\n[Training] Epoch 068 | Batch 110/204 | Current Loss: 0.100052\n[Training] Epoch 068 | Batch 120/204 | Current Loss: 0.100011\n[Training] Epoch 068 | Batch 130/204 | Current Loss: 0.100164\n[Training] Epoch 068 | Batch 140/204 | Current Loss: 0.100135\n[Training] Epoch 068 | Batch 150/204 | Current Loss: 0.100138\n[Training] Epoch 068 | Batch 160/204 | Current Loss: 0.099918\n[Training] Epoch 068 | Batch 170/204 | Current Loss: 0.100122\n[Training] Epoch 068 | Batch 180/204 | Current Loss: 0.100131\n[Training] Epoch 068 | Batch 190/204 | Current Loss: 0.100109\n[Training] Epoch 068 | Batch 200/204 | Current Loss: 0.099970\n\n[Training] Epoch 068 Summary:\n  Avg Loss: 0.100078\n  Current LR: 8.33e-05\n[EarlyStopping] Loss improved (0.100082 → 0.100078). Saving model...\n[Training] Epoch 069 | Batch 000/204 | Current Loss: 0.100116\n[Training] Epoch 069 | Batch 010/204 | Current Loss: 0.100067\n[Training] Epoch 069 | Batch 020/204 | Current Loss: 0.100091\n[Training] Epoch 069 | Batch 030/204 | Current Loss: 0.100017\n[Training] Epoch 069 | Batch 040/204 | Current Loss: 0.100068\n[Training] Epoch 069 | Batch 050/204 | Current Loss: 0.100077\n[Training] Epoch 069 | Batch 060/204 | Current Loss: 0.100112\n[Training] Epoch 069 | Batch 070/204 | Current Loss: 0.100131\n[Training] Epoch 069 | Batch 080/204 | Current Loss: 0.100035\n[Training] Epoch 069 | Batch 090/204 | Current Loss: 0.100081\n[Training] Epoch 069 | Batch 100/204 | Current Loss: 0.100056\n[Training] Epoch 069 | Batch 110/204 | Current Loss: 0.100061\n[Training] Epoch 069 | Batch 120/204 | Current Loss: 0.100126\n[Training] Epoch 069 | Batch 130/204 | Current Loss: 0.100119\n[Training] Epoch 069 | Batch 140/204 | Current Loss: 0.099982\n[Training] Epoch 069 | Batch 150/204 | Current Loss: 0.100101\n[Training] Epoch 069 | Batch 160/204 | Current Loss: 0.100021\n[Training] Epoch 069 | Batch 170/204 | Current Loss: 0.100134\n[Training] Epoch 069 | Batch 180/204 | Current Loss: 0.100023\n[Training] Epoch 069 | Batch 190/204 | Current Loss: 0.100088\n[Training] Epoch 069 | Batch 200/204 | Current Loss: 0.100151\n\n[Training] Epoch 069 Summary:\n  Avg Loss: 0.100070\n  Current LR: 2.21e-05\n[EarlyStopping] Loss improved (0.100078 → 0.100070). Saving model...\n[Training] Epoch 070 | Batch 000/204 | Current Loss: 0.100073\n[Training] Epoch 070 | Batch 010/204 | Current Loss: 0.100031\n[Training] Epoch 070 | Batch 020/204 | Current Loss: 0.100003\n[Training] Epoch 070 | Batch 030/204 | Current Loss: 0.100127\n[Training] Epoch 070 | Batch 040/204 | Current Loss: 0.100085\n[Training] Epoch 070 | Batch 050/204 | Current Loss: 0.099992\n[Training] Epoch 070 | Batch 060/204 | Current Loss: 0.100065\n[Training] Epoch 070 | Batch 070/204 | Current Loss: 0.100034\n[Training] Epoch 070 | Batch 080/204 | Current Loss: 0.100093\n[Training] Epoch 070 | Batch 090/204 | Current Loss: 0.100028\n[Training] Epoch 070 | Batch 100/204 | Current Loss: 0.100096\n[Training] Epoch 070 | Batch 110/204 | Current Loss: 0.100065\n[Training] Epoch 070 | Batch 120/204 | Current Loss: 0.100094\n[Training] Epoch 070 | Batch 130/204 | Current Loss: 0.100048\n[Training] Epoch 070 | Batch 140/204 | Current Loss: 0.100065\n[Training] Epoch 070 | Batch 150/204 | Current Loss: 0.100073\n[Training] Epoch 070 | Batch 160/204 | Current Loss: 0.100178\n[Training] Epoch 070 | Batch 170/204 | Current Loss: 0.100059\n[Training] Epoch 070 | Batch 180/204 | Current Loss: 0.100050\n[Training] Epoch 070 | Batch 190/204 | Current Loss: 0.100056\n[Training] Epoch 070 | Batch 200/204 | Current Loss: 0.100084\n\n[Training] Epoch 070 Summary:\n  Avg Loss: 0.100070\n  Current LR: 8.63e-04\n[EarlyStopping] Counter: 1/10\n[Training] Epoch 071 | Batch 000/204 | Current Loss: 0.100112\n[Training] Epoch 071 | Batch 010/204 | Current Loss: 0.100096\n[Training] Epoch 071 | Batch 020/204 | Current Loss: 0.100082\n[Training] Epoch 071 | Batch 030/204 | Current Loss: 0.100154\n[Training] Epoch 071 | Batch 040/204 | Current Loss: 0.100064\n[Training] Epoch 071 | Batch 050/204 | Current Loss: 0.100139\n[Training] Epoch 071 | Batch 060/204 | Current Loss: 0.100074\n[Training] Epoch 071 | Batch 070/204 | Current Loss: 0.100039\n[Training] Epoch 071 | Batch 080/204 | Current Loss: 0.100144\n[Training] Epoch 071 | Batch 090/204 | Current Loss: 0.100120\n[Training] Epoch 071 | Batch 100/204 | Current Loss: 0.100075\n[Training] Epoch 071 | Batch 110/204 | Current Loss: 0.100183\n[Training] Epoch 071 | Batch 120/204 | Current Loss: 0.100164\n[Training] Epoch 071 | Batch 130/204 | Current Loss: 0.100068\n[Training] Epoch 071 | Batch 140/204 | Current Loss: 0.100062\n[Training] Epoch 071 | Batch 150/204 | Current Loss: 0.100013\n[Training] Epoch 071 | Batch 160/204 | Current Loss: 0.099970\n[Training] Epoch 071 | Batch 170/204 | Current Loss: 0.100086\n[Training] Epoch 071 | Batch 180/204 | Current Loss: 0.100085\n[Training] Epoch 071 | Batch 190/204 | Current Loss: 0.100013\n[Training] Epoch 071 | Batch 200/204 | Current Loss: 0.100115\n\n[Training] Epoch 071 Summary:\n  Avg Loss: 0.100109\n  Current LR: 8.42e-04\n[EarlyStopping] Counter: 2/10\n[Training] Epoch 072 | Batch 000/204 | Current Loss: 0.100162\n[Training] Epoch 072 | Batch 010/204 | Current Loss: 0.099973\n[Training] Epoch 072 | Batch 020/204 | Current Loss: 0.100060\n[Training] Epoch 072 | Batch 030/204 | Current Loss: 0.100085\n[Training] Epoch 072 | Batch 040/204 | Current Loss: 0.100215\n[Training] Epoch 072 | Batch 050/204 | Current Loss: 0.100065\n[Training] Epoch 072 | Batch 060/204 | Current Loss: 0.100070\n[Training] Epoch 072 | Batch 070/204 | Current Loss: 0.100104\n[Training] Epoch 072 | Batch 080/204 | Current Loss: 0.100114\n[Training] Epoch 072 | Batch 090/204 | Current Loss: 0.100097\n[Training] Epoch 072 | Batch 100/204 | Current Loss: 0.100057\n[Training] Epoch 072 | Batch 110/204 | Current Loss: 0.100074\n[Training] Epoch 072 | Batch 120/204 | Current Loss: 0.100230\n[Training] Epoch 072 | Batch 130/204 | Current Loss: 0.100125\n[Training] Epoch 072 | Batch 140/204 | Current Loss: 0.100043\n[Training] Epoch 072 | Batch 150/204 | Current Loss: 0.100129\n[Training] Epoch 072 | Batch 160/204 | Current Loss: 0.100060\n[Training] Epoch 072 | Batch 170/204 | Current Loss: 0.100023\n[Training] Epoch 072 | Batch 180/204 | Current Loss: 0.100123\n[Training] Epoch 072 | Batch 190/204 | Current Loss: 0.100086\n[Training] Epoch 072 | Batch 200/204 | Current Loss: 0.100085\n\n[Training] Epoch 072 Summary:\n  Avg Loss: 0.100116\n  Current LR: 7.81e-04\n[EarlyStopping] Counter: 3/10\n[Training] Epoch 073 | Batch 000/204 | Current Loss: 0.100109\n[Training] Epoch 073 | Batch 010/204 | Current Loss: 0.100189\n[Training] Epoch 073 | Batch 020/204 | Current Loss: 0.100089\n[Training] Epoch 073 | Batch 030/204 | Current Loss: 0.100119\n[Training] Epoch 073 | Batch 040/204 | Current Loss: 0.100125\n[Training] Epoch 073 | Batch 050/204 | Current Loss: 0.100047\n[Training] Epoch 073 | Batch 060/204 | Current Loss: 0.100042\n[Training] Epoch 073 | Batch 070/204 | Current Loss: 0.100125\n[Training] Epoch 073 | Batch 080/204 | Current Loss: 0.100138\n[Training] Epoch 073 | Batch 090/204 | Current Loss: 0.100087\n[Training] Epoch 073 | Batch 100/204 | Current Loss: 0.100050\n[Training] Epoch 073 | Batch 110/204 | Current Loss: 0.100117\n[Training] Epoch 073 | Batch 120/204 | Current Loss: 0.100081\n[Training] Epoch 073 | Batch 130/204 | Current Loss: 0.100106\n[Training] Epoch 073 | Batch 140/204 | Current Loss: 0.100055\n[Training] Epoch 073 | Batch 150/204 | Current Loss: 0.100092\n[Training] Epoch 073 | Batch 160/204 | Current Loss: 0.100062\n[Training] Epoch 073 | Batch 170/204 | Current Loss: 0.100158\n[Training] Epoch 073 | Batch 180/204 | Current Loss: 0.100102\n[Training] Epoch 073 | Batch 190/204 | Current Loss: 0.100002\n[Training] Epoch 073 | Batch 200/204 | Current Loss: 0.100079\n\n[Training] Epoch 073 Summary:\n  Avg Loss: 0.100105\n  Current LR: 6.86e-04\n[EarlyStopping] Counter: 4/10\n[Training] Epoch 074 | Batch 000/204 | Current Loss: 0.100107\n[Training] Epoch 074 | Batch 010/204 | Current Loss: 0.100135\n[Training] Epoch 074 | Batch 020/204 | Current Loss: 0.100058\n[Training] Epoch 074 | Batch 030/204 | Current Loss: 0.100073\n[Training] Epoch 074 | Batch 040/204 | Current Loss: 0.100166\n[Training] Epoch 074 | Batch 050/204 | Current Loss: 0.100022\n[Training] Epoch 074 | Batch 060/204 | Current Loss: 0.100034\n[Training] Epoch 074 | Batch 070/204 | Current Loss: 0.100032\n[Training] Epoch 074 | Batch 080/204 | Current Loss: 0.100126\n[Training] Epoch 074 | Batch 090/204 | Current Loss: 0.100056\n[Training] Epoch 074 | Batch 100/204 | Current Loss: 0.100084\n[Training] Epoch 074 | Batch 110/204 | Current Loss: 0.100035\n[Training] Epoch 074 | Batch 120/204 | Current Loss: 0.100122\n[Training] Epoch 074 | Batch 130/204 | Current Loss: 0.100099\n[Training] Epoch 074 | Batch 140/204 | Current Loss: 0.099991\n[Training] Epoch 074 | Batch 150/204 | Current Loss: 0.100139\n[Training] Epoch 074 | Batch 160/204 | Current Loss: 0.100108\n[Training] Epoch 074 | Batch 170/204 | Current Loss: 0.100060\n[Training] Epoch 074 | Batch 180/204 | Current Loss: 0.100148\n[Training] Epoch 074 | Batch 190/204 | Current Loss: 0.100103\n[Training] Epoch 074 | Batch 200/204 | Current Loss: 0.100097\n\n[Training] Epoch 074 Summary:\n  Avg Loss: 0.100096\n  Current LR: 5.65e-04\n[EarlyStopping] Counter: 5/10\n[Training] Epoch 075 | Batch 000/204 | Current Loss: 0.100101\n[Training] Epoch 075 | Batch 010/204 | Current Loss: 0.100017\n[Training] Epoch 075 | Batch 020/204 | Current Loss: 0.100075\n[Training] Epoch 075 | Batch 030/204 | Current Loss: 0.100007\n[Training] Epoch 075 | Batch 040/204 | Current Loss: 0.100112\n[Training] Epoch 075 | Batch 050/204 | Current Loss: 0.100102\n[Training] Epoch 075 | Batch 060/204 | Current Loss: 0.100168\n[Training] Epoch 075 | Batch 070/204 | Current Loss: 0.100002\n[Training] Epoch 075 | Batch 080/204 | Current Loss: 0.100110\n[Training] Epoch 075 | Batch 090/204 | Current Loss: 0.100058\n[Training] Epoch 075 | Batch 100/204 | Current Loss: 0.100105\n[Training] Epoch 075 | Batch 110/204 | Current Loss: 0.100098\n[Training] Epoch 075 | Batch 120/204 | Current Loss: 0.100095\n[Training] Epoch 075 | Batch 130/204 | Current Loss: 0.100213\n[Training] Epoch 075 | Batch 140/204 | Current Loss: 0.099984\n[Training] Epoch 075 | Batch 150/204 | Current Loss: 0.100134\n[Training] Epoch 075 | Batch 160/204 | Current Loss: 0.100062\n[Training] Epoch 075 | Batch 170/204 | Current Loss: 0.100069\n[Training] Epoch 075 | Batch 180/204 | Current Loss: 0.100027\n[Training] Epoch 075 | Batch 190/204 | Current Loss: 0.100066\n[Training] Epoch 075 | Batch 200/204 | Current Loss: 0.100010\n\n[Training] Epoch 075 Summary:\n  Avg Loss: 0.100085\n  Current LR: 4.32e-04\n[EarlyStopping] Counter: 6/10\n[Training] Epoch 076 | Batch 000/204 | Current Loss: 0.100046\n[Training] Epoch 076 | Batch 010/204 | Current Loss: 0.100073\n[Training] Epoch 076 | Batch 020/204 | Current Loss: 0.100116\n[Training] Epoch 076 | Batch 030/204 | Current Loss: 0.100040\n[Training] Epoch 076 | Batch 040/204 | Current Loss: 0.100019\n[Training] Epoch 076 | Batch 050/204 | Current Loss: 0.100029\n[Training] Epoch 076 | Batch 060/204 | Current Loss: 0.100050\n[Training] Epoch 076 | Batch 070/204 | Current Loss: 0.100084\n[Training] Epoch 076 | Batch 080/204 | Current Loss: 0.100129\n[Training] Epoch 076 | Batch 090/204 | Current Loss: 0.100077\n[Training] Epoch 076 | Batch 100/204 | Current Loss: 0.100097\n[Training] Epoch 076 | Batch 110/204 | Current Loss: 0.099927\n[Training] Epoch 076 | Batch 120/204 | Current Loss: 0.100093\n[Training] Epoch 076 | Batch 130/204 | Current Loss: 0.100039\n[Training] Epoch 076 | Batch 140/204 | Current Loss: 0.100020\n[Training] Epoch 076 | Batch 150/204 | Current Loss: 0.100117\n[Training] Epoch 076 | Batch 160/204 | Current Loss: 0.100127\n[Training] Epoch 076 | Batch 170/204 | Current Loss: 0.100024\n[Training] Epoch 076 | Batch 180/204 | Current Loss: 0.100035\n[Training] Epoch 076 | Batch 190/204 | Current Loss: 0.100047\n[Training] Epoch 076 | Batch 200/204 | Current Loss: 0.100106\n\n[Training] Epoch 076 Summary:\n  Avg Loss: 0.100071\n  Current LR: 2.99e-04\n[EarlyStopping] Counter: 7/10\n[Training] Epoch 077 | Batch 000/204 | Current Loss: 0.100091\n[Training] Epoch 077 | Batch 010/204 | Current Loss: 0.100150\n[Training] Epoch 077 | Batch 020/204 | Current Loss: 0.100081\n[Training] Epoch 077 | Batch 030/204 | Current Loss: 0.099981\n[Training] Epoch 077 | Batch 040/204 | Current Loss: 0.100072\n[Training] Epoch 077 | Batch 050/204 | Current Loss: 0.099982\n[Training] Epoch 077 | Batch 060/204 | Current Loss: 0.100110\n[Training] Epoch 077 | Batch 070/204 | Current Loss: 0.100031\n[Training] Epoch 077 | Batch 080/204 | Current Loss: 0.099994\n[Training] Epoch 077 | Batch 090/204 | Current Loss: 0.100037\n[Training] Epoch 077 | Batch 100/204 | Current Loss: 0.100081\n[Training] Epoch 077 | Batch 110/204 | Current Loss: 0.100153\n[Training] Epoch 077 | Batch 120/204 | Current Loss: 0.100010\n[Training] Epoch 077 | Batch 130/204 | Current Loss: 0.100059\n[Training] Epoch 077 | Batch 140/204 | Current Loss: 0.100023\n[Training] Epoch 077 | Batch 150/204 | Current Loss: 0.100089\n[Training] Epoch 077 | Batch 160/204 | Current Loss: 0.100150\n[Training] Epoch 077 | Batch 170/204 | Current Loss: 0.100125\n[Training] Epoch 077 | Batch 180/204 | Current Loss: 0.100135\n[Training] Epoch 077 | Batch 190/204 | Current Loss: 0.100014\n[Training] Epoch 077 | Batch 200/204 | Current Loss: 0.100066\n\n[Training] Epoch 077 Summary:\n  Avg Loss: 0.100062\n  Current LR: 1.79e-04\n[EarlyStopping] Loss improved (0.100070 → 0.100062). Saving model...\n[Training] Epoch 078 | Batch 000/204 | Current Loss: 0.100171\n[Training] Epoch 078 | Batch 010/204 | Current Loss: 0.100086\n[Training] Epoch 078 | Batch 020/204 | Current Loss: 0.099968\n[Training] Epoch 078 | Batch 030/204 | Current Loss: 0.099986\n[Training] Epoch 078 | Batch 040/204 | Current Loss: 0.099982\n[Training] Epoch 078 | Batch 050/204 | Current Loss: 0.100023\n[Training] Epoch 078 | Batch 060/204 | Current Loss: 0.100012\n[Training] Epoch 078 | Batch 070/204 | Current Loss: 0.100052\n[Training] Epoch 078 | Batch 080/204 | Current Loss: 0.100109\n[Training] Epoch 078 | Batch 090/204 | Current Loss: 0.100112\n[Training] Epoch 078 | Batch 100/204 | Current Loss: 0.100123\n[Training] Epoch 078 | Batch 110/204 | Current Loss: 0.100025\n[Training] Epoch 078 | Batch 120/204 | Current Loss: 0.100065\n[Training] Epoch 078 | Batch 130/204 | Current Loss: 0.100008\n[Training] Epoch 078 | Batch 140/204 | Current Loss: 0.100043\n[Training] Epoch 078 | Batch 150/204 | Current Loss: 0.100079\n[Training] Epoch 078 | Batch 160/204 | Current Loss: 0.100022\n[Training] Epoch 078 | Batch 170/204 | Current Loss: 0.100031\n[Training] Epoch 078 | Batch 180/204 | Current Loss: 0.100063\n[Training] Epoch 078 | Batch 190/204 | Current Loss: 0.100082\n[Training] Epoch 078 | Batch 200/204 | Current Loss: 0.100057\n\n[Training] Epoch 078 Summary:\n  Avg Loss: 0.100056\n  Current LR: 8.33e-05\n[EarlyStopping] Loss improved (0.100062 → 0.100056). Saving model...\n[Training] Epoch 079 | Batch 000/204 | Current Loss: 0.100166\n[Training] Epoch 079 | Batch 010/204 | Current Loss: 0.100078\n[Training] Epoch 079 | Batch 020/204 | Current Loss: 0.100024\n[Training] Epoch 079 | Batch 030/204 | Current Loss: 0.100014\n[Training] Epoch 079 | Batch 040/204 | Current Loss: 0.100007\n[Training] Epoch 079 | Batch 050/204 | Current Loss: 0.100072\n[Training] Epoch 079 | Batch 060/204 | Current Loss: 0.100110\n[Training] Epoch 079 | Batch 070/204 | Current Loss: 0.100064\n[Training] Epoch 079 | Batch 080/204 | Current Loss: 0.100006\n[Training] Epoch 079 | Batch 090/204 | Current Loss: 0.100077\n[Training] Epoch 079 | Batch 100/204 | Current Loss: 0.100067\n[Training] Epoch 079 | Batch 110/204 | Current Loss: 0.100070\n[Training] Epoch 079 | Batch 120/204 | Current Loss: 0.100026\n[Training] Epoch 079 | Batch 130/204 | Current Loss: 0.100019\n[Training] Epoch 079 | Batch 140/204 | Current Loss: 0.100164\n[Training] Epoch 079 | Batch 150/204 | Current Loss: 0.100099\n[Training] Epoch 079 | Batch 160/204 | Current Loss: 0.100005\n[Training] Epoch 079 | Batch 170/204 | Current Loss: 0.100047\n[Training] Epoch 079 | Batch 180/204 | Current Loss: 0.099966\n[Training] Epoch 079 | Batch 190/204 | Current Loss: 0.100030\n[Training] Epoch 079 | Batch 200/204 | Current Loss: 0.100082\n\n[Training] Epoch 079 Summary:\n  Avg Loss: 0.100055\n  Current LR: 2.21e-05\n[EarlyStopping] Loss improved (0.100056 → 0.100055). Saving model...\n[Training] Epoch 080 | Batch 000/204 | Current Loss: 0.100064\n[Training] Epoch 080 | Batch 010/204 | Current Loss: 0.100032\n[Training] Epoch 080 | Batch 020/204 | Current Loss: 0.099996\n[Training] Epoch 080 | Batch 030/204 | Current Loss: 0.099976\n[Training] Epoch 080 | Batch 040/204 | Current Loss: 0.100031\n[Training] Epoch 080 | Batch 050/204 | Current Loss: 0.100090\n[Training] Epoch 080 | Batch 060/204 | Current Loss: 0.100026\n[Training] Epoch 080 | Batch 070/204 | Current Loss: 0.099992\n[Training] Epoch 080 | Batch 080/204 | Current Loss: 0.100086\n[Training] Epoch 080 | Batch 090/204 | Current Loss: 0.100066\n[Training] Epoch 080 | Batch 100/204 | Current Loss: 0.100100\n[Training] Epoch 080 | Batch 110/204 | Current Loss: 0.100036\n[Training] Epoch 080 | Batch 120/204 | Current Loss: 0.100039\n[Training] Epoch 080 | Batch 130/204 | Current Loss: 0.099988\n[Training] Epoch 080 | Batch 140/204 | Current Loss: 0.100038\n[Training] Epoch 080 | Batch 150/204 | Current Loss: 0.100001\n[Training] Epoch 080 | Batch 160/204 | Current Loss: 0.100037\n[Training] Epoch 080 | Batch 170/204 | Current Loss: 0.100092\n[Training] Epoch 080 | Batch 180/204 | Current Loss: 0.100017\n[Training] Epoch 080 | Batch 190/204 | Current Loss: 0.100050\n[Training] Epoch 080 | Batch 200/204 | Current Loss: 0.100152\n\n[Training] Epoch 080 Summary:\n  Avg Loss: 0.100053\n  Current LR: 8.63e-04\n[EarlyStopping] Loss improved (0.100055 → 0.100053). Saving model...\n[Training] Epoch 081 | Batch 000/204 | Current Loss: 0.100203\n[Training] Epoch 081 | Batch 010/204 | Current Loss: 0.100372\n[Training] Epoch 081 | Batch 020/204 | Current Loss: 0.101043\n[Training] Epoch 081 | Batch 030/204 | Current Loss: 0.100473\n[Training] Epoch 081 | Batch 040/204 | Current Loss: 0.100416\n[Training] Epoch 081 | Batch 050/204 | Current Loss: 0.100210\n[Training] Epoch 081 | Batch 060/204 | Current Loss: 0.100153\n[Training] Epoch 081 | Batch 070/204 | Current Loss: 0.100134\n[Training] Epoch 081 | Batch 080/204 | Current Loss: 0.100191\n[Training] Epoch 081 | Batch 090/204 | Current Loss: 0.100264\n[Training] Epoch 081 | Batch 100/204 | Current Loss: 0.100159\n[Training] Epoch 081 | Batch 110/204 | Current Loss: 0.100113\n[Training] Epoch 081 | Batch 120/204 | Current Loss: 0.100118\n[Training] Epoch 081 | Batch 130/204 | Current Loss: 0.100149\n[Training] Epoch 081 | Batch 140/204 | Current Loss: 0.100068\n[Training] Epoch 081 | Batch 150/204 | Current Loss: 0.100130\n[Training] Epoch 081 | Batch 160/204 | Current Loss: 0.100045\n[Training] Epoch 081 | Batch 170/204 | Current Loss: 0.100122\n[Training] Epoch 081 | Batch 180/204 | Current Loss: 0.100161\n[Training] Epoch 081 | Batch 190/204 | Current Loss: 0.100231\n[Training] Epoch 081 | Batch 200/204 | Current Loss: 0.100095\n\n[Training] Epoch 081 Summary:\n  Avg Loss: 0.100195\n  Current LR: 8.42e-04\n[EarlyStopping] Counter: 1/10\n[Training] Epoch 082 | Batch 000/204 | Current Loss: 0.100292\n[Training] Epoch 082 | Batch 010/204 | Current Loss: 0.100008\n[Training] Epoch 082 | Batch 020/204 | Current Loss: 0.100077\n[Training] Epoch 082 | Batch 030/204 | Current Loss: 0.099928\n[Training] Epoch 082 | Batch 040/204 | Current Loss: 0.100205\n[Training] Epoch 082 | Batch 050/204 | Current Loss: 0.100120\n[Training] Epoch 082 | Batch 060/204 | Current Loss: 0.100037\n[Training] Epoch 082 | Batch 070/204 | Current Loss: 0.100026\n[Training] Epoch 082 | Batch 080/204 | Current Loss: 0.100170\n[Training] Epoch 082 | Batch 090/204 | Current Loss: 0.100095\n[Training] Epoch 082 | Batch 100/204 | Current Loss: 0.100007\n[Training] Epoch 082 | Batch 110/204 | Current Loss: 0.100147\n[Training] Epoch 082 | Batch 120/204 | Current Loss: 0.100093\n[Training] Epoch 082 | Batch 130/204 | Current Loss: 0.100081\n[Training] Epoch 082 | Batch 140/204 | Current Loss: 0.100052\n[Training] Epoch 082 | Batch 150/204 | Current Loss: 0.100128\n[Training] Epoch 082 | Batch 160/204 | Current Loss: 0.099919\n[Training] Epoch 082 | Batch 170/204 | Current Loss: 0.100079\n[Training] Epoch 082 | Batch 180/204 | Current Loss: 0.100083\n[Training] Epoch 082 | Batch 190/204 | Current Loss: 0.100101\n[Training] Epoch 082 | Batch 200/204 | Current Loss: 0.100108\n\n[Training] Epoch 082 Summary:\n  Avg Loss: 0.100090\n  Current LR: 7.81e-04\n[EarlyStopping] Counter: 2/10\n[Training] Epoch 083 | Batch 000/204 | Current Loss: 0.099992\n[Training] Epoch 083 | Batch 010/204 | Current Loss: 0.100062\n[Training] Epoch 083 | Batch 020/204 | Current Loss: 0.100111\n[Training] Epoch 083 | Batch 030/204 | Current Loss: 0.100027\n[Training] Epoch 083 | Batch 040/204 | Current Loss: 0.100124\n[Training] Epoch 083 | Batch 050/204 | Current Loss: 0.100063\n[Training] Epoch 083 | Batch 060/204 | Current Loss: 0.100014\n[Training] Epoch 083 | Batch 070/204 | Current Loss: 0.099960\n[Training] Epoch 083 | Batch 080/204 | Current Loss: 0.100167\n[Training] Epoch 083 | Batch 090/204 | Current Loss: 0.100028\n[Training] Epoch 083 | Batch 100/204 | Current Loss: 0.100071\n[Training] Epoch 083 | Batch 110/204 | Current Loss: 0.100052\n[Training] Epoch 083 | Batch 120/204 | Current Loss: 0.100057\n[Training] Epoch 083 | Batch 130/204 | Current Loss: 0.100064\n[Training] Epoch 083 | Batch 140/204 | Current Loss: 0.100152\n[Training] Epoch 083 | Batch 150/204 | Current Loss: 0.100006\n[Training] Epoch 083 | Batch 160/204 | Current Loss: 0.099979\n[Training] Epoch 083 | Batch 170/204 | Current Loss: 0.100065\n[Training] Epoch 083 | Batch 180/204 | Current Loss: 0.100052\n[Training] Epoch 083 | Batch 190/204 | Current Loss: 0.100016\n[Training] Epoch 083 | Batch 200/204 | Current Loss: 0.100105\n\n[Training] Epoch 083 Summary:\n  Avg Loss: 0.100083\n  Current LR: 6.86e-04\n[EarlyStopping] Counter: 3/10\n[Training] Epoch 084 | Batch 000/204 | Current Loss: 0.100194\n[Training] Epoch 084 | Batch 010/204 | Current Loss: 0.099981\n[Training] Epoch 084 | Batch 020/204 | Current Loss: 0.100063\n[Training] Epoch 084 | Batch 030/204 | Current Loss: 0.100083\n[Training] Epoch 084 | Batch 040/204 | Current Loss: 0.100177\n[Training] Epoch 084 | Batch 050/204 | Current Loss: 0.099969\n[Training] Epoch 084 | Batch 060/204 | Current Loss: 0.100187\n[Training] Epoch 084 | Batch 070/204 | Current Loss: 0.100175\n[Training] Epoch 084 | Batch 080/204 | Current Loss: 0.099991\n[Training] Epoch 084 | Batch 090/204 | Current Loss: 0.100001\n[Training] Epoch 084 | Batch 100/204 | Current Loss: 0.100068\n[Training] Epoch 084 | Batch 110/204 | Current Loss: 0.100048\n[Training] Epoch 084 | Batch 120/204 | Current Loss: 0.099987\n[Training] Epoch 084 | Batch 130/204 | Current Loss: 0.100057\n[Training] Epoch 084 | Batch 140/204 | Current Loss: 0.100107\n[Training] Epoch 084 | Batch 150/204 | Current Loss: 0.100022\n[Training] Epoch 084 | Batch 160/204 | Current Loss: 0.100038\n[Training] Epoch 084 | Batch 170/204 | Current Loss: 0.100017\n[Training] Epoch 084 | Batch 180/204 | Current Loss: 0.100119\n[Training] Epoch 084 | Batch 190/204 | Current Loss: 0.100089\n[Training] Epoch 084 | Batch 200/204 | Current Loss: 0.100015\n\n[Training] Epoch 084 Summary:\n  Avg Loss: 0.100074\n  Current LR: 5.65e-04\n[EarlyStopping] Counter: 4/10\n[Training] Epoch 085 | Batch 000/204 | Current Loss: 0.100076\n[Training] Epoch 085 | Batch 010/204 | Current Loss: 0.100096\n[Training] Epoch 085 | Batch 020/204 | Current Loss: 0.100110\n[Training] Epoch 085 | Batch 030/204 | Current Loss: 0.100097\n[Training] Epoch 085 | Batch 040/204 | Current Loss: 0.100059\n[Training] Epoch 085 | Batch 050/204 | Current Loss: 0.100092\n[Training] Epoch 085 | Batch 060/204 | Current Loss: 0.100064\n[Training] Epoch 085 | Batch 070/204 | Current Loss: 0.100045\n[Training] Epoch 085 | Batch 080/204 | Current Loss: 0.100137\n[Training] Epoch 085 | Batch 090/204 | Current Loss: 0.099982\n[Training] Epoch 085 | Batch 100/204 | Current Loss: 0.100026\n[Training] Epoch 085 | Batch 110/204 | Current Loss: 0.100049\n[Training] Epoch 085 | Batch 120/204 | Current Loss: 0.100092\n[Training] Epoch 085 | Batch 130/204 | Current Loss: 0.100069\n[Training] Epoch 085 | Batch 140/204 | Current Loss: 0.100000\n[Training] Epoch 085 | Batch 150/204 | Current Loss: 0.100075\n[Training] Epoch 085 | Batch 160/204 | Current Loss: 0.100005\n[Training] Epoch 085 | Batch 170/204 | Current Loss: 0.100166\n[Training] Epoch 085 | Batch 180/204 | Current Loss: 0.099952\n[Training] Epoch 085 | Batch 190/204 | Current Loss: 0.100009\n[Training] Epoch 085 | Batch 200/204 | Current Loss: 0.100042\n\n[Training] Epoch 085 Summary:\n  Avg Loss: 0.100065\n  Current LR: 4.32e-04\n[EarlyStopping] Counter: 5/10\n[Training] Epoch 086 | Batch 000/204 | Current Loss: 0.100036\n[Training] Epoch 086 | Batch 010/204 | Current Loss: 0.100098\n[Training] Epoch 086 | Batch 020/204 | Current Loss: 0.100044\n[Training] Epoch 086 | Batch 030/204 | Current Loss: 0.100087\n[Training] Epoch 086 | Batch 040/204 | Current Loss: 0.100024\n[Training] Epoch 086 | Batch 050/204 | Current Loss: 0.100053\n[Training] Epoch 086 | Batch 060/204 | Current Loss: 0.100094\n[Training] Epoch 086 | Batch 070/204 | Current Loss: 0.100087\n[Training] Epoch 086 | Batch 080/204 | Current Loss: 0.100025\n[Training] Epoch 086 | Batch 090/204 | Current Loss: 0.100028\n[Training] Epoch 086 | Batch 100/204 | Current Loss: 0.100098\n[Training] Epoch 086 | Batch 110/204 | Current Loss: 0.100069\n[Training] Epoch 086 | Batch 120/204 | Current Loss: 0.099986\n[Training] Epoch 086 | Batch 130/204 | Current Loss: 0.100063\n[Training] Epoch 086 | Batch 140/204 | Current Loss: 0.100087\n[Training] Epoch 086 | Batch 150/204 | Current Loss: 0.100014\n[Training] Epoch 086 | Batch 160/204 | Current Loss: 0.100098\n[Training] Epoch 086 | Batch 170/204 | Current Loss: 0.100082\n[Training] Epoch 086 | Batch 180/204 | Current Loss: 0.100022\n[Training] Epoch 086 | Batch 190/204 | Current Loss: 0.100014\n[Training] Epoch 086 | Batch 200/204 | Current Loss: 0.100051\n\n[Training] Epoch 086 Summary:\n  Avg Loss: 0.100061\n  Current LR: 2.99e-04\n[EarlyStopping] Counter: 6/10\n[Training] Epoch 087 | Batch 000/204 | Current Loss: 0.100015\n[Training] Epoch 087 | Batch 010/204 | Current Loss: 0.100115\n[Training] Epoch 087 | Batch 020/204 | Current Loss: 0.100228\n[Training] Epoch 087 | Batch 030/204 | Current Loss: 0.100085\n[Training] Epoch 087 | Batch 040/204 | Current Loss: 0.099955\n[Training] Epoch 087 | Batch 050/204 | Current Loss: 0.100086\n[Training] Epoch 087 | Batch 060/204 | Current Loss: 0.100054\n[Training] Epoch 087 | Batch 070/204 | Current Loss: 0.100134\n[Training] Epoch 087 | Batch 080/204 | Current Loss: 0.099970\n[Training] Epoch 087 | Batch 090/204 | Current Loss: 0.100162\n[Training] Epoch 087 | Batch 100/204 | Current Loss: 0.100067\n[Training] Epoch 087 | Batch 110/204 | Current Loss: 0.100035\n[Training] Epoch 087 | Batch 120/204 | Current Loss: 0.100071\n[Training] Epoch 087 | Batch 130/204 | Current Loss: 0.100038\n[Training] Epoch 087 | Batch 140/204 | Current Loss: 0.100008\n[Training] Epoch 087 | Batch 150/204 | Current Loss: 0.099954\n[Training] Epoch 087 | Batch 160/204 | Current Loss: 0.100032\n[Training] Epoch 087 | Batch 170/204 | Current Loss: 0.100020\n[Training] Epoch 087 | Batch 180/204 | Current Loss: 0.100057\n[Training] Epoch 087 | Batch 190/204 | Current Loss: 0.100043\n[Training] Epoch 087 | Batch 200/204 | Current Loss: 0.100173\n\n[Training] Epoch 087 Summary:\n  Avg Loss: 0.100055\n  Current LR: 1.79e-04\n[EarlyStopping] Counter: 7/10\n[Training] Epoch 088 | Batch 000/204 | Current Loss: 0.099929\n[Training] Epoch 088 | Batch 010/204 | Current Loss: 0.100066\n[Training] Epoch 088 | Batch 020/204 | Current Loss: 0.100013\n[Training] Epoch 088 | Batch 030/204 | Current Loss: 0.099995\n[Training] Epoch 088 | Batch 040/204 | Current Loss: 0.099934\n[Training] Epoch 088 | Batch 050/204 | Current Loss: 0.099998\n[Training] Epoch 088 | Batch 060/204 | Current Loss: 0.100079\n[Training] Epoch 088 | Batch 070/204 | Current Loss: 0.100014\n[Training] Epoch 088 | Batch 080/204 | Current Loss: 0.100072\n[Training] Epoch 088 | Batch 090/204 | Current Loss: 0.099954\n[Training] Epoch 088 | Batch 100/204 | Current Loss: 0.100132\n[Training] Epoch 088 | Batch 110/204 | Current Loss: 0.100084\n[Training] Epoch 088 | Batch 120/204 | Current Loss: 0.100032\n[Training] Epoch 088 | Batch 130/204 | Current Loss: 0.100035\n[Training] Epoch 088 | Batch 140/204 | Current Loss: 0.100037\n[Training] Epoch 088 | Batch 150/204 | Current Loss: 0.100079\n[Training] Epoch 088 | Batch 160/204 | Current Loss: 0.099980\n[Training] Epoch 088 | Batch 170/204 | Current Loss: 0.100099\n[Training] Epoch 088 | Batch 180/204 | Current Loss: 0.100047\n[Training] Epoch 088 | Batch 190/204 | Current Loss: 0.100079\n[Training] Epoch 088 | Batch 200/204 | Current Loss: 0.100000\n\n[Training] Epoch 088 Summary:\n  Avg Loss: 0.100052\n  Current LR: 8.33e-05\n[EarlyStopping] Loss improved (0.100053 → 0.100052). Saving model...\n[Training] Epoch 089 | Batch 000/204 | Current Loss: 0.100071\n[Training] Epoch 089 | Batch 010/204 | Current Loss: 0.100091\n[Training] Epoch 089 | Batch 020/204 | Current Loss: 0.100017\n[Training] Epoch 089 | Batch 030/204 | Current Loss: 0.100020\n[Training] Epoch 089 | Batch 040/204 | Current Loss: 0.100040\n[Training] Epoch 089 | Batch 050/204 | Current Loss: 0.100132\n[Training] Epoch 089 | Batch 060/204 | Current Loss: 0.100013\n[Training] Epoch 089 | Batch 070/204 | Current Loss: 0.099994\n[Training] Epoch 089 | Batch 080/204 | Current Loss: 0.099998\n[Training] Epoch 089 | Batch 090/204 | Current Loss: 0.100079\n[Training] Epoch 089 | Batch 100/204 | Current Loss: 0.100098\n[Training] Epoch 089 | Batch 110/204 | Current Loss: 0.100123\n[Training] Epoch 089 | Batch 120/204 | Current Loss: 0.100112\n[Training] Epoch 089 | Batch 130/204 | Current Loss: 0.099907\n[Training] Epoch 089 | Batch 140/204 | Current Loss: 0.100043\n[Training] Epoch 089 | Batch 150/204 | Current Loss: 0.100058\n[Training] Epoch 089 | Batch 160/204 | Current Loss: 0.100049\n[Training] Epoch 089 | Batch 170/204 | Current Loss: 0.100024\n[Training] Epoch 089 | Batch 180/204 | Current Loss: 0.099959\n[Training] Epoch 089 | Batch 190/204 | Current Loss: 0.100015\n[Training] Epoch 089 | Batch 200/204 | Current Loss: 0.100050\n\n[Training] Epoch 089 Summary:\n  Avg Loss: 0.100043\n  Current LR: 2.21e-05\n[EarlyStopping] Loss improved (0.100052 → 0.100043). Saving model...\n[Training] Epoch 090 | Batch 000/204 | Current Loss: 0.099983\n[Training] Epoch 090 | Batch 010/204 | Current Loss: 0.100157\n[Training] Epoch 090 | Batch 020/204 | Current Loss: 0.099926\n[Training] Epoch 090 | Batch 030/204 | Current Loss: 0.099976\n[Training] Epoch 090 | Batch 040/204 | Current Loss: 0.100037\n[Training] Epoch 090 | Batch 050/204 | Current Loss: 0.100091\n[Training] Epoch 090 | Batch 060/204 | Current Loss: 0.100097\n[Training] Epoch 090 | Batch 070/204 | Current Loss: 0.100094\n[Training] Epoch 090 | Batch 080/204 | Current Loss: 0.100046\n[Training] Epoch 090 | Batch 090/204 | Current Loss: 0.099995\n[Training] Epoch 090 | Batch 100/204 | Current Loss: 0.100042\n[Training] Epoch 090 | Batch 110/204 | Current Loss: 0.100158\n[Training] Epoch 090 | Batch 120/204 | Current Loss: 0.100022\n[Training] Epoch 090 | Batch 130/204 | Current Loss: 0.100050\n[Training] Epoch 090 | Batch 140/204 | Current Loss: 0.100069\n[Training] Epoch 090 | Batch 150/204 | Current Loss: 0.100060\n[Training] Epoch 090 | Batch 160/204 | Current Loss: 0.099970\n[Training] Epoch 090 | Batch 170/204 | Current Loss: 0.100143\n[Training] Epoch 090 | Batch 180/204 | Current Loss: 0.100009\n[Training] Epoch 090 | Batch 190/204 | Current Loss: 0.100164\n[Training] Epoch 090 | Batch 200/204 | Current Loss: 0.100030\n\n[Training] Epoch 090 Summary:\n  Avg Loss: 0.100042\n  Current LR: 8.63e-04\n[EarlyStopping] Loss improved (0.100043 → 0.100042). Saving model...\n[Training] Epoch 091 | Batch 000/204 | Current Loss: 0.100034\n[Training] Epoch 091 | Batch 010/204 | Current Loss: 0.100094\n[Training] Epoch 091 | Batch 020/204 | Current Loss: 0.100057\n[Training] Epoch 091 | Batch 030/204 | Current Loss: 0.100085\n[Training] Epoch 091 | Batch 040/204 | Current Loss: 0.100124\n[Training] Epoch 091 | Batch 050/204 | Current Loss: 0.100119\n[Training] Epoch 091 | Batch 060/204 | Current Loss: 0.100068\n[Training] Epoch 091 | Batch 070/204 | Current Loss: 0.100052\n[Training] Epoch 091 | Batch 080/204 | Current Loss: 0.100128\n[Training] Epoch 091 | Batch 090/204 | Current Loss: 0.100103\n[Training] Epoch 091 | Batch 100/204 | Current Loss: 0.100085\n[Training] Epoch 091 | Batch 110/204 | Current Loss: 0.100062\n[Training] Epoch 091 | Batch 120/204 | Current Loss: 0.100041\n[Training] Epoch 091 | Batch 130/204 | Current Loss: 0.100025\n[Training] Epoch 091 | Batch 140/204 | Current Loss: 0.100092\n[Training] Epoch 091 | Batch 150/204 | Current Loss: 0.100228\n[Training] Epoch 091 | Batch 160/204 | Current Loss: 0.100050\n[Training] Epoch 091 | Batch 170/204 | Current Loss: 0.100071\n[Training] Epoch 091 | Batch 180/204 | Current Loss: 0.100197\n[Training] Epoch 091 | Batch 190/204 | Current Loss: 0.100138\n[Training] Epoch 091 | Batch 200/204 | Current Loss: 0.100098\n\n[Training] Epoch 091 Summary:\n  Avg Loss: 0.100085\n  Current LR: 8.42e-04\n[EarlyStopping] Counter: 1/10\n[Training] Epoch 092 | Batch 000/204 | Current Loss: 0.100062\n[Training] Epoch 092 | Batch 010/204 | Current Loss: 0.100179\n[Training] Epoch 092 | Batch 020/204 | Current Loss: 0.100132\n[Training] Epoch 092 | Batch 030/204 | Current Loss: 0.100041\n[Training] Epoch 092 | Batch 040/204 | Current Loss: 0.099993\n[Training] Epoch 092 | Batch 050/204 | Current Loss: 0.099999\n[Training] Epoch 092 | Batch 060/204 | Current Loss: 0.100128\n[Training] Epoch 092 | Batch 070/204 | Current Loss: 0.100124\n[Training] Epoch 092 | Batch 080/204 | Current Loss: 0.100039\n[Training] Epoch 092 | Batch 090/204 | Current Loss: 0.100000\n[Training] Epoch 092 | Batch 100/204 | Current Loss: 0.100154\n[Training] Epoch 092 | Batch 110/204 | Current Loss: 0.100119\n[Training] Epoch 092 | Batch 120/204 | Current Loss: 0.100170\n[Training] Epoch 092 | Batch 130/204 | Current Loss: 0.100109\n[Training] Epoch 092 | Batch 140/204 | Current Loss: 0.100009\n[Training] Epoch 092 | Batch 150/204 | Current Loss: 0.100080\n[Training] Epoch 092 | Batch 160/204 | Current Loss: 0.100044\n[Training] Epoch 092 | Batch 170/204 | Current Loss: 0.100045\n[Training] Epoch 092 | Batch 180/204 | Current Loss: 0.100141\n[Training] Epoch 092 | Batch 190/204 | Current Loss: 0.100095\n[Training] Epoch 092 | Batch 200/204 | Current Loss: 0.100087\n\n[Training] Epoch 092 Summary:\n  Avg Loss: 0.100086\n  Current LR: 7.81e-04\n[EarlyStopping] Counter: 2/10\n[Training] Epoch 093 | Batch 000/204 | Current Loss: 0.100086\n[Training] Epoch 093 | Batch 010/204 | Current Loss: 0.100162\n[Training] Epoch 093 | Batch 020/204 | Current Loss: 0.099972\n[Training] Epoch 093 | Batch 030/204 | Current Loss: 0.099955\n[Training] Epoch 093 | Batch 040/204 | Current Loss: 0.100121\n[Training] Epoch 093 | Batch 050/204 | Current Loss: 0.100058\n[Training] Epoch 093 | Batch 060/204 | Current Loss: 0.100182\n[Training] Epoch 093 | Batch 070/204 | Current Loss: 0.100103\n[Training] Epoch 093 | Batch 080/204 | Current Loss: 0.100100\n[Training] Epoch 093 | Batch 090/204 | Current Loss: 0.100156\n[Training] Epoch 093 | Batch 100/204 | Current Loss: 0.100068\n[Training] Epoch 093 | Batch 110/204 | Current Loss: 0.100117\n[Training] Epoch 093 | Batch 120/204 | Current Loss: 0.100097\n[Training] Epoch 093 | Batch 130/204 | Current Loss: 0.100023\n[Training] Epoch 093 | Batch 140/204 | Current Loss: 0.100090\n[Training] Epoch 093 | Batch 150/204 | Current Loss: 0.100103\n[Training] Epoch 093 | Batch 160/204 | Current Loss: 0.100023\n[Training] Epoch 093 | Batch 170/204 | Current Loss: 0.100082\n[Training] Epoch 093 | Batch 180/204 | Current Loss: 0.100186\n[Training] Epoch 093 | Batch 190/204 | Current Loss: 0.100048\n[Training] Epoch 093 | Batch 200/204 | Current Loss: 0.099999\n\n[Training] Epoch 093 Summary:\n  Avg Loss: 0.100083\n  Current LR: 6.86e-04\n[EarlyStopping] Counter: 3/10\n[Training] Epoch 094 | Batch 000/204 | Current Loss: 0.100036\n[Training] Epoch 094 | Batch 010/204 | Current Loss: 0.100038\n[Training] Epoch 094 | Batch 020/204 | Current Loss: 0.100102\n[Training] Epoch 094 | Batch 030/204 | Current Loss: 0.100042\n[Training] Epoch 094 | Batch 040/204 | Current Loss: 0.100125\n[Training] Epoch 094 | Batch 050/204 | Current Loss: 0.100138\n[Training] Epoch 094 | Batch 060/204 | Current Loss: 0.100000\n[Training] Epoch 094 | Batch 070/204 | Current Loss: 0.100204\n[Training] Epoch 094 | Batch 080/204 | Current Loss: 0.100077\n[Training] Epoch 094 | Batch 090/204 | Current Loss: 0.100213\n[Training] Epoch 094 | Batch 100/204 | Current Loss: 0.100104\n[Training] Epoch 094 | Batch 110/204 | Current Loss: 0.100094\n[Training] Epoch 094 | Batch 120/204 | Current Loss: 0.100141\n[Training] Epoch 094 | Batch 130/204 | Current Loss: 0.100067\n[Training] Epoch 094 | Batch 140/204 | Current Loss: 0.100098\n[Training] Epoch 094 | Batch 150/204 | Current Loss: 0.100068\n[Training] Epoch 094 | Batch 160/204 | Current Loss: 0.100083\n[Training] Epoch 094 | Batch 170/204 | Current Loss: 0.100012\n[Training] Epoch 094 | Batch 180/204 | Current Loss: 0.100139\n[Training] Epoch 094 | Batch 190/204 | Current Loss: 0.100003\n[Training] Epoch 094 | Batch 200/204 | Current Loss: 0.100058\n\n[Training] Epoch 094 Summary:\n  Avg Loss: 0.100063\n  Current LR: 5.65e-04\n[EarlyStopping] Counter: 4/10\n[Training] Epoch 095 | Batch 000/204 | Current Loss: 0.099956\n[Training] Epoch 095 | Batch 010/204 | Current Loss: 0.100027\n[Training] Epoch 095 | Batch 020/204 | Current Loss: 0.100091\n[Training] Epoch 095 | Batch 030/204 | Current Loss: 0.100183\n[Training] Epoch 095 | Batch 040/204 | Current Loss: 0.100131\n[Training] Epoch 095 | Batch 050/204 | Current Loss: 0.099884\n[Training] Epoch 095 | Batch 060/204 | Current Loss: 0.100110\n[Training] Epoch 095 | Batch 070/204 | Current Loss: 0.100028\n[Training] Epoch 095 | Batch 080/204 | Current Loss: 0.100074\n[Training] Epoch 095 | Batch 090/204 | Current Loss: 0.100075\n[Training] Epoch 095 | Batch 100/204 | Current Loss: 0.100092\n[Training] Epoch 095 | Batch 110/204 | Current Loss: 0.100037\n[Training] Epoch 095 | Batch 120/204 | Current Loss: 0.100177\n[Training] Epoch 095 | Batch 130/204 | Current Loss: 0.100086\n[Training] Epoch 095 | Batch 140/204 | Current Loss: 0.100042\n[Training] Epoch 095 | Batch 150/204 | Current Loss: 0.100013\n[Training] Epoch 095 | Batch 160/204 | Current Loss: 0.100002\n[Training] Epoch 095 | Batch 170/204 | Current Loss: 0.100095\n[Training] Epoch 095 | Batch 180/204 | Current Loss: 0.100130\n[Training] Epoch 095 | Batch 190/204 | Current Loss: 0.100075\n[Training] Epoch 095 | Batch 200/204 | Current Loss: 0.100095\n\n[Training] Epoch 095 Summary:\n  Avg Loss: 0.100060\n  Current LR: 4.32e-04\n[EarlyStopping] Counter: 5/10\n[Training] Epoch 096 | Batch 000/204 | Current Loss: 0.100041\n[Training] Epoch 096 | Batch 010/204 | Current Loss: 0.100097\n[Training] Epoch 096 | Batch 020/204 | Current Loss: 0.100092\n[Training] Epoch 096 | Batch 030/204 | Current Loss: 0.100037\n[Training] Epoch 096 | Batch 040/204 | Current Loss: 0.100004\n[Training] Epoch 096 | Batch 050/204 | Current Loss: 0.099999\n[Training] Epoch 096 | Batch 060/204 | Current Loss: 0.100007\n[Training] Epoch 096 | Batch 070/204 | Current Loss: 0.100041\n[Training] Epoch 096 | Batch 080/204 | Current Loss: 0.100027\n[Training] Epoch 096 | Batch 090/204 | Current Loss: 0.100053\n[Training] Epoch 096 | Batch 100/204 | Current Loss: 0.100063\n[Training] Epoch 096 | Batch 110/204 | Current Loss: 0.100008\n[Training] Epoch 096 | Batch 120/204 | Current Loss: 0.100043\n[Training] Epoch 096 | Batch 130/204 | Current Loss: 0.100077\n[Training] Epoch 096 | Batch 140/204 | Current Loss: 0.100039\n[Training] Epoch 096 | Batch 150/204 | Current Loss: 0.100040\n[Training] Epoch 096 | Batch 160/204 | Current Loss: 0.100038\n[Training] Epoch 096 | Batch 170/204 | Current Loss: 0.099979\n[Training] Epoch 096 | Batch 180/204 | Current Loss: 0.100114\n[Training] Epoch 096 | Batch 190/204 | Current Loss: 0.100132\n[Training] Epoch 096 | Batch 200/204 | Current Loss: 0.100083\n\n[Training] Epoch 096 Summary:\n  Avg Loss: 0.100052\n  Current LR: 2.99e-04\n[EarlyStopping] Counter: 6/10\n[Training] Epoch 097 | Batch 000/204 | Current Loss: 0.100048\n[Training] Epoch 097 | Batch 010/204 | Current Loss: 0.100115\n[Training] Epoch 097 | Batch 020/204 | Current Loss: 0.100024\n[Training] Epoch 097 | Batch 030/204 | Current Loss: 0.100060\n[Training] Epoch 097 | Batch 040/204 | Current Loss: 0.099959\n[Training] Epoch 097 | Batch 050/204 | Current Loss: 0.100065\n[Training] Epoch 097 | Batch 060/204 | Current Loss: 0.100040\n[Training] Epoch 097 | Batch 070/204 | Current Loss: 0.100130\n[Training] Epoch 097 | Batch 080/204 | Current Loss: 0.100166\n[Training] Epoch 097 | Batch 090/204 | Current Loss: 0.099957\n[Training] Epoch 097 | Batch 100/204 | Current Loss: 0.100068\n[Training] Epoch 097 | Batch 110/204 | Current Loss: 0.100057\n[Training] Epoch 097 | Batch 120/204 | Current Loss: 0.100041\n[Training] Epoch 097 | Batch 130/204 | Current Loss: 0.100122\n[Training] Epoch 097 | Batch 140/204 | Current Loss: 0.099969\n[Training] Epoch 097 | Batch 150/204 | Current Loss: 0.100019\n[Training] Epoch 097 | Batch 160/204 | Current Loss: 0.099955\n[Training] Epoch 097 | Batch 170/204 | Current Loss: 0.100052\n[Training] Epoch 097 | Batch 180/204 | Current Loss: 0.100017\n[Training] Epoch 097 | Batch 190/204 | Current Loss: 0.100137\n[Training] Epoch 097 | Batch 200/204 | Current Loss: 0.100043\n\n[Training] Epoch 097 Summary:\n  Avg Loss: 0.100048\n  Current LR: 1.79e-04\n[EarlyStopping] Counter: 7/10\n[Training] Epoch 098 | Batch 000/204 | Current Loss: 0.100015\n[Training] Epoch 098 | Batch 010/204 | Current Loss: 0.100065\n[Training] Epoch 098 | Batch 020/204 | Current Loss: 0.100039\n[Training] Epoch 098 | Batch 030/204 | Current Loss: 0.099982\n[Training] Epoch 098 | Batch 040/204 | Current Loss: 0.099995\n[Training] Epoch 098 | Batch 050/204 | Current Loss: 0.100032\n[Training] Epoch 098 | Batch 060/204 | Current Loss: 0.100035\n[Training] Epoch 098 | Batch 070/204 | Current Loss: 0.100007\n[Training] Epoch 098 | Batch 080/204 | Current Loss: 0.100027\n[Training] Epoch 098 | Batch 090/204 | Current Loss: 0.100097\n[Training] Epoch 098 | Batch 100/204 | Current Loss: 0.100069\n[Training] Epoch 098 | Batch 110/204 | Current Loss: 0.100065\n[Training] Epoch 098 | Batch 120/204 | Current Loss: 0.099926\n[Training] Epoch 098 | Batch 130/204 | Current Loss: 0.100018\n[Training] Epoch 098 | Batch 140/204 | Current Loss: 0.100031\n[Training] Epoch 098 | Batch 150/204 | Current Loss: 0.099971\n[Training] Epoch 098 | Batch 160/204 | Current Loss: 0.100065\n[Training] Epoch 098 | Batch 170/204 | Current Loss: 0.100143\n[Training] Epoch 098 | Batch 180/204 | Current Loss: 0.099957\n[Training] Epoch 098 | Batch 190/204 | Current Loss: 0.100068\n[Training] Epoch 098 | Batch 200/204 | Current Loss: 0.100148\n\n[Training] Epoch 098 Summary:\n  Avg Loss: 0.100039\n  Current LR: 8.33e-05\n[EarlyStopping] Loss improved (0.100042 → 0.100039). Saving model...\n[Training] Epoch 099 | Batch 000/204 | Current Loss: 0.099960\n[Training] Epoch 099 | Batch 010/204 | Current Loss: 0.100007\n[Training] Epoch 099 | Batch 020/204 | Current Loss: 0.100052\n[Training] Epoch 099 | Batch 030/204 | Current Loss: 0.100059\n[Training] Epoch 099 | Batch 040/204 | Current Loss: 0.100074\n[Training] Epoch 099 | Batch 050/204 | Current Loss: 0.099987\n[Training] Epoch 099 | Batch 060/204 | Current Loss: 0.099999\n[Training] Epoch 099 | Batch 070/204 | Current Loss: 0.100085\n[Training] Epoch 099 | Batch 080/204 | Current Loss: 0.100072\n[Training] Epoch 099 | Batch 090/204 | Current Loss: 0.099969\n[Training] Epoch 099 | Batch 100/204 | Current Loss: 0.100046\n[Training] Epoch 099 | Batch 110/204 | Current Loss: 0.100122\n[Training] Epoch 099 | Batch 120/204 | Current Loss: 0.100131\n[Training] Epoch 099 | Batch 130/204 | Current Loss: 0.100022\n[Training] Epoch 099 | Batch 140/204 | Current Loss: 0.100015\n[Training] Epoch 099 | Batch 150/204 | Current Loss: 0.099977\n[Training] Epoch 099 | Batch 160/204 | Current Loss: 0.100035\n[Training] Epoch 099 | Batch 170/204 | Current Loss: 0.100004\n[Training] Epoch 099 | Batch 180/204 | Current Loss: 0.100078\n[Training] Epoch 099 | Batch 190/204 | Current Loss: 0.099945\n[Training] Epoch 099 | Batch 200/204 | Current Loss: 0.100070\n\n[Training] Epoch 099 Summary:\n  Avg Loss: 0.100033\n  Current LR: 2.21e-05\n[EarlyStopping] Loss improved (0.100039 → 0.100033). Saving model...\n[Training] Epoch 100 | Batch 000/204 | Current Loss: 0.100098\n[Training] Epoch 100 | Batch 010/204 | Current Loss: 0.099982\n[Training] Epoch 100 | Batch 020/204 | Current Loss: 0.099899\n[Training] Epoch 100 | Batch 030/204 | Current Loss: 0.099982\n[Training] Epoch 100 | Batch 040/204 | Current Loss: 0.099958\n[Training] Epoch 100 | Batch 050/204 | Current Loss: 0.099964\n[Training] Epoch 100 | Batch 060/204 | Current Loss: 0.099975\n[Training] Epoch 100 | Batch 070/204 | Current Loss: 0.100034\n[Training] Epoch 100 | Batch 080/204 | Current Loss: 0.100088\n[Training] Epoch 100 | Batch 090/204 | Current Loss: 0.100015\n[Training] Epoch 100 | Batch 100/204 | Current Loss: 0.099967\n[Training] Epoch 100 | Batch 110/204 | Current Loss: 0.100085\n[Training] Epoch 100 | Batch 120/204 | Current Loss: 0.100106\n[Training] Epoch 100 | Batch 130/204 | Current Loss: 0.100013\n[Training] Epoch 100 | Batch 140/204 | Current Loss: 0.100095\n[Training] Epoch 100 | Batch 150/204 | Current Loss: 0.099948\n[Training] Epoch 100 | Batch 160/204 | Current Loss: 0.099996\n[Training] Epoch 100 | Batch 170/204 | Current Loss: 0.100001\n[Training] Epoch 100 | Batch 180/204 | Current Loss: 0.099961\n[Training] Epoch 100 | Batch 190/204 | Current Loss: 0.099921\n[Training] Epoch 100 | Batch 200/204 | Current Loss: 0.100149\n\n[Training] Epoch 100 Summary:\n  Avg Loss: 0.100031\n  Current LR: 8.63e-04\n[EarlyStopping] Loss improved (0.100033 → 0.100031). Saving model...\n\n[Training] Final training completed with best loss: 0.100031\n\n[Training] Model saved to stolen_encoder_final.pt with best loss: 0.100031\n\n==================================================\nExporting Model to ONNX\n==================================================\n\n[Export] Converting model to ONNX format...\n[Export] Model saved to stolen_encoder_optimized2.onnx\n\n[Export] Verifying ONNX model...\n[Export] Verification successful!\n[Export] Output shape: (1, 1024)\n\n[Main] Pipeline completed successfully!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"submit_model() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T18:11:04.571263Z","iopub.execute_input":"2025-06-21T18:11:04.571599Z","iopub.status.idle":"2025-06-21T18:11:10.536104Z","shell.execute_reply.started":"2025-06-21T18:11:04.571581Z","shell.execute_reply":"2025-06-21T18:11:10.535462Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nSubmitting Model to Server\n==================================================\n[Submission] Sending model to server...\n[Submission] Server response:\n{'L2': 4.706040859222412}\n","output_type":"stream"}],"execution_count":5}]}